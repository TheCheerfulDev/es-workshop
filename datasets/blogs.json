{"index":{}}
{"author":"Aaron Aldrich","category":"Engineering","publish_date":"2017-12-26T00:00:00.000Z","url":"/blog/elastic-advent-calendar-2017-week-four","seo_title":"","content":" As we mentioned in our , the Engineering team here at Elastic wanted to celebrate the end of the 2017 Calendar via our own tech-advent series. We took a lot of inspiration from both the (fully in Japanese) and (in English) and we’d like to thank them for providing the awesome quality we have aspired to maintain. We have summarised weeks , and in previous blog posts, and this post covers the last and final (all be it short) week and also provides a summary of all the topics that were posted in the series. Here’s all 25 topics:  Dec 1: by Mark Walkom Dec 2: by Jun Ohtani Dec 3: by Tyler Hannan Dec 4: by David Pilato Dec 5: by Tal Levy Dec 6: by Jongmin Kim Dec 7: by Christopher Wurm Dec 8: by Medcl Zeng Dec 9: by Jordan Sissel Dec 10 by Philipp Krenn Dec 11: by Thiago Souza Dec 12: by Atonio Bonuccelli Dec 13: by Aaron Aldrich Dec 14: by Jun Ohtani Dec 15: by Abdon Pijpelink Dec 16: by David Pilato Dec 17: by Mat Schaffer Dec 18: by Jongmin Kim Dec 19: by Tyler Langlois Dec 20: by Medcl Zeng Dec 21: by Sherry Ger Dec 22: by Thiago Souza Dec 23: by Bhavya Mandya Dec 24: by Philipp Krenn Dec 25: by Mark Walkom Thank You! We will be keeping all the of the topics available on the so you can refer back to them at any time. And, as these are Discuss topics, you can also continue the conversation with the authors! Thanks for following on through this series, we hope it’s provided some useful inspiration for your use of the Elastic Stack. If you’d like us to repeat this, or if you have ideas for next year, please let us know via or feel free to create a topic in our with your comments. We hope 2017 has been a great year and we look forward to 2018 being even better! ","locales":"","title":"The Elastic Advent Calendar 2017, Week 4"}
{"index":{}}
{"author":"Elastic Engineering","category":"Engineering","publish_date":"2018-01-08T00:00:00.000Z","url":"/blog/elastic-cloud-and-meltdown","seo_title":"","content":" Elastic is aware of the and we are addressing them for Elastic Cloud. We know that you entrust your data to our cloud service, and we take the confidentiality of all data very seriously. At this time, we are not aware of any exploit on our cloud service that utilized the Meltdown or Spectre vulnerabilities. Impact Assessment The Meltdown or Spectre vulnerabilities apply when untrusted code can execute on a system. At the host infrastructure level, we know that both our infrastructure providers (AWS and GCP) have patched their systems, and are no longer vulnerable. At the Elastic Cloud service level, Elastic Cloud allows you to upload some artifacts, such as plug-ins, dictionaries, and scripts. These uploads provide a potential vector of attack that could exploit Meltdown. Old Elasticsearch clusters on version 1.x are more vulnerable. Except uploads from you, Elastic Cloud does not allow for untrusted code execution. Based on our assessment, we believe the impact of Meltdown and Spectre to Elastic Cloud to be small. We have focused our efforts on mitigation and control while we carry out our regular process for operating system patches in an accelerated fashion. Mitigation We disabled non-sandboxed scripting for all Elasticsearch 1.x clusters as a primary, customer-visible mitigation. We have also disabled self-service uploads of custom bundles from you until we have fully completed our patching. Behind the scenes, we’ve further increased our observability of system-level calls and isolated clusters running version 1.x of Elasticsearch on their own hosts. Patching and Customer Impact We are using an accelerated version of our regular maintenance procedure to perform OS-level updates while maintaining service availability for user clusters. Your clusters will not experience downtime from this operating system patching. There is considerable speculation on the internet about the performance impact of the patches to address Meltdown. We have begun Elasticsearch-specific testing to establish the impact for our common benchmarking scenarios and will publish a blog post with this information as soon as we have it. Further Assistance As always, our support team is available to answer questions you have regarding Meltdown, Spectre, and our handling of them. Please use your standard support channel to ask those questions. ","locales":"","title":"Elastic Cloud and Meltdown"}
{"index":{}}
{"author":"Andrew Cholakian","category":"The Logstash Lines","publish_date":"2018-01-02T00:00:00.000Z","url":"/blog/logstash-lines-2018-01-02","seo_title":"","content":" We're back from the holidays with some great new features!In 6.0 we released the Logstash Pipeline Viewer. To get to this view, users would first have to go through the Pipelines view. This view listed the user's pipelines as cards with each card listing out the pipeline's versions.We are targeting 6.2 for a redesigned tPipelines view to not only serve as a navigational tool to the Pipeline Viewer but also to provide users with a summary view of their pipelines' health.Many thanks to several Elasticians who volunteered their time to help usability test designs for the new Pipelines view.With  users will be able to specify the pipeline ID from the CLI from 6.2.0 onward with the new --pipeline.id option. This is useful for those not using the pipelines.yml file. More consistent handling of IP addr lookups (Targetting LS 7.0) in TCP input as of Better handling of empty source fields in geoip plugin as of We have a tool that can generate a list of LS deps + their licenses for Legal compliance reasons. ","locales":"","title":"Logstash Lines: Update for January 2, 2018"}
{"index":{}}
{"author":"Adrien Grand","category":"Engineering","publish_date":"2018-01-01T00:00:00.000Z","url":"/blog/this-year-in-elasticsearch-and-apache-lucene-2017","seo_title":"","content":" As the Earth's rotation reaches the point where we close out another Gregorian calendar year, we wanted to share one last week in Lucene.Lucene is the core component that Elasticsearch is built on, we've seen some users that may have not even known about Lucene without Elasticsearch, and there are    that mention that Lucene bugs were found via Elasticsearch. The work we do on both projects is a commitment that we take great pride in.Here is a non-exhaustive list, in no particular order, of improvements that we made to Lucene over the course of 2017 that we hope you find interesting.And, as a bit of history, here is Shay Banon's , way back in 2006.We're really looking forward to growing this list of Elastic contributors in 2018. ","locales":"","title":"This Year in Elasticsearch and Apache Lucene - 2017"}
{"index":{}}
{"author":"Jim Goodwin","category":"Kurrently in Kibana","publish_date":"2017-12-26T00:00:00.000Z","url":"/blog/keeping-up-with-kibana-2017-12-26","seo_title":"","content":" Welcome to This is a weekly series of posts on new developments in the Kibana project and any related learning resources and events. Let's talk breaking changes. Wondering how we migrated to be compatible with 6.0? Tyler walks through how the team navigated the removal of mapping types. — elastic (@elastic) Hi all, Well, this is the last post in this series for 2017, and it has been an incredibly productive year for Kibana. Thank you for all of the enhancement requests, PRs, error reports, AMA questions, and forum posts, they help us to make Kibana better for everyone. I won't reiterate all of the changes from 2017, but if you haven't upgraded Kibana in a while you should really read these release posts below, there is a lot of value there and we have a packed road map for 2018.That's all for this week, have a great end of 2017, and a happy new year!Cheers, Jim ","locales":"","title":"Kibana: This week in Kibana for December 26, 2017"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2018-01-05T00:00:00.000Z","url":"/blog/brewing-in-beats-password-keystore","seo_title":"","content":" Did you know that is already available? Try it and let us know what you think. If you are curious to see the Beats in action, we just published the . This update includes the changes over the last two weeks. Password keystoreWe have merged the which allow users to define sensitive information into an obfuscated data store on disk instead of having them defined in plaintext in the yaml configuration.# create new keystore to disk ./metricbeat keystore create # add a new key to the store. ./metricbeat keystore add elasticsearch_password # remove a key from the store ./metricbeat keystore remove elasticsearch_password # list the configured keys without the sensitive information ./metricbeat keystore list You can then reference the keys from the keystore using the same syntax that we use for the environment variables: password: \"${elasticserarch_password}\" In the current implementation, the passwords are not encrypted into the keystore, only obfuscated. This new feature is planned to be released with the 6.2 release. Structured logging in libbeatThis refactors the logging of libbeat and adds support for structured logging. The new logging implementation is based on , which is one of the most efficient structured logging libraries for Golang. To switch to the JSON format, simply add to the configuration file. Another enhancement is that the Beats can also . By setting , all logs will be written to the Application log. The source name will be the name of the Beat. Besides this, there are no changes to the user facing logging configuration. The non-JSON logger output has some format differences, but, in general, it will have a more consistent format across outputs. These changes are only in the master branch at the moment, but we will likely include it in 6.2. Metricbeat: Read HAProxy metrics over HTTPThanks to , the HAProxy module can in addition to the TCP socket. This means HTTP authentication is also supported when reading the stats. The improvement will be available in the 6.2 release. Other changes:Repository: elastic/beatsAffecting all BeatsChanges in master: MetricbeatChanges in 6.1: PacketbeatChanges in master: AuditbeatChanges in master: TestingChanges in master: Changes in 6.1: Changes in 6.0: InfrastructureChanges in master: DocumentationChanges in 6.1: ","locales":"","title":"Brewing in Beats: Password Keystore"}
{"index":{}}
{"author":"Daniel Cecil","category":"Culture","publish_date":"2017-12-22T00:00:00.000Z","url":"/blog/solving-the-small-but-important-issues-with-fix-it-fridays","seo_title":"","content":" Question: How many engineers does it take to change a light bulb? Answer: The light bulb works fine on the system in my office... OK. It isn’t a great joke. But it’s the perfect setup for discussing an important topic here at Elastic: How do busy engineers, often working on large and gnarly projects, handle the small issues — like changing a metaphorical light bulb — that inevitably pop up from time to time? The answer: Fix-It Friday. The Elasticsearch code is housed in a public repository and accessible to anyone. When a user finds bugs, spots missing features, or wants to make a specific request, they can flag it using the issues tab by simply submitting a new issue. The process is open and transparent — just the way we like it. Each day, someone on the Elasticsearch team is assigned to a role called support dev help. In this role, the engineer has the dual duty of aiding the Elastic support team while looking for fresh issues in the Elasticsearch repository. When a new issue arises, the engineer will add a label to help the team prioritize when to tackle it, and how much effort it might take to solve it. However, not all issues have a simple diagnosis, nor an easy fix. “If there’s enough information, but it’s not clear that the issue is something we really want to handle due to policy, or maybe the person handling the ticket doesn’t have enough knowledge in the issue area to make a decision on it, then we can mark the ticket ‘discuss’ and it goes into the queue for Fix-It Friday,” said Colin Goodheart-Smithe, Elasticsearch Software Engineer.   Elasticsearch Team Lead Clint Gormley created the Fix-It Friday initiative a little over three years ago as a time when these small issues were given to engineers to solve. That ambitious concept didn’t last very long. The team quickly learned that small issues often turned out to be big ones in disguise. (Think: the filament in the light bulb looks dead, but in reality the electricity is out.) So, the scope of Fix-It Friday evolved into a get together for discussing user requests and finding solutions. Since the Elastic team is distributed, the meetup also became a weekly opportunity to get off Slack and email and get focused on a team video call. “It’s a good time,” said Gormley, “getting a group with such a wide range of expertise in one virtual room — it’s amazing.” About 10 issues are discussed during a typical one hour Fix-It Friday session. Issues are later fixed and implemented or de-escalated. When asked whether there was a particular issue from a Fix-It Friday meeting that jumped out at him, or that he thought was quirky or fun, Gormley laughed. “We’ve only been through 12,000 issues or something ….” But one seemingly small bug hiding something larger did spring to mind. Users reported heavy queries submitted to Elasticsearch never timing out, and Gormley recalled queries which ran for hours. “Usually, our queries run milliseconds, so if one runs for an hour, you know you have a problem,” he explained. In these situations users, thinking nothing is happening, run the query again. So, instead of one running for an hour, they actually have two — or more. This isn’t exactly an issue that could break anything, but it had the potential to slow results and reduce resources. The issue was marked for discussion at a Fix-It Friday session. After a lengthy debate, Elastic engineers considered adding a default timeout, meaning in one hour’s time, the query got canceled. It seemed like a good idea at first. But with several eyeballs on the issue, another perspective developed. Data is stored in indexes mapped out to shards, which are situated on different machines. When you run a query, it reaches out to all the shards, gathering the results and providing those results to the user. But what happens if one of the shards is missing due to a dying node on the sh","locales":"","title":"Solving the Small but Important Issues with Fix-It Fridays"}
{"index":{}}
{"author":"Heinrich Wendel","category":"User Stories","publish_date":"2017-12-22T00:00:00.000Z","url":"/blog/iprice-group-and-the-largest-ecommerce-catalog-in-southeast-asia-powered-by-elasticsearch","seo_title":"","content":" E-commerce is a very young and fragmented space in Southeast Asia (SEA). Unlike the United States, where Amazon is well established as the no. 1 player in online shopping, there are tens of thousands of entrepreneurs fighting for the favor of local shoppers with no clear leader in sight. Moreover, customers in our seven countries that constitute SEA, namely , , , , , and , have their individual preferences and unique tastes in accordance to their local culture.  Here at iPrice, we set out with the mission to build SEA’s one-stop-shopping destination, aggregating the product catalogs of all these merchants into a single shopping experience for the end user. They wouldn’t have to visit each merchant one-by-one to find the products they are searching for:  instead, iPrice categorizes the products and presents them to shoppers in a well-organized and visually-appealing fashion. The idea of product discovery was born, with the goal to make e-commerce more accessible and credible to the . Targeting 250 million SKUs to a population of almost 600 million peopleAt the beginning we had to ask ourselves the all too well-known question, “What technology platform to base iPrice on?” While a traditional SQL approach would have secured us easy access to developer talent, we were concerned about scalability. The two most popular e-commerce stores in SEA were launched just less than five years ago, but we knew that the region was about to experience its internet moment in a similar way to how China did a few years earlier. We were looking for a solution that was simple to set up with a small start-up team while we had little traffic and only a couple of million products, but scaled easily whenever the internet-burst would happen. From a functional perspective, e-commerce is all about search. Shoppers are trying to find one or two items they want to buy out of a catalog of hundreds of thousands of items. We are not a simple store but an aggregator of all stores—meaning we would have to deal with a scale that is one order of magnitude higher, carrying hundreds of millions of items. Our eyes naturally fell on Elasticsearch, a Lucene-based solution which was already renowned for its full-text search capabilities and had already gathered a decent reputation. Still undecided if we should stage Elasticsearch with a SQL-based primary data store, we thought through the customer purchasing journey through online portals. We found that customers only want to see the most recent information, meaning if they click on a product and it turns out to be out-of-stock on the merchant’s site, they can’t buy it and we lose a potential lead.  As such, we had to make sure that our product catalog is always up-to-date, while avoiding any additional replication or synchronization of the data which would potentially take a couple of hours.  While product data is the core of what we deal with, we also have to store the navigation structure of our portals, supplemental content that provides shoppers with contextual information about products they are interested in, and last but not least, data about the shoppers’ behavior on the site. Again, the question was whether to add a secondary SQL database or not, but the nature of this kind of data is also not very relational and Elasticsearch was already renowned for holding large amounts of log information. We settled on implementing our own CMS on top of Elasticsearch as our primary data store, going completely NoSQL in our approach, and this has benefited us in the long run. Importing >630 GB every 24 hours into a cluster with 320 GB of memoryThe following diagram illustrates our architecture in a nutshell. At the end of every day, our partners provide us with their latest product catalog in the form of CSV or XML files. After midnight, when it is unlikely tha","locales":"","title":"iPrice Group & the largest e-commerce catalog in Southeast Asia – powered by Elasticsearch"}
{"index":{}}
{"author":"Mark Walkom","category":"Engineering","publish_date":"2017-12-22T00:00:00.000Z","url":"/blog/elastic-advent-calendar-2017-week-three","seo_title":"","content":" Week three of the leads us towards the last of the series. If you’re just joining us, this calendar is how we’re celebrating the end of the year — by sharing a daily Elastic Stack tip in our community . You can catch up on the first two weeks by heading . French, Korean and English feature this week, and we have topics ranging from Elastic Cloud Enterprise, through to building your own crawler that indexes pages to Elasticsearch and a fantastic post on monitoring Kubernetes using the Elastic Stack. As usual you can follow along live by checking out or subscribing to the , or watch for the daily tweeting on . Here’s a sample of what we’ve posted from our third week:  Dec 15: by Abdon Pijpelink Dec 16: [[FR][Elasticsearch] Tests de performance pour votre plugin Elasticsearch]]() by David Pilato Dec 17: by Mat Schaffer Dec 18: by Jongmin Kim Dec 19: by Tyler Langlois Dec 20: by Medcl Zeng Dec 21: by Sherry Ger ! If you’re looking for other great pieces of reading, we recommend checking out our inspiration calendars — the (fully in Japanese) and (in English). And don’t be afraid to leave some feedback on the posts — we’d love to hear your thoughts! ","locales":"","title":"Elastic Advent Calendar 2017, Week 3"}
{"index":{}}
{"author":"Michelle Carroll","category":"Culture","publish_date":"2017-12-20T00:00:00.000Z","url":"/blog/applications-for-django-girls-san-francisco-workshop-now-open","seo_title":"","content":" We’re excited to be hosting a Django Girls workshop in San Francisco on Sunday, February 25! . If you’re not familiar with the organization, Django Girls is on a mission to inspire a love of programming for newcomers, and especially for underrepresented folks in tech (like women). The organization enables local teams of volunteers to set up one-day workshops, with an emphasis on achieving small successes in a supportive environment. It’s an organization and a mission we’re passionate about — democratizing technology, keeping things fun, and having fantastic documentation to boot. We strongly believe in the value of Django Girls. We are not only hosting this event, but we have sponsored 18 other events all over the world. This is our second year hosting a workshop around Elastic{ON}, and it’s been an amazing experience for everyone involved. (Want an additional perspective? .) As part of a distributed company, is a rare opportunity to bring many employees together in one place and work together on passion projects — like being coaches for the workshop. We’re also super-jazzed to be hosted by in their awesome space. While most of the folks who are using the Elastic Stack have some experience programming, we know that’s not universal — and folks who are in dev, ops, QA, analyst, or other programming-heavy roles often get asked for recommendations on learning how to code. Help us spread this education by getting the word out on this free workshop. Share this blog post or the , and don’t forget that applications close January 30! And finally, we recommend checking out the to see what workshops are coming up (and when they’re looking for coaches or applicants) and to get a sense of what it takes to organize one in your local region. Django Girls. Dream. Create. Code. We hope to see you there! ","locales":"","title":"Applications for Django Girls San Francisco Workshop Now Open"}
{"index":{}}
{"author":"Andrew Cholakian","category":"The Logstash Lines","publish_date":"2017-12-19T00:00:00.000Z","url":"/blog/logstash-lines-2017-12-19","seo_title":"","content":" Hello Logstashers! We're glad to present you all the past couple week's news in convenient digest form! This past week has been mostly about fixes. We have some big picture things we're working on, but nothing in enough shape to share yet. ","locales":"","title":"Logstash Lines: Update for December 19, 2017"}
{"index":{}}
{"author":"Tyler Smalley","category":"Engineering","publish_date":"2017-12-20T00:00:00.000Z","url":"/blog/kibana-6-removal-of-mapping-types","seo_title":"","content":" When Elasticsearch in 6.0.0, it was a major breaking change that affected any application that uses indices with multiple types. Kibana is one such application, relying on multiple types to store objects like visualizations, dashboards, index patterns, and more. How did we successfully migrate Kibana to be compatible with Elasticsearch 6? We share this strategy with you below, and hope that it gives you ideas for how to convert your multi-type indices to single-type. First, deciding what the new mapping was going to look like. There are a few common alternatives to mapping types: We chose adding a custom type field and nesting the data under the name of the type. This allowed us to have fields with the same name under different types, which was previously a limitation. Here is a visual of what this Elasticsearch document transformation looks like: Once we had the mapping, the next step was migrating the data. When creating the new index, we need to ensure is , otherwise we will not be able to fall-back to the single type. Additionally, 5.6 has an option which mimics the behavior in 6.0. After creating the new index with the desired mapping, we reindexed the data into the new format. To do this, we leverage the reindex API to transform the documents, setting the type field and nesting the previous data under the name of the type. Since IDs are only unique within a type, we also prefix the ID with the type. Using an alias, we can swap out the index in a single atomic action without downtime. Details of this full migration process can be found in our . Kibana 5.6 is considered a compatibility release, allowing users to perform a rolling upgrade to 6.0 without downtime. This means Kibana 5.6 needs to seamlessly handle both single and multiple mapping types. To allow for this, we introduced a which has become the preferred way of programmatically interacting with Kibana data. This allows for a consistent interface, regardless of the underlying Elasticsearch document structure. In 5.6, we still default to using multiple types, but fall back to a single type when we identify the data has been migrated. Here are a few examples of how we built a fallback system. Create When creating a document we will receive a if the type does not exist. We receive this error after the data has been migrated, allowing us to re-try with the single-type format. POST /.kibana/doc/index-pattern:abc123/_create { \"type\":\"index-pattern\", \"index-pattern\":{ \"title\":\"Test pattern\" } } GetInstead of making two requests, we can perform a single search to capture the document. Here we use a boolean query containing both of the formats. POST /.kibana/_search { \"query\":{ \"bool\":{ \"should\":[{ \"bool\":{ \"must\":[{ \"term\":{ \"_id\":\"abc123\" } }, { \"term\":{ \"_type\":\"index-pattern\" } }] } }, { \"bool\":{ \"must\":[{ \"term\":{ \"_id\":\"index-pattern:abc123\" } }, { \"term\":{ \"type\":\"index-pattern\" } }] } }] } } } DeleteWhen deleting a document, we use , providing the same query used for fetching a document. UpdateWhen using the _update API, we receive a if the document is missing. We receive this after the data has been migrated, allowing us to re-try with the single-type format. POST /.kibana/doc/index-pattern:abc123/_update { \"index-pattern\":{ \"title\":\"My new title\" } } These changes allowed us to migrate the Kibana index at any time during a 5.6 installation. In X-Pack, we have made this process even easier for users through our . If you have indices created in 2.x, you must reindex them before upgrading to Elasticsearch 6.0. In addition, the internal Kibana and X-Pack indices must be reindexed to upgrade them to the format required in 6.0. The Reindex Helper identifies these indices and provides a button to perform the reindex. Have a special use case or strategy for how to migrate your multiple type indices to single type? Share it with us! You can also ask us for advice on our . ","locales":"","title":"Kibana's Road to 6.0 and the Removal of Mapping Types"}
{"index":{}}
{"author":"David Roberts","category":"Engineering","publish_date":"2017-12-19T00:00:00.000Z","url":"/blog/smarter-machine-learning-job-placement-in-elasticsearch","seo_title":"Smarter Machine Learning Job Placement in Elasticsearch","content":" Ever since we , ML jobs have been automatically distributed and managed across the Elasticsearch cluster. To recap, you specify which nodes you’re happy for ML jobs to run on by configuring the and settings to on these nodes. Then, when you the cluster runs the job’s associated analytics process on one of those nodes, and the job will continue to run there until it’s closed or the node is stopped. Prior to version 6.1 this node allocation was done in a very simple way: a newly opened job was always allocated to the node running the fewest jobs subject to a couple of : Different ML job configurations and data characteristics can require different resources. For example, a single metric job generally uses very little resource, whilst a multi-metric job analysing 10,000 metrics will require more memory and CPU. But no account was taken of the expected or actual resource usage of each job when allocating jobs to nodes, which could lead to: To mitigate these problems, starting in version 6.1 ML will allocate jobs based on estimated resource usage. Each ML job runs in a separate process, outside of the Elasticsearch JVM. In 6.1 we have added a new , , to control the percentage of memory on a machine running Elasticsearch that may be used by these processes associated with ML jobs. This is a dynamic cluster setting, so the same number will apply to all nodes in the cluster, and it can be changed without restarting nodes. By default the native processes associated with ML jobs are allowed to use 30% of memory on the machine. ML will never allocate a job to a node where it would cause any of these constraints to be violated: For nodes where none of the hard constraints would be violated, we will continue to allocate jobs to the least loaded ML nodes. However, instead of number of jobs being the definition of load it is now estimated job memory. Job memory is estimated in one of two ways: The first method is preferred, but cannot be used very early in the lifecycle of a job. The estimate of job memory use is based on actual model size when the following conditions are met: Once jobs are “established” according to these criteria, we should be able to make pretty good decisions about whether opening a new job is viable. However, if many jobs are created and opened around the same time then we will tend to be restricted by the configured in the . Before version 6.1 our default was 4GB, and this was excessive in many cases. Therefore, in version 6.1 we have cut the default to 1GB. If you are creating advanced jobs that you expect to have high memory requirements we’d encourage you to explicitly set this limit to a higher value when creating the job. And there should be less scope to hog resources if you accidentally create a job that would use a lot of memory if it were allowed to run unconstrained. Similarly, if you’re creating jobs that you expect to have very low memory requirements, we’d encourage you to set in the to a much lower value than the default. We’ve done this in the job creation wizards in Kibana: single metric jobs created using the wizard now have their set to 10MB, and multi-metric job created by the UI wizard have it set to 10MB plus 32KB per distinct value of the split field. Because of the smarter limiting of ML jobs based on memory requirements, in version 6.1 we’ve increased the default value for from 10 to 20. Of course, if you have large jobs or little RAM then the memory limit will kick in and you won’t be able to open 20 jobs per node. But if you have many small jobs then you’ll no longer be artificially restricted. If you’ve previously customized the value of then you may wish to revisit your setting taking account of the new functionality. During rolling upgrades to version 6.1 ML jobs might be running in mixed version clusters where some nodes are running version","locales":"","title":"Smarter Machine Learning Job Placement in Elasticsearch"}
{"index":{}}
{"author":"Clinton Gormley","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2017-12-18T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2017-12-18","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. automatically optimises away the need to track versions of in-memory buffered documents while indexing if all documents in the ram buffer are guaranteed to have no duplicates and are documents using auto-generated IDs. This reduces the GC overhead drastically in high-throughput scenarios (up to 50%) and offers a depending on the workload. This change will come in 6.2. Elasticsearch 6.2.0 will be the first release of Elasticsearch to officially support JDK 9. Elasticsearch 6.2.0 will run out-of-the-box on both JDK 8 and JDK 9. We as of the JDK, but Elasticsearch will move forward with the JDK ecosystem. When , releases of Elasticsearch will stop supporting JDK 9:  we intend to support JDK 10 but there is no guarantee of that at this time. Support for JDK 8 will continue until end-of-life in of the JDK. A new () has been added to master and is planned to be backported to 6.2. The ranking evaluation API can be used to evaluate the quality of ranked search results over a set of typical search queries. Users can supply a set of typical queries together with a list or manually rated documents, and the API will perform the queries and calculate common information retrieval metrics like mean reciprocal rank, precision or discounted cumulative gain on it. The API is currently marked as experimental and will probably change a bit in the foreseeable future. More details about the current state can be found in the . Ranking via the API is a very manual process at the moment, so we only expect to see traction around this feature once we have a UI to make interaction much more point-and-click. Brainstorming in progress with the Kibana team. Changes in 5.6: Changes in 6.0: Changes in 6.1: Changes in 6.2: Changes in 7.0: Apache Lucene There is an ongoing to release Lucene 7.2.0, which is going well so far. and . ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2017-12-18"}
{"index":{}}
{"author":"Court Ewing","category":"Releases","publish_date":"2017-12-19T00:00:00.000Z","url":"/blog/kibana-6-1-1-released","seo_title":"","content":" Today, we released Kibana 6.1.1 with a fix for a high severity security vulnerability in the Time Series Visual Builder. All administrators of Kibana 6.1.0 are urged to upgrade Kibana immediately. Versions prior to 6.1.0 are not affected. If you had any Kibana 6.1.0 instances on , we’ve automatically upgraded them, so no further action is required. For folks that cannot upgrade from 6.1.0 at this time, you can disable time series visual builder entirely by specifying in kibana.yml and restarting Kibana. Note, this will require a full “optimize” run, which can take a few minutes. Math aggregations and remote code execution In Kibana 6.1.0, we released a new feature for “math aggregations” in the Time Series Visual Builder which allowed users to apply mathematical operations to their TSVB results. Unfortunately, this new feature has a vulnerability that could allow an attacker to execute arbitrary code on the Kibana server. We’ve in 6.1.1. Removing a feature is never something we take lightly, especially in a patch release, but the issue is severe and there isn’t a reliable way to permanently fix it. We do want to have this sort of math capability in Kibana at some point, but we need to take a more holistic view on its security before releasing it again. There are a couple of other bug fixes in this release as well, so check out the for all the details. If you’re not using , head on over to our page to get the release now. ","locales":"","title":"Kibana 6.1.1 released"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2017-12-14T00:00:00.000Z","url":"/blog/brewing-in-beats-monitor-raid-status-with-metricbeat","seo_title":"","content":" Did you know that is already available? Try it and let us know what you think. If you are curious to see the Beats in action, we just published the . Metricbeat: RAID metricsetAdded upon request from our own Cloud team, this polls and records software RAID specific metrics. The metricset is part of the system module and is planned to be released in 6.2. New community Beat: TracebeatCreated by , this sends traceroute pings and indexes the results into Elasticsearch. Other changesRepository: elastic/beatsAffecting all BeatsChanges in master: Changes in 6.1: Changes in 6.0: MetricbeatChanges in master: Changes in 6.1: Changes in 6.0: PacketbeatChanges in master: FilebeatChanges in master: WinlogbeatChanges in master: Changes in 6.1: Changes in 6.0: ProcessorsChanges in master: Machine learning jobsChanges in master: TestingChanges in master: Changes in 6.1: Changes in 6.0: InfrastructureChanges in master: Changes in 6.1: Changes in 6.0: PackagingChanges in master: DocumentationChanges in 5.3: Changes in master: Changes in 5.6: Changes in 6.1: Changes in 6.0: Repository: elastic/gosigarChanges in master: ","locales":"","title":"Brewing in Beats: Monitor RAID status with Metricbeat"}
{"index":{}}
{"author":"Jay Modi","category":"Engineering","publish_date":"2017-12-18T00:00:00.000Z","url":"/blog/default-password-removal-elasticsearch-and-x-pack-6-0","seo_title":"Default Password Removal in X-Pack 6.0","content":" The Elasticsearch team takes pride in making software that is easy to get started with, which allows developers to make progress on their projects at a faster pace. The team wanted the same experience for X-Pack security features and out of this desire, the addition of built-in user accounts was born. X-Pack ships with a built in administrator account and accounts for Kibana and Logstash system users. In 5.x, these accounts have a default password of ‘changeme', which was chosen with the hopes that users would heed the advice embedded in the password and, well, change the password. Hope is not good enough when it comes to securing applications:  relying on hope means we assume our users know about these accounts and the default password and also know why these need to be changed. As a company, relying on hope is like rolling the dice for becoming the next major piece of software featured in the news cycle as the culprit responsible for a bad data leak. In order to provide better security, we made for 6.0 that removed the default password altogether. The removal of the default password has the effect of adding a single step to the getting started process for Elasticsearch and we felt that this tradeoff was the right one to make when it came to shipping software that is secure. Getting this process down to a single step was not easy:  there were a lot of ideas and a lot of back-and-forth discussions on how we accomplish this. The solution makes use of an auto-generated seed value on each node. This seed value serves as the initial password for the elastic user. The seed value alone could have been a fine solution but it has its own issues:  the most important being that we have a different password for the elastic user on each node. In terms of usability, the seed value as the elastic password would complicate the getting started experience as it would require additional manual steps to configure passwords for other users such as the `kibana` user. More work was needed to make getting started a nice experience. Moving beyond the seed value, a new tool, ‘’, has been added to make the initial password setting as easy as possible. The tool has both an interactive mode where the user can provide their own passwords and an automated mode that sets the passwords to a random value, which is then sent to standard out. Let’s take a look at how easy it is to get started with X-Pack: ","locales":"","title":"Default Password Removal in Elasticsearch and X-Pack 6.0"}
{"index":{}}
{"author":"Jim Goodwin","category":"Kurrently in Kibana","publish_date":"2017-12-18T00:00:00.000Z","url":"/blog/keeping-up-with-kibana-2017-12-18","seo_title":"","content":" Welcome to This is a weekly series of posts on new developments in the Kibana project and any related learning resources and events. 6.1 has a new homepage with even more dashboard customization options and input controls. — elastic (@elastic) Hi all, The big event last week was shipping the 6.1 release of Kibana, you can see a link to the blog post in the tweet above.Work on integrating into Kibana continues and it hopefully will be available for use in the 6.2 release. One of the important parts of EUI is its documentation, including a new set of guidelines for writing content for Kibana. You can see some of the work in progress here: I'd also like to point out some recent Webinars featuring members of the Kibana team:With some significant changes to the visualize API, using existing Kibana visualizations in your own plugins has become much easier. Join Alex Francoeur, Thomas Neirynck, and Peter Pisljar for a live demonstration to learn how to develop visualizations in Kibana. Version 6.0 introduces many new features and improvements across all components of the Elastic Stack. In this webinar, Archana Sriram, Mike Baamonde, and George Kobar will walk you through the considerations, best practices, and caveats for upgrading your Elastic Stack to 6.0. That's all for this week. Cheers, Jim ","locales":"","title":"Keeping up with Kibana: This week in Kibana for December 18, 2017"}
{"index":{}}
{"author":"Michelle Carroll","category":"","publish_date":"2017-12-14T00:00:00.000Z","url":"/blog/elastic-advent-calendar-2017-week-two","seo_title":"","content":" Week two of the has been a busy one! If you’re just joining us, this calendar is how we’re celebrating the end of the year — by sharing a daily Elastic Stack tip in our community . You can catch up on the first week by heading . In this batch, we’ve got posts in Chinese, English, German, Portuguese, Italian, and Japanese, and covered everything from text analysis to migrating data with the reindex API to analyzing your hockey game and cryptocurrencies with the Elastic Stack. You can follow along live by checking out or subscribing to the , or watch for the daily tweeting on . Here’s a sample of what we’ve posted from our second week Dec 8: by Medcl Zeng Dec 9: by Jordan Sissel Dec 10 by Philipp Krenn Dec 11: by Thiago Souza Dec 12: by Atonio Bonuccelli Dec 13: by Aaron Aldrich Dec 14: by Jun Ohtani ! If you’re looking for other great pieces of reading, we recommend checking out our inspiration calendars — the Elastic Stack calendar on Qiita (fully in Japanese) and SysAdvent (in English). And don’t be afraid to leave some feedback on the posts — we’d love to hear your thoughts! ","locales":"","title":"Elastic Advent Calendar 2017, Week 2"}
{"index":{}}
{"author":"Sam Reid","category":"Engineering","publish_date":"2017-12-14T00:00:00.000Z","url":"/blog/how-to-build-a-site-search-ui","seo_title":"","content":" .w3-btn,.w3-button{border:none: display:inline-block: outline:0: padding:12px 20px: vertical-align:middle: overflow:hidden: text-decoration:none: color:inherit: background-color:inherit: text-align:center: cursor:pointer: white-space:nowrap} .w3-btn:hover{box-shadow:0 8px 16px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19)} .w3-btn,.w3-button{-webkit-touch-callout:none: -webkit-user-select:none: -khtml-user-select:none: -moz-user-select:none: -ms-user-select:none: user-select:none} .w3-disabled,.w3-btn:disabled,.w3-button:disabled{cursor:not-allowed: opacity:0.3}.w3-disabled *,:disabled *{pointer-events:none} .w3-btn.w3-disabled:hover,.w3-btn:disabled:hover{box-shadow:none} .w3-black,.w3-hover-black:hover{color:#fff!important: background-color:#00BFB3!important} Note: The original extended post is available on the . Search nirvana: A powerful backend + well-designed UIDepending on the purpose and scale of your website, search can be a critical feature that enables your users to quickly find the information they need. Elasticsearch makes it significantly easier to architect a search engine that delivers relevant results, but building your search backend is only part of the work of implementing a search experience. Without an intuitive search interface, your users may not get the full value of your search engine. What we’ve learned as search providersAt (an Elastic Company), we provide search as a service to completely handle the backend of your search engine, and we also help you to build well-designed search UIs. Swiftype is built on the Elastic Stack which has enabled us to support over 10,000 production search engines and serve over 5 billion queries a month. It’s safe to say that we’ve learned a thing or two about search over the years as we’ve helped small and large companies like Lyft, AT&T, Twilio, Asana, and Samsung provide top-notch search experiences. If you’d like to learn more about Swiftype’s architecture and use of the Elastic Stack, . In our experience helping these organizations and many others with search, we’ve been able to see what works well when it comes to search UIs and apply those learnings to both our out-of-the-box search interface as well as our jQuery libraries that we support for creating fully custom UIs. While building a great search interface can take some effort, we’ve consistently seen companies reap the rewards of implementing well-designed search UIs which include revenue growth and better user engagement. Implementing your search UISwiftype can help you to get a jump start on building your next search experience, including the UI (or ). In 3 steps, you can have a functioning search UI implemented. — Index your data into Swiftype — Tune your search results — Implement your search bar ","locales":"","title":"How to Build a Site Search UI"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2017-12-13T00:00:00.000Z","url":"/blog/elasticsearch-6-1-0-released","seo_title":"Elasticsearch 6.1.0 released","content":" Today we are pleased to announce the release of , based on . This is the latest stable release, and is already available for deployment on , our Elasticsearch-as-a-service platform. Latest stable release in 6.x: You can read about all the changes in the release notes linked above, but there are a few changes which are worth highlighting: ","locales":"","title":"Elasticsearch 6.1.0 released"}
{"index":{}}
{"author":"Steve Dodson","category":"Releases","publish_date":"2017-12-14T00:00:00.000Z","url":"/blog/machine-learning-6-1-0-released","seo_title":"","content":" On Demand Forecasting Smarter Node Allocation for ML Jobs Automatic Job Creation for Known Data Types Data Visualizer This tool summarizes the key features in the data, such as cardinality of fields, sparsity and counts of key values. Moving forward we will extend this view to help you create more effective analysis configurations for time series ML jobs: Population Analysis Job Wizard Job GroupsOverall Buckets GET _xpack/ml/anomaly_detectors/job-1,job-2,job-3/results/overall_buckets { \"overall_score\": 75, \"start\": \"1403532000000\", \"top_n\": 2 } { \"count\": 1, \"overall_buckets\": [ { \"timestamp\" : 1403532000000, \"bucket_span\" : 3600, \"overall_score\" : 75.0, \"jobs\" : [ { \"job_id\" : \"job-1\", \"max_anomaly_score\" : 80.0 }, { \"job_id\" : \"job-2\", \"max_anomaly_score\" : 70.0 }, { \"job_id\" : \"job-3\", \"max_anomaly_score\" : 14.0 } ], \"is_interim\" : false, \"result_type\" : \"overall_bucket\" } ] } ","locales":"","title":"Machine Learning 6.1.0 Released"}
{"index":{}}
{"author":"Andrew Cholakian","category":"Engineering","publish_date":"2017-12-13T00:00:00.000Z","url":"/blog/logstash-pipeline-viewer-6-0","seo_title":"","content":" Logstash’s strength is its flexibility. With its minimalist syntax and rich set of plugins, users have been able to conjure up all kinds of Logstash pipelines — from to . As you create increasingly sophisticated pipelines, you may have discovered that understanding these pipelines becomes harder. You might want to see the overall shape of your pipeline, understand where branches in data flow might be, and how the various parts of your pipeline perform under actual production conditions. To help you answer some of these questions, we have introduced the Logstash Pipeline Viewer UI. Concepts and Terminology Before we talk some more about the Pipeline Viewer, it’s necessary to understand some associated concepts and terminology first. Every Logstash pipeline has an . If you define your pipelines using the or command-line options, your pipelines will have their ID set to “main”. If you define your pipelines or , you will be able to set your own pipeline IDs to something more descriptive like, say, “Apache Access Logs Processing”. For every (uniquely ID’d) pipeline, there can be multiple of that pipeline. What differentiates one version of a pipeline from another version of that pipeline are its contents. For instance, say you created a pipeline with ID = “p1” containing one input, one filter, and one output plugin. At this point, only one version of “p1” exists. At some point in the future, say you modified “p1” so it contained the same plugins as before but also included an additional filter plugin. Now “p1” would have two versions in existence - the original one and the changed one. Since the version of a pipeline is based on its contents, Logstash will automatically calculate a version for you. It will look something like this: . This version currently only shows up in the Monitoring UI in Kibana (where the Pipeline Viewer feature lives), and even there it often shows up in a shortened form for ease of readability. The Tour Great! Now that you understand pipeline IDs and versions, let’s dive into the Pipeline Viewer UI itself. We assume that you have one or more Logstash pipelines currently running and to Elasticsearch. Navigate to the Monitoring tab in Kibana and scroll down to the Logstash section. You might notice that there is a new item in this section called “Pipelines”. Clicking here shows you all pipelines across your Logstash deployment that have been running during the time window set by the Kibana Time Picker (top right of screen). For each pipeline the list of its versions that existed during the Kibana Time Picker time window are shown. Clicking on a pipeline’s version takes you to the Pipeline Viewer, the centerpiece of this blog post. It shows a rendering of that specific version of the pipeline. The pipeline is visualized as a directed acyclic graph, with vertices and edges. Vertices in the graph can represent one of three elements in a Logstash pipeline: plugins, if conditions, or the queue in Logstash that exists between the input and filter stages of a pipeline. Edges represent paths that events can take as they travel through the pipeline. Plugin vertices show the ID of the plugin (if one is specified by the pipeline creator via the property), the plugin’s throughput (in events per second), and the latency (in milliseconds) introduced by the plugin as it processes events passing through it. If condition vertices show the condition being tested. They have two types of edges emanating from them: “true” or “T” edges, representing the path an event would take if the condition is met, and “false” or “F” edges, representing the path an event would take if the condition is not met. Today we do not show metrics for if condition vertices or the queue vertex but this is something we plan to add in the future. Applications Now that you know your way around the Pipeline Viewer, here are some of its practical applications. A","locales":"","title":"You know, for visualizing your Logstash pipelines"}
{"index":{}}
{"author":"Andrew Cholakian","category":"Releases","publish_date":"2017-12-13T00:00:00.000Z","url":"/blog/logstash-6-1-0-released","seo_title":"","content":" Logstash 6.1.0 has launched! We've got some great new features to talk about! Read about it here, or just head straight over to our  and give it a shot! However, you may want to take a minute to read about  and  first. Read on for what's new in Logstash 6.1.0. We’re proud to announce a great new way to extend Logstash functionality in 6.1.0. Complex modification of events in Logstash is now much easier due to our new feature, file based Ruby scripting via the Logstash Ruby filter. While the Ruby filter already lets you use custom ruby code to modify events, that code must live inside the Logstash configuration file itself, which doesn’t work well for longer pieces of code, and is hard to debug. Additionally, there’s never been a good way to reuse or test that code. File based Ruby scripts can take arguments, letting you reuse code within your Logstash configs easily. This new feature lets you write Ruby code in an external file, with tests inline in that same file, and reuse that anywhere you’d like. Another nice feature here is that we can generate accurate line numbers in stack traces for exceptions in file based Ruby scripts, making them much easier to debug. The full details for configuring this are available in the . For a short example see below: To configure the ruby filter with a file use: filter { ruby { # Cancel 90% of events path => \"/etc/logstash/drop_percentage.rb\" script_params => { \"percentage\" => 0.9 } } } The file 'drop_percentage.rb' would look like: def register(params) @should_reject = params[\"reject\"] end def filter(event) return [] if event.get(\"message\") == @should_reject event.set(\"field_test\", rand(10)) extra_processing(event) [event] end def extra_processing(event) # .. end test \"non rejected events\" do parameters do { \"reject\" => \"hello\" } end in_event { { \"message\" => \"not hello\" } } expect(\"events to flow through\") do |events| events.size == 1 end expect(\"events to mutate\") do |events| events.first.get(\"field_test\").kind_of?(Numeric) end endLogstash 6.1.0 brings some exciting new changes to the Logstash internals. We’ve been working on a full rewrite of the internal execution engine in Logstash. This rewrite moves the core execution logic from JRuby to Java/JVM Bytecode. With this approach we’ll be able to pave the way to more performance improvements in the future, as well as the ability to write optimized Logstash plugins in any JVM language.This feature is currently disabled by default, and users should note that it is experimental and not yet ready for production. To enable this feature, you’ll need to use the '--experimental-java-execution' flag. We encourage users to try this flag out in test and staging environments and report any bugs found. Our hope is to make this the default execution method sometime in the 6.x timeframe. ","locales":"","title":"Logstash 6.1.0 Released"}
{"index":{}}
{"author":"Monica Sarbu","category":"Releases","publish_date":"2017-12-13T00:00:00.000Z","url":"/blog/beats-6-1-0-released","seo_title":"","content":" We’re pleased to announce the Beats 6.1.0 release. This is the latest stable version and it comes with lots of new modules and an exciting Autodiscovery feature. Docker AutodiscoveryBeats 6.1 brings in the first phase of autodiscovery support. Autodiscovery allows the user to configure providers, that watch for system changes and emit events to a common bus. Based on these events, the Autodiscovery system detects situations when there is something new that we can monitor and instantiates new Beats modules for it. In general, Autodiscovery allows the Beats to react and adapt to changes in the ever more dynamic infrastructures. The first provider watches for Docker events. It supports config mapping from container metadata to config templates, so new modules are created when a container starts. metricbeat.autodiscover: providers: - type: docker templates: - condition: equals.docker.container.image: redis config: - module: redis metricsets: [\"info\", \"keyspace\"] hosts: \"${data.host}:${data.port}\" The above is an example configuration that instantiates the Metricbeat Redis module every time a new redis container (defined by having the redis image) is started. Note that the connection information (host/ports) is filled in by the autodiscovery support via a template. Future releases will add more Autodiscovery providers, for example for Kubernetes events and package managers. New Metricbeat and Filebeat modulesEach Beats release adds a few new Metricbeat and Filebeat modules, but 6.1 really sets the bar higher. Many of these modules are contributed by our users (Thank You!). Let’s go through the list: TLS support in PacketbeatPacketbeat 6.1 adds , which is one of the most anticipated Packetbeat features. It doesn’t mean decrypting traffic, but it parses the initial handshake and extracts data like ciphers supported by the client and the server, the client and server certificate chains, the subject alternative name (SAN), validity dates, raw certificates, and so on. This data is super valuable for debugging TLS issues and also for intrusion detection and auditing. The implementation also comes with support for the extension to TLS, which allows Packetbeat to detect, for example, whether HTTP/2 or HTTP/1 are used as an application protocol on top of the TLS connection. Docker JSON-file prospector in FilebeatFilebeat 6.1 comes with an (experimental) Docker prospector that implements the default . Filebeat could already read Docker logs via the log prospector with JSON decoding enabled, but this new prospector makes things easier for the user. It abstracts the format, so there is no need to manually configure JSON decoding. Here is an example config, which captures the logs from a single container specified by its ID: prospectors: - type: docker containers.ids: - c3ec7a0bd9640151a768663b7e78c115d5b1a7f87fba572666bacd8065893d41 It also parses the timestamp from the JSON file, something that wasn’t previously possible with Filebeat alone (it required Logstash or Ingest Node). This new prospector works great with the Docker Autodiscovery provider. New Auditbeat dashboards Auditbeat 6.1 comes with several in the default configuration file, which makes it easier to get started with. To match the use cases, we also have three new dashboards: FeedbackIf you want to make use of the new features added in Beats 6.1.0, please , install it, and let us know what you think on Twitter () or in our . ","locales":"","title":"Beats 6.1.0 released"}
{"index":{}}
{"author":"Jim Goodwin","category":"Releases","publish_date":"2017-12-13T00:00:00.000Z","url":"/blog/kibana-6-1-0-released","seo_title":"","content":" You may be thinking \"But wait, you just released 6.0?\"... I know, right! But 6.1.0 has been percolating for months and now it is here. New in this release we have:   You can now create input control visualization components which when placed on a Dashboard allow users to select particular values from a terms aggregation from a multi-select drop down control or select a range from a min/max aggregation using a range slider control. This will make it easy to guide users to important filtering values for the dashboard and make it simple for them to apply filters and explore the information on the dashboard.Kibana has a homepage! Finally clicking on the Kibana logo in the upper left will do something useful! And finally new users will get some great pointers as to what to do with Kibana when they launch it for the first time! Please watch this space, there is a lot more coming...Beginning with X-Pack monitoring 6.1, the Monitoring UI will automatically use to load data from remote clusters defined using the Elasticsearch connection's remote cluster list . That means if you only define elasticsearch.url in your kibana.yml, and configure your dedicated monitoring clusters as remote clusters of it, then the Monitoring UI will detect and show all clusters! We also took the time to optimize the experience so that it routes requests to specific monitoring clusters whenever possible. This provides multiple advantages over the existing behavior:Using Cross Cluster Search is now the preferred way to talk to a dedicated monitoring cluster because of these benefits and simplifications. We hope that it helps you to both improve your Elastic Stack monitoring and simplify it at the same time.Newly introduced visualizations can now be part of labs-mode. Visualizations in labs-mode introduce new more cutting-edge functionality and can be subject to change across minor releases. Labs-mode can be turned off in the advanced settings. Labs-visualization will then no longer be available to the user. The Time Series Visual Builder is not part of labs-mode, it continues to be an experimental feature. The input controls are the first to be flagged as a lab visualization.Pie charts now support data labels making it easy to understand the values being presented without having to look back and forth to a legend.We have improved the use of Region Maps for deployment in environments without internet access. Similar to the Coordinate Map visualization, the Region map can now use a WMS-service as a base-layer. Admins can now also setup Kibana to opt-out of connection to the Elastic Maps Service. Users can now opt-out of having the visualization display warnings.Reporting now has the ability to render Dashboards in a WYSIWYG manner to PDF preserving the locations and sizes of panels on the dashboard.There are now additional options for customizing the content on Dashboards. We've added a new option to \"use margins\" to add separation between Dashboard panels. We've also added the capability to customize the panel titles or hide them altogether. The Management application now supports managing the license for your cluster including seeing your current license level and expiration information, links out to obtain a Basic or a paid license and support for uploading and installing a new license. We've also made sure that you'll be able to log into Kibana to use this tool even if your license has expired.Some settings are sensitive, and relying on filesystem permissions to protect their values is not sufficient. For this use case, Kibana provides a keystore, and the kibana-keystore tool to manage the settings in the keystore. [More information here: ] Please , try it out, and let us know what you think on Twitter () or in our . You can report any problems on the . ","locales":"","title":"Kibana 6.1.0 is released"}
{"index":{}}
{"author":"Tyler Hannan","category":"Releases","publish_date":"2017-12-13T00:00:00.000Z","url":"/blog/elastic-stack-6-1-0-released","seo_title":"","content":" 6.1.0 is here. Fresh on the heels of the 6.0.0 GA release, we are pleased to introduce you to the capabilities of 6.1.0. You should download it now or use it on (your favourite hosted Elasticsearch and Kibana provider. The line between products and features is often a blurry one. And we feel that pain – or experience that delight – as keenly as many. When there are so many features to highlight in a release, where do you even start? Either you craft the next great novel or you choose to provide links to details. Happy reading and…more importantly…happy searching, analyzing, and visualizing. Let’s start with some features that are, without question, worth mentioning: APM After announcing that we joined forces with Opbeat, and an alpha (in the 6.0 timeframe), we are super pleased to share that Elastic APM is now in beta. This includes not only all of the goodness we have described in the past, but also features a brand new UI. It is available in X-Pack Basic (free!) and you can read more information in the . Machine Learning Unsupervised, Automated, Expedited…adjectives abound when describing machine learning solutions. But, as our team has said, Elastic Machine Learning ‘catches what you might miss, all by itself.’ In 6.1.0 this expanded to include a series of substantive new features including On Demand Forecasting (based on the past, what values would you expect in the future), smarter allocation for efficiently assigning jobs to ML nodes, and automatic job creation for known data types. The fun doesn’t stop there, and all the features are described in the post. Elasticsearch The summary, or aggregation, of features is in the . Kibana Visualize the future of interacting with your data in the because there is MUCH more than can be listed here. Logstash Grok the details in the . Beats If you want all the details, ‘Go’ read the . Get It Now! ","locales":"ja-jp","title":"Elastic Stack 6.1.0 Released"}
{"index":{}}
{"author":"Mark Walkom","category":"Engineering","publish_date":"2017-12-12T00:00:00.000Z","url":"/blog/custom-region-maps-in-kibana-6-0","seo_title":"","content":" Grab Your Cartometers In Kibana 5.5 we added the ability to define your own , which are also also known as choropleth maps. This allows users to define custom geo-boundaries as , then overlay them on the Elastic Map Service to display aggregations for custom geographical areas natively in Kibana. This is a simple yet very powerful method for providing localised insights from your datasets. We've touched on this previously in a , but let's dig into this some more and run through a practical example of how to deploy it step by step in the Elastic Stack! If you'd like to replicate this blog post as you read it, the code for doing so is all . Ready, Set…. Before we start, let's define a few words that we will use throughout this post that you may be unfamiliar with, or have heard elsewhere in different contexts:  Sourcing Your Map has a number of high quality maps and for this post we'll be using the Australian State file. If you download the file, extract it and take a look in it, you will find a repetitive structure, with the geoshapes we need and other fields that we will use later. There are a number of other sources out there that build and provide geojson files for you to download and use, your favourite search engine can help locate them for you. Ultimately, if you can't find what you want on the internet then you can build your own geojson files. That's outside the scope of this blog post, but luckily exploratory.io themselves have an excellent blog post on how to do that right ! Configuring Your Source Now that we have our geojson file, we need to let Kibana know how to read it and map the geoshapes it holds onto the tiles that the Elastic Map Service provides. kibana.yml Using the extracted aus_state.geojson from the \"Australia States\" archive, we need to configure some custom settings in your kibana.yml file. Open the yaml file in your favourite editor and add this on the end:  # Custom Region Maps regionmap: layers: - name: \"Australian States\" url: \"http://localhost:8000/aus_state.geojson\" attribution: \"https://exploratory.io/maps\" fields: - name: \"STATE_NAME\" - description: \"State Name\" These settings are explained in of the documentation, but let's break it down here:  The fields.name we define above is extremely important. It tells Kibana how to take the documents in Elasticsearch and then figure out which shapes in the geojson file it needs to place that document for the aggregation results to be displayed. We will run through this in detail and step by step, so don't worry if that's a bit confusing for right now. Serving Up The Geojson File The last step we need to do here is to serve the geojson file from a web server, as was defined in url. For this post we kept it simple and used this awesome little tool from npm, , and ran this command via our shell from the same directory that the aus_state.geojson lives in:  http-server --cors=Access-Control-Allow-Origin -p 8000 You can test this by opening from your browser, and you should see the contents of the geojson file. There are also dedicated products to serve your geo-data, like that would be better suited for ongoing, production level deployments. We'd definitely recommend investigating them as more permanent solutions. Deploying Your Configuration It's all been pretty text heavy so far, especially for what is ultimately a visualisation. So let's take a look at what this all translates to in the Kibana UI. The Aggregation Settings As with most things in Kibana, we need to build an aggregation to show our data on our custom geoshapes. For this example dataset we will change the default Count Aggregation, under Metrics, to a Max and select the Population field to run the agg on. Then we Bucket the data using a Terms Aggregation on the Name field. Here's what we mean:  Note - these are shown side-by-side to save room on this page. In Kibana they are displayed as a single vertical column on the left of the browser window. The Map Settings Now","locales":"","title":"Custom Region Maps in Kibana 6.0"}
{"index":{}}
{"author":"Rasmus Makwarth","category":"Releases","publish_date":"2017-12-12T00:00:00.000Z","url":"/blog/elastic-apm-beta-released","seo_title":"","content":" We're excited to share that Elastic APM is now in beta! In 6.0, we released the alpha, which included the open source APM Server and agents. With this release, we're now also providing you with a dedicated Kibana UI to easily visualize and debug performance bottlenecks and errors in your code. We want to help developers spend less time in the develop-test-deploy loop, and be able to ship code changes with confidence. To accomplish that, we've designed an UI that is designed specifically for the developers that wrote the code. The UI is available for free via X-Pack with a Basic license, and you get it out of the box with the 6.1 release. Here's a preview of the UI in action: Zero UI configuration Once you install the Elastic APM agent library in your application, the application will automatically appear in the UI. Installing agents is as easy as installing the Elastic APM agent for your programming language and copy/pasting a few lines of configuration. The agent will automatically instrument your application and send performance data through to the APM Server. The APM Server will process and index the data into Elasticsearch. Visualize application bottlenecks APM monitors transactions and errors in your application. A transaction can be a request to your server or a batch job or a custom transaction type. Out of the box, you'll get response times, requests per minute, status codes per endpoint, plus the ability to dive into a specific request sample and get a complete waterfall view of what your application is spending its time on - like database queries, cache calls, external requests, etc. This lets you easily compare and debug fast responses to slow responses. For each incoming request and each application error you also get access to contextual information, like request header, user information, system values or custom data that you can manually attach to the request. Having access to application-level insights with just a few clicks will drastically decrease spent on debugging 500s, slow response times and crashes. Correlate APM with other data sources using dashboards If you're already using the Elastic Stack for logging and server-level metrics, you can easily import the prepacked APM dashboards that comes with the APM Server. This will enable you to easily grab APM specific visualizations and use those to correlate APM data with other data sources. To get the dashboards, run the following command in the APM Server 6.1 directory Get started today The UI is now available as part of Kibana and activated via X-Pack with a Basic license. It's free - and you can . If you have questions or feedback, please drop us a note in the . ","locales":"","title":"Elastic APM enters Beta with new UI"}
{"index":{}}
{"author":"Andrew Cholakian","category":"The Logstash Lines","publish_date":"2017-12-12T00:00:00.000Z","url":"/blog/logstash-lines-2017-12-07","seo_title":"","content":" Hello Logstashers! We're glad to present you all the past couple week's news in convenient digest form!We've now for resetting logging settings changed via the Logstash web API back to their defaults. We've added some much improvements fixes for Logstash's log4j logging. We will cap log files at 100MB per file, and gzip files as they're rolled over automatically. We're targeting 6.2 with this change. The Logstash HTTP input previously exhibited poor behavior when the queue was blocked. If a client connected and timed out, LS would not release the connection, but rather, block indefinitely, causing the client to potentially time out. The HTTP protocol doesn’t deal well with long running requests. This plugin will now either return a 429 (busy) error when Logstash is backlogged, or it will time out the request.If a 429 error is encountered clients should sleep, backing off exponentially with some random jitter, then retry their request.This plugin will block if the Logstash queue is blocked and there are available HTTP input threads. This will cause most HTTP clients to time out. Sent events will still be processed in this case. This behavior is not optimal and will be changed in a future release. In the future, this plugin will always return a 429 if the queue is busy, and will not time out in the event of a busy queue.While doing this work, we discovered that the HTTP input had some synchronization bottlenecks which were unnecessary. Users might see a nice perf boost on multicore systems.One of the big changes in 6.0 is our work on a new execution backend as well as a new IR for our compiler. We've seen a number of small discrepancies pop up in the 6.x series that we've been ironing out with both the new IR (used by all users) and the new execution engine (will be optional with 6.1). We've done this work in a . ","locales":"","title":"Logstash Lines: Update for December 12, 2017"}
{"index":{}}
{"author":"Clinton Gormley","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2017-12-11T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2017-12-11","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. A new dynamic cluster setting in 7.0 tracks the number of buckets created in an aggregation and fails the request if this number reaches the limit. The default for 7.0 is set to 10000, this means that any request that tries to return more than this number will fail. The number of buckets is checked in the coordinating node and also in each shard when we build the response. This setting should help to reject bad requests that were not caught by the circuit breaker but could push memory use over the limit. This setting will also be present in 6.2 but it will be disabled by default. Requests in this version that hit the default limit (10,000) will log a deprecation in order to prepare users for the migration to 7.0. X-Pack security will now filter the mappings fields returned by get index, get mappings, get field mappings and field capabilities APIs. This means that fields that a user cannot access due to field-level security will no longer be returned from these APIs. Since 5.0, it has been possible to remove index or cluster settings by setting them to . This should have worked for wildcard settings too (eg ) but a bug prevented that from working correctly. Finally, unknown settings from 2.x indices/cluster were moved to the namespace, but it was impossible to delete settings from indices as this was rewritten to . Both of these issues have been . The jvm.options file syntax has changed in 6.2 in order to support a breaking change in command line arguments in Java 9. Each option now needs to be preceded by a . Apache Lucene We are fixing  before building the first release candidate. We should hopefully have a release in the coming weeks. was merged, allowing for great speedups when only the top matches are needed. This optimization requires scorers to expose a maximum score that they may contribute and currently only works well with BM25, but we are working on to work well with this optimization and . We are also looking into how we could , or maybe even the per-norm per-term maximum term frequencies in order to , which would in-turm make the optimization more efficient. It turns out the same API could be used to , when the term term frequency of the most frequent term is not high enough to produce a competitive score. At index time, documents are first buffered in an in-memory . Seeing it as a single buffer is a bit of a simplification though: there is actually a set of index buffers. This helps with concurrency since different threads can write to different index buffers concurrently. When refreshing, each index buffer writes a segment.In order to make multi-tenancy easy, Elasticsearch adds an abstraction layer on top of Lucene called  which tries to make sure that each shard can use as much memory as possible for these buffers at index-time, because it makes indexing faster, while also ensuring that the total amount of memory that is spent on the index buffers across shards does not exceed . The issue is that when this shared limit is reached, Elasticsearch tells Lucene to do a refresh, which writes _all_ per-thread buffers to disk. This new feature is a way to tell Lucene to release some of the memory it spends on indexing, only flushing one of the largest per-thread buffers. This means we will write larger index buffers on average, which should make indexing more efficient. ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2017-12-11"}
{"index":{}}
{"author":"Jim Goodwin","category":"Kurrently in Kibana","publish_date":"2017-12-11T00:00:00.000Z","url":"/blog/keeping-up-with-kibana-2017-12-11","seo_title":"","content":" Welcome to This is a weekly series of posts on new developments in the Kibana project and any related learning resources and events. We’re proud to present 6.0 with accessibility improvements, one-click CSV exports, tighter access control with Dashboard Only Mode, and more. Learn more: — elastic (@elastic) Hi all, This blog is going to be a different style going forward, instead of being mostly a change log, I'm going to try to highlight some important topics in Kibana development each week. So, here we go... A while ago we decided to move towards React as our rendering technology and away from Angular. We've also been developing a new UI design for Kibana which we refer to as K7 (who knows what the 7 means...) and if you follow along in github.com there has been a frenzy of activity around it. This includes the creation of a set of reusable React components that we call EUI ( ) and a complete redesign of our CSS structure.  In order to reach our goals we're going to start using EUI in Kibana and migrating our UIs to it during the 6.x series of releases. We've got a skin that closely mimics the 6.x design using the EUI components. This will allow us to continue feature development and keep back-porting sane. This week we'll be merging EUI into Kibana and upgrading to React 16, and then cleaning up any rendering issues that come up. Our plan is to have things stable quickly.  You can read more at \"EUIfying Kibana\" . Earlier this year at  ( at 1h28m ) a prototype of a new application in Kibana called Canvas. Canvas is a truly innovative presentation tool that integrates live data and analysis from Elasticsearch into info-graphic-like presentations. This week we have a technology preview of Canvas that you can  as well as a Canvas blog so that you can follow along with all the updates.That's all for this week.Cheers,Jim ","locales":"","title":"Keeping up with Kibana: This week in Kibana for December 11, 2017"}
{"index":{}}
{"author":"Aaron Aldrich","category":"Engineering","publish_date":"2017-12-07T00:00:00.000Z","url":"/blog/elastic-advent-calendar-2017-week-one","seo_title":"","content":" This year, the Elastic Engineering team wanted to do something special to celebrate the end of the 2017 Calendar year. Drawing heavy inspiration from the (fully in Japanese) and (in English), we’ve decided to join the tech-advent tradition. From the 1st through 25th of December the team is publishing a series of Elastic Stack tips each day at 00:00 UTC. And —as we are a distributed organization with community members from all corners of the globe— you’ll be seeing these threads in a variety of our native languages. Just this week we have contributions in English, Japanese, French, and Korean! If you want to be actively notified of new topics, subscribe to the ! We’ll also be posting weekly summary blog posts (like this one) if you prefer to follow along here, and we encourage you to join in the conversation either way:  topics are open for feedback and questions and we’d love to hear from you. Here’s what we’ve posted from our first week It’s a great collection of content packed into some mighty small space, and we’d love to hear your feedback on the posts. Happy reading! ","locales":"","title":"Welcome to the Elastic Advent Calendar! Looking back on Week One"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2017-12-07T00:00:00.000Z","url":"/blog/brewing-in-beats-recursive-file-watching-on-macos-with-auditbeat","seo_title":"","content":" Did you know that is already available? Try it and let us know what you think. If you are curious to see the Beats 6.0 in action, we just published the . This update covers the last two weeks Auditbeat: recursive file watching on macOSAuditbeat now supports . This functionality is based on the  library. One drawback of FSEvents is that in the case of multiple events on the same file, they have coalesced in a single notification. The PR orders the set of actions in a single event to be meaningful depending if the file existed in the beat database and if it doesn’t exist anymore at the moment of processing the event. Packetbeat: add_kubernetes_metadataAfter Filebeat and Metricbeat, Packetbeat is the next in line to get Kubernetes support. The `add_kubernetes_metada` processor is now with the pods and enhance the events with Kubernetes metadata. This feature was merged into master and is scheduled to be released in 6.2. Packetbeat: Several TLS support enhancementsPacketbeat now includes a for the TLS data. It can also report the , which is defined as the time spent between first packet and completion of the handshake. Finally, it can now calculate for the client TLS sessions. The JA3 fingerprints are efficient for detecting malware or unauthorized applications. These features are merged into master and are scheduled to be released with 6.2. Filebeat: use the local timezone in the system moduleAn issue that we had in Filebeat modules was that the Ingest Node pipelines assume the incoming logs have timestamps in UTC. In 6.1, Elasticsearch is getting the ability to parse timestamp in the timezone specified by . We now , so the local timezone can be correctly used when decoding the timestamp. This feature will be present in 6.1 but disabled by default. Filebeat and Metricbeat modules for Logstash monitoringThe Filebeat module for Logstash was in time for 6.1. The Metricbeat module got a with basic event stats, also in time for 6.1. Other changes:Repository: elastic/beatsAffecting all BeatsChanges in master: Changes in 6.1: Changes in 6.0: MetricbeatChanges in master: Changes in 5.6: Changes in 6.1: Changes in 6.0: PacketbeatChanges in master: FilebeatChanges in master: HeartbeatChanges in master: ProcessorsChanges in master: TestingChanges in master: Changes in 6.1: Changes in 6.0: InfrastructureChanges in master: Changes in 6.1: PackagingChanges in master: Changes in 6.0: DocumentationChanges in master: Changes in 6.1: Changes in 6.0: Repository: elastic/gosigarChanges in master: ","locales":"","title":"Brewing in Beats: Recursive file watching on macOS with Auditbeat"}
{"index":{}}
{"author":"Rashid Khan","category":"Engineering","publish_date":"2017-12-06T00:00:00.000Z","url":"/blog/canvas-tech-preview","seo_title":"","content":" Hey. You there. Want a sneak peek at a product in progress? We showed off an early prototype of Canvas at Elastic{ON} 17 earlier this year and since then we’ve been hacking on translating the concepts from that prototype into something we can share with you. Let’s be clear: This is still early stuff. But we’re having enough fun with it in the lab that we want to share fun with you. Well, probably you anyway.Is this preview for me? Have you ever taken something apart with no intention of putting it back together, just to see how it works? Can you make a half passable drink in the absence of a half passable liquor cabinet? Would you ride your bike down a road just because you’ve never ridden a bike down that road before? Do you do things just to prove to yourself that you can? If you answered yes, or no, to any or all, of these questions, then Canvas might be for you. These rough cuts of Canvas are for those with a bit of curiosity, some imagination and a whole lot of crazy. One more question: Do you run everything and anything in production? If so, Canvas isn’t for you. Canvas is marching forward rapidly, so we recommend that you try it out in an environment that won’t anger the masses should a hiccup occur. Ok, what is it? Hold on, we’re getting ahead of ourselves. There’s a section on that later. In short, Canvas is a composable, extendable, creative space for live data.  The  goal of Canvas is to be flexible, and allow you to tweak all bits and pieces required to get to the result you desire. Canvas presents you with a blank page to which you can add a selection of elements. These elements can be connected to data and configured with a simple UI. Within the sidebar you can play with palettes, fonts, background, borders and more. When there’s a style you have to have, but there isn’t a button, Canvas gives you the option of using raw CSS. Canvas today can’t do everything (though we’re working on that!) but its capabilities go much deeper than what you see on the surface. Sometimes you need to manipulate the style, or even the data, in a way that no graphical interface can easily express. The Canvas interface is intentionally compact because many of the most interesting and powerful features of Canvas are contained within a small window at the bottom of the screen. Hidden behind a button that says is the nerve center of Canvas.  The interfaces you touch in Canvas, are in fact manipulating the Canvas expression in this text box. The expression describes everything Canvas needs to do to create your element. When you change something in the sidebar, for example a color palette, Canvas updates that expression in the right place.  Not only can Canvas update the expression, but you can too and Canvas will do its best to create an interface to whatever you type. For example if you add or change the palette in the expression, the sidebar will update to reflect the expression in turn. There isn’t an interface to every function and argument, but we’re steadily adding new ones. Canvas expressions are simple, but like any sufficiently powerful concept, there is a learning curve. At its heart it is a pipe based syntax, and the output of one function flows into the next: Here we’re using the function to search for the string “error” and retrieve the \"@timestamp\" and \"bytes\" fields for the most recent 100 document. Then we’re using the function to assign those fields to dimensions, and finally piping into to put the points on a simple chart. This is, of course, a very simple example, all of these functions have arguments we aren't using. Canvas has dozens of functions and many more capabilities including table transforms, type casting and sub-expressions. The best way to learn right now is to open the code screen and play with the sidebar to see how it manipulates the expression. If you really want to dig deep see the . What doesn’t it do? (aka: What’s coming?) See, I told you there was a s","locales":"","title":"Canvas Technology Preview"}
{"index":{}}
{"author":"Jim Goodwin","category":"Releases","publish_date":"2017-12-06T00:00:00.000Z","url":"/blog/kibana-5-6-5-and-6-0-1-released","seo_title":"","content":" Hello, and welcome to the 5.6.5 and 6.0.1 release of Kibana!  These releases of Kibana include an important security fix, we recommend that you upgrade either to 5.6.5 or 6.0.1 to correct the problem.Kibana 5.6.5 and 6.0.1 are available on our and on . Please review the release notes for and for the rest of the enhancements and bug fixes. ","locales":"","title":"Kibana 5.6.5 and 6.0.1 released"}
{"index":{}}
{"author":"Peter Pišljar","category":"Engineering","publish_date":"2017-12-06T00:00:00.000Z","url":"/blog/developing-new-kibana-visualizations","seo_title":"Creating new Kibana visualizations and embedding them into your own plugin","content":" Creating new visualization typesIn 6.0 we made some significant changes to the visualize API and how visualizations are implemented. We also which should provide basic description of the interface. This tutorial should help you get started quickly. Let us discuss some terms firstWe will build a visualization similar to Kibana's built-in Metric visualization, except we will try to keep it simple without any advanced options. When you start with creating new metric visualization in Kibana you are presented with a screen looking something like this: On the left side we have the default editor and on the right side there is the actual visualization. If you look at the side editor you notice it consists of few parts, which are numbered in the image above. Bootstrapping the pluginNew visualizations are just Kibana plugins with the right . We won't go over all the details. Instead, let's start with creating a folder inside Kibana's plugins directory. We will use a super smart name of 'test_vis'. { \"name\": \"test_vis\", \"version\": \"kibana\" } export default function (kibana) { return new kibana.Plugin({ uiExports: { visTypes: [ 'plugins/test_vis/test_vis' ] } }):  } Visualization definitionThe file referenced in above file will define a new visualization type and register it. As it is there are multiple factories depending on the rendering technology you are using. You could even extend it with your own. But in this tutorial we will use the , which is the one we recommend to use. Base visualization type does not make any assumptions about the rendering technology. Let's look at our example . First we import our styles and option template (we will talk about it later) as well as our visualization controller, which will take care of rendering the visualization. import './test_vis.less':  import optionsTemplate from './options_template.html':  import { VisController } from './vis_controller':  import { CATEGORY } from 'ui/vis/vis_category':  import { VisFactoryProvider } from 'ui/vis/vis_factory':  import { VisTypesRegistryProvider } from 'ui/registry/vis_types':  import { VisSchemasProvider } from 'ui/vis/editors/default/schemas':  function TestVisProvider(Private) { const VisFactory = Private(VisFactoryProvider):  const Schemas = Private(VisSchemasProvider):  } The method of the accepts the definition object. Take a look at the documentation to get a better idea of what these properties do. The important parts are that we define the name for our visualization, the controller class which will take care of rendering, the default configuration of the visualization itself and the configuration for the editor. return VisFactory.createBaseVisualization({ name: 'test_vis', title: 'Test Vis', icon: 'fa fa-gear', description: 'test vuis', category: CATEGORY.OTHER, visualization: VisController, visConfig: { defaults: { // add default parameters fontSize: '30' }, }, We will be using the default Kibana editor in this tutorial. This is the side editor you see in many Kibana visualizations. We need to provide the , which should be the angular template for the options tab. We also need to provide definition, which tells which aggregations can be configured. In the below example our schema definition contains a single object of group metrics. The minimum is set to one (so users will have to configure at least one metric), some aggregations are excluded from the list and the default configuration is provided. editorConfig: { optionsTemplate: optionsTemplate, schemas: new Schemas([ { group: 'metrics', name: 'metric', title: 'Metric', min: 1, aggFilter: ['!derivative', '!geo_centroid'], defaults: [ { type: 'count', schema: 'metric' } ] } ]), } }):  } At the end we need to register our provider function with the . // register the provider with the visTypes registry VisTypesRegistryProvider.register(TestVisProvider):  Visualization OptionsIn the visualiz","locales":"","title":"Developing new Kibana visualizations"}
{"index":{}}
{"author":"Akli Rahmoun","category":"User Stories","publish_date":"2017-11-27T00:00:00.000Z","url":"/blog/elasticsearch-at-rte-blackout-prevention-through-weather-prediction","seo_title":"","content":" About RTEAt the core of the power system, RTE (Réseau de transport d’électricité) keeps the balance between power consumption and generation. Twenty-four hours a day and seven days a week, we play a key role in directing the flow of electricity and maximizing power system efficiency for our customers and the community. We convey electricity throughout mainland France, from power generation facilities to industrial consumers who are connected to the transmission grid, and to the distribution grid which provide the link between RTE and end users. We operate France's high and extra-high voltage transmission system, the biggest in Europe. Our Daily ChallengeThe electrical resistance of a power line causes it to produce more heat as the current it carries increases. If this heat is not sufficiently dissipated, the metal conductor in the line may soften to the extent that it sags under its own weight between supporting structures. If the line sags too low, a flash over to nearby objects (such as trees) may occur, causing a transient increase in current. Automatic protective relays detect the excessively high current and quickly disconnect the line, with the load previously carried by the line transferred to other lines. If the other lines do not have enough spare capacity to accommodate the extra current, their overload protection will react as well, causing a cascading failure. Eventually, this can lead to a widespread power outage (blackout), like the one that occurred in Northeastern and Midwestern United States and the Canadian province of Ontario on Thursday, August 14, 2003. This incident had major adverse effects on the proper functioning of the regional economy, administration, public services, and more generally, on people’s daily lives. Power plants went offline to prevent damage in the case of an overload, forcing homes and businesses to limit power usage. Some areas lost water pressure because pumps lacked power, causing potential contamination of the water supply. Railroad service, airports, gas stations, and oil refineries had to interrupt service due to lack of electricity. Cellular communication devices were disrupted and cable television systems were disabled. Large numbers of factories were closed in the affected area, and others outside the area were forced to close or slow work because of supply problems and the need to conserve energy while the grid was being stabilized. Unleashing the Power of Numerical Weather Prediction DataBasically, the problem we are trying to solve consists of dynamically determining the sag margin without violating clearance requirements. A way of solving this problem is Dynamic Line Rating (DLR). The DLR prediction model aims to answer this simple question: What is a transmission line’s maximum instantaneous current carrying capacity after accounting for the effects of weather (temperature, wind, and solar radiation) on thermal damage and line sag? To answer that question, we used data provided by Météo France, the French national meteorological service. This data is formatted into GRIB2 files that can be sourced from Météo France’s open data platform. is a file format for the storage and transport of gridded meteorological data, such as output from the Numerical Weather Prediction model. It is designed to be self-describing, compact, and portable across computer architectures. The GRIB standard was designed and is maintained by the World Meteorological Organization. How did the Elastic Stack Help us Respond to the Challenge?The goal of the POC was to provide the easiest and most powerful access to this weather data to end users. We found out that Elasticsearch’s powerful ingest capabilities, geo indexing, and query features could help us achieve our goal efficiently and at scale both in terms of throughput and storage size. Let’s go deeper in how we built the data processin","locales":"","title":"Elasticsearch at RTE: Blackout Prevention through Weather Prediction"}
{"index":{}}
{"author":"Carlos Pérez-Aradros","category":"Engineering","publish_date":"2017-11-27T00:00:00.000Z","url":"/blog/shipping-kubernetes-logs-to-elasticsearch-with-filebeat","seo_title":"Shipping Kubernetes logs with Filebeat","content":" We recently wrote about the new Filebeat features to , and since the 6.0 release, you can leverage the same technology when running Kubernetes. Metadata is key When shipping logs from containers infrastructure it’s important to include context metadata to ensure we can correlate logs later. This becomes especially important for the Kubernetes case. You may want to watch logs from a full deployment, a namespace, pods with a specific label, or just a single container. Metadata is key to ensure you can filter logs to focus on what’s important to you. Metadata is also useful to correlate events from different sources. When troubleshooting an issue it’s very common to check logs and metrics together, thanks to Kubernetes metadata we can filter both at the same time. Add Kubernetes metadata We use processors across all Beats to modify events before sending them to Elasticsearch, some of them are used to add metadata, as part of the 6.0.0 release we added to the list! enriches logs with metadata from the source container, it adds pod name, container name, and image, Kubernetes labels and, optionally, annotations. It works by watching Kubernetes API for pod events to build a local cache of running containers. When a new log line is read, it gets enriched with metadata from the local cache. Deployment Shipping logs from Kubernetes with Filebeat is pretty straightforward, we provide to do it. Filebeat is deployed as a , this ensures one agent is running on every Kubernetes node. Docker logs folder from the host is mounted in the Filebeat container, Filebeat tails all container logs and enriches them using . To deploy and see it yourself, just follow these simple steps: Logs will start flowing into Elasticsearch, enriched with Kubernetes metadata! You can now use it to filter logs: and try it yourself. ","locales":"","title":"Shipping Kubernetes Logs to Elasticsearch with Filebeat"}
{"index":{}}
{"author":"Suyog Rao","category":"Releases","publish_date":"2017-11-27T00:00:00.000Z","url":"/blog/elastic-cloud-enterprise-1-1-1-released","seo_title":"","content":" We are happy to announce the availability of ECE 1.1.1, a maintenance release. Please see the here or head straight to our to get it! Bug fixesECE 1.1.1 corrects an important upgrade issue that we discovered and last week. Before this release, upgrading from 1.0.2 to 1.1.0 and subsequently restarting allocators or adding more capacity could lead to authentication errors in Kibana. This upgrade issue has now been fixed in 1.1.1. In addition to this bug fix, we added a new configuration parameter which allows you to control timeout values during installation. Existing ECE installations can be live upgraded to version 1.1.1. ","locales":"","title":"Elastic Cloud Enterprise 1.1.1 released"}
{"index":{}}
{"author":"Suyog Rao","category":"Engineering","publish_date":"2017-11-22T00:00:00.000Z","url":"/blog/elastic-cloud-enterprise-1-1-0-upgrade-issues","seo_title":"","content":" Last week we with new features and bug fixes. Since then, we've discovered a critical bug that leads to Kibana being unavailable post-upgrade. This bug only affects users who upgraded from 1.0.x versions of ECE to 1.1.0.  IssueIf you have upgraded to ECE 1.1.0 and any of the following are true: If you have done those things, you will see Kibana authentication issues and unavailability. Monitoring will also be affected. Immediate WorkaroundAs an immediate workaround, we've created a script that can fix this issue on your deployment, which is . You can run this script anywhere that has access to the coordinator node on port . The script requires two pieces of information: Here's what it looks like:[user@localhost ~]$ curl -O https://download.elastic.co/cloud/fix-ece-1.1.0-upgrade.sh [user@localhost ~]$ chmod a+x ./fix-ece-1.1.0-upgrade.sh [user@localhost ~]$ ./fix-ece-1.1.0-upgrade.sh Please enter IP address of the coordinator:<your ip here> Please enter root password to admin console:<your password here>The script will test connectivity, and if successful, make the changes. If not, it will let you know. After that has happened, it will prompt you to remove from every host. This is a manual step, which you can do with this command on every host running ECE:docker rm -f frc-services-forwarders-services-forwarder Once this is removed, ECE will automatically restart the above service with the patched configuration.Official patched version We are working on a 1.1.1 release that addresses this situation. We are targeting a 1.1.1 release early next week which will allow you to upgrade from 1.1.0 or any older versions.If you are planning to upgrade your existing deployment to 1.1.0, we strongly recommend to hold off until we release 1.1.1.We apologize for the inconvenience this may have caused and look forward to releasing 1.1.1 ASAP. Please contact us if you have any questions about this. ","locales":"","title":"Elastic Cloud Enterprise 1.1.0 upgrade issues and workaround"}
{"index":{}}
{"author":"Christian Dahlqvist","category":"Engineering","publish_date":"2017-11-22T00:00:00.000Z","url":"/blog/why-am-i-seeing-bulk-rejections-in-my-elasticsearch-cluster","seo_title":"","content":" Elasticsearch supports a wide range of use-cases across our user base, and more and more of these rely on fast indexing to quickly get large amounts of data into Elasticsearch. Even though Elasticsearch is fast and index performance is continually improved, it is still possible to overwhelm it. At that point you typically see parts of bulk requests getting rejected. In this blog post we will look at the causes and how to avoid it. This is the second installment in a series of blog posts where we look at and discuss your common questions. The first installment discussed and provided guidelines around \"\"What happens when a bulk indexing request is sent to Elasticsearch? Let’s start at the beginning and look at what happens behind the scenes when a bulk indexing request is sent to Elasticsearch. When a bulk request arrives at a node in the cluster, it is, in its entirety, put on the bulk queue and processed by the threads in the thread pool. The node that receives the request is referred to as the coordinating node as it manages the life of the request and assembles the response. This can be a node dedicated to just coordinating requests or one of the data nodes in the cluster. A bulk request can contain documents destined for multiple indices and shards. The first processing step is therefore to split it up based on which shards the documents need to be routed to. Once this is done, each bulk sub-request is forwarded to the data node that holds the corresponding primary shard, and it is there enqueued on that node’s bulk queue. If there is no more space available on the queue, the coordinating node will be notified that the bulk sub-request has been rejected. The thread pool processes requests from the queue and documents are forwarded to replica shards as part of this processing. Once the sub-request has completed, a response is sent to the coordinating node. Once all sub-requests have completed or been rejected, a response is created and returned to the client. It is possible, and even likely, that only a portion of the documents within a bulk request might have been rejected. The reason Elasticsearch is designed with request queues of limited size is to protect the cluster from being overloaded, which increases stability and reliability. If there were no limits in place, clients could very easily bring a whole cluster down through bad or malicious behaviour. The limits that are in place have been set based on our extensive experience supporting Elasticsearch for different types of use-cases. When using the HTTP interface, requests that results in at least a partial rejection will return with response code 429, 'Too many requests'. The principle also applies when the transport protocol is used, although the protocol and interface naturally is different. Applications and clients may report these errors back to the user in different ways, and some may even attempt to handle this automatically by retrying any rejected documents. How can we test this in practice? In order to illustrate the practical impact of this behaviour, we devised a simple test where we use to run bulk indexing requests against a couple of with varying number of data nodes. Configuration and instructions on how to run Rally is available in . The same indexing workload was run against three different Elastic Cloud clusters. We have been indexing with one replica shard configured wherever possible. The clusters consisted of one, two and three data nodes respectively, with each data node having 8GB RAM (4GB heap for Elasticsearch, 4GB native memory). Invoking the API we could see that each data node by default had a fixed bulk thread pool size of two with a queue size of 200: %> curl -XGET http://<es_url>:<es_port>/_nodes/thread_pool</es_port></es_url> \"bulk\": { \"type\": \"fixed\", \"min\": 2, \"max\": 2, \"queue_size\": 200 } During the test we indexed into a varying number of shard","locales":"","title":"Why am I seeing bulk rejections in my Elasticsearch cluster?"}
{"index":{}}
{"author":"Nicolás Bevacqua","category":"Engineering","publish_date":"2017-11-21T00:00:00.000Z","url":"/blog/meet-the-new-ece-ui","seo_title":"","content":" A few months ago, Dave Snider (a design lead at Elastic) showed me the latest of what the design team had been working on since July. This was a minimalistic set of UI components that worked well together and, more importantly, had a coherent design. We’re calling it Elastic UI, or EUI for short. The designs are purely CSS based, while the interactive bits have been added in React, but in such a way that we could easily take just the CSS, or make a version of the components using a different JavaScript framework. As we were going through the designs I wasn’t merely impressed, I wanted Elastic Cloud Enterprise to leverage this component system right away! When the team met in Berlin, I was eager to discuss the possibility of releasing this as part of . A minor bump we faced was that EUI was at the time embedded in a branch of Kibana, and thus not readily available to other teams within Elastic that may have wanted to leverage all of the design work that went into it. This was because, at the time of its inception, Kibana was the only intended consumer of EUI. Thankfully they received us with open arms and we were able to extract EUI into its own repository the day after, so that everyone across the company could consume it. Armed with the new repository, we set out to redesign the entire Elastic Cloud Enterprise UI. The plan was to experiment a little and see how far we could get in a week’s time. The problem was that there was an upcoming release in two weeks, and virtually everyone in the team was taking one week vacations at some point before the release. Alas, we excitedly pushed forward. Experimenting with EUI The way React applications are typically built often encourages a highly componentized structure, meaning we would have our own share of reusable components like a , a to require confirmation for dangerous actions, code highlighting components, and so on. By changing these components’ implementations to use EUI, we were able to reimplement large portions of our codebase to rely on EUI without having to change the interfaces of our own reusable components – a huge time saver. Another benefit in doing such a large scale migration with React is that components are easy to swap. Where you had an , you now have an and an statement. Same thing with buttons. Suddenly the Login screen looks great! The first few bits of the redesign involved bringing in the new page layout, and replacing with EUI components all of the styles we had previously implemented ourselves. Things like spacing and responsiveness ended up being the more challenging aspects. The Platform page, for example, gives you an overview of how your entire fleet is doing. One of the nice aspects of EUI is that it has accessibility built into it, so we can be sure to provide an optimal experience to screen readers while using the slick new designs. We’re excited about and can’t wait to hear your feedback! . ","locales":"","title":"Meet the New Elastic Cloud Enterprise UI!"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2017-11-21T00:00:00.000Z","url":"/blog/brewing-in-beats-autodiscovery-with-docker","seo_title":"","content":" Autodiscovery - use Docker events to auto-configure BeatsThe autodiscovery feature with the first provider (for Docker) has been . Autodiscovery allows the user to define different providers, that watch for system changes and emit events to a common bus. Then the autodiscovery module detects situations when there is something we can monitor and instantiates new modules for it. The first provider watches for Docker events. It supports config mapping from container metadata to config templates, so new modules are created when a container starts. metricbeat.autodiscover: providers: - type: docker templates: - condition: equals.docker.container.image: redis config: - module: redis metricsets: [\"info\", \"keyspace\"] hosts: \"${data.host}:${data.port}\" The above is an example configuration that instantiates the Metricbeat Redis module every time a new Redis container (defined by having the redis image) is started. Note that the connection information (host/ports) is filled in by the autodiscovery support via a template. This feature will be released in Metricbeat and Filebeat 6.1. Configure the number of routing shards in the Elasticsearch templateElasticsearch 6.1 will have an API for . To enable this feature, applications like Beats need to set the config option at index time creation. The actual number of shards must be a factor of the number of routing shards. This adds configuration option in Beats for the routing shards, with a default of 30. We chose 30 as it is a multiple of 1, 3 and 5, our current number of default shards in Beats and ES. The new configuration option will be present in Beats 6.1. Packetbeat: support for reading TLS envelopesThis PR adds , which is one of the most anticipated Packetbeat features. It doesn’t mean decrypting traffic, but it parses the initial handshake and extracts data like ciphers supported by the client and the server, the client and server certificate chains, the subject alternative name (SAN), validity dates, raw certificates, and so on. This data is super valuable for debugging TLS issues and also for intrusion detection and auditing. The implementation also comes with support for the extension to TLS, which allows Packetbeat to detect, for example, whether HTTP/2 or HTTP/1 are used as an application protocol on top of the TLS connection. This feature will be released in Packetbeat 6.1. Filebeat: Docker JSON-file prospectorThis PR adds an (experimental) written by the default JSON logging driver. Filebeat could already read Docker logs via the prospector with JSON decoding enabled, but this new prospector makes things easier for the user. It abstracts the format, so there is no need to manually configure JSON decoding. Here is an example config, which captures the logs from a single container specified by its ID: prospectors: - type: docker containers.ids: - c3ec7a0bd9640151a768663b7e78c115d5b1a7f87fba572666bacd8065893d41 It also parses the timestamp from the JSON file, something that wasn’t possible with Filebeat alone (it required Logstash or Ingest Node). This new prospector will be released with Filebeat 6.1. Other changesRepository: elastic/beatsAffecting all BeatsChanges in master: MetricbeatChanges in master: FilebeatChanges in master: HeartbeatChanges in master: AuditbeatChanges in master: TestingChanges in master: Changes in 6.0: DocumentationChanges in master: Changes in 5.6: Changes in 6.0: Repository: elastic/gosigarChanges in master: ","locales":"","title":"Brewing in Beats: Autodiscovery with Docker"}
{"index":{}}
{"author":"Rich Collier","category":"Engineering","publish_date":"2017-11-20T00:00:00.000Z","url":"/blog/machine-learning-anomaly-scoring-elasticsearch-how-it-works","seo_title":"","content":" We often get questions about Elastic’s Machine Learning “anomaly score” and how the various scores presented in the dashboards relate to the “unusualness” of individual occurrences within the data set. It can be very helpful to understand how the anomaly score is manifested, what it depends on, and how one would use the score as an indicator for proactive alerting. This blog, while perhaps not the full definitive guide, will aim to explain as much practical information as possible about the way that Machine Learning (ML) does the scoring. The first thing to recognize is that there are three separate ways to think about (and ultimately score) “unusualness” - the scoring for an individual anomaly (a “record”), the scoring for an entity such as a user or IP address (an “influencer”), and the scoring for a window of time (a “bucket”). We will also see how these different scores relate to each other in a kind of hierarchy. Record Scoring The first type of scoring, at the lowest level of the hierarchy, is the absolute unusualness of a specific instance of something occurring. For example: Each of the above occurrences has a calculated probability, a value that is calculated very precisely (to a value as small as 1e-308) - based upon the observed past behavior which has constructed a baseline probability model for that item. However, this raw probability value, while certainly useful, can lack some contextual information like: Therefore, to make it easier for the user to understand and prioritize, ML normalizes the probability such that it ranks an item’s anomalousness on a scale from 0-100. This value is presented as the “anomaly score” in the UI. To provide further context, the UI attaches one of four “severity” labels to anomalies according to their score - “critical” for scores between 75 and 100, “major” for scores of 50 to 75, “minor” for 25 to 50, and “warning” for 0 to 25, with each severity denoted by a different color. Here we see two anomaly records displayed in the Single Metric Viewer, with the most anomalous record being a “critical” anomaly with a score of 90. The “Severity threshold” control above the table can be used to filter the table for higher severity anomalies, whilst the “Interval” control can be used to group the records to show the highest scoring record per hour or day. If we were to in ML’s API to ask for information about anomalies in a particular 5 minute time bucket (where was the name of the job): We would see the following output: Here we can see that during this 5-minute interval (the of the job) the record_score is 90.6954 (out of 100) and the raw is 1.75744e-11. What this is saying is that it is very unlikely that the volume of data in this particular 5 minute interval should have an actual rate of 179 documents because “typically” it is much lower, closer to 60. Notice how the values here map to what’s shown to the user in the UI. The value of 1.75744e-11 is a very small number, meaning it is very unlikely to have happened, but the scale of the number is non-intuitive. This is why projecting it onto a scale from 0 to 100 is more useful. The process by which this normalization happens is proprietary, but is roughly based on a quantile analysis in which probability values historically seen for anomalies in this job are ranked against each other. Simply put, the lowest probabilities historically for the job get the highest anomaly scores. A common misconception is that the anomaly score is directly related to the deviation articulated in the “description” column of the UI (here “3x higher”). The anomaly score is purely driven by the probability calculation. The “description” and even the value are simplified bits of contextual information in order to make the anomaly easier to understand. Influencer Scoring Now that we’ve discussed t","locales":"","title":"Machine Learning Anomaly Scoring and Elasticsearch - How it Works"}
{"index":{}}
{"author":"Clinton Gormley","category":"Engineering","publish_date":"2017-11-16T00:00:00.000Z","url":"/blog/removal-of-mapping-types-elasticsearch","seo_title":"Removal of mapping types in Elasticsearch 6.0","content":" ","locales":"","title":"Removal of Mapping Types in Elasticsearch 6.0"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2017-11-14T00:00:00.000Z","url":"/blog/elasticsearch-6-0-0-released","seo_title":"Elasticsearch 6.0.0 GA released","content":" With 2236 pull requests by 333 commiters added since the release of Elasticsearch 5.0.0, we are proud to announce the release of , based on .A big thank you to all the who tested early versions and opened bug reports, and so helped to make this release as good as it is. ","locales":"","title":"Elasticsearch 6.0.0 GA released"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2017-11-16T00:00:00.000Z","url":"/blog/brewing-in-beats-monitor-logstash-with-beats","seo_title":"","content":" Monitor Logstash with Metricbeat With this , Metricbeat gets a new module for monitoring Logstash. The module is in its early stages, more metrics will be added in future PRs. The module is experimental and it will be released in 6.1. Metricbeat: Export more Ceph informationThanks to , the Ceph module exports , that is important for understanding and monitoring the structure of a Ceph cluster. It collects the weight, status, primary_affinity and other useful info of each OSD node. Other changes Repository: elastic/beatsAffecting all BeatsChanges in master: Changes in 6.0: MetricbeatChanges in master: PacketbeatChanges in master: HeartbeatChanges in 6.0: TestingChanges in master: Changes in 6.0: DocumentationChanges in master: Changes in 5.6: Changes in 6.0: ","locales":"","title":"Brewing in Beats: Monitor Logstash with Beats"}
{"index":{}}
{"author":"Monica Sarbu","category":"Releases","publish_date":"2017-11-14T00:00:00.000Z","url":"/blog/beats-6-0-0-released","seo_title":"","content":" Today is a big day for Elastic and the entire community: the Elastic Stack 6.0 is now generally available (GA). In this blog post we’ll highlight the main new features of Beats 6.0. If you’ve been following the Alpha and Beta blog posts, you know already what’s in, so we won’t stay in your way. Go to the Beats product page to . Before upgrading from Beats 5.x, review the docs and the guide. If you are planning to upgrade the whole Elastic stack at once, we also have a for that. Logs and metrics out of Kubernetes and DockerFilebeat and Metricbeat have gained several processors and modules that make container observability with the Elastic stack a breeze. You can use the and processors to enhance application logs and metrics with Docker and Kubernetes metadata. These processors query the Docker and Kubernetes APIs and enhance the events with the container name, image, pod name, labels, and so on. Depending on the Beat, these processors can use different logic to obtain the metadata. For example Filebeat takes the path of the log file, extracts the container ID from it, and uses the ID to retrieve metadata about the container/pod from which the log message originated. See this blog post about . Metricbeat gets a new Kubernetes module, which works by interrogating periodically the Kubernetes API. It gives you details about the running container’s pods, including the CPU usage, memory usage, bytes exchanged over the network, and info about the file system. The sample Kibana dashboard provided with the module shows you at a glance the monitoring status of your Kubernetes cluster. We also provide Kubernetes deployment manifests for Metricbeat and Filebeat. You can find more details about deploying Filebeat and Metricbeat on Kubernetes in the docs, but as a sneak peak, it’s as easy as: curl -L -O https://raw.githubusercontent.com/elastic/beats/6.0/deploy/kubernetes/filebeat-kubernetes.yaml # edit the YAML file to set the Elasticsearch connection information kubectl create -f filebeat-kubernetes.yaml The above commands install Filebeat as a DaemonSet, ensuring one agent is running on each Kubernetes node, and configure Filebeat to pick up the logs from , unwrap them from the JSON objects, and automatically enhance them with Kubernetes metadata. Auditbeat - easy operational securityYou can think of Auditbeat as a friendlier version of that is perfectly integrated with the Elastic Stack. It is based on the Linux audit framework, which means it can hook into every system call and capture them on particular conditions. You can use Auditbeat to very efficiently detect things like short-lived connections and processes, unauthorised attempts to open files, privilege escalations, and so on. Auditbeat automatically correlates events together, resolves UIDs ito names, and outputs JSON objects directly to Elasticsearch or Logstash. Auditbeat also has a file integrity module. It watches files and directories for changes, and when a file changes it computes the MD5, SHA1, and SHA256 hashes and publishes them to Elasticsearch. The hashes can be compared against known malicious files, as shown in this . This functionality is available on Windows, macOS, and Linux. New commands and configuration layoutWe rethought the way the Metricbeat and Filebeat modules are enabled and configured. Instead of one huge configuration file, we now have a directory with individual configuration files for each module. The Beats also get commands to list, enable, or disable modules. For example: $ metricbeat modules list $ metricbeat modules enable redis $ metricbeat modules disable redis And these are not the only useful new commands. There are also commands to export the configuration, export the Elasticsearch mapping template, do a test fetch for a Metricbeat module, and even test the connectivity with Logstash or Elasticsearch. More efficient Metricbeat storageWe took a good look at the data that Metricbeat generates and how it is stored into Elasticsearch","locales":"","title":"Beats 6.0.0 GA released"}
{"index":{}}
{"author":"Jim Goodwin","category":"Releases","publish_date":"2017-11-14T00:00:00.000Z","url":"/blog/kibana-6-0-0-released","seo_title":"","content":" With 1,280 pull requests by 208 contributors added since the release of Kibana 5.0.0, we are proud and happy to announce the release of . We'd like to thank all the who tested early versions and reported bugs helping to make this a great release of Kibana! This release has a lot of new features including:   Did someone say CSV export? We’re pretty sure we heard someone ask for CSV export. Just to be safe, we built CSV export. Search for the documents you want to export in the Discover app, and then export matching documents as a CSV file via the reporting menu. CSV export comes with X-Pack basic, which is our free license. In 6.0 we made changes across Kibana to improve Accessibility, one of those efforts is to make the colors in the UI have appropriate contrast for folks who have different forms of color blindness. We've redone the styling for Kibana to address these issues. Here are some sample screens:     We've also improved screen reading and keyboard navigation throughout Kibana: [Continue reading: ] We've introduced a new UI for creating and editing alerts based on thresholds. It includes a builder experience with type-ahead suggestions and graphical feedback based on previewing the alert constraints. It supports sending alert messages with template values to the log, email, or slack. See the demonstration animation below for a quick look at the new functionality: [] You can now enter full screen mode when viewing a dashboard. This hides the browser chrome and the top nav bar. If you have any filters applied, you'll see the filter bar, otherwise that will be hidden as well. To exit full screen mode, hover over and click the Kibana button on the lower left side of the page, or simply press the ESC key. This mode complements the Dashboard Only Mode introduced in alpha2, and together they make a great solution for monitors in NOCs, SOCs and other Kiosks around the office!   Ever wish you could share your Kibana dashboards without the risk of someone accidentally deleting or modifying them? Do you want to show off your dashboards without the distraction of unrelated applications and links? In version 6.0 we’re making it easier than ever to set up a restricted access user, with limited visibility into Kibana. It’s already possible to create read only users, but new in 6.0 is a UI to match, and we’ve made it simple to set up. All you have to do is assign the new, reserved, built-in kibana_dashboard_only_user role, along with the appropriate data access roles, to your user and they will be in dashboard only mode when they log in to Kibana. [Continue reading:  ] Cluster Alerts in Monitoring was added in the 5.4 release, but until now the alerts only appeared on the Overview page of the Monitoring app. This new feature allows you to receive email notifications when the alerts are triggered. To use it, go to the Advanced Settings page in Kibana Management, enter an email address for `xpack:defaultAdminEmail`, and click Save: The built-in alerts will send an email to that address when they initially trigger, and when they're resolved.    Using this feature does require that your Elasticsearch nodes are configured for the ability to send emails from watches. If you haven't set that up yet, take a look at the X-Pack documentation for \"Configuring Email Accounts\":   When we released the first phase of Cluster Alerts, we promised there will be more alert types to come, and we're delivering on that promise with the new X-Pack License Expiration alert. This alert will tell you when your X-Pack license is close to expiration. It starts as a low-priority alert when expiration is 30 days away, becomes a medium-priority alert when expiration is 15 days away, then becomes a high-priority alert when the expiration is 7 days away.   In  we introduce an Experimental Kibana Query Language it is disabled by default and can be enabled through the Kibana configuration.  Kibana currently provides four different search mec","locales":"","title":"Kibana 6.0.0 is released"}
{"index":{}}
{"author":"Tyler Hannan","category":"Releases","publish_date":"2017-11-14T00:00:00.000Z","url":"/blog/elastic-stack-6-0-0-released","seo_title":"","content":" 6.0.0 is here. Not much more needs saying. You should download it now or use it on (your favourite hosted Elasticsearch and Kibana provider.) If you haven’t followed the release cadence over the past few months, you may be surprised by today’s announcements. Today represents the culmination of thousands of pull requests and the effort of hundreds of committers. This has culminated in two alpha releases, two beta releases, two release candidates, and – finally – general availability (GA). This milestone would have been impossible to achieve without the effort of a variety of teams within Elastic. And, importantly, the perspective and feedback of our users who chose to participate in the . Not only are we releasing the entirety of the Elastic Stack, today also marks the release of that includes 6.0 support, offline installation, and a variety of UX changes simplifying the provisioning, management, and monitoring of clusters. And, because just GA’ing multiple products on the same day isn’t enough…APM is still in Alpha and we invite your participation in testing it on 6.0.0. When there are so many features to highlight in a release, where do you even start? Either you craft the next great novel or you choose to provide links to details. Happy reading and…more importantly…happy searching, analyzing, and visualizing. We are also hosting a , with Shay and Elastic engineering leads. Join us for 6.0 demos, AMAs, and more during our live 6.0 launch celebrations. Elasticsearch An entirely new zero-downtime upgrade experience, the addition of fast, op-based recovery thanks to sequence IDs, improved handling of sparse data, faster query times, distributed Watch execution, and the list keeps going. The summary, or aggregation, of features is in the . Kibana A Dashboard Only mode, a Full Screen mode, the ability to export saved searches to .csv, Alert creation via a UI in X-Pack Gold and above, a migration assistant in X-Pack Basic, and all of it is made more accessible via contrast changes, keyboard accessibility enhancement and more. Visualize the future of interacting with your data in the . Logstash Multiple, self-contained pipelines in the same Logstash instance and the addition of UI components - Pipeline Viewer in X-Pack Basic and Logstash pipeline management in X-Pack Gold. Grok the details in the . Beats Beats <3 containers and, also, Beats <3 modules (and improving the dashboard for those modules). Combine this with a new commands and configuration layout generally and more efficient storage in Metricbeat. Also, say ‘Heya’ to Auditbeat. If you want all the details, ‘Go’ read the . ES-Hadoop First class support for Spark’s Structured Streaming has landed in 6.0, alongside a re-write of connector mapping code to better support multiple mappings. Support for reading and writing the new join fields has been added as well. Users can also now take advantage of script types other than inline for update operations. This is just a reduced view though, so map your eyes to the details in the . Get It Now! ","locales":"de-de,fr-fr,ja-jp,ko-kr,zh-chs","title":"Elastic Stack 6.0.0 GA is Released"}
{"index":{}}
{"author":"James Baiera","category":"Releases","publish_date":"2017-11-14T00:00:00.000Z","url":"/blog/es-hadoop-6-0-0-released","seo_title":"","content":" Major releases don’t come every day, which is why I am astonishingly excited to announce the release of Elasticsearch for Apache Hadoop (aka ES-Hadoop) built and tested against the latest and greatest Elasticsearch 6.0.0. This release has been a culmination of a monumental effort across Elastic as well as our awesome community. A special thank you to all who checked out the preview releases and provided invaluable feedback on them. And now, on to the shiny new stuff! What’s new? Spark 2.2.0 and Stable Support for Spark Structured Streaming Spark 2.2.0 landed on July 11th and we spared no time in making sure we work seamlessly with it. What’s with all the excitement? Why, is no longer an “Experimental” feature in this release! This means that we’re treating our Structured Streaming integration in ES-Hadoop as an evolving integration as of this beta release. Please note that due to its experimental nature in prior versions, we will only be supporting our Structured Streaming integration on Spark versions 2.2.0 and above. Don’t fret though - this doesn’t impact our existing Spark integrations at all. RIP Elasticsearch on YARN The Elasticsearch on Apache YARN (ES-on-YARN) beta integration has been removed in this release. ES-on-YARN was an experiment for deploying Elasticsearch on top of Hadoop’s YARN cluster resource negotiator. The project was never recommended for production use and has been in perpetual beta status since its inception. The core limitations for the project have been YARN’s lack of formal support for long-running services, which is a requirement for Elasticsearch to achieve production level stability. The ecosystem around long-running services in YARN has improved since the start of the beta, but much of the improvement is based in systems that sit on top of YARN like Apache Slider. These systems are still fairly young and would require quite a bit of work to migrate toward. With all this in mind, we have decided to cease development of the ES-on-YARN project. We’re always eager to hear your feedback, so if you have any about ES-on-YARN make it known on the . Have no fear though. When one door closes, another one opens: for users looking to easily orchestrate and manage a fleet of Elasticsearch clusters, either on-prem or in the cloud, is the recommended solution. Support for new Join Fields The days are numbered for Multi-typed indices in Elasticsearch. Users who work with Parent-Child based data need not worry about the future due to the advent of the new “join” field type in Elasticsearch. We’ll be rolling out support for reading and writing data with this new field type in this release. We’re excited to hear your feedback on this new feature! Multiple Mappings and Multiple Index Reads We took a long hard look at how we handle Elasticsearch mappings in the connector. After that long hard look we re-wrote a healthy chunk of code to fix an unhealthy bunch of problems. In this release you will no longer be bitten by common errors when reading from multiple indices (each with varying field types). ES-Hadoop will also alert you when the indices you’re reading from have conflicting mappings in them. Check Out Our Bug Collection Nested Java Bean serialization problems, field exclusion problems on Pig and SparkSQL, partial document reads and serialization exceptions, parsing errors from index auto-creation, backwards compatibility errors with scroll id’s, missing support for timestamps in params and much more all fixed in this release. Take a look at in this release! ","locales":"","title":"Elasticsearch for Apache Hadoop 6.0.0 GA is Released"}
{"index":{}}
{"author":"Suyog Rao","category":"Releases","publish_date":"2017-11-14T00:00:00.000Z","url":"/blog/elastic-cloud-enterprise-1-1-0-released","seo_title":"","content":" We are pleased to announce that ECE 1.1 is available to ! ECE 1.1 is packed with important enhancements and bug fixes, so please read on. You can find the release notes for ECE 1.1 . ","locales":"","title":"Elastic Cloud Enterprise 1.1.0 released"}
{"index":{}}
{"author":"Maxime Greau","category":"Engineering","publish_date":"2017-11-13T00:00:00.000Z","url":"/blog/sha512-checksums-for-elastic-stack-artifacts","seo_title":"","content":" Each time we do a release of the Elastic Stack, are generated. Until now, we provided SHA-1 checksum files alongside those artifacts to verify file integrity. The , announced this year, has increased the awareness to move to a safer alternative for all applications that rely on SHA-1 for digital signatures, file integrity, or file identification. That’s what we have done by using , . Elastic Stack 5.6.2+: SHA-1 and SHA-512 checksums While we encourage you to move to the SHA-512 checksum files quickly, we still generate the SHA-1 checksums for 5.6.x releases for backward compatibility. Elastic Stack 5.6.2 was the first version released with both SHA-1 and SHA-512 checksum files available. The same format is used to produce the contents of the and files. Once you have downloaded one Elastic Stack artifact, e.g. Kibana 5.6.3, and its related checksum files from , you can check the file integrity: Then you have to write a script to check that the checksum downloaded from is the same that the one generated locally, based on the binary artifact downloaded. Elastic Stack 6.0.0 checksums: new SHA-512 format, no more SHA-1 With the upcoming Elastic Stack 6.0.0 release, we have decided to: You can already validate the integrity of Elastic Stack files with the new SHA-512 file format with the last week: The new format, used by files, contains the hash value and the artifact’s filename associated on the same line (separated by two spaces): Now, with this format, it’s easier to check the integrity of a downloaded Elastic Stack artifact, by simply using the option: If the SHA-512 checksums matches, is printed along the filename, while a message will be printed to the standard output in case of error: Elasticsearch plugins The Elasticsearch team has updated that checks the integrity of the plugins files before installing it. All Elasticsearch official plugins have been updated with The expected behavior is based on the above explanations in this article, so: Conclusion To summarize, we encourage that you always check the integrity of the files you download. If you already have a script doing that with the Elastic Stack artifacts, please for all Elastic releases. ","locales":"","title":"SHA-512 checksums for Elastic Stack artifacts"}
{"index":{}}
{"author":"Renuka Hermon","category":"Culture","publish_date":"2017-11-10T00:00:00.000Z","url":"/blog/monitoring-the-dark-army-with-kibana-mr-robot","seo_title":"Monitoring the Dark Army with Kibana on Mr. Robot","content":" Elliot uses Kibana to visualize the Dark Army’s efforts to steal data in \"eps3.4_runtime-error.r00.\" Elastic users across the globe do a double take. We try to calmly write a blog post. Mr. Robot is a company-wide favorite for reasons that are likely obvious. The show is well-known for depicting cybersecurity scenarios with realistic detection strategy, tools, and response. So when the Mr. Robot team reached out to us, we were thrilled to hear that they wanted to feature Kibana in an upcoming episode. We, of course, said “yes!” We couldn’t wait to see our software in action in Elliot's world. By the way, you’ll notice that it’s not the latest version of Kibana, but to fit with the show's timeline.   In true form, the technical minds at Mr. Robot wanted to build an authentic Kibana dashboard populated with data, created using the real tools. Can you see why we are fans? , Technical Consultant for Mr. Robot, indulges us all in the cybersecurity background behind each episode. In his latest post, he dives into how he made a Kibana dashboard that depicts malicious activity performed by the Dark Army. To start, Ryan built an ELK (today referred to as the Elastic Stack) VM comprised of Elasticsearch, Logstash, and Kibana. Then he populated it with data from Windows and Linux systems. For more behind-the-scenes commentary about how he built each section of the dashboard, head over to . We love hearing about all the ways users put the Elastic Stack to work. Seeing Elliot use Kibana to visualize the Dark Army’s attacks, well, it was pretty surreal. Here’s a taste of reactions from fans on our team: “Mr. Robot's writers and consultants have created a refreshing, technically honest show. They don't skimp on the detail, but it's all approachable. It rings familiar. I am humbled they chose to use the Elastic Stack.” - Nick Waringa, Senior Security Analyst “Our whole family set a Mr. Robot weekend-binge-watch record in preparation for the Kibana episode.” - Beth McAnerney, Netsuite Administrator “The last time I was this excited to be a nerd was when I read Snow Crash!” - Mark Walkom, Solutions Architect / Developer Advocate “Finally I work somewhere cool enough to make it on Mr. Robot!” - Grant Murphy, Cloud Engineer ","locales":"","title":"Monitoring the Dark Army with Kibana on Mr. Robot"}
{"index":{}}
{"author":"Josh Bressers","category":"Engineering","publish_date":"2017-11-08T00:00:00.000Z","url":"/blog/sense-chrome-plugin-malware-issue","seo_title":"","content":" Elastic has recently been made aware that the Chrome webstore has marked the Sense browser plugin as malware. The plugin in question is not published by, or affiliated with, Elastic. A long time ago Elastic wrote a plugin called Sense. Sense was the first version of what we now call the . The idea is that when curl is just too complicated you can interact with JSON using this tool. It gave developers the ability to easily write, debug, and modify the JSON being sent to Elasticsearch. The project was rather useful and used by many. As many successful open source projects go, Sense evolved and became part of something bigger than itself. Sense was added to Kibana version 4 as a plugin. In Kibana version 5 we renamed Sense to Console and include it with every copy of Kibana installed. It proved to be such a useful tool we wanted everyone to have easy access to it. When we decided to stop supporting the initial version of Sense the project was forked. In fact the project is still on . Anyone is welcome to fork the code and work on a project of their own. This is how open source works, the ability to fork a project or maintain your own version is incredible. Sometimes though things don’t always work out the way we’d like them to. We recently that the Google Chrome webstore has flagged a forked version of the Sense plugin as malware. We have a copy of this plugin, we looked at the contents and scanned it using VirusTotal, nothing obviously wrong stands out. That however doesn’t always mean it’s “safe”. It just means VirusTotal didn’t find anything wrong with it. Google has a pretty good track record about things like this, it’s likely there was something wrong with that plugin, it’s probably not a virus. Sometimes they flag things that are using extremely old and insecure libraries or even plugins that are doing something suspicious. Regardless, if you were using this plugin, you’d be wise to scan your system for possible problems. Chrome will automatically remove plugins from a running system that it believes contain malware. Even if you have a copy of this plugin and try to install it, Chrome will remove it eventually. If you were using the Sense Chrome plugin we encourage you to use the Console feature in Kibana. It has similar functionality and is part of a well maintained and actively developed project. There is a lesson here for everyone about software pedigree. Before installing things you find on the Internet, even through the Chrome store, you should note where it came from. Elastic was not the publisher of this particular plugin. There is a lot of dodgy software out there, some of it’s bad on purpose, most is accidentally bad. Elastic takes issues like this very seriously, we have teams of people who help us watch for problems like this and prevent them from happening in our products and services. There is a saying “software ages like milk, not like wine”. Old software can also be risky software. ","locales":"","title":"Sense Chrome plugin malware issue"}
{"index":{}}
{"author":"Andrew Cholakian","category":"Releases","publish_date":"2017-11-14T00:00:00.000Z","url":"/blog/logstash-6-0-0-released","seo_title":"Logstash 6.0.0 Released","content":" We’re glad to announce that Logstash 6.0.0 has launched! Today marks the first day of 6.0’s inter-planetary mission of making life easier for systems administrators and engineers. Can’t wait another minute to try it? Head over to our  and give it a shot! However, you may want to take a minute to read about  and  first. Read on for what's new in Logstash 6.0!Streamline Processing with Multiple PipelinesOne pipeline no longers rules them all. Logstash 6.0 introduces the ability to run multiple pipelines concurrently for different use cases. The pipelines run together in the same instance, but with independent inputs, filters, and outputs to enable users to isolate processing logic per data source. This keeps your pipeline logic focused and concise. Historically, many Logstash users combine multiple use cases into a single pipeline which requires adding complex conditional logic. With multiple pipelines that’s no longer necessary:  now, you can organize your config more cleanly, and execute your pipeline more efficiently by using dedicated pipelines per use case.To add to the fun, each pipeline also has its own independent settings and lifecycles which you can tune to match the respective workload profile desired for that data source. For instance, you may want to allocate more pipeline worker threads for a high volume logging pipeline, and throttle back resources for a lower intensity local metrics pipeline. For more details on multiple pipelines, take a peek at the  and !Manage Pipelines Centrally with the Elastic StackIn the past, managing pipeline configurations was either a manual task, or you would use config management tools like Puppet or Chef to assist with operational automation. With Logstash 6.0, the centralized pipeline management feature now enables you to manage and automatically orchestrate your Logstash deployments directly with the Elastic Stack through the Kibana single pane of glass. This feature brings a Pipeline Management UI to Kibana, which you can use to create, edit, and delete pipelines. Underneath this UI we use Elasticsearch to store your pipeline configurations. With a few simple settings, your Logstash nodes can be configured to watch for changes on these pipelines, letting you seamlessly push out pipeline changes without additional operational infrastructure beyond the Elastic Stack.Centralized pipeline management is available as an X-Pack feature. To learn more, take a look at the  and the !Visualize Pipeline Logic and PerformanceWe’re also proud to announce the long awaited in this 6.0 release, which is a welcome addition to the Logstash Monitoring UI. With this new tool you can both visualize your pipeline configuration and troubleshoot performance bottlenecks at the plugin level. The pipeline viewer displays your Logstash pipelines as a DAG (Directed Acyclic Graph). Overlaid on this DAG are relevant performance metrics for individual inputs, filters, and outputs. Potential performance problems are highlighted to let you quickly determine which portions of your pipeline may be bottlenecks.Users should note that this is a beta feature and may be subject to change. One known issue is the lack of ability to cleanly render very large pipelines. This is an area of active work, so expect significant improvements to this feature as we improve it further and move it into general availability.A Smooth Path from Ingest Node to LogstashSome users enjoy the convenience of Elasticsearch Ingest Nodes when getting started with the Elastic Stack. However, at a certain level of complexity, Ingest Nodes may not have all the features required to solve your problem and a migration to Logstash may be in order. To ease these transitions we’ve created an  that ships with Logstash 6.0. The converter takes an Ingest Node pipeline as an input and spits out the respective Logstash pipeline. See the for more insight!Now With JRuby 9kWe’ve spent a lot of time movin","locales":"","title":"Logstash 6.0.0 GA Released"}
{"index":{}}
{"author":"Anna Ossowski","category":"News","publish_date":"2017-11-10T00:00:00.000Z","url":"/blog/apply-for-an-elasticon-opportunity-grant-today","seo_title":"","content":" For the past few years, our developer relations team has been running an informal scholarship program of sorts to help folks from underrepresented groups in technology attend Elastic{ON}. Last year, this program brought ten individual scholarship attendees from five different organizations to the conference. Hearing their perspectives on Elastic{ON} and learning about the insights they took home when the conference wrapped inspired us to step up our scholarship game, and the was born. For more information, please . ","locales":"","title":"Apply for an Elastic{ON} Opportunity Grant Today!"}
{"index":{}}
{"author":"Shay Banon","category":"News","publish_date":"2017-11-09T00:00:00.000Z","url":"/blog/swiftype-joins-forces-with-elastic","seo_title":"","content":" I am thrilled to announce that Swiftype is joining forces with us. Swiftype is the creator of a highly-regarded, popular SaaS-based Site Search product and a newly launched Enterprise Search product. Meet Matt and Quin in a coming up on November 29. Some of you may be wondering, why is a company focused on building SaaS search applications joining forces with a search technology company? As I’ve said, I’ve always viewed ‘search’ as a wonderful foundation to solve many different use cases, whether it is search embedded in an application:  search used for logging, security, or metrics:  or search being used to create a whole set of new applications and products. Well, Swiftype did this. They built an entire company focused on making it simple for users to put a search box on their websites or within their applications and an enterprise search product for organizations to manage disparate content from various web applications. Swiftype’s first product is . If you go to websites and help centers for companies like Asana, Shopify, SurveyMonkey, and TechCrunch, that’s the Swiftype Site Search product powering the search box experience. And under the hood of Site Search, is Elasticsearch. In fact, Swiftype has been using Elasticsearch for a long time, since Elasticsearch version 0.90 for indexing and storing searchable content. Like so many other successful SaaS companies do, Swiftype created an amazing user interface and a lot of infrastructure around Elasticsearch to provide an incredible SaaS-first Site Search experience. I’m excited to announce that Site Search will be offered with a new introductory subscription plan starting at $79/month (). This will allow customers to grow at their own pace. In addition, Swiftype's Site Search also provides an ideal migration path for customers. Earlier this year, Swiftype released its second product, Swiftype . This product has web crawlers and out-of-the-box connectors to cloud applications like Atlassian, Box, Dropbox, Github, Google Apps, Microsoft Outlook, Salesforce, Slack, Zendesk, and an API to build custom connectors. With the EOL of traditional enterprise search products like , Swiftype Enterprise Search meets the needs of today’s modern organization using many cloud-based shared and private content repositories. Effective immediately, Swiftype’s Enterprise Search product will be available as a beta via trial request. As we move towards making it GA, our combined engineering teams will integrate more capabilities of the Elastic Stack and X-Pack into this product, as well as make this product available as both a SaaS service and on-premise solution. I’d like to welcome the entire Swiftype team, customers and community to the Elastic family. It’s really exciting that Swiftype’s founders -- Matt Riley and Quin Hoxie -- and the Swiftype team are joining us to further extend our vision to offer tailored solutions on top of the Elastic Stack. Now some words from Swiftype’s founders. Swiftype set out to build a cloud-based search platform that dramatically simplified the process of creating powerful, high-quality search experiences. With the ever-growing amount of content published on the web, and with consumers expecting intuitive search tools, the need for world-class search capabilities is greater than ever before for businesses of all sizes. Our ongoing goal has been to stay ahead of this need by delivering incredible search for any team, technically savvy or not. When we began designing our own infrastructure, we made an early bet on Elasticsearch as a foundational technology in our system — and it turned out to be a good one. Elasticsearch not only powers our primary search functionality, but has also grown to support a wide variety of other product and operational use cases. Suffice to say, we have been power users of Elasticsearch for almost as long as is possible, and we’re thrilled to now be part of this incredible team spearheadin","locales":"","title":"Swiftype Joins Forces with Elastic"}
{"index":{}}
{"author":"Alexander Reelsen","category":"Engineering","publish_date":"2017-11-08T00:00:00.000Z","url":"/blog/elasticsearch-6.0-counter-measures-against-filling-up-disks","seo_title":"","content":" More checks, less problemsThis post will inform you about upcoming changes in Elasticsearch 6.0 with regards to the disk allocation decider, as there is one big change coming up you should be aware of. In addition we will quickly talk about some improvements in our logging infra as well, as this affects disk space usage.The new disk threshold decider behaviourIn a previous post we already about disk space savings, just by upgrading. That's an awesome thing, but of course only one part of the equation. You can still run out of disk space. There are dozens of reasons this can happen. For instance your monitoring is broken, you are receiving an insane data spike (maybe due to a DDoS attack), huge merges are going on, or one of your nodes goes offline and relocation happens.Elasticsearch has a list of allocation deciders, which check if a shard should be allocated on a node. For example these deciders make sure that no primary and replica shard are on the same node. Allocation deciders also take the shard allocation filtering rules into account or the total shard limit per node. Each of those deciders basically returns a decision telling the caller, if it is ok to put a shard on this node or not.One of those allocation deciders is called the which checks if there is enough space in order to allocate a shard. This decider allows you to configure a low and a high watermark. The low watermark is used to decide if a shard should be allocated on this node based on the remaining disk space. The high watermark is used to move away shards, once a certain amount of the disk is used. This allows the remaining shards to have some more breathing room.So far, so good. One precondition of this decider is that it is able to properly read the available disk space. Most of the time this is a not an issue, but you still might want to check. The easiest way to find out is to use the and check the information. You could do this viaGET _nodes/stats/fs?human # another way of checking GET /_cluster/allocation/explain?include_disk_info=true Ok, so this is good, right? We get close to running out of disk space, we move the shard away, everything is awesome. Yeah, no. Only sometimes.What if there is no space on other nodes to move the shard or there is only one node? Then we cannot move it, and at some point we will run out of disk space, risking data corruption.Wouldn't it be great to just stop indexing in case we risk running out of disk space? Yes, it would. That's why we did it from Elasticsearch 6.0 onwards.A new watermark has been added to the . If that watermark is passed (by default it is set to ), indices are marked as read-only. Which indices will be affected? Every index, which is writeable and contains a shard on this affected node. In addition, the indices are not fully read-only, deletes are still allowed, as they just need to update a small tombstone file.One important tidbit needs be taken care of by the cluster administrator. Once the indices are switched into this read-only mode, you have to manually mark them as writeable again. There is no automatic mechanism to switch back to writeable, once enough space has been reclaimed.In order to re-enable an index for writing again and to remove that setting, executePUT my_index/_settings { \"index.blocks.read_only_allow_delete\" : null } Wait, there's more!So, this is a great protection against running out of disk space. But this is not the only way to run out of disk space. Elasticsearch produces logs which get written into dedicated log files. If you have a rogue query that hits you several hundred times per second, this query might generate lots of log entries. This is one of the reasons you want to have your data and your logs on different partitions or even on different disks, so that logging I/O does not affect your query/index I/O.With Elasticsearch 6.0 we will do a couple of things (some are new, some hav","locales":"","title":"Elasticsearch Preview: Countermeasures against filling up disks"}
{"index":{}}
{"author":"David Pilato","category":"Culture","publish_date":"2017-11-08T00:00:00.000Z","url":"/blog/free-lunch-for-open-source-engineers","seo_title":"","content":" When I started at elastic , we were a few people in the company and I was feeling pretty much alone in France. I’d been hired to write some code, help people on the forum (now ) and also continue evangelism efforts in France. That has always been a good balance for me. I mean that staying in front of my computer all day long is not the ideal thing. I need to talk with real people in real life and not only over Zoom or Google Hangout. I spoke one day with about that need and he told me that he would like to start doing BBL sessions. I asked: “?” Brown Bag Lunches aka BBL The brown bag lunch is the typical American bag you take out from a restaurant when you want to eat at your office. The idea was quite simple: Some people also call them “Lunch and Learn” sessions. I found that idea brilliant as it would exactly fill my needs in term of: So I started a day after with a Tweet: — David Pilato🇪🇺🇫🇷 (@dadoonet) And answered me that Deal! The 1st BBL at SocGen So I gave . We were something like 15-20 attendees. It was a similar session of the one I gave at . Feedback has been very positive and I found out 4 key points: What about a website? Some other people started as well to run BBL in France. At some point , , and myself found that we should have a website to reference all the speakers/sessions so companies would be able to contact us. started that way. It’s super easy to add a talk… Well, it’s super easy as soon as you are a developer because you need to on GitHub a JSON document. :) The website helped a lot with getting more visibility. I think I’m getting 15 to 20% of my contacts through it. We started to a bit as the number of speakers/talks have been growing dramatically. Sadly, I did not find time to finish the job yet but thanks to and we have a running on CleverCloud and anytime a PR is merged the NodeJS hook fetch the data, transform it and upload that inside . The goal is to add a cool search engine on top of brownbaglunch.fr capable to deal with typos, find speakers around you using geolocation features, filter by label, using faceted navigation, giving autocompletion… 5 years later… I spoke with some of my colleagues about this and they started to organize BBL sessions in their respective countries. So I started to internationalize the website as well. We might want people to simply fork the original concept or host a global one for the entire world. We’ll see where it goes. Speaking at BBLs represents half of my evangelist activities. In 2017, I did it 17 times in Paris, Amiens, Lille… I went to companies like , , , , … Most of the time, I have around 15-20 attendees. Sometimes, companies don’t have enough seats for everyone, so I’m doing 2 separate sessions. Sometimes, we have people attending the session remotely over Zoom or Google Hangout. Sometimes we do instead of a BBL… Sometimes I’m getting super emotional as it can touch my heart really deeply. I had the opportunity to speak at in 2014. For those who don’t know Meetic, it’s a dating web/mobile application available in many countries. It’s Tinder-like, but Tinder was created years after Meetic. I was super excited to speak there for a very personal reason. In 2004, after a divorce, I met my wife on Meetic and in 2007 we got a baby, Max. I’d say that thanks to this website (in 2004, there was no mobile application!), my life changed totally. You can find a picture of Max searching for logs here! One of the BBL attendees was working at Meetic in 2004 so I was super thankful and it was a great pleasure to share my knowledge with the team. I heard later that , which is even better! Seeds, Harvest Speaking at BBLs is a great opportunity to share your knowledge, your experience with a community and to make growing happen even faster. At BBLs, I find a lot of people who are building a POC or running a project in production already. And they are happy to share their story with the com","locales":"","title":"Free Lunch for Open Source Engineers"}
{"index":{}}
{"author":"Christoph Wurm","category":"Engineering","publish_date":"2017-11-07T00:00:00.000Z","url":"/blog/deploying-elasticsearch-on-microsoft-azure","seo_title":"Deploying Elasticsearch on Microsoft Azure","content":" I recently had the chance to present at Azure OpenDev, giving an overview of what running and using Elasticsearch and the Elastic Stack looks like today. What I'd like to do today is spend a few minutes going into more detail on some of the topics. But first, here's the recording:  See the  for the other talks by Microsoft, GitHub, CloudBees / Jenkins, Chef, and HashiCorp. Azure MarketplaceAs demonstrated in the talk, the easiest way to get started with Elastic on Azure is to use the . You can deploy it directly from the Azure Portal and it's going to handle all of the steps required to get Elasticsearch and Kibana up and running: Provisioning instances and storage, deploying and configuring the software, setting up networking and finally bringing everything up. We have a  about how to use it. Now, there is another way of using it besides running it from the Marketplace. Since it is open source (the sources are ) you can also choose to deploy it from the command line. This allows you to automate things, and to make any customizations to its workings that you might want to do. We have about that. If you're using the template, please drop us some feedback at . HardwareSome of the most often asked questions when deploying on Azure are: Which instances should I use?  We found the to be a good fit. Like every other data store Elasticsearch is very dependent on the amount of memory available to itself (as the JVM heap) and to the underlying host system (used for the important filesystem cache). You can read a bit more about how memory should be assigned in . We also recommend using . Backed by Solid State Drives (SSD) it allows Elasticsearch to reach its stored data quickly - and users will benefit from improved response times. also come with encryption at rest (via ). For bigger clusters, we recommend having three dedicated master nodes. They will not be storing any data, but will handle cluster management tasks like creating new indices and rebalancing shards. Small D series instances are most often good enough. Same as master nodes, Kibana has relatively light resource requirements. Most computations are pushed down to Elasticsearch, so you can usually run Kibana on small D series instances as well. Since it typically does a lot of processing it is best deployed on . AvailabilityOftentimes you will want to deploy a highly available Elasticsearch cluster that stays online even in the face of instance or zone failure. Azure has several concepts that help you design redundancy into your deployment and is a good read. Azure has geographical regions around the world. Each region then contains multiple data centers very close together. You will most likely want to choose whichever region is closest to you, or closest to the users of the system. All nodes of an Elasticsearch cluster should be deployed in the same region. Each tier of the Elastic Stack should be in . Two instances of Kibana should be in one, two instances of Logstash in another, and the Elasticsearch nodes in a third set. Azure distributes instances across . During a planned maintenance event, only one update domain is going to be rebooted at a time, while only the machines in the same fault domain are sharing a power source and a network switch. Distributing your instances across domains ensures the availability of instances in expected and unexpected circumstances. Azure is previewing the concept and is supporting . Going forward, this is likely going to be the best way to deploy Elasticsearch. Each zone should have one master-eligible node (or a dedicated master node) and data nodes should be distributed across zones and tagged appropriately using . When using dedicated Elasticsearch master nodes (see above), using  is a good way to scale the Elasticsearch data nodes up and down as needed. BackupElasticsearch has a to ship index files to a remote backup location. ","locales":"ja-jp","title":"Deploying Elasticsearch on Microsoft Azure"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2017-11-06T00:00:00.000Z","url":"/blog/brewing-in-beats-new-dashboards-for-auditbeat","seo_title":"","content":" Welcome to ! With this weekly series, we're keeping you up to date with what's new in Beats, including the latest commits and releases. New Auditbeat dashboardsWith this , Auditbeat gets new configuration samples in the and three new dashboards: These new dashboards will be present in Auditbeat 6.1 Windows services metricsetThanks to our regular contributor , Metricbeat gets a metricset in the module that collects information about which services are running and data about each of them. Fields are things like “name”, “display_name”, “uptime”, “state”, “start_type”. This new metricset is scheduled to be released in Metricbeat 6.1. Other changesRepository: elastic/beatsAffecting all BeatsChanges in master: Changes in 6.0: PacketbeatChanges in 6.0: FilebeatChanges in master: PackagingChanges in master: Changes in 6.0: DocumentationChanges in master: Repository: elastic/kibanaTime series visualizationsChanges in master: ","locales":"","title":"Brewing in Beats: New Dashboards for Auditbeat"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2017-10-31T00:00:00.000Z","url":"/blog/elasticsearch-6-0-0-rc2-released","seo_title":"Elasticsearch 6.0.0-rc2 released","content":" We are excited to announce the release of , based on . This is the sixth in a series of pre-6.0.0 releases designed to let you test out your application with the features and changes coming in 6.0.0, and to give us feedback about any problems that you encounter. Open a bug report today and become an . This is a pre-release and is intended for testing purposes only. Indices created in this version . Upgrading 6.0.0-rc2 to any other version is not supported. Also see: ","locales":"","title":"Elasticsearch 6.0.0-rc2 released"}
{"index":{}}
{"author":"Court Ewing","category":"Releases","publish_date":"2017-11-07T00:00:00.000Z","url":"/blog/kibana-5-6-4-released","seo_title":"","content":" We’re pleased to announce the release of Kibana 5.6.4, which includes some valuable stability improvements. One of the bugs that was fixed in Kibana 5.6.4 caused the browser window to crash when you used shift+return in console. We also fixed an issue where saved object import would fail under certain circumstances as a result of saved visualizations being imported before their associated saved searches were successfully imported. You can get Kibana 5.6.4 on our page and on . As usual, you can review all changes in the . ","locales":"","title":"Kibana 5.6.4 released"}
{"index":{}}
{"author":"Luisa Antonio","category":"Culture","publish_date":"2017-11-07T00:00:00.000Z","url":"/blog/three-ways-to-get-more-out-of-aws-reinvent-for-elasticsearch-users","seo_title":"Three Ways to Get More Out of AWS re:Invent for Elasticsearch Users","content":" AWS re:Invent has gotten so big, they don’t just have an expo floor plan, they have a and it’s all taking place in a deceivingly short 4.5 days. There’s so much to get out of re:Invent, make it a two-for-one when you stop by the Elastic booth for personalized hands-on-demos and unlimited Q&A time with Elasticsearch technical experts, plus receive your own pair of Kibana socks. Find us at booth #2031 in the Sands Expo Hall located in The Venetian. We are towards the back right of the expo hall, just across from Cloudreach. If you passed the Salesforce booth, you went too far. Elastic folks will be available at this booth during all expo hall hours. for expo hall hours. This year will mark the third year Elastic will be at AWS re:Invent and we’re excited to make this event even more valuable for our Elasticsearch users. Seeing is believing and we’ve got some dynamic demos in store. and sign up for one of the sessions listed below or request one that works for your schedule.Elastic Cloud Enterprise (ECE) Demo: You know, the software that makes Elastic Cloud (our hosted and managed Elasticsearch product). Take a behind-the-scenes look at the architecture and see firsthand how to upgrade in a few clicks and monitor your deployments from a single console.Demo TimesSecurity Analytics with Machine Learning Demo: Watch the Elastic Stack in action. Machine learning, search, and analytics join forces to track a real-world data exfiltration attack.Demo TimesHave questions after attending your re:Invent Elasticsearch sessions? We’ll have Elastic Stack experts on hand to take your toughest search, logging, analytics, and metrics questions and talk through product developments coming down the pipeline. Ask us about the difference between Amazon’s Elasticsearch Service and , how many shards you should have in your cluster, or anything that’s on your mind. You can literally ask us anything! Not sure what to ask? What’s better than using Elasticsearch? Why, sporting Elastic socks of course!You know what’s even better? These socks will visualize your data on the go. Well, not really, but we’re working on it. . Of course we’ll have stickers galore and other swag for folks who drop by. If you’ve downloaded the , it’s also an opportunity to earn some points!What are you waiting for? Don’t waste time thinking about. Screenshot these deets before heading to AWS re:Invent and come say “hi!”AWS re:inventNovember 27 - December 1. 2017Hall B | Booth #2031Sands Expo | The Venetian201 Sands AveLas Vegas, NV 89169Not attending AWS re:Invent? Come visit us at our User Conference, Elastic{ON}, February 27 - March 1 in San Francisco. Early bird prices available for a limited time! ","locales":"","title":"Three Ways to Get More Out of AWS re:Invent for Elasticsearch Users"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2017-11-02T00:00:00.000Z","url":"/blog/brewing-in-beats-kubernetes-deployment-files","seo_title":"","content":" Welcome to ! With this weekly series, we're keeping you up to date with what's new in Beats, including the latest commits and releases. Kubernetes deployment filesWe are making it easier to deploy Filebeat and Metricbeat 6.0 on Kubernetes by providing deployment manifest files. You can find more details about deploying Filebeat and Metricbeat on Kubernetes in the , but it can be summarized as: curl -L -O https://raw.githubusercontent.com/elastic/beats/6.0/deploy/kubernetes/filebeat-kubernetes.yaml # edit the YAML file to set the Elasticsearch connection information kubectl create -f filebeat-kubernetes.yaml The above commands install Filebeat as a DaemonSet, ensuring one agent is running on each Kubernetes node, and configure it to pick up the logs from , unwrap them from the JSON objects, automatically enhance them with Kubernetes metadata (pod names, labels, etc.), and ship them to Elasticsearch. Metricbeat follows a similar . In progress: Filebeat modules for Elasticsearch and LogstashWe have work in progress PRs for adding Filebeat modules for and . Kibana is coming next. This is part of making the Elastic Stack easier to monitor with the Elastic Stack. Both modules are planned to be released with 6.1. Thanks HacktoberfestWe are pleasantly surprised to how many of you participated to Beats as part of Hacktoberfest this year, and we would like to thank the following contributors: We are looking forward for the next Hacktoberfest. Other changesRepository: elastic/beatsAffecting all BeatsChanges in master: Changes in 5.6: MetricbeatChanges in master: Changes in 6.0: PacketbeatChanges in master: Changes in 5.6: FilebeatChanges in master: Changes in 6.0: TestingChanges in master: Changes in 6.0: PackagingChanges in master: Changes in 6.0: DocumentationChanges in master: Changes in 6.0: ","locales":"","title":"Brewing in Beats: Kubernetes deployment files"}
{"index":{}}
{"author":"Mark Harwood","category":"Engineering","publish_date":"2017-11-01T00:00:00.000Z","url":"/blog/using-the-elastic-stack-and-graph-to-tackle-toxic-content","seo_title":"","content":" This is a challenging time for media organizations that distribute content they don't author themselves. Increasingly, sites that serve social media, music, or video are paying closer attention to the messages they're delivering. While free speech is a widely cherished principle, in practice, most commercial organizations exercise some form of content review outlined in their own terms of service. These restrictions on content aren't motivated by an organisation's desire to act as some form of moral arbiter - often the pressures are external. While search engines have historically helped surface content that's desirable, they can equally be applied to the challenge of identifying content that's undesirable, as we'll see in this post. Volume of content and the complexity of making judgement calls, however, can make this task more challenging. (Not to mention the issue of balancing false-positives and false-negatives.) There are a couple of approaches (which are not mutually exclusive) to do so: We'll explore both scenarios using real data and the Elastic Stack. Proactively identifying content The good news is you don't necessarily need to adopt complex analysis of your text, audio, or video content to identify candidates for review. We can use the same \"people who liked X also like Y…\" techniques employed by recommendation algorithms. The difference is we are focusing on users with undesirable tastes rather than desirable ones. Let's start by looking at some real data (that we've anonymised for this post) in the form of user profiles from a music streaming service where each user profile contains their list of favourite artists. If we start with a single artist name (we'll refer to them here as fictitious \"Band X\") that's known to be associated with hate speech we can query the user profiles to see which other artists are favoured by these Band X fans. We can walk these connections simply by hitting the \"+\" button in X-Pack's Graph UI. First, a word on meaningful relationships Before we look at any connections, it's important to appreciate that the Graph API that underpins the UI uses some special relevance-ranking logic from our search engine heritage that is not found in typical graph databases. Let's turn this special feature off in the settings to see what problems these significance algorithms help avoid: With this feature turned off our top suggestions for the interests of Band X fans look like this: Our resulting graph shows a link between Band X and The Ramones, a popular group not known to be associated with hate speech. When we click on the link between Band X and the \"ramones\" suggestion, we can drill down into the stats of how many users like Band X and The Ramones using a Venn diagram. There are 97 Band X fans who like The Ramones, and while that may be a large number it is not significant — The Ramones are generally popular (just like The Beatles or ) and a huge majority of their fans have no interest in hate speech. The Ramones are not relevant to this content exploration -- they are off-topic -- and should not appear in the top recommendations. It should be obvious from this example that popularity is not the same thing as significance. Let's turn the default significance setting back on: Now when we walk the top connections let's see what significant suggestions we find: The Ramones are no longer present and instead we find \"Band Q\". Only 41 of the Band X fans like Band Q — but there are only 53 Band Q fans in total. That's a huge overlap and suggests that the two bands are strongly associated even though they are less popular than The Ramones. \"Band Q\" is an anonymised name but you only need to see the real band names from this dataset to know that the significance algorithms are staying on topic while following the connections. This is the first hop in what could be a multi-step expansion. We started with a single known-bad, not kn","locales":"","title":"Using the Elastic Stack and Graph to Tackle Toxic Content"}
{"index":{}}
{"author":"Tyler Hannan","category":"Releases","publish_date":"2017-10-31T00:00:00.000Z","url":"/blog/elastic-stack-6-0-0-rc2-released","seo_title":"","content":" If you’ve been following along closely you will be aware that we’ve been busy in preparation for 6.0.0 GA. It is our intent, and hope, that rc2 represents as close to final code as is possible before the GA release. This also means that – if you want to test in advance of GA – this could be close to your final chance. While releasing all the things on the same day is no trick (but quite the feat of coordination), we do hope you find it a treat to play with upcoming features. We continue to work hard squashing bugs and adding features to each product because, like mummies, we are afraid to unwind. Before you get too excited, keep in mind that this is still a release candidate so don’t put it into production. There is no guarantee that any of the 6.0.0 pre-release versions will be compatible with other pre-releases, or the 6.0.0 GA. During the 5.0 release, we introduced the Elastic Pioneer Program and are continuing the with the 6.0 preview releases. Keep in mind that you can . That’s correct, release candidates are now available on your favourite hosted Elasticsearch provider. All of the products have multiple changes of varying complexity. We’d encourage you to test with the entire Elastic Stack. Elasticsearch Get It Now! ","locales":"","title":"Elastic Stack 6.0.0-rc2 released"}
{"index":{}}
{"author":"Renuka Hermon","category":"Culture","publish_date":"2017-10-30T00:00:00.000Z","url":"/blog/thank-you-for-speaking-elasticon-tour-toronto","seo_title":"","content":" Dog parks, hockey, craft coffee . . . it’s like the city of Toronto has an insider’s view of our Slack channels at Elastic. My first Elastic{ON} event was bound to be great.Strolling in at 7 a.m. on a boiling hot, late-September day in Toronto (weather anomaly anyone?), I saw the Sony Centre transformed into a vibrant space surging with Elastic{ON} energy. I’ll come out and say it, our Events and Design teams don’t mess around. All eyes were drawn to the stage. Between chatting with folks at registration and fetching extra computer chargers for attendees who were rapidly note taking, I saw turn to the crowd, ready to kick off his presentation. As he started discussing how they use the Elastic Stack to report on and monitor usage on the public Wi-Fi offering that they’ve built in the Toronto and NYC subway systems, we all knew we were in for a treat. He’s one of us, an Elastic user with a great sense of humor. He started his talk in one of the best ways possible: laughter. Here we see his favorite (impossible?) captchas that prevented users from joining the Wi-Fi. Then he dove right into the heart of the story. He described his adoption of the Elastic Stack and how BAI Canada uses it to “.” Jeremy walked us through his journey specifically around creating more robust reporting capabilities and gave us a peek into what their future holds for machine learning, analytics, and modeling. Although happy hour was yet to begin, there was plenty of humor, raw storytelling, and humility: qualities of those who create. That’s why we (community members and employees alike) attend. We get to chat face-to-face with the people who love these tools as much as we do and use them to make lives better, simpler, and happier. Writing as someone who would much rather sit behind a keyboard than stand in front of a crowd, I discovered a few points that may be helpful if you (like me) are considering speaking at an event, , or our .If you're looking for motivation to speak, remember this:Elastic users are clever and apply the software in ways we hadn’t predicted. Your use case is interesting and I guarantee someone in the crowd will have a light bulb switch on thanks to you. Newcomers, veterans, those who tinker, and those who have helped build the Elastic Stack are all in the crowd. If you’re considering applying to speak at an Elastic event, your fan base is built in.  Have you always been an open source fan? Did your boss give you pushback on utilizing new software? Did the office hold a party in your honor because you found a way to visualize your security logs in a useful way? People in the crowd will relate.  Behind the scenes, I heard a few veteran speakers mention pre-show jitters and post-show adrenaline rush. Of course, preparation is key. Daniel Palay and Livia Decurtins, who head up Elastic{ON} External Speaker Relations, make sure that no speaker question goes unanswered and that each presentation slide is spotless. Who’s that person commanding the room with Elastic Cloud Team Lead, Suyog Rao? That’s Jeremy, but next time it could be you . . . or me. Elastic engineers are sharing roadmaps at the same event, oftentimes on the same stage. And I will leave you with this: we also have incredible at Elastic{ON} Tour and Conference. Don’t miss out. Thank you, Toronto, for being an incredible host. ","locales":"","title":"Thank You for Speaking: Motivation from Elastic{ON} Tour Toronto"}
{"index":{}}
{"author":"Andrew Cholakian","category":"The Logstash Lines","publish_date":"2017-10-31T00:00:00.000Z","url":"/blog/logstash-lines-2017-10-31","seo_title":"","content":" Welcome back to The Logstash Lines! In these weekly posts, we'll share the latest happenings in the world of Logstash and its ecosystem. I'm really excited to announce a new patch that just landed in master and will most likely be released in Logstash 6.1 native java execution with runtime . This is all thanks to the amazing work of , who's leveraged his significant expertise to deliver us this patch. You can test it out by using the special flag on the master branch of Logstash.  Logstash will have two ways of executing in releases with this patch: Our goal is to continue to improve the feature flagged code generation and get feedback from users willing to try it out as an experimental feature, then make it the default execution sometime in 6.x once it's been tested more in the field. If you're interested in giving it a shot it's probably best to wait till it's out of master and in a proper release behind the aforementioned feature flag as this feature is still under active development. ","locales":"","title":"Logstash Lines: Experimenting With Bytecode Generation"}
{"index":{}}
{"author":"Shay Banon","category":"News","publish_date":"2017-10-30T00:00:00.000Z","url":"/blog/elasticon-2018-announcing-the-fourth-official-elasticsearch-user-conference","seo_title":"","content":" I personally believe that the culture of a company is created not by any one decision, but by thousands of small decisions made by hundreds of individuals. The culture of Elastic{ON} is no different. It is not the product of where we hold the conference or what we put on the walls, but of the thousands of personal interactions, meaningful conversations, and individual lightbulb moments that happen when you bring committed users from an open source community into the same physical space.This community inspires us to build better products that end up living up to what you, our users, expect us to bring to the table. Whether it’s through simply using our products, being innovative around new use cases, contributing code and effort in documentation, or opening issues around something that just doesn’t work, you continue to be the most critical drivers of new products and ideas.And from 27 February to 1 March of next year, more than 400 of your biggest fans from Elastic will be anxiously waiting to meet and talk with you at . Why? Because there is no “average” Elastic{ON} attendee. Every one of you is doing something creative that we want to hear about, bumping into problems we want to try and solve, and coming up with use cases we haven’t even dreamed about. We care deeply about the user experience you have when you embrace and deploy our products. We try to imagine, what is the first download experience that a user has? What is it like, that moment when you go and click download into your laptop and you’re there to try to solve a problem? And are our products living up to solving that problem? We want to talk with you about it at Elastic{ON} 2018. This year, there are a few things that I, personally, am really excited about. It really is mind blowing to think about how much the conference has grown since 2015, but the culture and the heart of Elastic{ON} remains the same. At Elastic we continue to work each day to make simple things simple and difficult things possible. At Elastic{ON} we get to share what the future holds and see what kind of amazing and difficult things our users are doing and what new tools we need to build for them. . ","locales":"","title":"Elastic{ON} 2018: Announcing the Fourth Official Elasticsearch User Conference"}
{"index":{}}
{"author":"Kiju Kim","category":"Engineering","publish_date":"2017-10-26T00:00:00.000Z","url":"/blog/using-korean-analyzers","seo_title":"","content":" Hangul (Korean alphabet) was created in 1443 by King Sejong the Great. Before that, Korean people used Chinese characters but only Yangban, the ruling class people could actually learn and use it and the ordinary people could hardly use it because Chinese language is so different from Korean language and it was too difficult for the ordinary people to spare time to learn. Hangul is a phonetic alphabet and consists of 24 characters: 14 consonants (ㄱ[g], ㄴ[n], ㄷ[d], ㄹ[l/r], ㅁ[m], ㅂ[b], ㅅ[s], ㅇ[null/ng], ㅈ[j], ㅊ[ch], ㅋ[k], ㅍ[p], ㅌ[t], and ㅎ[h]) and 10 vowels (ㅏ[a], ㅑ[ja], ㅓ[ə], ㅕ[jə], ㅗ[o], ㅛ[jo], ㅜ[u], ㅠ[ju], ㅡ[ɯ], and ㅣ[i]). We can combine them and make 11172 characters (syllables), e.g. ㅎ+ㅏ+ㄴ=한. A Korean analyzer is required to search Korean documents effectively. Korean is an agglutinative language whereas English is an inflectional language and Chinese is an isolated language. A predicate changes its form according to its ending (e.g., ‘먹다’ and ‘먹고’), and a noun is usually followed by one or more postpositions (e.g., 엘라스틱서치(noun)+를(postposition)). If we query without a Korean analyzer, we can only get a single form of the predicates or nouns. For example, if we query ‘엘라스틱서치’, we don’t get the documents including ‘엘라스틱서치를’. A Korean analyzer analyzes ‘엘라스틱서치를 이용해서 한국어 문서들을 효과적으로 검색하려면 한국어 분석기가 필요합니다’ and extracts tokens such as ‘엘라스틱서치’, ‘를’, ‘이용’, ‘해서’, ‘한국어’, ‘문서’, ‘들’, ‘을’,  ‘효과적’, ‘으로’, ‘검색’, ‘하려면’, ‘한국어’, ‘분석기’, ‘가’, ‘필요’, and ‘합니다’. With these tokens, we can query ‘엘라스틱서치’ and get the documents including either ‘엘라스틱서치’ or ‘엘라스틱서치를’. Currently Elasticsearch has commercial and open source analyzers and provides APIs to implement analyzers. Among them, seunjeon, arirang, and open-korean-text are the widely used open source Korean analyzers. Open-korean-text supports only Elasticsearch 5.x. I installed these three Korean analyzers on Elasticsearch 5.5.0 and measured time and memory consumption during analysis. seunjeon arirang open-korean-text To see the effect of the JIT compiler, I ran the same test twice when I measure the analysis time. I used ‘time’ (http://man7.org/linux/man-pages/man1/time.1.html) to measure time to analyze a Korean text (see at.sh, ot.sh, and st.sh in the appendix). I ran it once just after starting Elasticsearch, and then ran it again without restarting Elasticsearch.Fig. 1 Time to analyze a Korean textArirang is the fastest in the both runs, but the 2nd run of seunjeon is much faster than its 1st run. Open-korean-text is similar or a bit slower than seunjeon.Memory (Java heap) consumption was measured at the three points: just before the analysis, maximum usage during the analysis, and just after the analysis. I used ‘jstat -gc’ () to measure the memory consumption (see am.sh, om.sh, and sm.sh in the appendix). Again, I ran the same test twice. I ran it once just after starting Elasticsearch, and then ran it again without restarting Elasticsearch.Arirang showed almost no difference among them, whereas seunjeon showed big increase during the analysis. Open-korean-text showed moderate increase during the analysis but it is mostly released after analysis. Fig. 2 Memory consumption during Korean text analysis (1st run) Fig. 3 Memory consumption during Korean text analysis (2st run)Finally let’s see the analysis results. The original string is the preface King Sejong wrote when he invented Hangul. I used the same string for the time and memory consumption test.  “나라의 말이 중국과 달라 한자와는 서로 맞지 아니할새 이런 까닭으로 어리석은 백\\xEC","locales":"ko-kr","title":"Which Korean analyzer shall I use?"}
{"index":{}}
{"author":"Tony Sleva","category":"Culture","publish_date":"2017-10-26T00:00:00.000Z","url":"/blog/elasticon-tour-stops-and-their-ama-heroes","seo_title":"","content":" “The AMA is worth the price of admission, as far as I’m concerned.” — Michael Alexander, Senior Software Engineer After attending my first Elastic{ON} Tour event, the above quote may most accurately encapsulate my takeaway from the . For all that the event offered, including talks by Shay, multiple Elastic product managers, and special guests from X-Box, the highlight for many attendees was the Ask Me Anything booth staffed by authentic Elastic software engineers. I should stop for a moment. You likely already know about , but there’s a chance you haven’t heard of the . Elastic{ON} Tour stops are one-day events in cities around the world (50% of this year’s stops are outside of the U.S.) that offer a localized Elastic{ON} experience — content tailored to your region with users and engineers from your local community. Each tour stop is made up of a series of speaker sessions (roadmap, best practices, use case success stories, etc.) with breaks interspersed throughout to allow for mental digestion and professional networking. But while all that is going on, away from the presentations, closer to the food spreads, is a table stocked with Elastic engineers. Specifically, Elastic engineers that know a whole lot about the Elastic Stack (because they helped build it). Even more specifically, Elastic engineers that will answer any question you can think of because their only job for the day is to answer your questions about Elastic. Go ahead, ask them anything. I dare you. Not sure where to start? Here are just some of the many questions that could be overheard in Seattle: Those are all questions that would usually be directed towards our support team by customers with a commercial . But at Elastic{ON} (and its tour stops), our AMA engineers are available for your questions all event long — no subscription required. And the Seattle attendees, knowing that this would be the case, weren’t about to let an opportunity this good slip through their fingers. Here’s what some of them had to say of their AMA experience: “It was really helpful to be able to talk to an actual developer from Logstash. They were able to quickly fix a problem that had been going on for two to three weeks.” — Jonathan Li, Software Engineer “An initial index design, while functional, was not scaling well due to the mapping and high number of deletes. The AMA engineers really helped and gave me some great ideas on how to approach re-architecting.”  Look at that. Real-time answers from real-life Elastic engineers. How you leave happy? I learned a lot standing around that table. The first thing I learned was that our AMA engineers fear no question, and they will talk Elastic for hours (days, maybe). Next, I learned that a lot of answers start with, “,” which is a pretty good indicator to take a seat, because you’re about to dive deep into a use case exploration. Finally, and most importantly, I learned that AMAs are an invaluable resource for Elastic{ON} (and Tour) attendees, are supremely appreciated by all who stop by, and in some cases, the driving impetus for attendance. ","locales":"","title":"Elastic{ON} Tour Stops and Their AMA Heroes"}
{"index":{}}
{"author":"Alex Francoeur","category":"Engineering","publish_date":"2017-10-25T00:00:00.000Z","url":"/blog/time-series-annotations-and-anomalies-with-kibana","seo_title":"","content":" It's been a long busy summer and we've taken longer than to close out our Time Series Visual Builder blog series. If you haven't had a chance to view the first two video-blogs or want a refresher, we highly recommend setting aside a few minutes to watch them both (, ). Today we'll be going over how to add annotations to a time series visualization from anomalies detected by a machine learning job. If you'd like to follow along, we'll be using the latest version of Kibana with machine learning feature installed and logs from our . For this demo specifically, we are using the Kibana 6.0.0-RC1 build. The latest preview release can be found . In this video, you'll learn to do the following: Ready to dive in? Watch and follow along in the video below. We're moving fast with the Time Series Visual Builder! If you have a feature you'd like to see, I invite you to open an issue in the and add a label. ","locales":"","title":"Time Series, Annotations, and Anomalies with Kibana"}
{"index":{}}
{"author":"Christoph Wurm","category":"Engineering","publish_date":"2017-10-20T00:00:00.000Z","url":"/blog/getting-started-with-elasticsearch-and-the-elastic-stack-on-microsoft-azure","seo_title":"Getting Started with Elasticsearch and the Elastic Stack on Microsoft Azure","content":" As cloud adoption grows, we’re keeping pace at Elastic, developing integrations and making it easier to use our software wherever you are. These days, a number of our users are using Microsoft Azure for their deployments. For example, you can read about . Microsoft itself is an Elastic user in its own right. Elasticsearch is - a Top 50 website, and social network Yammer is . There are several ways to run the components of the Elastic Stack (Elasticsearch, Kibana, Logstash, and Beats) - and to ingest data from various applications and Azure components. Installing Elastic from the Azure Marketplace The easiest way to get started is to use the . We’ve partnered with Microsoft to create a configurable, UI-driven deployment template that you can use to create an Elasticsearch cluster with a Kibana instance on top. You can find more information on this in . And you’ll find the template itself . Let us know at if you have any questions or feedback. Ingesting Data using Beats and Logstash As the primary ingest technologies for the Elastic Stack, you can use both these components to get your data into Elasticsearch on Azure. is a data collection framework with implementations for many common data types: Log files, system and application metrics, network data, audit logs, or Windows Event data. You can view a list of official Beats , and a curated list of community-developed Beats . In contrast to the Beats which collect data from the source, Logstash is commonly used to receive data from the Beats for further processing - or to pull data from intermediary systems. Logstash supports a number of data sources, e.g. Syslog, various message queues like Kafka or Redis, and a number of Azure-specific data sources. And, of course, it can receive data from any Beat for further processing and enrichment. See for a list of input plugins and for a list of data transformations that are supported out of the box. Oct 25: Join Us Virtually at Azure OpenDev! Join us online for the next edition of Azure OpenDev where we will dive deeper into this. This is a live community-focused series of technical demonstrations centered around building open source solutions on Azure. On , we will be presenting together with , , , and on how to build a DevOps pipeline and bring an enterprise app to the Azure cloud. . ","locales":"","title":"Getting Started with the Elastic Stack on Microsoft Azure"}
{"index":{}}
{"author":"Loek van Gool","category":"Engineering","publish_date":"2017-10-24T00:00:00.000Z","url":"/blog/psd2-architectures-with-the-elastic-stack-part-ii","seo_title":"","content":" At Elastic, we :heart: APIs because developers love to work with them to get things done. APIs also have the power to change (or disrupt) an industry quickly and decisively, as is the case with The Revised Payment Service Directive (PSD2). APIs make it possible to seemlessly switch from Web browsers to apps, to deploy content to any platform, and to find the best deals among thousands of suppliers. PSD2 sets out to standardize APIs between EU banks and abolish the existing lock-ins that still exist in the industry. Because while financial institutions are closer to the forefront of the innovation curve than almost any other industry, the point can be made that this has not resulted in wide-spread open access to the core banking ecosystems - namely accounts and transactions. PSD2 is a directive from the European Union that will make banks open up access to their, otherwise private, core banking functions in ways that we have not seen before. PSD2 legislation introduces a breadth of opportunity for retail banks, while also introducing new risk. The Elastic Stack plays a vital role in many of the world’s banks today, and that will especially be true for PSD2 architectures. This is Part II of a series on PSD2 in which we will focus on creating “observability” in a public API architecture, that is to say at all times knowing the status of the business service, its anomalies that require attention and all historical raw data around individual users and requests. focuses on using the Elastic Stack for running next-generation retail banking APIs and also gives a general introduction of PSD2 regulation and strategic options for EU retail banks. A Shopping ListAt Elastic we get to see many customers running production, value-add installations, the successful deployments provide the business with a platform to leverage for insight. The commonality that can be extracted from these installations include but are not limited to: The Elastic Stack for Logging and Metrics At the highest level, Elastic is functioning as the data platform for all logs, metrics, and traces that are generated in the Elastic data platform. A separate cluster will ensure separation of resources and data. Data agents generate and collect relevant data into a pipeline that transforms the data before ingesting it into a permanent data store. From ingestion, that data is immediately available for automated and manual analytics: machine learning, dashboarding, ad-hoc queries, and the likes. The Elastic Stack for Logging and Metrics More specifically, the logical architecture looks like pictured above. The Elastic Stack offers a complete suite of products for API observability architectures: The Elastic Stack logical architecture for Observability combines all these products into an end to end platform with accompanying services, like Consulting and Expert Support. As you have probably read a bunch of times by now, Elastic :heart: APIs. That is why the Elastic Stack products natively supports REST API endpoints for easy integration into any architecture. Keeping an Eye on Things, All Things are composed of documents in the 1st Normal Form (1NF), usually with a timestamp. 1NF is important to achieve linear scalability: it is not feasible to arbitrarily join multiple datasets of hundreds of terabytes while the user or a real-time process is waiting for the answer. Of course, it’s a good idea to join those datasets at time of ingestion! That still allows us to scale to billions of events per day without slowing down. Millions of similar events will stream into the Elastic platform using the Elastic Beats data agent towards Logstash, Elastic’s data processing product. Logstash will be able to enrich, lookup, filter and transform the data in transit before storing it in Elasticsearch. After Logstash, the same document might look like this. It has relevant information added to it ","locales":"","title":"PSD2: Monitoring Modern Banking API Architectures with the Elastic Stack, Part II"}
{"index":{}}
{"author":"Shay Banon","category":"News","publish_date":"2017-10-13T00:00:00.000Z","url":"/blog/alibaba-cloud-to-offer-elasticsearch-kibana-and-x-pack-in-china","seo_title":"","content":" Heya (Ni Hao) to Aliyun customers … Today, we're announcing a multi-year collaboration and strategic partnership with Alibaba Cloud to offer a new service called . This includes Elasticsearch, Kibana, and all features hosted on Alibaba Cloud, deployable by customers with a just few clicks from Alibaba's site. By collaborating with Alibaba Cloud, we'll be able to provide Alibaba's customers with the latest versions and features of the Elastic Stack (formerly known as the ELK Stack) and X-Pack and work together with Alibaba Cloud to build and launch new services such as logging. It's been an absolute privilege for me to be in Hangzhou at Alibaba's to make this important announcement in front of thousands of developers, startup entrepreneurs, and IT professionals. We view China as an important market and we love the pace of innovation that is happening all across the country with our software in gaming apps, mobile apps, web apps, and within traditional IT systems. While this super exciting for us, I'd like to recognize the wonderful community that's been built in China. It's these users who have contributed to putting Elasticsearch on the map. In the last two years, we've done over 50 meetups and developer events all across China. We've been to Beijing, Shanghai, Guangzhou, Shenzhen, Hangzhou, and many more cities. Our community is more than 5,000 users, and keeps growing daily. And this was all made possible by our first hire in China, Medcl Zeng, our engineering evangelist, and many volunteers who have helped us along the way. Alibaba Cloud team, thank you (Xie Xie) for a great week in China. It's going to be an exciting next few years for us. Simon Hu, President of Alibaba Cloud and Shay Banon, CEO of Elastic Shay Banon, CEO of Elastic, announces partnership with Alibaba Cloud at The Computing Conference 2017 ","locales":"zh-chs","title":"Alibaba Cloud to Offer Elasticsearch, Kibana, and X-Pack in China"}
{"index":{}}
{"author":"Jim Goodwin","category":"Releases","publish_date":"2017-10-10T00:00:00.000Z","url":"/blog/kibana-5-6-3-released","seo_title":"","content":" Hello, and welcome to the 5.6.3 release of Kibana!  This release of Kibana includes an important enhancement to improve the experience when importing dashboards and visualizations from pre-5.4 releases of Kibana allowing you to choose an existing index pattern for references in dashboards and visualizations. Kibana 5.6.3 is available on our and on . Please review the  for rest of the enhancements and bug fixes. ","locales":"","title":"Kibana 5.6.3 released"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2017-09-28T00:00:00.000Z","url":"/blog/elasticsearch-6-0-0-rc1-released","seo_title":"Elasticsearch 6.0.0-rc1 released","content":" We are excited to announce the release of , based on . This is the fifth in a series of pre-6.0.0 releases designed to let you test out your application with the features and changes coming in 6.0.0, and to give us feedback about any problems that you encounter. Open a bug report today and become an . This is a pre-release and is intended for testing purposes only. Indices created in this version . Upgrading 6.0.0-rc1 to any other version is not supported. Also see: ","locales":"","title":"Elasticsearch 6.0.0-rc1 released"}
{"index":{}}
{"author":"Ron Cohen","category":"Releases","publish_date":"2017-09-28T00:00:00.000Z","url":"/blog/elastic-apm-enters-alpha","seo_title":"","content":" We’re very excited to announce that Elastic APM is now in alpha. It is open source and brings support for monitoring Node.js and Python applications. Since Opbeat became part of Elastic, the team has been very hard at work reworking central components from opbeat.com to make them work on the Elastic Stack. This means Elastic APM is available for you to and try out on your local machine today. ArchitectureElastic APM consists of four components: are open source libraries written in the same language as your application. You install them into your application as you would install any other library. is an open source application written in Go which runs on your servers. It listens on port by default and receives data from agents periodically. To visualize the data after it's sent to Elasticsearch, you can use the pre-built Kibana Dashboards that come with APM Server. Later this year the UI will get a massive upgrade when we release a dedicated APM UI, similar to the intuitive and easy-to-use interface that is known from Opbeat today. The UI will be delivered as a Kibana plugin and will be included in the Beta release. Try it today! The alpha release that we’re making available today has been running on different sites for some time. While this software is alpha level, we encourage you to try it out - we’re always looking for feedback from users! Getting started with Elastic APM is straightforward: For more detailed getting started guide, follow the . Feedback wantedPlease don’t hesitate to get in touch if you run into any issues or if you have ideas for improvements. You can ask question on the or open issues directly on the . Finally, we’re also keen to know more about your stack to help inform our continued development. Please help us by answering this . ","locales":"","title":"Elastic APM enters alpha"}
{"index":{}}
{"author":"Chris Scobell","category":"User Stories","publish_date":"2017-10-11T00:00:00.000Z","url":"/blog/lexers-upgrade-to-elasticsearch-5-4-1-improved-search-speeds-by-30-40","seo_title":"Lexer's upgrade to Elasticsearch 5.4.1 improved search speeds by 30-40%","content":" Scaling upLexer provide the data, tools and team to help companies genuinely understand and engage current and prospective customers. Elasticsearch helps us process large volumes of data and present it to our clients for insight and action.   When we first began using Elasticsearch, our was managing around one million pieces of social content per day. Today that figure has climbed to 30 million per day, so upgrading towards a more robust cluster running Elasticsearch 5.4.1 was a crucial step in building a scalable product going forward.  Elasticsearch 5.4.1 features dramatically improved indexing performance making it faster to get new data into the system. Plus, it comes with a new default scripting language () that is . Resilience is also a key focus: searches keep running even if hardware fails, or someone gets greedy with a huge complicated search. Needless to say, we were pretty eager to move in, so the first step was to work out how we were going to pack up the boxes in preparation for a full-scale migration.   Preparing for migrationThe first step for us was moving our data into smaller indexes. We moved from a single index containing 2.8 billion tweets, comments, messages, articles and blogs to 90 smaller indexes of about 30 million objects each. These new indexes made the process of migration much more streamlined, and, more broadly, allowed our clients to make more efficient requests within Lexer.  The next step was to ensure our searches were compatible with Elasticsearch 5.4.1 by updating our query generation library to ensure the queries generated in the interface (i.e. term matches or author searches) would work on the new search cluster.  Here’s an example of the type of queries that had to be migrated, \"and\" and \"or\" filters which had to be translated into \"bool\" queries using \"must\" and \"should\": Old Query { \"query\": { \"filter\": { \"and\": [ { \"or\": [ { \"query\": { \"query_string\": { \"query\": \"car AND (blue OR red)\", \"default_field\": \"content.content\" } } }, { \"query\": { \"query_string\": { \"query\": \"bob\", \"default_field\": \"content.author\" } } } ] }, { \"query\": { \"query_string\": { \"query\": \"facebook.com\", \"default_field\": \"content.source\" } } } ] } } } New Query: { \"query\": { \"bool\": { \"must\": [ { \"query\": { \"query_string\": { \"query\": \"facebook.com\", \"default_field\": \"content.source\" } } } ], \"should\": [ { \"query\": { \"query_string\": { \"query\": \"car AND (blue OR red)\", \"default_field\": \"content.content\" } } }, { \"query\": { \"query_string\": { \"query\": \"bob\", \"default_field\": \"content.author\" } } } ], \"minimum_should_match\": 1 } } } We could translate these queries easily because we never stored Elasticsearch queries directly but instead store them in our own domain specific logical query structure. All we had to do was modify the library that translates our internal query format into Elasticsearch queries so that it would output queries that were accepted by Elasticsearch 5. So our concerns here were not so much with common queries like keyword searches but instead with the more complicated searches, like geolocation filters. Our interface allows users to draw a box on a map that is effectively 4 lat/long points, which is converted into an Elasticsearch query and run against the cluster.  Here’s what this looks like in , our social analytics tool. In this example, the user is searching for people posting about the Australian Football League (AFL) in the vicinity of its biggest stadium, the Melbourne Cricket Ground. All of the charts and tables in this example are the result of Elasticsearch aggregations using the geo filter.  The \"From Location\" map filter translates into an Elasticsearch query like this: { \"geo_bounding_box\": { \"geography.point\": { \"top_left\": { \"lat\": -37.8169206345321, \"lon\": 144.978375434875 }, \"bottom_right\": { \"lat\"","locales":"","title":"Lexer's upgrade to Elasticsearch 5.4.1 improved search speeds by 30-40%"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2017-10-10T00:00:00.000Z","url":"/blog/elasticsearch-5-6-3-released","seo_title":"Elasticsearch 5.6.3 released","content":" Today we are pleased to announce the release of , based on . Elastisearch 5.6.3 is the latest stable release, and is already available for deployment on , our Elasticsearch-as-a-service platform.Latest stable release in 5.x:Please , try it out, and let us know what you think on Twitter () or in our . You can report any problems on the . ","locales":"","title":"Elasticsearch 5.6.3 released"}
{"index":{}}
{"author":"Tyler Hannan","category":"Releases","publish_date":"2017-09-28T00:00:00.000Z","url":"/blog/elastic-stack-6-0-0-rc1-released","seo_title":"","content":" 6.0.0-rc1 is available today! And, because multiple releases on the same day are amazing, we also . Not only are we approaching, rapidly, the GA of 6.0.0, we are super excited that this release includes the alpha release of . Although it is in alpha, it has been running on multiple sites for some time and it wouldn’t be a proper dogfooded release if we didn’t sip the champagne while mixing our metaphors. If you want more history on why Elastic APM, is a great place to start . During the 5.0 release, we introduced the Elastic Pioneer Program and are continuing the with the 6.0 preview releases. Before you get too excited, keep in mind that this is still a release candidate so don’t put it into production. There is no guarantee that any of the 6.0.0 pre-release versions will be compatible with other pre-releases, or the 6.0.0 GA. We strongly recommend that you keep this far, far away from production. But we also recommend that you . That’s correct, now that we have a release candidate it is available on your favourite hosted Elasticsearch provider. (Or will be by the time you read this…). Elasticsearch For more detailed information, peruse the Elasticsearch . Kibana Visualize the future of your interacting with your data in the Kibana . Logstash Grok the details of rc1 in the Logstash . Beats We don’t ‘let the beat drop’ but we drop the updates in a . Get It Now! ","locales":"","title":"Elastic Stack 6.0.0-rc1 released"}
{"index":{}}
{"author":"Stacey Gammon","category":"Engineering","publish_date":"2017-10-10T00:00:00.000Z","url":"/blog/kibana-dashboard-only-mode","seo_title":"Kibana Dashboard Only Mode","content":" Ever wish you could share your Kibana dashboards without the risk of someone accidentally deleting or modifying them? Do you want to show off your dashboards without the distraction of unrelated applications and links? In version 6.0 we’re making it easier than ever to set up a restricted access user, with limited visibility into Kibana. It’s already possible to create read only users, but new in 6.0 is a UI to match, and we’ve made it simple to set up. All you have to do is assign the new, reserved, built-in role, along with the appropriate data access roles, to your user and they will be in when they log in to Kibana. The Experience When a user in Dashboards Only Mode first logs into Kibana, they will only see the Dashboard app in the left navigation pane. All edit and create controls will be hidden. When a dashboard is opened, they will also have a limited visual experience, with no add or edit controls. Dashboard only mode pairs well with full screen mode. Share your dashboards with the team responsible for putting them up on the “big screen”, and be confident they will remain safe and indestructible. How to Set it Up Under Management > Security > Users, edit or create a new user and assign them the role, along with roles that grant the user appropriate data access. Advanced Configuration The built-in role grants read only access to the .kibana index, so if you have a multi-tenant setup, or are using a custom kibana index, you’ll have to use an advanced configuration. You can do this by creating your own roles, and tagging them as “Dashboard only mode” in Advanced Settings. You can find this configuration in Management > Advanced Settings, called . By default this will be set to . Here you can add as many additional roles as you like. When creating your custom dashboard only mode roles, you should grant them read only access to your custom index. Roles are stored in Elasticsearch, but because Advanced Settings are stored in your kibana index, you will have to modify this setting for each custom index you are using. Try It Out Dashboard only mode is available in 6.0.0-RC-1, which you can download and try out here: . We’d love for you to try it out and give us any feedback you have. As an added bonus, by finding and filing new bugs, and be eligible for Elastic swag. ","locales":"","title":"Kibana Dashboard Only Mode"}
{"index":{}}
{"author":"Jim Goodwin","category":"Kurrently in Kibana","publish_date":"2017-10-02T00:00:00.000Z","url":"/blog/keeping-up-with-kibana-2017-10-2","seo_title":"","content":" Welcome to This is a weekly series of posts on new developments in the Kibana project and any related learning resources and events. 7 is in 6.0.0-rc1! Plus, security levels up thanks to you, our early testers. See what else is new, just in time for weekend projects: — elastic (@elastic) V6.0.0 stabilization edition. These have no impact on existing v5 users. ","locales":"","title":"Keeping up with Kibana: This week in Kibana for October 2, 2017"}
{"index":{}}
{"author":"Uri Cohen","category":"Releases","publish_date":"2017-09-28T00:00:00.000Z","url":"/blog/announcing-the-ga-of-elastic-cloud-hosted-elasticsearch-on-google-cloud-platform-gcp","seo_title":"","content":" Great things happen in pairs, especially when the company who popularized the search box for the Internet partners with the world’s most popular open source, distributed search engine. This happened in April when to offer our hosted Elasticsearch product on Google Cloud Platform (GCP).  Less than six months after this announcement, is now GA in four regions: US West (Oregon), US Central (Iowa), Europe (Belgium), and Europe (Frankfurt). As Google’s cloud business continues to rapidly grow and more and more developers adopt Elastic’s products for mission critical use cases like search, logging, security, metrics, and analytics, users can now deploy, manage, and scale their Elasticsearch clusters on GCP with a few clicks. Based on what we continually hear from our users, they want to use a hosted Elasticsearch service created by Elastic, not to be . Elastic Cloud is the only product on the market that comes with , support provided by Elastic technical engineers, and many other features like one-click upgrades, snapshots every 30 minutes, custom plugin support, Elastic’s popular map service for Kibana geo-visualizations, and a comprehensive set of out-of-the-box monitoring metrics and tools. With Google, we get an incredible partner and a powerful cloud platform to offer Elastic Cloud on. Our users and customers now get more choices for where to run their Elasticsearch and Kibana workloads. A Standard subscription to Elastic Cloud on GCP starts at $45/month and users can freely upgrade to . In the future, we look forward to expanding into more GCP regions and adding additional features and use cases. Spin up your now! ","locales":"de-de,fr-fr,ja-jp","title":"Announcing the GA of Elastic Cloud on Google Cloud Platform (GCP), More Options to Host Elasticsearch"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2017-10-10T00:00:00.000Z","url":"/blog/brewing-in-beats-enhance-data-with-azure-metadata","seo_title":"","content":" Welcome to ! With this weekly series, we're keeping you up to date with what's new in Beats, including the latest commits and releases. Enhance the event with Azure metadata Thanks to , the add_cloud_metadata processor gets for Azure. If configured, it enhances each event with . This is useful in case you want to see what logs came from a specific Azure instance or what metrics are associated with it. This feature is scheduled to be released in 6.1. Support for TLS renegotiation TLS renegotiation is disabled by default in the Go standard library (and therefore in Beats), because it significantly complicates the state machine and has been the source of security issues in the past. However, it is sometimes , so this adds support for enabling TLS renegotiation. The setting is and the options are (default), , and . The new setting can be used anywhere TLS is configured (the outputs, the http module in Metricbeat, Heartbeat, etc.). This feature will be available in 6.1 and is being ported to 5.6 as well. Work in progress: Autodiscovery We have started the work around autodiscovery in Beats. Once it’s finished, autodiscovery (the name still to be discussed) will allow for scenarios like: In general, auto discovery will allow the Beats to react and adapt to changes in the ever more dynamic infrastructures. See this for the general approach we take for the building blocks, as well as the first implementation for Docker. This work is scheduled to be released in the 6.x time frame. Participate to Hacktoberfest with Beats You can participate in the fourth annual Hacktoberfest by contributing to the Beats open source project. Just start by looking at the GitHub issues tagged with the hacktoberfest label. Other changesRepository: elastic/beatsAffecting all BeatsChanges in master: Changes in 6.0: MetricbeatChanges in master: PacketbeatChanges in 6.0: FilebeatChanges in master: Changes in 6.0: AuditbeatChanges in 6.0: DashboardsChanges in master: TestingChanges in master: Changes in 5.6: Changes in 6.0: DocumentationChanges in master: Changes in 5.6: Changes in 6.0: ","locales":"","title":"Brewing in Beats: Enhance data with Azure metadata"}
{"index":{}}
{"author":"Monica Sarbu","category":"Releases","publish_date":"2017-09-28T00:00:00.000Z","url":"/blog/beats-6-0-0-rc1-released","seo_title":"","content":" Today we are excited to announce the first public release candidate for the Beats 6.0. The 6.0 release day is approaching! The release is primarily about bug fixes and performance improvements, but we do have a change we want to highlight. But before that, let’s get those handy links out of the way: Lower number of shards in default configurations Starting with 6.0, the number of shards and other Elasticsearch mapping templates can be changed directly from the Beats configuration files. This is convenient because it lets you easily adapt the size of the shards depending on how much data your Beats create. In RC1, we took this a step further and added explicit settings for the number of shards in the default configuration files. These settings overwrite the default number of shards that Elasticsearch has, which is 5. With this release we changed the default number of shards to 1 for the Beats that create metrics (Metricbeat and Heartbeat) and 3 for the Beats that create events (Filebeat, Winlogbeat, Auditbeat, and Packetbeat). This change should result in a lower amount of shards created for a small to medium installation using the default configuration. If you have lots of Beats, or for other reasons expect high indexing throughput, you should consider increasing the number of shards in the Beats configuration. Become a Pioneer A big “Thank You” to everyone that has tried the alpha or beta releases and posted issues or provided feedback. We’d like to also remind you that if you post a valid, non-duplicate bug report during the pre-GA period against any of the Elastic stack projects, you are entitled to a special gift package. You can find more details about the . ","locales":"","title":"Beats 6.0.0-rc1 released"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2017-09-28T00:00:00.000Z","url":"/blog/brewing-in-beats-add-opensuse-support","seo_title":"","content":" Welcome to ! With this weekly series, we're keeping you up to date with what's new in Beats, including the latest commits and releases. Official support for openSUSE 42 We have an openSUSE 42 VM to our packaging tests in , so we can officially support it starting with version 6.0. Note that this doesn’t change our stance on SUSE or openSUSE 11, which is that we provide binaries but they are not officially supported. The main reason is that while openSUSE has switched to systemd, SUSE still uses init scripts, and the init script that we ship with the RPMs doesn’t work on SUSE out of the box. Audibeat: Kibana 5.x version for the dashboards Auditbeat contains dashboards compatible with Kibana 5.x, so that it’s convenient to use with the previous version of the stack as well. This change didn’t make it for 6.0.0-rc1, but we intend to include it in 6.0.0-rc2. Other changes: Repository: elastic/beats Affecting all Beats Changes in master: Metricbeat Changes in master: Filebeat Changes in master: Testing Changes in master: Changes in 5.6: Documentation Changes in master: Changes in 5.6: Changes in 6.0: ","locales":"","title":"Brewing in Beats: Add openSUSE support"}
{"index":{}}
{"author":"Jared Carey","category":"Engineering","publish_date":"2017-09-27T00:00:00.000Z","url":"/blog/tls-elastic-stack-elasticsearch-kibana-logstash-filebeat","seo_title":"","content":" Transport Layer Security (TLS) can be deployed across the entire Elastic Stack, allowing for encrypted communications so you can rest easy at night knowing that the data transmitted over your networks is secured. It may not seem all that necessary, but then again consider the impossible situation of making sure that no developer starts logging sensitive data into the logs that you are shipping to a central location. Sensitive data is what most would believe to be passwords, customer's personal information, etc. However, this definition of sensitive data is far too narrow for the era of cyber security that we live in. Imagine a compromised router that allows an attacker to peer into the raw unencrypted data on the network, where seeing the logging data could provide the software and operating system versions of all the software being used on a network. This provides every detail necessary for the attacker to look up known software vulnerabilities that could allow the attacker to gain direct access to these servers. The security of an entire organization hinges on the weakest link, and in today's world of cybersecurity attacks - don't let your logging / search system be that weakest link. 6.x will require Elasticsearch TLS node to node communication when using X-Pack security for a multi node cluster. Read for more details. This blog will guide you through the process of setting up and configuring TLS to keep all data private from Filebeat -> Logstash -> Elasticsearch -> Kibana -> your web browser. You'll need a system that has some memory available in order to run each of these, as you will be setting up two Elasticsearch nodes (1gb memory per node by default, starting the second node is optional if you need to conserve some memory), one Logstash server (1gb memory by default), one Kibana server (~200mb memory), and one Filebeat (~10mb). You will likely need 6gb total system memory, but 8gb would be ideal since I have no way of telling what other software or memory hungry browser (with 50 tabs open) you are running. Understanding TLS / CertsFirst we should start with some of the fundamentals, by discussing what a certificate is and how it will be used. A certificate holds the public information, including the public key, that helps encrypt data between two parties:  where only the party with the matched private key could decrypt data from an initial handshake. Without wading too far into the details, the public and private key represent a hard-to-compute but easy-to-verify computational puzzle, where the private key holds very large numbers that allow for easily solving this computational puzzle. The hard-to-compute means the math necessary to try to solve this computational puzzle, in modern public key cryptography, would take thousands of years with current compute resources. The private key holds the source numbers necessary to easily verify the puzzle, and the certificate is generated from the private key to contain the necessary inputs to this math puzzle. Additionally, the certificate will contain a form of identities, known as the Common Name (CN). Throughout the examples to follow, I will use a server's DNS name for the identity. All public parts of the puzzle and the identity of a certificate are first created through generating a Certificate Signing Request (CSR). The CSR allows to request for identity to be inserted and signed by an authority, at which point the certificate is created. The certification / verification of the certificate is important, since the handshake that will take place between the client and server requires the client trusting the signing authority of the server's certificate. The client must either trust the server's certificate directly or it should be signed by an authority that the client trusts. For example, your OS / browser has a preset list of certificates that are \"publicly\" trusted. When you visit goog","locales":"","title":"TLS for the Elastic Stack: Elasticsearch, Kibana, Beats, and Logstash"}
{"index":{}}
{"author":"Andrew Cholakian","category":"The Logstash Lines","publish_date":"2017-09-27T00:00:00.000Z","url":"/blog/logstash-lines-2017-09-27","seo_title":"","content":" Welcome back to The Logstash Lines! In these weekly posts, we'll share the latest happenings in the world of Logstash and its ecosystem.We've finished transitioning logstash core away from Travis to our own Jenkins infrastructure. Being on fast, stable hardware has really improved the reliability of our tests. Additionally, this move has reduced our PR test time from 45 minutes to often under 10 minutes per commit. We now split up some of the work among multiple jobs.Another part of this work has been stabilizing tests. The tests for Logstash are green more often now than they have been in a long time:  all while the amount and coverage of testing in Logstash has grown astronomically. In Logstash 6+ all commercially supported plugins will now be part of the package. This has been a problem historically for users in air-gapped environments, who would need to go through the cumbersome process of downloading plugins that are part of our supported list. (#8318).  The added plugins are: the aggregate filter, anonymize filter, de-dot filter, elasticsearch filter, jdbc_streaming filter, truncate filter, email output, and lumberjack output.We've continued to make strides improving our windows support. We've been steadily making our Windows experience more consistent and correct, as well as improving test support in a variety of PRs. ","locales":"","title":"Logstash Lines: More Bundled Plugins and Testing Improvements"}
{"index":{}}
{"author":"Steve Dodson","category":"Engineering","publish_date":"2017-09-26T00:00:00.000Z","url":"/blog/machine-learning-for-nginx-logs","seo_title":"","content":" Getting insight from nginx log files can be complicated. This blog shows how machine learning can be used to automatically extract operational insights from large volumes of nginx log data. Overview Data science can be a complicated, experimental process where it is easy to , or the . Therefore, a key design goal for the Machine Learning group at Elastic is to develop tools that empower a wide spectrum of users to get insight out of Elasticsearch data. This lead to us to develop features such as “” and “” wizards in X-Pack Machine Learning, and we are planning to simplify analysis and configuration steps even more in upcoming releases. In parallel to these wizards, we are also planning to shrink-wrap job configurations on known Beats and Logstash data sources. For example, if you are collecting data with the , we can provide a set of shrink-wrapped configurations and dashboards to help users apply machine learning to their data. These configurations are also aimed at showing how we develop Machine Learning configurations internally based on our experience. Help us prioritize the next set of modules that should include preconfigured machine learning jobs by . The details of how to install these configurations will be covered in a subsequent blog. This blog is aimed at describing the use cases and configurations. Use Case Notes The configuration options for X-Pack Machine Learning are extensive, and often new users are tempted to start with complex configurations and select large numbers of attributes and series. These types of configurations can be very powerful and expressive, but require care as the results can be difficult to interpret. We therefore recommend that users start with simple, well-defined use cases, and build out complexity as they become more familiar with the system. (Note, often the best initial use cases come from automating anomaly detection on charts on the Operations teams core dashboards.) Example Data Description The data used in these examples is from a production system consisting of 4 load balanced nginx web servers. We analysed 3 months data (~29,000,000 events, ~1,100,000 unique visitors, ~29GB data). Note, the data shown here has been anonymised. nginx : Sample log message: Once processed by Filebeat’s NGINX module configuration, we get the following JSON document in Elasticsearch: Use Case 1: Changes in Website Visitors Operationally, system issues are often reflected in changes in visitor rate. For example, if the visitor rate declines significantly in a short period of time, it is likely that there is a system issue with the site. Simple ways to understand changes in visitor rate are to analyse overall event rate, or the rate number of distinct visitors. Job 1.1: Low Count of Website Visitors This job can simply be configured using the ‘Single Metric Job’ wizard: Job configuration summary: This analysis shows a significant anomaly on February 27th where the total event rate drops significantly: (Note this analysis of the 29,000,000 events took a total of 16s on a m4.large AWS instance) Job 1.2: Low Count of Unique Website Visitors Event counts can be strongly influenced by bots or attackers, and so a more consistent feature to analyse the number of unique website visitors. Again this can simply be configured using the ‘Single Metric Job’ wizard: Again there is a significant anomaly on February 27th where the number of unique visitors per 15m drops from a typical 1487 to 86: Combining Job 1.1 and 1.2: Using the the results from both jobs can be temporary correlated to give an ‘Overall’ view into the anomalousness of the system based on these features: This clearly shows in a single view, that there was a significant anomaly on February 27th between 10:00-12:00 where the total event rate dropped, and the number of unique visitors dropped. The operations team confirmed the site had significant issues at ","locales":"","title":"Machine Learning for Nginx Logs - Identifying Operational Issues with Your Website"}
{"index":{}}
{"author":"Court Ewing","category":"Releases","publish_date":"2017-09-26T00:00:00.000Z","url":"/blog/kibana-5-6-2-released","seo_title":"","content":" Kibana 5.6.2 is released today with a critical fix to the Upgrade Assistant in X-Pack basic as well as a fix for a metric visualization regression. The change to the Upgrade Assistant is critical for any users that started using Kibana prior to 5.5 and intend to upgrade to 6.0. The Upgrade Assistant helps users migrate their .kibana index to a single-type format that is compatible with Elasticsearch 6.0, but prior to version 5.6.2 it would not properly disable dynamic mappings for the .kibana index after migration, so saving an object in Kibana would result in an unexpected new type in the .kibana index, which would put Kibana into a permanent red status. If your Kibana install began in 5.5 or later, or if you haven’t run the upgrade assistant yet, just go ahead and upgrade to 5.6.2 to make sure this issue is fixed going forward. If your Kibana install predates 5.5, and you’ve already upgraded the .kibana index via the Upgrade Assistant in 5.6.0 or 5.6.1, then you must follow a manual process to fix the data corruption issue, which is documented in this , and you should upgrade to 5.6.2 as well. The regression with metric visualizations did not respect style properties that were set on existing metric visualizations. Upgrading to 5.6.2 should resolve this issue. Kibana 5.6.2 is available on our as well as on . As always, check out the . ","locales":"","title":"Kibana 5.6.2 released"}
{"index":{}}
{"author":"Suyog Rao","category":"Engineering","publish_date":"2017-09-26T00:00:00.000Z","url":"/blog/logstash-centralized-pipeline-management","seo_title":"","content":" Introduction Motivation behind this feature Multi-tenancy and self-service Security considerations Roadmap Longer term, we are working on providing syntax highlighting, autocomplete, and snippets support for the config editor. Version based rollback, you ask? On it. Oh, and did I mention we’re already thinking about a drag-n-drop UI to create pipelines? I can go on and on about all the exciting stuff that's brewing in our heads, but I’ll stop here. :) We would like to hear from you! Please let us know how you like this feature or if you have ideas for enhancements. You could also win plenty of swag when you provide feedback via our . What are you waiting for?! Get ‘stashin! ","locales":"","title":"Centralized Pipeline Management in Logstash"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2017-09-26T00:00:00.000Z","url":"/blog/elasticsearch-5-6-2-released","seo_title":"Elasticsearch 5.6.2 released","content":" Today we are pleased to announce the release of , based on . Elastisearch 5.6.2 is the latest stable release, and is already available for deployment on , our Elasticsearch-as-a-service platform.Latest stable release in 5.x:Please , try it out, and let us know what you think on Twitter () or in our . You can report any problems on the . ","locales":"","title":"Elasticsearch 5.6.2 released"}
{"index":{}}
{"author":"Juan Cidade","category":"User Stories","publish_date":"2017-09-21T00:00:00.000Z","url":"/blog/using-elastic-machine-learning-to-automate-complex-data-analysis-at-sunhotels","seo_title":"","content":" An Interview with Juan Cidade, Head of IT Ops, SunhotelsHow has the exponential growth of incoming data affected the way you do business?As an online business providing travel services to European businesses, Sunhotels has always been data-driven at its core. Consequently, we have always embraced open source technologies - particularly Elasticsearch - to help us gain insight from our customer data and improve our services as a result. Now, as part of Australian travel group Webjet, we’re focusing heavily on the use of next generation data technologies to drive ongoing innovation and growth. Recently, the volume of data we handle has grown exponentially. We process search and booking requests for thousands of travel agencies and B2B suppliers across Europe. This includes enquiries from tens of thousands of travel agencies, and a huge swathe from third party aggregators. In two years, the average number of requests per second has grown from 600 to over 4000. That’s 250 million requests and thousands of bookings per day, originating from a host of different sources. We needed a platform that could process this kind of volume of data at speed and scale but, importantly, one that would reveal meaningful insight. How have Elastic technologies helped you manage the data revolution?Elasticsearch has been a part of the business for many years. Originally, we wanted to have some idea about search response times, and to find a way to differentiate between kinds of searches and their outcomes. Soon, we wanted to capture different strands of metadata from each search:  helping us to understand more about each interaction, and be more responsive to issues. We started logging around 15 different fields across each search and booking:  response time, destination, customer type, product contract (third party or direct etc). When putting all the raw data into Elasticsearch, we not only see response times across the board at a granular level, but we can analyse and understand all the different factors that might affect availability in one hotel: low availability, seasonality, price sensitivity. Using this information, we work with individual hotels, destinations, providers or clients to suggest solutions:  change pricing, improve contracting/mapping/availability, connecting/disconnecting providers. Working with SQL, it was hard to track what was being searched versus what was being booked. It was particularly hard to find blind spots in sales. Now, the wider business uses the same dashboards that operations use, taking the insight to inform long-term strategies. Being able to track ‘look to book’ ratios and analyse how many searches result in a booking, per destination, per client, allows Sunhotels to tune sales and contracting strategies:  useful when onboarding new providers and clients. We can make educated, profile-based assumptions and deliver a service based on historical insight. This enables us to optimize traffic. More bookings with less traffic:  win-win. How does this help Sunhotels with its ongoing strategy of innovation and growth?There are two main ways in which our data management infrastructure benefits both us and our partners. Firstly we want to detect changes in behaviour more quickly so we can respond proactively. When we see trends are appearing over time (e.g. January is a busy time for beach holiday booking in Scandinavia versus the Spanish habit of booking last minute), we’re able to predict behaviours and align the service accordingly. Predicting behaviour from historical insight means we can be proactive in offering our sales/contracting department the information they need to act swiftly, too. This might mean boosting the number of properties available in a certain area or at a certain price point, if we expect demand to be high. This could also mean increasing activities in quieter regions: ","locales":"fr-fr","title":"Using Elastic Machine Learning to Automate Complex Data Analysis at Sunhotels"}
{"index":{}}
{"author":"Jason Zucchetto","category":"Engineering","publish_date":"2017-09-20T00:00:00.000Z","url":"/blog/minimize-index-storage-size-elasticsearch-6-0","seo_title":"","content":" Elasticsearch 6.0 ships with two great improvements to help minimize index storage size. The best part about the improvements is they will require no special configuration changes or re-architectures, and in most cases will only require a simple upgrade and a newly created index. To illustrate the improvements, we’ll use , a lightweight tool for ingesting metrics into Elasticsearch. After running Metricbeat for several days on both Elasticsearch 5.6 and Elasticsearch 6.0 (6.0 beta2), index sizes were 41.5% smaller for the Metricbeat workload on Elasticsearch 6.0:GET _cat/indices?v health status index uuid pri rep docs.count docs.deleted store.size pri.store.size yellow open metricbeat-2017.09.16 0b46voluSDmzfwCdYmYvZg 5 1 1694709 0 508.6mb 508.6mb yellow open metricbeat-2017.09.17 UKrTuwevS3urZkjeU8GFhg 5 1 1694385 0 500.7mb 500.7mb yellow open metricbeat-2017.09.18 dxFeMlabR_anYZ_C6BBq4A 5 1 1696223 0 512.7mb 512.7mb Total storage size over 3 days: GET _cat/indices?v health status index uuid pri rep docs.count docs.deleted store.size pri.store.size yellow open metricbeat-2017.09.16 7IK6c1bfSQCaFAp1i3axUQ 5 1 1696571 0 299.1mb 299.1mb yellow open metricbeat-2017.09.17 CcBCgLdfRESXH0UaGV6YCA 5 1 1695385 0 295.4mb 295.4mb yellow open metricbeat-2017.09.18 sZfCXx8ZReGzLIsjSFO4hA 5 1 1697063 0 296.1mb 296.1mb Total storage size over 3 days: (41.5% improvement in storage space from Elasticsearch 5.6)The test above used Metricbeat with the module.Deprecated _all fieldThe “_all” field was deprecated in Elasticsearch 6.0, this is the first part in the explanation for the storage space savings we’re seeing. If you’re unfamiliar with the “_all” field, it’s a special field used to concatenate all values together, making it easy to search everything. The “_all” field made it easy to get started with Elasticsearch, however, the “_all” field uses a lot of additional storage space (especially as values are duplicated). PUT /user_profiles/profile/1 { \"userid\" : \"john123\", \"first\": \"John\", \"middle\": \"James\", \"last\": \"Smith\", \"city\": \"Alamo\", \"state\": \"California\" } The “_all” field for the document above now contains the terms [ “john123”, “john”, “james”, “smith”, “alamo”, “california” ].Using the “_all” field, it was easy to search across all fields, however, we’re now duplicating a lot of values, and using more storage space:GET /user_profiles/_search { \"query\": { \"match\": { \"_all\": \"john123 john james smith alamo california\" } } } With the deprecation of the “_all” field in Elasticsearch 6.0, we save space on indexing duplicate data. And if “_all” field functionality is still needed, the parameter should still be used in the index mapping.We ran a follow-up test to look at the effects of disabling “_all” in relational to the storage improvements we saw. When isolated, the deprecation of “_all” accounted for almost 40% of our index saving improvements (the other 60% was due to the sparse field improvements we’ll visit next).Sparse Field ImprovementsElasticsearch 6.0 includes Lucene 7.0, which has a major storage benefit in how sparsely populated fields are stored (). Metricbeat, used in our test above, happens to use a lot of sparsely populated fields.If you recall, doc values (the columnar data store in Elasticsearch) have allowed us to escape the limitations of JVM heap size to support scalable analytics on larger amounts of data. Doc values are a very good fit for dense values, where every document has a value for every field.  But they have been a poor fit for sparse values (many fields, with few documents having a value for each field), where the matrix structure ends up wasting a lot of space. If you’re unsure of what a sparsely populate field is, it’s a field that contains a value for a small percentage of documents. For instance, if we go back to our user profile example, the “middle”, “city”","locales":"","title":"Space Saving Improvements in Elasticsearch 6.0"}
{"index":{}}
{"author":"Greg Marzouka","category":"Engineering","publish_date":"2017-09-20T00:00:00.000Z","url":"/blog/generating-an-elastic-cloud-enterprise-client","seo_title":"","content":" At Elastic, we use the OpenAPI specification, formerly known as Swagger, for documenting the Elastic Cloud Enterprise REST API. The Elastic Cloud Enterprise (ECE) API allows for creating and managing clusters, performing upgrades and repairs, and other general automation tasks within ECE. See our previous blog post, , for a great overview of the API. What is the ? From the OpenAPI website: The OpenAPI Specification (OAS) defines a standard, language-agnostic interface to RESTful APIs which allows both humans and computers to discover and understand the capabilities of the service without access to source code, documentation, or through network traffic inspection. Adopting the OpenAPI specification allows us to generate our , providing a concise set of documentation:  even more importantly, it empowers users to generate REST clients in the language of their choice. This ability to generate REST clients is incredibly useful for automating and developing your own software layer around Elastic Cloud Enterprise. In fact, we use our OpenAPI specification to generate many of the internal tools we use to manage Elastic Cloud. In this blog post, we're going to focus on client generation and how you can leverage our specification to generate a client in whatever supported language you desire. Getting the specificationIf you have a running ECE installation, the OpenAPI specification can be retrieved through the API by issuing a GET request to the following endpoint: on the coordinator host: $ curl -XGET https://ece-host:12443/api/v1/api-docs/swagger.json Otherwise, you can download the specification . Just change the version portion of the URL to the desired version of ECE. Inspecting the specification JSON a bit, you'll notice that each API endpoint is represented as an object. This object contains: For instance, the specification for shutting down an Elasticsearch cluster that’s running in ECE looks like this: \"/clusters/elasticsearch/{cluster_id}/_shutdown\": { \"post\": { \"security\": [{ \"basicAuth\": [] }], \"description\": \"Shuts down a running cluster and removes all nodes belonging to the cluster. The plan for the cluster is retained. Warning: this will lose all cluster data that is not saved in a snapshot repository.\", \"x-doc\": { \"tag\": \"Clusters - Elasticsearch - Commands\" }, \"tags\": [\"ClustersElasticsearch\"], \"operationId\": \"shutdown-es-cluster\", \"parameters\": [{ \"name\": \"cluster_id\", \"in\": \"path\", \"description\": \"Identifier for the Elasticsearch cluster\", \"type\": \"string\", \"required\": true }], \"summary\": \"Shut down cluster\", \"responses\": { \"202\": { \"description\": \"The shutdown command was issued successfully, use the \\\"GET\\\" command on the /{cluster_id} resource to monitor progress\", \"schema\": { \"$ref\": \"#/definitions/EmptyResponse\" } }, \"404\": { \"description\": \"The cluster specified by {cluster_id} cannot be found (code: 'clusters.cluster_not_found')\", \"schema\": { \"$ref\": \"#/definitions/BasicFailedReply\" } }, \"449\": { \"description\": \"When running as an administrator (other than root), sudo is required (code: 'root.needs_sudo')\", \"schema\": { \"$ref\": \"#/definitions/BasicFailedReply\" } } } } } Also, note that the Swagger version is available in the JSON definition. At the time of this blog post, we’re on version 2.0 of the specification: \"swagger\": \"2.0\" Swagger Editor The Swagger website provides a great online tool called which parses the specification and produces friendly documentation, which you can navigate through (similar to our API reference). Now that you have the spec, go ahead and try pasting it into the editor. Hopefully, the benefits of using the OpenAPI specification will become obvious! Generating a clientGenerating a client is actually very straightforward. The Swagger Editor gives you the ability to generate clients in many different languages with just the click of a button. This is great for experimentation. However, in practice you'll likely want to a use a li","locales":"","title":"Generating an Elastic Cloud Enterprise Client"}
{"index":{}}
{"author":"Tyler Langlois","category":"Engineering","publish_date":"2017-09-19T00:00:00.000Z","url":"/blog/kubernetes-vault-integration-devops-team","seo_title":"Pods, Tokens, and a Little Glue: Integrating Kubernetes and Vault on the Elastic DevOps Team","content":" On Elastic's Infrastructure team, we are always looking for opportunities to introduce automation and reduce operational burdens. Function-as-a-service solutions such as AWS Lambda have allowed us to simplify operations for some services, but applications that require persistent runtimes have different challenges. There are many potential solutions to this problem: fleets of autoscaled AWS ECS instances, platforms such as Google App Engine or AWS Elastic Beanstalk, and more. Our work on this has led to investigating various container schedulers - and with such a large community and broad feature set, has proven a useful platform for our use case.Adopting a solution as powerful and flexible as Kubernetes entails some non-trivial work to bring it into an existing operations workflow, not the least of which is providing secrets or otherwise sensitive information to running applications within a or . This post will explain our approach to integrating (our chosen secret management solution) with Kubernetes.IntroductionIntegrating Kubernetes and Vault should meet a few requirements: Moreover, applications should be able to migrate to Kubernetes without dramatic changes in order to consume secrets - this is critical to aid in migration from traditional platforms like AWS EC2 to Kubernetes.BackgroundBefore diving into the implementation, there are a few details specific to our solution that are worth highlighting: ImplementationThere are a few steps that come together to achieve the goals outlined in the introduction: first, connecting Kubernetes to Vault:  second, providing a mechanism to expose secrets to running applications:  and third, creating reusable tools to aid in generic solutions for additional applications.Connecting Kubernetes and VaultFortunately, the project provides a well-designed solution to this problem. ensures that tokens are passed to applications securely in-transit, meeting the first requirement for integrating with Vault.Our deployment of kubernetes-vault looks similar to the one outlined in the project's quick start guide, with the exception that Vault is run of Kubernetes as an independent service. This works equally well as running Vault within Kubernetes itself, which is important as a variety of services outside of our Kubernetes cluster also consumes Vault for different use cases.One important consideration when deploying the kubernetes-vault deployment is to ensure that the token passed to the service is . When interacting with Vault as a limited-privilege user, any generated tokens are subject to expiry when their parent token is revoked or expired.Exposing Secrets to ApplicationsLeveraging kubernetes-vault brings us to the point that a JSON-formatted file is available with several values including the Vault token. While consuming this file can be done with a little code for applications, retrofitting existing applications can be challenging.Fortunately, many applications can easily consume environment variables, which provides a convenient way to pass secret values without needing to write them out to a persistent file or in a Pod/Deployment spec. Another tool called can automate this process.To illustrate how this works, consider a for Logstash (which can ). How can we pass an HTTP basic authentication username and password to the to securely index events to an external, secured cluster?First, we define a defining where to reference secrets from within Vault:apiVersion: v1 kind: ConfigMap metadata: name: logstash-secrets data: logstash.secrets: | ELASTICSEARCH_USERNAME=elasticsearch/production#username ELASTICSEARCH_PASSWORD=elasticsearch/production#passwordThis instructs to expose a secret stored in the under the path with the key as the environment variable . This is mounted into the pod at as a de facto directory f","locales":"","title":"Pods, Tokens, and a Little Glue: Integrating Kubernetes and Vault in Elastic Infrastructure"}
{"index":{}}
{"author":"Lauren Johnson","category":"Culture","publish_date":"2017-09-19T00:00:00.000Z","url":"/blog/top-10-elasticon-2017-talks","seo_title":"Top 10 Elastic{ON} 2017 Talks","content":" The fourth annual Elasticsearch user conference is on the horizon, and we can't wait to put more all-star speakers up on stage in 2018. Sifting through CFP applications got us thinking about all-star talks from Elastic{ON} 2017 that have stood the test of time (and playbacks). We'd be pretty jazzed if these popular videos inspire you to or when we open conference registration in October. Adding machine learning features to the Elastic Stack was a big announcement for the 2017 conference (because that's the sort of splash you can expect from Elastic{ON}). This presentation from Sophie Chang and Steve Dodson introduces fast, effective anomaly detection for jaw-dropping amounts of data and shows you how to solve real-world business problems by applying those machine learning capabilities to the Elastic Stack. This talk features not only the incomparable Clinton Gormley (and his famously smooth baritone) but also an in-depth look at 5.x features like cross-cluster search, the Java high-level REST client, and multi-word synonyms. Plus there's a sneak peek at doc values, index sorting, rolling upgrades, and other features coming in 6.0. Kibana creator Rashid Khan gives you a hands-on introduction to Timelion, which he likes to call the “clawing, gnashing, zebra-killing, pluggable time series interface for everything.” Learn about Timelion's expression syntax, multiple manners of munging data, and plugins that can help you do so much more. Monitoring for malicious activity and handling the resulting alerts is vital to the success of Slack's security program. See how they scaled their efforts by creating a reliable logging pipeline and integrating with a communication platform that helps users look at more data by delegating event management directly to the affected individuals. Court Ewing and Jim Goodwin walk you through new data visualizations and other rad Kibana features that came out after 5.0, then highlight some of the most exciting changes still to come, including heatmap, log context, pipeline aggregations, CSV export, and more. Steve Dodson and Tom Veasey dive deep into Elastic machine learning features so you can see how they work for real datasets. Peek under the hood at the unsupervised machine learning techniques Elastic uses to detect anomalies in your time series data. Walmart uncovered the hidden potential of its data to gain insights into customer purchasing patterns and track store performance metrics and holiday analytics. Kevin Conway and Srinivas Singanamalla show you how they leverage X-Pack and the Elastic Stack to create actionable business intelligence as part of the Walmart technology team. A six-pack of Elastic experts walk you through the X-Pack roadmap from developments in management and monitoring for Elasticsearch to security improvements, PDF reports and CSV export options, the future of alerts, and new Graph UI features. Costin Leau offers up a taste of a new Elastic {re}search project and some of the recipes behind it, plus a behind-the-scenes looks at what we're doing to make Elasticsearch more accessible to users, applications, and systems. Get a detailed walkthrough of Tagcloud and heatmap, new visualizations in Kibana 5.2, as well as insight into new visualizations coming down the pipeline, including a behind-the-scenes perspective on the evolving world of visualizations and how it may affect your custom visualization plugins. Did these talks get your creative juices flowing? at our fourth annual user conference and . Oh, and if you're using the Elastic Stack to do good in the world, . We can't wait to see all of you awesome folks on stage at Elastic{ON} 2018. ","locales":"","title":"Top 10 Elastic{ON} 2017 Talks"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2017-09-19T00:00:00.000Z","url":"/blog/brewing-in-beats-performance-improvements","seo_title":"","content":" Welcome to ! With this weekly series, we're keeping you up to date with what's new in Beats, including the latest commits and releases. Last week, we released the second beta release of the Elastic Stack 6.0. Read more details in the about what’s new in the Beats 6.0.0-beta2 release. Performance improvements Steffen spent some time last week benchmarking and profiling the publisher pipeline, which was heavily refactored for 6.0. This has resulted that are tuning the default configuration and are improving on some slow paths, so that Beats 6.0 will generally be faster and consume less CPU than Beats 5.x on similar loads. New community Beat: Cloudwatchmetricbeat collects the metrics from , and stores them in Elasticsearch for further analysis. The user can configure the metrics that they are interested in. This is a companion for , also from the community, that can retrieve data from AWS Cloudwatch Log Groups. Other changes: Repository: elastic/beats Affecting all Beats Changes in master: Changes in 6.0: Filebeat Changes in master: Changes in 6.0: Auditbeat Changes in 6.0: Testing Changes in 5.6: Documentation Changes in master: Changes in 5.6: Changes in 6.0: ","locales":"","title":"Brewing in Beats: Performance improvements"}
{"index":{}}
{"author":"Jim Goodwin","category":"Kurrently in Kibana","publish_date":"2017-09-18T00:00:00.000Z","url":"/blog/keeping-up-with-kibana-2017-9-18","seo_title":"","content":" Welcome to This is a weekly series of posts on new developments in the Kibana project and any related learning resources and events. Elastic Stack 5.6.1 is released, there is an important bug fix for users upgrading from 2.x details here: — elastic (@elastic) ","locales":"","title":"Keeping up with Kibana: This week in Kibana for September 18, 2017"}
{"index":{}}
{"author":"Jim Goodwin","category":"Releases","publish_date":"2017-09-18T00:00:00.000Z","url":"/blog/kibana-5-6-1-released","seo_title":"","content":" Hello, and welcome to the 5.6.1 release of Kibana!   The 5.6.1 release of the Elastic Stack has an important fix for an issue affecting users upgrading to 5.6.0 with indexes created in Elasticsearch 2.x and earlier where Kibana becomes unavailable.  Anyone upgrading a cluster that has indexes created with Elasticsearch 2.x or prior. This does not affect users installing a fresh 5.6 cluster nor those who started their Elastic journey with the 5.x releases. The problem is most visible through Kibana. After attempting to login to Kibana post-upgrade, the following error appears: It may also manifest as the following error: Or if  is in use: In any case, the Elasticsearch cluster itself (including all of your data in Elasticsearch) is not affected and correctly reports itself as in Green status. Upgrading to 5.6.1 is the best solution to the issue. You can read the details of the problem and about mitigation if you are already experiencing the issue in this . If you are an Elastic Cloud user then Elastic Cloud support is working to mitigate this issue for you. Kibana 5.6.1 is available on our and on . Please review the  for rest of the enhancements and bug fixes. ","locales":"","title":"Kibana 5.6.1 released"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2017-09-18T00:00:00.000Z","url":"/blog/elasticsearch-5-6-1-released","seo_title":"Elasticsearch 5.6.1 released","content":" Today we are pleased to announce the release of , based on . Elastisearch 5.6.1 is the latest stable release, and is already available for deployment on , our Elasticsearch-as-a-service platform.Latest stable release in 5.x:Please , try it out, and let us know what you think on Twitter () or in our . You can report any problems on the . ","locales":"","title":"Elasticsearch 5.6.1 released"}
{"index":{}}
{"author":"Asawari Samant","category":"Engineering","publish_date":"2017-09-18T00:00:00.000Z","url":"/blog/simplify-data-ingest-into-elasticsearch-with-modules","seo_title":"","content":" Struggling to collect and analyze data for a particular use case? Read on. The 30 seconds read: Which modules should we build next? . It will take only couple of minutes. We promise. Need a little more context? Read on. The 5 minutes read:Modules were build on the principle that simple things should be simple. They let you go from raw data to dashboards in literally minutes. Or dare I say seconds. With just one command, your data is collected, parsed, enriched and indexed into Elasticsearch, and ready for exploration in prepackaged Kibana dashboards (which also ship with the module). Below is one such example of a prepackaged module dashboard, this one’s from the system module in Metricbeat. A little history lessonThe concept of modules was initially introduced in Metricbeat 5.0 last year, and tried to streamline data to dashboard experience for metrics from the OS and other common services (Redis, Apache, etc) running in your infrastructure. The idea really took off with the addition of modules to Filebeat in version 5.3.0. With Filebeat modules, we really aimed to get the experience to be turn-key from the beginning. Filebeat modules also extended the concept from metrics to common log formats, such as Apache and NGINX access logs. In version 5.6, Logstash entered the modules party with ArcSight and Netflow modules. Going beyond dashboards So, what’s next for modules? More modules for more data sources is an obvious direction. But, we have recently started exploring how modules could further enrich the getting started experience, by going beyond data to dashboard. Why stop at dashboards? Why not data to insight?In 5.6, we tested this concept by adding preconfigured machine learning jobs to the NGINX module in Filebeat. Now, with just one command you not only have NGINX logs visualized in Kibana dashboards, but you also have machine learning jobs (requires X-Pack) that track anomalies on common metrics (such as visitor rate, request rate, etc) in those logs. The journey ahead (and we need your inputs)In the last year, the ecosystem has grown to 40+ modules across , , and . We want to continue down this path by building more modules and expanding this simple getting started experience to more data sources. As we start planning for the 6.x releases, we would love a little help from you, our community, to help prioritize the next wave of modules. to let us know which modules you are currently using and which modules we should build next. It’s a super short survey and should take only a couple of minutes to complete. ","locales":"","title":"A survey on modules, data sources, and ingestion experience"}
{"index":{}}
{"author":"Christian Dahlqvist","category":"Engineering","publish_date":"2017-09-18T00:00:00.000Z","url":"/blog/how-many-shards-should-i-have-in-my-elasticsearch-cluster","seo_title":"","content":" Elasticsearch is a very versatile platform, that supports a variety of use cases, and provides great flexibility around data organisation and replication strategies. This flexibility can however sometimes make it hard to determine up-front how to best organize your data into indices and shards, especially if you are new to the Elastic Stack. While suboptimal choices  will not necessarily cause problems when first starting out, they have the potential to cause performance problems as data volumes grow over time. The more data the cluster holds, the more difficult it also becomes to correct the problem, as reindexing of large amounts of data can sometimes be required.When we come across users that are experiencing performance problems, it is not uncommon that this can be traced back to issues around how data is indexed and number of shards in the cluster. This is especially true for use-cases involving multi-tenancy and/or use of time-based indices. When discussing this with users, either in person at events or meetings or via our , some of the most common questions are “How many shards should I have?” and “How large should my shards be?”.This blog post aims to help you answer these questions and provide practical guidelines for use cases that involve the use of time-based indices, e.g. logging or security analytics, in a single place. What is a shard?Before we start, we need to establish some facts and terminology that we will need in later sections.Data in Elasticsearch is organized into . Each index is made up of one or more shards. Each shard is an instance of a Lucene index, which you can think of as a self-contained search engine that indexes and handles queries for a subset of the data in an Elasticsearch cluster.As data is written to a shard, it is periodically published into new immutable Lucene segments on disk, and it is at this time it becomes available for querying. This is referred to as a refresh. How this works is described in greater detail in .As the number of segments grow, these are periodically consolidated into larger segments. This process is referred to as . As all segments are immutable, this means that the disk space used will typically fluctuate during indexing, as new, merged segments need to be created before the ones they replace can be deleted. Merging can be quite resource intensive, especially with respect to disk I/O.The shard is the unit at which Elasticsearch distributes data around the cluster. The speed at which Elasticsearch can move shards around when rebalancing data, e.g. following a failure, will depend on the size and number of shards as well as network and disk performance.Index by retention periodAs segments are immutable, requires Elasticsearch to first find the existing document, then mark it as deleted and add the updated version. also requires the document to be found and marked as deleted. For this reason, deleted documents , which can consume a lot of system resources.Elasticsearch allows complete indices to be deleted very efficiently directly from the file system, without explicitly having to delete all records individually. This is by far the most efficient way to delete data from Elasticsearch.Are indices and shards not free?For each Elasticsearch index, information about mappings and state is stored in the cluster state. This is kept in memory for fast access. Having a large number of indices in a cluster can therefore result in a large cluster state, especially if mappings are large. This can become slow to update as all updates need to be done through a single thread in order to guarantee consistency before the changes are distributed across the cluster.Each shard has data that need to be kept in memory and use heap space. This includes data structures holding information at the shard level, but also at the segment level in order to define where data reside on disk. The size of the","locales":"","title":"How many shards should I have in my Elasticsearch cluster?"}
{"index":{}}
{"author":"Loek van Gool","category":"Engineering","publish_date":"2017-09-14T00:00:00.000Z","url":"/blog/psd2-architectures-with-the-elastic-stack","seo_title":"","content":" At Elastic, we :heart: APIs because developers love to work with them to get things done. APIs also have the power to change (or disrupt) an industry quickly and decisively, as is the case with The Revised Payment Service Directive (PSD2). APIs make it possible to seemlessly switch from Web browsers to apps, to deploy content to any platform, and to find the best deals among thousands of suppliers. PSD2 sets out to standardize APIs between EU banks and abolish the existing lock-ins that still exist in the industry. Because while financial institutions are closer to the forefront of the innovation curve than almost any other industry, the point can be made that this has not resulted in wide-spread open access to the core banking ecosystems - namely accounts and transactions. PSD2 is a directive from the European Union that will make banks open up access to their, otherwise private, core banking functions in ways that we have not seen before. PSD2 legislation introduces a breadth of opportunity for retail banks, while also introducing new risk. The Elastic Stack plays a vital role in many of the world’s banks today, and that will especially be true for PSD2 architectures. So, banks that operate in the European Union can strategize on three axes: Interactions with our clients indicate that most banks, if not all, opt to go beyond the requirements that PSD2 demands of them to become a single open ecosystem between merchants, banks and users. A simplified architecture of a PSD2-compliant retail banking ecosystem already shows the important role that APIs will play. A big change is that more APIs will have to be opened up to more external parties. Apart from the obvious security concerns, this also means that you will no longer control the usage of your own APIs, other parties will use them as well. Having and sharing actionable data will be the Norm for AISPsAISPs (we’ll repeat: ) add value to customers by ‘knowing it all’. To become an AISP, a bank must have a complete picture of a user’s financial transactions and accounts. On top of that, an AISP should know what the user wants to achieve, what merchants the user likes, all with user consent (GDPR, anyone?). On top of that, AISPs should strive to have the best experiences (that includes user interfaces, alerts, brand image, trustworthiness) to get in a position of advising the user. Luckily, APIs also help out to get data from merchants and ASPSPs into the AISP. Real-time query engines are able to react and predict to users, transactions, and the like. Having offline batch processing is a great way to extract intelligence out of data in some cases, but to get that intelligence online, a fast data store is needed with millisecond response times. European banks have been experimenting with personal finance features on their platforms for years. But they have always been based on the partial picture of the user’s finances, and were arguably not as functional a users have come to expect in recent years. With PSD2, we expect a surge of new personal finance tools that will be completely automated, intelligent and responsive. It requires real-time analytics and natural language processing (NLP) at scale, such as aggregations, fuzzy queries, multi-language, and predictions. PSD2 ArchitecturesBanks already operate using internal APIs that connect modern, scalable front-end applications to core account and payment systems. Typically, the core banking systems are legacy systems that don’t scale effortlessly, so they offload part of their responsibilities to various modern data stores to save cycles on the core systems. PSD2-compliant architectures will have to make those APIs accessible to 3rd parties that are, at best, under the bank’s , not control. This means that the APIs will make or break access to bank’s most basic functions. A Shopping List for PSD2 ArchitecturesA PSD2-compliant architecture requires a","locales":"","title":"PSD2: Modern Banking API Architectures with the Elastic Stack"}
{"index":{}}
{"author":"Alexander Reelsen","category":"Engineering","publish_date":"2017-09-14T00:00:00.000Z","url":"/blog/distributed-watch-execution-elasticsearch-6.0","seo_title":"","content":" Ever since introducing watcher in Elasticsearch 1.x we had one big issue on our roadmap: Making the execution of watches distributed. With the Elasticsearch 6.0 release, we will finally do it. This blog post describes the journey to distributed watch execution.How it all startedEverything should start simple. New features should start simple and become powerful over time, once the simple solution has proven to be useful. This is also how Watcher started. A set of small features, that allows you to execute an input, check if it matches a condition and then optionally have a set of actions being executed. All good so far. But how do we manage those executions? Where are they executed? How do we prevent parallel executions of the same watch? We decided for the simple solution first. Preventing parallel executions in the cluster can be solved easily when you decide to just run on the master node. Preventing parallel executions on a node can be solved with some smart locking. And all of this can run in a dedicated thread pool just for watcher executions.This solution helped us to get off the ground, but came with some drawbacks. First and foremost, we are using the master node for a task that is usually not a master node task. This implies that additional resources are required. You need some memory to hold the watches in memory for execution, you will have higher IO due to the watch execution itself and this node will execute searches and index operations on the local cluster. Those are not typical tasks for a dedicated master node. On top of that this also means there is a scalability limit, as we can only have a single master in our cluster and all the other resources would go unused.So while this solution worked for some time, we knew we had to improve.Laying out a pathOne of the good things of this implementation was the fact, that it is very internal, so the user does not really notice, where a watch is executed or loaded. The user is just putting watches and expects them to be executed. This means, we did not have to do any API changes for this feature.So, how did we approach the problem? Should we introduce some fancy consistent hashing algorithm to scale the watch execution across all the nodes? That might work, but it turns out we already have a great mechanism to distribute data across a cluster, so why not piggyback on that one? This mechanism is called a shard and is basically the unit of scale in Elasticsearch. What if we execute the watches where the data is? So if you have one shard, you would only have one node where watches are executed. If you have 2 shards and 1 replica, watches would be executed across four nodes. Each shard would be responsible for executing 1/th of the data, with being the number of total shards to store watches.By using shards as our base for watch execution we get a few things completely for free.First, we now have data locality. As the execution always happens on the node that is holding a shard, there is no network traffic happening. When a watch execution starts, the watch is always fetched locally and everything happens on that node.Second, scaling out becomes simple. We can scale by adding more primaries or replicas. In the first graphic you would see, that only two nodes are executing watches, by adding a replica, you could make sure that all three nodes are actually executing watches.Third, we can now easily control where the execution of watches should happen, without having to write any code for this. Elasticsearch itself already supports this, thanks to . We could run a command like this to ensure watch shards are only placed on nodes with a certain attributePUT .watches/_settings { \"index.routing.allocation.include.role\": \"watcher\" } That's almost it, folks!While working on this feature we got rid of some locking code, which should improve single node scalability, and reduced memory usage by removing an additional in memor","locales":"","title":"Distributed Watch Execution in Elasticsearch 6.0"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2017-09-13T00:00:00.000Z","url":"/blog/brewing-in-beats-running-auditbeat-side-by-side-with-auditd","seo_title":"","content":" Welcome to ! With this weekly series, we're keeping you up to date with what's new in Beats, including the latest commits and releases. Last week, we released the second beta release of the Elastic Stack 6.0. Read more details in the about what’s new in the Beats 6.0.0-beta2 release. Auditbeat: run side-by-side with auditdStarting with the Linux kernel 3.16, it’s possible to receive the kernel audit logs over a multicast socket. This allows for multiple recipients, which is great because now you can have Auditbeat and the auditd daemon running on the same server. We in go-libaudit and Auditbeat, which will have this feature in 6.0.0-rc1. The feature is enabled by default if the kernel is newer than 3.16. Lower number of shards in default configurationsWe have added a while ago the possibility to change the number of shards and other Elasticsearch mapping templates directly from the Beats configuration files. When we did that, we also changed the number of shards to 1 in the default Metricbeat configuration file, but didn’t change the other Beats. We have  so that the Beats that create events have a default of 3, and the Beats that create metrics have a default of 1. This should result in a lower amount of shards created for a typical installation of a few Beats and default config. The new configuration files will be present in 6.0.0-rc1. Fix: Keep Docker and Kubernetes pod annotations longerIn some cases pod annotations are needed after the container/pod is deleted, for instance when Filebeat is reading the log behind the container. This makes sure we keep the metadata after a pod is gone. By storing access times we ensure that it's available as long as it's being used. Other changesRepository: elastic/beatsAffecting all BeatsChanges in master: Changes in 6.0: MetricbeatChanges in master: Changes in 6.0: PacketbeatChanges in master: FilebeatChanges in master: Changes in 6.0: TestingChanges in master: Changes in 5.5: Changes in 6.0: DocumentationChanges in master: Changes in 5.5: Changes in 5.6: Changes in 6.0: ","locales":"","title":"Brewing in Beats: Running Auditbeat side-by-side with auditd"}
{"index":{}}
{"author":"Thomas Neirynck","category":"Engineering","publish_date":"2017-09-13T00:00:00.000Z","url":"/blog/region-maps-gauge-kibana","seo_title":"","content":" With version 5.5, two new visualization types made their way into Kibana, Region Maps and Gauge charts. Admittingly, our attentive readers may interject that calling them “new” is somewhat overstating it. Indeed, early users of Kibana may recognize the Region Map visualization from v3, and the gauge charts already have a presence as one of the chart types in the experimental Time Series Visual Builder. Nonetheless, these Visualizations deserve some exposure, so without further ado, here they are. (Again!) Region Maps Region Maps, sometimes called Choropleth maps, use a color gradient to symbolize the value of a metric, where lighter colors indicate lower values and darker colors indicate higher values. Linking a terms aggregation to a vector layer These maps are configured by matching the results of a Terms aggregation, to features in a GeoJSON FeatureCollection based on a shared property. The field of the term-aggregation must match a property value in the shape layer. For example, in a map of world countries this field would have a value of country name or an ISO country identifier. In the example below, we are running a term aggregation on the field on a logstash index. This is a two-digit ISO country identifier that can be produced by the GeoIp filter plugin. We are counting the number of requests. This terms-aggregation can now be joined to the world layer. Here we are using the default ‘World Countries’ file that is available from the Elastic Maps Service and match it to that field with the two letter abbreviation field from our Terms aggregation This configuration will produce the following map: If items from the term-aggregation cannot be joined to a feature in the Geojson layer, Kibana will show a warning. These warnings can be turned off by turning off the . Adding your own data layers Kibana comes readily available with layers of world countries and USA states based on data from . Both these layers are served with the , which is a freely available data service hosted by Elastic supporting Kibana’s mapping visualizations. You can have Kibana use your own GeoJSON layers as well. Configure this by linking them in the . Any layer that is reachable on the network over http can be linked. If the files are hosted on a differnet domain than Kibana, this does require that the service hosting these files are CORS enabled. In Kibana, these are then added to the drop down list. It would look something like: When you CORS enable the server, also make sure you echo all request header names submitted by the Kibana application in the header-property of the server’s response. Depending on your server technology (Apache, NGINX, IIS, …), this may require some custom configuration. If you prefer to explicitly list the allowed headers, ensure the header is part of the server’s response. This header is present on every request coming from the Kibana application. The region map is subject to the same zoom limitations as the Coordinate Map. To remove the restriction on the number of zoom levels, install X-Pack Basic for free alongside Kibana and Elasticsearch. Gauge and Goal charts Gauge and Goal charts look very similar, but they are used for different purposes. The Goal chart is used to track your progress toward a fixed goal. Let’s say you are counting your website visitors and you are trying to hit 1M visitors. This is the perfect use case for a Goal chart. A Gauge chart on the other hand is used to track the current value. A well known use case for gauges to show speed or RPM in your car. Using an example from the Elastic world, gauges would be suitable for tracking your server response times. You know something about the value you are tracking like its minimum and maximum: . You can also think of your values in ranges: values within the green range are good, values in the yellow range are still ok and values in the red range are bad. Looking forward to 6.x In the upcoming major v6 release of Kiban","locales":"","title":"Region Maps and Gauges in Kibana"}
{"index":{}}
{"author":"Jim Unger","category":"Engineering","publish_date":"2017-09-13T00:00:00.000Z","url":"/blog/creating-a-threshold-alert-in-elasticsearch-is-simpler-than-ever","seo_title":"","content":" was one of the themes at Elastic{ON} ‘17, our annual user conference where we connect with our users. During and prior to the user conference, we received many requests for a simple and easy to use UI to create alerts. As it turns out, creating a single UI to work effectively for all types of alerts is pretty hard. For example, a UI that can create an alert when the average CPU utilization goes over 50% looked pretty different from a UI that can create an alert when there are many concurrent logins from the same IP address. Go use this feature and check out our for more details. As you dive into the 6.0 preview releases, we'd love to hear your feedback as part of our . Your insights make our software better! ","locales":"","title":"Creating a threshold alert in Elasticsearch is simpler than ever."}
{"index":{}}
{"author":"Luca Cavanna","category":"Engineering","publish_date":"2017-09-13T00:00:00.000Z","url":"/blog/the-elasticsearch-java-high-level-rest-client-is-out","seo_title":"","content":" Part of the success of Elasticsearch has probably been its large number of supported language clients right from the start. Its powerful REST API can be accessed using HTTP and JSON, which makes it easy to integrate with using any language that provides those capabilities.Nevertheless, there hasn’t been an official Elastic Java REST client for a long time. This was partly due to the existence of the (also sometimes called the TransportClient), that doesn’t communicate with the cluster via REST but via a binary protocol. However, there are several drawbacks of this client, like security aspects or tighter coupling to JVM and cluster versions. For more details, we recommend reading the blog post, where we also described our future plans around the Java clients for Elasticsearch.Enter the Low-Level Client As a first step to improve the situation, the low-level REST client was released with version 5.0 of the Elastic Stack. It solved the compatibility and dependency problems by providing strict access via the REST API. However, this came at the price of losing all the comforts of the Java API, which people were used to when using the transport client, like search request and query builders and response objects that are convenient to use in Java. That’s why next we started working on the high-level REST client: a client that sends requests using the low-level client, but that is also able to accept higher level request objects and returns response objects similar to those the native client already uses.Let's Level UpRight after the release of version 5.0 we started working on the high-level client, with the primary goal to make migration from the current transport client easy and to initially support the core APIs index, bulk, get, delete and search. It soon became clear that in order to progress and to ease migration at the same time, we would still need the current search and query builders and support the current response objects. The work then mostly involved implementing parsing Elasticsearch REST responses back to Java objects, a tedious issue if you consider the richness especially of the search response with all its varying aggregation responses. One of the gives a bit of an insight into the work that has gone into this.For this reason, the high-level client still depends on Elasticsearch, like the Java API does today. This may not be ideal, as it still ties users of the client to depend on a certain version of Elasticsearch, but this decision allows users to migrate away more easily from the transport client. We would like to get rid of this direct dependency in the future, but since this is a separate long-term project, we didn’t want this to affect the timing of the client’s first release. The first release of the Java high-level REST client went out with the of the Elastic Stack. It allows you to communicate via REST with Elasticsearch nodes running 6.0.0-beta1, but it is not fully compatible with 5.x nodes. The of the Elastic Stack recently went out and includes the 5.6.0 version of the high-level client, which works against Elasticsearch nodes running version 5.6. It is unfortunately not compatible with previous 5.x versions of Elasticsearch. What’s Up Next?The high-level client will eventually replace the transport client in the future, hence we invite all our Java users to try it out and migrate to it if possible. The current release supports the following : index, get, delete, update, bulk, search, scroll and clear scroll. Support for other APIs will be added step by step, also according to the feedback received from the community. As described in our , it is possible meanwhile to use the low-level REST client to perform requests that are not yet supported by the high-level one. The documentation for the high-level REST client can be found , and its javadoc . We are looking forward to your feedback! Even better: become an and receive a spec","locales":"","title":"The Elasticsearch Java High-Level Rest Client is Out"}
{"index":{}}
{"author":"Alvin Chen","category":"News","publish_date":"2017-09-12T00:00:00.000Z","url":"/blog/elasticsearch-arcsight-integration","seo_title":"","content":" IntroductionIf you've been following our engineering blogs for the past 9 months or so (see Editor's note at the end for links), you'll recall that we've posted a half dozen examples of how to use the Elastic Stack to augment the ArcSight SIEM by taking a copy of the ArcSight CEF-formatted event data stream, ingesting it into the Elastic Stack, and then searching, analyzing, visualizing, and alerting on these events using various Elastic products and features. We've had such a good response to the blog series, that we decided to make the integration even better and easier to get started. Today, we are very excited to introduce the Logstash ArcSight module that simplifies the Elastic Stack ArcSight integration down to one command. Run one command and start exploring your ArcSight events in Kibana in literally minutes. Before we dig into the details of this new Logstash module, let's take a look at why this integration with ArcSight makes sense. Security Analytics is a Growing Use Case for the Elastic StackThe security threat landscape has fundamentally changed in the past 5 years. While traditional methods usually involved searching for known bad activity in their log data, today's analysts often need to interactively investigate wherever the data leads them in real time. Additionally, the volumes of security-relevant log data that used to be measured in TB per day are now growing to PB per day. Depending on their specific role, security teams may need tools and platforms optimized for their particular workloads. Factors affecting platform requirements include: Necessarily, there is not a one-size-fits-all solution for log-based security analytics. Driven by these factors, security analytics has become one of the fastest growing use cases for the Elastic Stack. The Elastic Stack is integrated in popular open-source security projects like , , and , and is featured in security training courses. The recent introduction of machine learning capabilities to the Elastic Stack (via X-Pack) are enabling even more automated detection of attack behaviors for security analysts, as described in this .  Good Security Analytics Requires “Good Data”Security operations teams have learned over the years that their ability to detect threats and respond to cyber incidents depends upon having “good data” to monitor and analyze. Good data means that the security events generated by systems and devices have been normalized into a common format with common field names, and filtered so that visualization, correlation, and alerting can be implemented efficiently across multiple data sources. Easier said than done! Collecting “good” security data is hard. In fact, , security industry analyst Anton Chuvakin cites “dirty data” and “trouble with collecting data” as top reasons for failure in security analytics or security data lake initiatives. He comments on the difficulty of the data collection problem, noting that SIEM vendors have spent 10+ years debugging and innovating their “data collectors” for a reason. Users of the Elastic Stack already address the “good data” challenge with , our suite of specialized open source data shippers. They install as lightweight agents and send data from hundreds or thousands of machines to Logstash or Elasticsearch. We wanted to make it easy for ArcSight customers who have deployed a security event collection layer using ArcSight Smart Connectors (ArcSight supports connectors for 350-400 devices/systems) to get their security event data into the Elastic Stack to enable new ways to search, analyze, and visualize their data. Last year, ArcSight introduced the ArcSight Data Platform (ADP) which embraces an open architecture and lets ArcSight users send their CEF-formatted security event data to a variety of destinations beyond the ArcSight Logger and ESM SIEM products. The ADP also added a queuing system, called the ArcSight Event Broker, which provides a mec","locales":"","title":"Introducing the Elastic Stack ArcSight Integration"}
{"index":{}}
{"author":"Nikolaj Richers","category":"Engineering","publish_date":"2017-09-12T00:00:00.000Z","url":"/blog/live-upgrading-elastic-cloud-enterprise","seo_title":"","content":" Welcome to the next step in our story of bringing you great new features! Where previous versions of Elastic Cloud Enterprise either required you to reinstall or included only new Elastic Stack packs, you can now live upgrade a production ECE installation to a new version with little or no downtime. What is “little or no downtime?” If your ECE installation includes more than one proxy and if you are using a load balancer as recommended, there is no downtime. (Installations with a single proxy or no load balancer will experience a short outage as each proxy is upgraded.) You simply initiate the upgrade process and let ECE upgrade itself: ECE will replace the containers with its own software and dependencies one by one, on all hosts. Upgrading ECE does not touch any of the containers that run your Elasticsearch clusters and Kibana instances. Your applications continue to perform as expected and your users won’t know that you are replacing their spaceship piece by piece in mid-flight. We like to call this operational happiness.There are two ideas behind our new live- upgrade functionality: to make the upgrade process safe and to make it simple. It’s all designed to just work. Before you start upgrading, we recommend that you . There is some small print you should know around free disk space and things you shouldn’t be doing whilst you upgrade. (Don’t add or remove hosts during the upgrade process, for example.) Ready? To upgrade your installation: 1. Download and run the latest installation script for Elastic Cloud Enterprise 1.0.2 with the action from a host that holds the director role: bash <(curl -fsSL https://download.elastic.co/cloud/elastic-cloud-enterprise.sh) upgrade You can follow along whilst each container for ECE is upgraded on the hosts that are part of your installation. After the upgrade process completes, that ship with ECE 1.0.2 to get every last little bit of the new version. To learn more about how the upgrade process works, see . And if you are curious to know what else is in version 1.0.2, take a look at our . ","locales":"","title":"Live Upgrading Elastic Cloud Enterprise"}
{"index":{}}
{"author":"Jim Goodwin","category":"Releases","publish_date":"2017-09-11T00:00:00.000Z","url":"/blog/kibana-5-6-0-and-5-5-3-released","seo_title":"","content":" Hello, and welcome to the 5.6.0 and 5.5.3 release of Kibana!   The 5.6.0 release is an important one because it contains the features to allow upgrading to 6.0 including backward and forward compatibility and the Upgrade Assistant. We didn't include any significant new features in 5.6.0 as our focus was on upgrading and compatibility. As part of X-Pack Basic (free), we are providing an upgrade assistant to aid in migrating to version 6 of the Elastic stack. The upgrade assistant identifies breaking changes to index and cluster settings and allows you to enable deprecation logging. Internal indices used by the stack will need to be reindexed, in addition to any indices created in 2.x. The assistant provides a one-click solution to these migrations. After resolving deprecation issues and migrating indices, you are ready to upgrade your stack. Elasticsearch 5.6 supports rolling upgrades to 6.x, and Kibana 5.6 can run in tandem with an Elasticsearch cluster undergoing a rolling upgrade. Once the full Elasticsearch cluster has been upgraded, you can upgrade Kibana to 6.x. Kibana 5.6.0 is available on our and on . Please review the  for rest of the enhancements and bug fixes. The 5.5.3 release contains a number of bug fixes and enhancements. Kibana 5.5.3 is available on our   under \"past releases\" and on .  You should read the for information about the bug fixes and enhancements. ","locales":"","title":"Kibana 5.6.0 and 5.5.3 released"}
{"index":{}}
{"author":"Nikolaj Richers","category":"Releases","publish_date":"2017-09-12T00:00:00.000Z","url":"/blog/elastic-cloud-enterprise-1-0-2-released","seo_title":"","content":" We are happy to announce the release of ECE 1.0.2! This new version includes: - Our main improvement in this version. We now support live upgrading to new ECE versions without downtime for managed clusters. The upgrade process is designed to be simple and safe, and updates all hosts that are part of your Elastic Cloud Enterprise installation to the latest version of ECE with a single command. Upgrading replaces the containers that ECE itself requires to run without touching your Elasticsearch clusters and Kibana instances. To learn more, see our blog post and our . - ECE now ships with Elasticsearch 2.4.6 and Kibana 4.6.5, along with the previously included Elasticsearch and Kibana version 5.4.1. To learn more about these versions of the Elastic Stack, see: Additional are also available for use with Elastic Cloud Enterprise. - A number of bug fixes improve the installation and stability of ECE. For example: Existing ECE installations on versions 1.0.1 and 1.0.0 can be to version 1.0.2. ","locales":"","title":"Elastic Cloud Enterprise 1.0.2 Released"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2017-09-11T00:00:00.000Z","url":"/blog/elasticsearch-5-6-0-released","seo_title":"Elasticsearch 5.6.0 released","content":" Today we are pleased to announce the release of , based on . This is the latest stable release, and is already available for deployment on , our Elasticsearch-as-a-service platform.Latest stable release in 5.x:You can read about all the changes in the release notes linked above, but there are a few changes which are worth highlighting: ","locales":"","title":"Elasticsearch 5.6.0 released"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2017-09-11T00:00:00.000Z","url":"/blog/elasticsearch-5-5-3-released","seo_title":"Elasticsearch 5.5.3 released","content":" Today we are pleased to announce the release of , based on . Elastisearch 5.5.3 is the latest stable release, and is already available for deployment on , our Elasticsearch-as-a-service platform.Latest stable release in 5.x:Please , try it out, and let us know what you think on Twitter () or in our . You can report any problems on the . ","locales":"","title":"Elasticsearch 5.5.3 released"}
{"index":{}}
{"author":"Suyog Rao","category":"Releases","publish_date":"2017-09-11T00:00:00.000Z","url":"/blog/logstash-5-6-0-released","seo_title":"","content":" We are excited to announce that Logstash 5.6.0 has been released! You can find the binaries on our and the release notes . Logstash 5.6.0 is stacked with great features and important bug fixes. Here are the highlights: Introducing Logstash modulesMake way for Logstash modules! Modules contain pre-packaged Logstash pipelines, Kibana dashboards and other metadata files to ease the set up of the Elastic Stack for certain data sources. The goal of these modules are to provide an end-to-end, getting started experience for users to start exploring data sources within minutes. This idea is based on the existing module feature in Filebeat and we've provided a in Logstash as well. We are shipping two modules in the 5.6 release: bin/logstash-plugin install x-pack bin/logstash --modules arcsight --setup -M \"arcsight.var.inputs=smartconnector,eventbroker\" bin/logstash --modules netflow --setupupporting string escapes in configurationOne of the oldest in LS — first reported in 2013 — is shipping with 5.6. Before this change, providing escaping quotes and control characters was not possible in a Logstash config. For example, \"\\n\" was literally a backslash and lowercase n in the config — as a result, you couldn't use mutate filter to split strings with “\\n”. Escape sequences such as this are now supported in the configuration. To keep backward compatibility with existing configurations, a feature flag ( is required to enable this in your Logstash deployment () Performance improvementsWe spend significant time investigating performance issues and hot-spots in our pipeline. With these tests, we identified that since 5.4, the metrics infrastructure had added to the throughput, in some cases up to 40%! One reason for this is that we're simply using the stats more with the introduction of the monitoring UI and the plugins recording more stats. In this release, we're bringing the performance back in line with pre-5.4 release. Overall, in this release, there is lower load average, less GC and higher throughput when running Logstash. GeoIP FilterThis release adds support for the free GeoIPLite2-ASN database from Maxmind to be able to look up ASN data out of the box. FeedbackPlease , try it out, and let us know what you think on Twitter () or in our . You can report any problems on the ","locales":"","title":"Logstash 5.6.0 released"}
{"index":{}}
{"author":"Monica Sarbu","category":"Releases","publish_date":"2017-09-11T00:00:00.000Z","url":"/blog/beats-5-6-0-released","seo_title":"","content":" Today we are pleased to announce the  Beats 5.6 release, the latest stable release of the 5.x series. Before going through the highlights, let’s get those handy links out of the way: Load Machine Learning jobs from Filebeat modulesFilebeat 5.6 release comes with the support to load Machine Learning (ML) job configurations directly from the Filebeat modules. This means that besides Kibana dashboards and Elasticsearch Ingest Node pipelines, Filebeat modules can also contain ML configurations to be used for anomaly detection. As part of this release, Filebeat comes with five ML job configurations which were already tuned to work perfectly with the data collected by the Nginx module. If you have Xpack installed and ML enabled, simply run:filebeat -e -modules=nginx -setup And you will see the following job configurations created for you: After you let Filebeat ingest some data, simply click the Run button on the ML jobs, and : anomaly detection for your access logs. And that’s not all! For any incident detected by ML, you can drill down in dedicated Kibana dashboards which are ready made for you to investigate these incidents. Preparing for a smooth transition to 6.0If you are on a 5.x version of Beats, you will need to upgrade to 5.6 before making the jump to 6.0. This is because we’ve made some changes to 5.6 to smoothen the upgrade experience. Notably, the setting is no longer disabled in the Beats 5.6 mapping templates. This is because Elasticsearch is removing support for the field in 6.0, and having settings for it in the template is problematic at upgrade time. Kibana 6.0 will use a new internal format and a new API for importing and exporting dashboards. To make sure Beats 5.6 will work well with Kibana 6.0, we added support to import the dashboards via the new API in this release already. For this, you can run the `import_dashboards` script with the flag: ./scripts/import_dashboards -kibana http://localhost:5601 If Kibana 5.x is used, the dashboards are imported directly into Elasticsearch. ./scripts/import_dashboards FeedbackIf you want to make use of the new features added in Beats 5.6.0, please , install it, and let us know what you think on Twitter () or in our . ","locales":"","title":"Beats 5.6.0 released"}
{"index":{}}
{"author":"Tyler Hannan","category":"Releases","publish_date":"2017-09-11T00:00:00.000Z","url":"/blog/elastic-stack-5-6-0-released","seo_title":"","content":" 5.6 is here! In the recent past, we’ve made quite a bit of noise about 6.0 (tl: dr - , give use feedback, join the . During this release cycle, we have not forgotten about the 5.x series. In fact, 5.6.0 is an important milestone and should be treated as the ‘Upgrade Release’. If you want to perform a rolling restart from 5.x to 6.x, you will need to update your Elasticsearch cluster to 5.6.0 or higher first. Add a migration assistant to the mix, and 5.6.0 might be your favourite release (at least until 6.0.0 is available). The hits don’t stop there. We began our modular journey with the introduction of Beats modules. This same paradigm is replicated in Logstash with both Arcsight and Netflow modules. And, we want to understand how modules impact your user experience, so and let us know. As per usual, it is available – right now – on our favourite hosted Elasticsearch service. Elasticsearch For more detailed information, and many other features, peruse the Elasticsearch . Kibana ‘Visualize’ all the features, and ‘Discover’ more information in the . Logstash For more information, grok the . Beats We don’t ‘let the beat drop’ but we drop the updates in a . Get It Now! ","locales":"ja-jp","title":"Elastic Stack 5.6.0 Released"}
{"index":{}}
{"author":"Jongmin Kim (KR)","category":"Engineering","publish_date":"2017-09-07T00:00:00.000Z","url":"/blog/ship-metrics-with-metricbeat-to-hosted-elasticsearch-elastic-cloud","seo_title":"Monitoring Your System with Metricbeat and Elastic Cloud","content":" Elastic Cloud is the official hosted Elasticsearch and Kibana offering, from the creators of Elasticsearch, Kibana, Beats, and Logstash. This is not the same as Amazon's AWS Elasticsearch Service. You can see details in this . Elastic Cloud offers a 14-day free trial. Your email address is all you need to sign up, no credit card is required.In this post, we will see how to: 1. Create an Elastic Cloud accountGo to the Elastic Cloud web page. Start an . You can skip to step 2 if you have already have an Elastic Cloud account.  Enter your email address and click the  button. You will receive an email verification message in your inbox. Click the  button to continue.    Set the password for your Elastic Cloud account. 2. Start an Elasticsearch cluster and Kibana instanceOnce you have created your password, it will automatically take you to the Elastic Cloud UI. You can get back later by going to . Click on  to create your first cluster. Choose the cluster size and region then click .  It will create and provision your Elasticsearch cluster. Once you have finished creating your Elasticsearch cluster, it will pop up the generated password for the superuser, .    If you lose the password, you can reset it from the Security menu in the Elastic Cloud console. Navigate to the  menu. You will see Elasticsearch and Kibana access URLs.   3. Create a new superuserClick on the Kibana URL, which will take you to the Kibana log in screen. Use \"elastic\" for the user, along with the automatically generated password (which you copied somewhere safe) to log in.  Elastic Cloud includes several X-Pack features, such as security and monitoring. If you prefer to use a more familiar username and password, you can create a new superuser using the Role Based Access Control functionality.  Visit the  tab, and select  in the Security section:   Click the  button.   Fill in the username, password, full name, email, enter  for the Role, and save it. This will create your favorite new superuser account Try to logout and log back in with your new account to make sure everything works ok. 4. Configure and run MetricbeatNow that we have Elasticsearch and Kibana configured, let's send some data its way. In this tutorial, we will use  for this purpose. Metricbeat collects all kinds of metrics from your system and ships them to your Elasticsearch or Logstash instance. It runs as a Binary file, so you will have to download the correct file for your operating system. Visit the to , download the correct binary for your OS, and then unpack it. Once unpacked, you will find several files in the metricbeat directory. The file  (or  for Windows OS) will start your Metricbeat. The default configuration is set to send data to Elasticsearch running at  by default, so we will have to change the configuration to send it to our Elastic Cloud cluster. Open  with your favorite code editor. Replace  with our new Elasticsearch cluster URL.  output.elasticsearch: #Array of hosts to connect to. hosts: [\"localhost:9200\"] Put your newly created username and password (if you skipped the user creation you can use elastic and the auto-generated password). username: \"<user id>\" password: \"<password>\" Be careful with yaml grammar, you have to make right space in each line. username:  password: should be indented by 2 spaces , while the rest should start with no space.  Put your newly created . If you skipped user creation, you can use elastic and the auto generated password for this step.Save the file. Then run metricbeat, supplying the option, which will tell the system to load the default metrics dashboards to Kibana. TIP: if you add the  option when running Metricbeat you can see the logs while running. 5. Check the Metricbeat dashboardsWe added the  option when we started","locales":"ko-kr","title":"Monitor Your System with Metricbeat and Elastic Cloud"}
{"index":{}}
{"author":"Petr Dobrovolný","category":"User Stories","publish_date":"2017-09-11T00:00:00.000Z","url":"/blog/using-elastic-graph-to-evaluate-the-imec-ic-link-partner-network","seo_title":"","content":" In this blog, we would like to share our experiences with the Elastic Stack including the X-Pack extension used for our business analysis activities. Next to many other interesting results, we were able to deliver a unique analysis of our business partner network as a social network and quantify it by degree of separation parameter expressing interaction intensity among our partners. Imec.IC-link – The Semiconductor Manufacturing Division at ImecImec is a world-leading nano-electronics R&D center, headquartered in Leuven Belgium. Currently, there are about 3,500 researchers working on a broad variety of topics, ranging from sub-10nm complementary metal-oxide-semiconductor (CMOS), wearable healthcare devices, image sensors and vision systems, solar cells, wireless communication, to gallium nitride power electronics. The imec.IC-link division is the semiconductor manufacturing division of imec. We help innovators, entrepreneurs, and universities realize their ideas in silicon. For our customers, we enable access to leading-edge foundries for volumes ranging from small-scale Multi-Project Wafer (MPW) prototyping runs to full volume production. We also provide a wide range of related services across the entire application-specific integrated circuit (ASIC) value chain. Currently, we are serving more than 300 small and medium-sized enterprises (SMEs) and about 700 universities and every year we support the production of more than 500 integrated circuits. Why Elastic?During 20 years of history, we gathered a huge amount of technology and business data. It includes over three TB of technology data related to all foundry semiconductor manufacturing and related services. The latest, complex technologies further accelerate the size of datasets that need to be maintained. In addition, our business activities generate a substantial amount of data that is continuously increasing alongside a growing partner network. The standard imec infrastructure for storing, maintaining, and processing financial and CRM information is based on a robust SQL implementation and is well-suited to satisfy our requirements. However, our needs for flexible, predictive, and prescriptive analytics exploiting the merge of technology and business data is going far beyond the possibilities of the current imec infrastructure. We decided to set up a program exploring the growing potential of our business with the help of business analytics generated by a new software infrastructure capable of processing the large amount of heterogenous data we have to maintain. Combining existing business/financial data with technology data could bring more insights and generate analysis that couldn’t be retrieved before. However, the business/financial data are supplied in the form of classical SQL tabular datasets, while technology data appears in a large variety of different forms. These include text documents, tables, presentations, even e-mail messages and many others, usually in proprietary data formats. Converting this data into some structured, tabular form would be very tedious and cumbersome and, for some of them, like free form text documents, even impossible. We had to find a way to store and search through this diverse data, which is why we built a solution based on the Elastic Stack, combining a distributed document storage with a distributed search and analytics engine. The core of our new infrastructure named imec.IC-link Business Analytics bot (BA bot) is shown below. Figure 1: Business Analytics (BA) bot set up in imec.IC-linkThe infrastructure was designed with heterogenous data in mind. Data of various formats enter the Elastic Stack from several different sources. SQL-like datasets are first stored in an intermediate SQL storage and then transformed and sent to Elasticsearch using Logstash. Logstash is a data processing pipeline that can input, transform, and then send","locales":"","title":"Using Elastic Graph to Evaluate the imec.IC-link Partner Network"}
{"index":{}}
{"author":"Daniel Palay","category":"News","publish_date":"2017-09-06T00:00:00.000Z","url":"/blog/elasticon-2018-call-for-presentations-your-story-our-stage-you-in","seo_title":"Elastic{ON} 2018 Call for Presentations: Your Story. Our Stage. You In?","content":" In 174 days, the worldwide Elastic community will gather in San Francisco for Elastic{ON} 2018, and once again our guest speakers will be front and center. Since 2015, we’ve heard stories from big companies (did you know Optum uses the Elastic Stack as the backbone of their Security Big Data Lake?) and small companies (Giant Oak is using the Elastic Stack to bring human traffickers to justice). We’ve had folks talk about what it’s like to analyze metric and log data coming back from the Mars Rover and what goes into analyzing the effectiveness of every notification sent out by Netflix. And now we are ready to find our next wave of speakers for Elastic{ON} 2018. While the application process is very similar to past years, we’re making some changes too. In light of feedback from past speakers and applicants, we’ve spent several months revamping the tools we offer to help applicants craft their submissions. Here is a sampling of the changes and new features we’re launching with the 2018 CFP: Those are some of the biggest changes, but there are a few more that we’ll be working on through the CFP application period, so stay tuned. But for now, if you think you have a great story to tell about what you are up to at work, at home, or just for fun, ","locales":"","title":"Elastic{ON} 2018 Call for Presentations: Your Story. Our Stage. You In?"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2017-09-04T00:00:00.000Z","url":"/blog/brewing-in-beats-plugin-support-for-filebeat-prospectors","seo_title":"","content":" Welcome to ! With this weekly series, we're keeping you up to date with what's new in Beats, including the latest commits and releases. Last week, we released the second beta release of the Elastic Stack 6.0. Read more details in the about what’s new in the Beats 6.0.0-beta2 release. Filebeat: Prospectors now have a plugin interface The Filebeat prospectors are responsible for the actual data collection. The most used one is , which can tail rotating files, but we also have , and we plan more. This makes the prospectors be pluggable internally, just like the libbeat outputs, processors, Metricbeat modules, a.s.o. This means that it is now easier to create a Beat that uses “Filebeat as a library” and that adds it’s custom prospector. This change is currently present only in the master branch. Metricbeat: use less memory in the Windows perfmon module The Windows perfmon module was allocating more memory than it’s needed to receive the performance counters. The memory wasn’t leaked, but just remained unused until it’s cleaned up by GC later.  The will be available in the next 6.0 release. Other changes Repository: elastic/beats Affecting all Beats Changes in master: Changes in 6.0: Metricbeat Changes in master: Changes in 6.0: Packetbeat Changes in master: Changes in 6.0: Filebeat Changes in master: Testing Changes in master: Changes in 6.0: Packaging Changes in master: Changes in 6.0: Documentation Changes in master: Changes in 5.4: Changes in 5.5: Changes in 6.0: ","locales":"","title":"Brewing in Beats: Plugin support for Filebeat prospectors"}
{"index":{}}
{"author":"Tyler Hannan","category":"Releases","publish_date":"2017-08-31T00:00:00.000Z","url":"/blog/elastic-stack-6-0-0-beta2-released","seo_title":"","content":" 6.0.0-beta2 is available today! Before you get too excited, keep in mind that this is still a beta so don’t put it into production. There is no guarantee that any of the 6.0.0-beta versions will be compatible with other pre-releases, or the 6.0.0 GA. We strongly recommend that you keep this far, far away from production. And, since it is an beta, it is not yet available on Elastic Cloud. During the 5.0 release, we introduced the Elastic Pioneer Program and are continuing the with the 6.0 preview releases. Download the preview release, provide feedback, become a Pioneer. Elasticsearch For more detailed information, peruse the Elasticsearch . Kibana The road to 6.0 GA continues in Kibana with optimization, bug fixes, and more. Learn more information in the . Logstash For more information, grok the . Beats We don’t ‘let the beat drop’ but we drop the updates in a . Get It Now! ","locales":"ja-jp","title":"Elastic Stack 6.0.0-beta2 released"}
{"index":{}}
{"author":"Dale McDiarmid","category":"Engineering","publish_date":"2017-09-05T00:00:00.000Z","url":"/blog/a-full-stack-in-one-command","seo_title":"Launch Elasticsearch, Kibana, Beats, and Logstash in One Comman","content":" ConceptAt Elastic, we are constantly looking for ways to make it easy for new users to experience the magic of the Elastic Stack. How can we shorten the time from \"I have heard about this Elasticsearch thing\" to \"Oh, drill downs in Kibana are so amazing\"? During the recent reorganization of our (contributions always welcome!!), we updated the legacy Docker examples to reflect the publication of our . And then we had a whacky idea. Can we provide examples for our new users to experience the full stack in a single command? One command. Uno. Now, if you have been following the evolution of the Elastic Stack, you would know that recent releases have focused on simplifying the getting started experience. They build on the idea that simple things should be simple. and are both steps in that direction, providing both the necessary ingest pipelines for parsing data, as well as supporting dashboards for common data sources. We wanted to simplify this even further for new users in the exploratory mode looking to simply \"get a feel\" for the capabilities of the stack. Remember, all down to one command. TechnologyThe above restrictions were relevant when deciding on the appropriate technology for this problem. Despite all the recent developments in orchestration tooling, we decided still represents the easiest way of formulating a full stack example targeted at a single machine. Compose is a tool for defining and orchestrating multiple Docker containers to form an application. The containers, and their respective interactions, are largely defined through YAML files. These YAML files can be executed by Compose with a single command, including the initialization and startup all of the containers defined. ArchitectureOn confirming the technology, we had to decide what specifically to include.  Ideally, the Compose example would simply deploy a complete stack.  On closer inspection this aim was a little unrealistic. Our Beats modules, especially and , have grown rapidly, allowing a wide range of technologies to be monitored.  For now, we have therefore focused on deploying a range of Beats modules only, whilst ensuring appropriate data sources are available and automatically ingested to populate any dashboards. We will, however, update the example on the release of Logstash modules in 5.6. We settled on the following architecture which captures and populates as much data as possible: As illustrated above, as well as starting containers for Elasticsearch, Kibana and each of our Beats, we spin up instances of NGINX, Apache2 and MySQL.  These provide interfaces for Metricbeat modules (, , ) to monitor, as well as generating logs that can be consumed by equivalent modules in Filebeat (, , ). Furthermore, with some careful bind-mounting of local filesystem locations, Metricbeat can be used to monitor both the host's system statistics (via the ) and the Docker containers themselves (via the ).  Filebeat can additionally be used to collect and ingest the host's system logs using its equivalent of the , as well as the Docker generated as a result of the containers sending their output to stdout. We use Packetbeat to collect and monitor any DNS, HTTP, or other layer-7 traffic traffic occurring on the host, including MySQL transaction data. Although not illustrated above (to avoid a spider web of connecting lines), Heartbeat monitors all other containers via ICMP,  performs health checks against Kibana, Elasticsearch, Apache2 and NGINX over HTTP, and against MySQL through a raw TCP socket connection. All of the above provides a fairly comprehensive set of monitoring (and duplication for purposes of example), for an architecture you might deploy, whilst maximising the number of modules deployed and dashboards populated. A full list of the dashboards for which data will be available is listed . Deploying more modules would unfortunately require a prohibitive number of co","locales":"","title":"A Full Stack in One Command"}
{"index":{}}
{"author":"Suyog Rao","category":"Releases","publish_date":"2017-08-31T00:00:00.000Z","url":"/blog/logstash-6-0-0-beta2-released","seo_title":"","content":" We are excited to announce the release of beta2 for Logstash 6.0.0! As always, your feedback is super important to us, so please download this release and provide us any feedback you have! Talking about feedback, we hope you are familiar with our for pre-releases! You could win plenty of swag, and even a ticket to ElasticON 2018 when you provide feedbackImprovements to Pipeline VisualizationWe introduced graph based visualization for Logstash pipelines in . Based on user feedback, we've made a number of usability improvements in this release:We now inform users about the lack of explicit plugin ID in their pipeline configuration. Plugin IDs are semantic names users can add as a label to each of their plugin configuration. Having a unique, meaningful ID for each plugin section makes it easy to troubleshoot in the UI. Imagine a configuration which has multiple grok plugins or multiple ES outputs. The pipeline UI will show a specific grok is slow, but having a meaningful label makes it even more effective to zoom in on the right grok pattern.We now explicitly label the conditionals edges as “T” or “F” to indicate true and false branch in the data flow. Previously, we just used different colors — yellow and blue.FeedbackPlease , try it out, and let us know what you think on Twitter (@elastic) or in our forum. You can report any problems on the . ","locales":"","title":"Logstash 6.0.0-beta2 released"}
{"index":{}}
{"author":"Monica Sarbu","category":"Releases","publish_date":"2017-08-31T00:00:00.000Z","url":"/blog/beats-6-0-0-beta2-released","seo_title":"","content":" Today we are excited to announce the Beats 6.0.0-beta2 release. As we are getting closer to the GA of 6.0, this release is primarily about bug fixes, but there are also a few new features. Before going through the highlights, let’s get those handy links out of the way: More consistency in Beats dashboardsAs more and more dashboards are shipped with each Beat, it becomes challenging to identify which dashboard or visualization comes from which module. So, in the Beta2 release, we renamed the dashboards, searches, and visualisations of Filebeat and Metricbeat to follow a set of common naming guidelines: the dashboards start with , and the visualizations and searches end with the same identifier. We plan to extend this naming practice across all dashboards created by Elastic products. Configurable index name in dashboardsThe dashboards that are shipped with each Beat are correlated with the name of the index, where the Beat’s data is stored in Elasticsearch. In case you are you using a different index than the default , then you would need to adjust the dashboards before using them. Starting with the Beta2 release, this step is much easier as you can configure the index name when loading the Beats dashboards. ./metricbeat setup -E setup.dashboards.index=testbeat-* -e where is the index name where Metricbeat is storing its data. Add support for newer Kafka versionsThe Beta2 release comes with support for newer versions of Kafka, including the latest 0.11.0.0 release. This is a result of upgrading the client library that Beats is using to connect to Kafka. The new Kafka versions are supported by the Kafka output as well as by the Metricbeat Kafka module. This release also brings support for lz4 compression to the Kafka output. Known issuesThere's a couple of issues that we have discovered and fixed but are still present in Beta2. Don't worry, we already have solutions for them and they will be closed by the time 6.0 become GA. If you are affected by any of these bugs, look in the associated tickets for workarounds. Become a PioneerA big “Thank You” to everyone that has tried the alpha or beta releases and posted issues or provided feedback. We’d like to also remind you that if you post a valid, non-duplicate bug report during the alpha/beta period against any of the Elastic stack projects, you are entitled to a special gift package. You can find more details about the Pioneer Program in the . ","locales":"","title":"Beats 6.0.0-beta2 released"}
{"index":{}}
{"author":"Tim Roes","category":"Engineering","publish_date":"2017-08-30T00:00:00.000Z","url":"/blog/making-kibana-accessible","seo_title":"","content":" Inclusivity and diversity are primary ingredients of Elastic’s culture. Those two values are present in just about every aspect of our company. Elastic consists of nearly 600 globally distributed employees in more than 30 countries around the world, all with very unique backgrounds and life stories. We have company wide discussions that are not only internal updates, but refreshingly optimistic and unbiased responses to some of the more worrisome issues in recent news. Our CEO, Shay Banon, will send follow up emails that result in paid time off to volunteer at your favorite charity and direct responses to . We even have a slack channel focused specifically on the topics of diversity, inclusion and belonging. The list goes on.At our core, Elasticians are open-minded and inclusive of all walks of life. We believe our products should be an extension of our company culture and uphold the same values. It is for this very reason that we’ve started our journey to make Kibana accessible to all. Adios, Kibana Pink ","locales":"","title":"Making Kibana Accessible"}
{"index":{}}
{"author":"Maciej Rakowicz","category":"User Stories","publish_date":"2017-08-30T00:00:00.000Z","url":"/blog/get-a-break-and-search-fast-with-snaptrip-and-elasticsearch","seo_title":"","content":" specialises in last-minute staycations, providing savings of up to 60% on cottages, lodges, and apartments in the UK. Making Search Real FastOne of our priorities is to make search fast, accurate, and detailed for our customers. Our site lists many thousands of properties, but users don't want to scroll through page after page choosing the right property for them. Understandably, given what we do, we want them to be able to find properties that are available, and match their requirements, quickly and easily. To do this, we use Elasticsearch as we had great experiences with it in the past and it allows us to have a cluster in the cloud that we can provision, upgrade and manage with little dev-ops effort. We've looked at other cloud solutions mainly to test their real life support, however, Elasticsearch & Elastic Cloud seemed to be the best fit for us from the start. Our key search functions include complex feature filtering, map zoom pan (bounding box search), and the combination of property specs with availability information from multiple sources to bring in the most accurate results. In particular, the Elasticsearch sorting flexibility is amazing as we can play with it without having to run a full re-indexing which helps a lot (re-indexing is heavy in our case, there's a lot of bits and pieces that build a single document). Elasticsearch is basically used as an aggregations/cache layer that powers all website searches. It combines data from actual systems of records and makes it available via a common, query-able interface. We keep documents in sync by running background jobs (batch processes) throughout a day. In addition, whenever we notice a price or availability change an event is fired and the aggregation & reindexing task kicks in to process it asynchronously. We have them carefully scheduled to meet business requirements but also to throttle load on any other components of our infrastructure and external infrastructures we integrate with (basically external APIs).In the grand scheme of things, we store a very small volume of data, but a lot of nested documents and the overall size of data indexable documents is not trivial. It's challenging for memory especially during heavy batch job. Improving Customer ExperienceEarlier this year, we were also able to quickly implement a new and novel site feature – 'travel time'. It uses a complex polygon search to restrict results to properties within a certain driving distance of the user's location. Search for driving distance The polygon query uses a geo_polygon filter with a pre-calculated polygon points. ... more filters ... \"geo_polygon\": { \"location\": { \"points\": [a set of polygon points, precalculated] } } ... more filters ... where \"location\" is mapped as: \"type\": \"geo_point\". We use the (version 1.0.6). The query expressed in Ruby looks as shown above, a Ruby hash. However, polygon generation is the complex bit. We are running a relatively complex recursive isochrone algorithm that uses our self-hosted graph/routing service. It eventually spits out a GeoJSON that can be passed as is to the Elasticsearch filter. Our lessons learned mainly focus on the isochrone algorithm - balancing between accuracy and real time performance. Elasticsearch performs well for the polygons of up to 100 points that we supply. We're also working on improving the results users see based on various user and inventory metadata, and Elasticsearch makes this very complex sort ordering possible. Technical & Business Benefits Thinking back, we made a really quick start with Elasticsearch with only one senior developer engineer with minor previous Elasticsearch production experience who set it all up. Today, Elasticsearch greatly helps us reduce the level of work required to aggregate data for results, and build complex search features for our small, lean development team. It uses an expressive query language which h","locales":"","title":"Get a Break and Search Fast with Snaptrip and Elasticsearch"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2017-08-31T00:00:00.000Z","url":"/blog/elasticsearch-6-0-0-beta2-released","seo_title":"Elasticsearch 6.0.0-beta2 released","content":" We are excited to announce the release of , based on . This is the fourth in a series of pre-6.0.0 releases designed to let you test out your application with the features and changes coming in 6.0.0, and to give us feedback about any problems that you encounter. Open a bug report today and become an . This is a beta release and is intended for testing purposes only. Indices created in this version . Upgrading 6.0.0-beta2 to any other version is not supported. Also see: ","locales":"","title":"Elasticsearch 6.0.0-beta2 released"}
{"index":{}}
{"author":"Suyog Rao","category":"The Logstash Lines","publish_date":"2017-08-30T00:00:00.000Z","url":"/blog/logstash-lines-2017-08-30","seo_title":"","content":" Welcome back to The Logstash Lines! In these weekly posts, we'll share the latest happenings in the world of Logstash and its ecosystem. Filters/Output execution in Java (In-progress, 6.1)In 6.0, LS engine can read existing LS configurations and convert them to a graph representation which is used by the pipeline visualization. We're now to the second phase of this project where the graph model is now used for executing the filters and outputs section. Previously, the filter and output phases would be consolidated to a big ruby function and executed. Now all of this will happen in Java. Here's a good way to summarize the evolution of LS execution: 5.x and before: 6.0: In parallel, 6.1: The other part of this project is to execute the conditionals in the config natively in Java. All this work will directly benefits users with complex configurations with plenty of conditionals. Specifically this will add: 1. Type safety for filters and conditionals 2. Performance improvement because we reduce crossing the JRuby/Ruby boundary. 3. Performance improvement because the JVM can now inline conditional execution. 4. Better unit testing for conditionals. 5. Better reporting of errors in conditionals (previously left to the Ruby interpreter and exposed to the user). In early testing, we've seen ~40% throughtput improvement in processing apache pipeline. Changes in master Changes in 5.6 ","locales":"","title":"Logstash Lines: Java execution for filters/outputs"}
{"index":{}}
{"author":"Jim Goodwin","category":"Kurrently in Kibana","publish_date":"2017-08-29T00:00:00.000Z","url":"/blog/keeping-up-with-kibana-2017-8-28","seo_title":"","content":" Welcome to This is a weekly series of posts on new developments in the Kibana project and any related learning resources and events. This week we have an upgrade to 1.6.5 of Angular, a change of our markdown module to 'markdown-it', more progress on our K7 design build-out in the UI framework and an upgrade of Leaflet.Please read all the details below! Operations ","locales":"","title":"Keeping up with Kibana: This week in Kibana for August 28, 2017"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2017-08-29T00:00:00.000Z","url":"/blog/brewing-in-beats-add-support-for-new-kafka-versions","seo_title":"","content":" Welcome to ! With this weekly series, we're keeping you up to date with what's new in Beats, including the latest commits and releases. Update sarama (Kafka) library We have the that we use in the Kafka output. This adds compatibility with newer versions of Kafka (between and ) and a lz4 compression option. The updated client library is in our 6.0 branch and will be released with 6.0.0-beta2. Metricbeat: Total CPU usage metrics This  introduces two new metrics in cpu metricset of the system module: and . While these metrics could be computed based on other existing metrics, they are so commonly needed that it makes sense to export them directly. They will be available in 6.0.0-beta2. Filebeat: Add registry for prospectors With this , the prospectors in Filebeat use the same module pattern as we have in Metricbeat and other places, making it easy to create custom Beats that use Filebeat as a framework and add new prospector types. New community Beat: Cloudfrontbeat Created by  reads log events from Amazon Web Services  and publishes them to Elasticsearch for further analysis. Other changes: Repository: elastic/beats Affecting all Beats Changes in master: Changes in 6.0: Metricbeat Changes in master: Changes in 6.0: Packetbeat Changes in master: Changes in 6.0: Dashboards Changes in master: Changes in 6.0: Infrastructure Changes in master: ","locales":"","title":"Brewing in Beats: Add support for new Kafka versions"}
{"index":{}}
{"author":"Michael Muckel","category":"User Stories","publish_date":"2017-08-29T00:00:00.000Z","url":"/blog/powering-the-glomex-marketplace-with-elasticsearch","seo_title":"","content":" glomex – is a one-of-a-kind, open marketplace for premium video content. Bred from the ProSiebenSat.1 Group, Germany’s number one TV broadcaster both in the TV-advertising and in the audience market, glomex provides content owners, publishers and advertisers with a web-based platform where they can easily and fairly trade content for reach. Figure 1: glomex Media Exchange homepageWhen the glomex engineering team started to build the glomex marketplace from scratch, we focused on some key principles that our architecture needed to provide. First, launching a new product required flexibility in our architecture to include new features quickly and learn from our users’ behavior. Secondly, due to the projected exponential increase of requests to our services, we needed a technology platform that was easily and quickly scalable. To meet both of these core requirements, we designed a scalable microservice architecture based on Amazon Web Services (AWS). Since the team had a lot of positive experience building large-scale products with Elasticsearch, one central decision we made early on was to make Elasticsearch a core component of our architecture. At glomex, Elasticsearch powers two of our central systems: Search through our growing content catalogue and our Real-Time Data Platform. The following section will provide an overview of both use cases and highlights a couple of technical considerations about how glomex integrated Elasticsearch into its architecture. Search at ScaleSearch plays a critical role on the . Offering an extensive catalogue of video content requires a superior search experience for our business customers, who require easy and fast selection of content to be published on their websites. To satisfy the requirements of our customers, the search interface in the portal provides various features like faceted search, autocompletion, and the support for multiple languages. The following figure presents the glomex Extended Search interface for customers within the portal. This interface is targeting B2B customers, like editorial teams of both small and large content portals, searching for relevant content to be embedded into their websites. This presents challenges for the user experience of the search interface: the time users spend within the portal performing searches needs to be minimized, but at the same time provide enough depth for very specific queries.     Elasticsearch is a great fit for this set of requirements since many of the features we need (like faceted search, query-time boosting, multi-language support) are automatically available. Elasticsearch provides fast results - due to reasonable defaults. In addition, Elastic and the community provide an extensive set of documentation and best practises for various areas to implement more specific features (like search-term auto-completion, search-term suggestions). While search via the portal is a central feature of the glomex marketplace, the majority of requests hitting our service infrastructure result from the integrations on our customer's websites via our glomex Embed-Video-Player or from our APIs. Since we cannot predict which content will go viral at which point in time, our architecture is designed to be highly scalable, leveraging several layers of caching. glomex Search ArchitectureAll glomex products are running on a microservice-based architecture. The figure presents a high-level overview of the services involved in powering the glomex search experience and the API. Figure 3: MES - Search ArchitectureIn an earlier version of the architecture, Elasticsearch was used as the central datastore for all video metadata. While this is a very common architecture adapted by many projects, we migrated our video metadata to a separate data store using Amazon DynamoDB and use Elasticsearch to focus on indexing the data. One key benefit resulting from this separation of concerns is that b","locales":"","title":"Powering the glomex Marketplace with Elasticsearch"}
{"index":{}}
{"author":"Mats Iremark","category":"User Stories","publish_date":"2017-08-28T00:00:00.000Z","url":"/blog/how-the-elastic-stack-helped-collector-bank-drive-innovation","seo_title":"","content":" is the leading digital niche bank in the Nordics today. For the past three years, Elasticsearch has been our absolutely most important product to ensure quality in our deliveries. In this blog post, I would like to tell our Elastic journey, provide insights into our architecture in the Microsoft Azure Cloud, explain a few challenges we were facing along the way and also how and why we plan to move on to Elastic Cloud Enterprise (ECE) in the short-term. Our BackgroundCollector Bank has been around for 18 years. We provide payment solutions to thousands of companies in five different countries serving more than 4 million customers. Private clients can lend and save money using Collector at the market's best rates. We also provide factoring and other B2B solutions. This autumn we will also release a very innovative bank app that will integrate a lot of 3rd party solutions. Technology-wise, Collector is leading the fight for fintech companies to use the public cloud. For the past three years, almost all new products and services that have been built at Collector have been deployed to the Microsoft Azure Cloud. We have, for example, built a new bank system for loans and savings accounts and a solutions for our partners. Therefore our whole infrastructure around Elasticsearch is very Azure specific. Figure 1: Collector bank homepageThe JourneyAbout three years ago, Collector had a handful of big systems taking care of the payment solutions, factoring and private loans. Only the online payment system was built by ourselves - while all other systems were bought. At that time, most systems wrote application logs to files and we were very dependent on a few people to be able to find and access those files. This led to a very ineffective development process. An installation of Splunk was up back then, but was only used for one system. From my previous assignment as a consultant, my team members and I had discovered the benefits of using Elasticsearch over any file or relational database logging solutions. So to make a short story as short as it really was, we began using Elasticsearch at Collector Bank. In the beginning we built a lot of microservices and were able to search and correlate logs between systems. This was a game changer for the architecture and workflow at Collector, which lead to more and more teams discovering how logging with the Elastic Stack and visualizing data in Kibana helped them develop services with higher quality in shorter amount of time. Where We are Today Figure 2: Monitoring dashboard to get insights into the e-commerce platformThere are now 18 teams working in software development at Collector. In the office landscape, each team has at least one tv-screen up showing Kibana dashboards that provide them with information about their systems. Both for alerts and monitoring, but also for the morale boost to see what's going on in real-time. For example, if there are any problems with the payments or payments, we have watches/alerts telling us that immediately. Along with other devops tools/services like PagerDuty and Slack, the reality today is that if there any problems with our systems or integrations with partners, the responsible team is notified immediately on Slack. The ability to visualize things has proven to be a booster for the teams and a way to measure the value of completed stories. The user story we delivered last week – How many customers has so far used it? Figure 3: Response time monitoring dashboardOur Azure ArchitectureCollector builds most services using C# and deploys them as Azure Web Apps. We use Serilog as our main logging framework (take a look at our for some enrichers and sinks we've released). All those services log to an Azure Event Hub. Why? It's designed for that: handling millions and millions of events. This is where most other companies use Redis or Kafka. All three products/services act as a cache. ","locales":"","title":"How the Elastic Stack Helped Collector Bank Drive Innovation"}
{"index":{}}
{"author":"Armin Braun","category":"Engineering","publish_date":"2017-08-24T00:00:00.000Z","url":"/blog/ingest-node-to-logstash-configuration-converter","seo_title":"","content":" MotivationWhether you want to use more than just Elasticsearch as a target for your data ingestion, configure a more complex set of transformations than ingest nodes allow for or use Logstash's new persistent queue feature to make your data ingestion pipeline more resilient, Logstash 6.0's new Ingest to Logstash config migration tool has you covered. It allows you to automatically convert Ingest filter configurations into equivalent Logstash filter configurations and thus removes a large part of the manual effort currently required in the migration from Ingest node to Logstash node. While transforming a simple Ingest configuration like the one shown below to Logstash is relatively easy to do by hand. { \"description\": \"Pipeline to parse Apache logs\", \"processors\": [ { \"append\": { \"field\" : \"client\", \"value\": [\"host1\", \"host2\"] } } ] } becomes:filter { mutate { add_field => { \"client\" => [ \"host1\", \"host2\" ] } } } output { elasticsearch { hosts => \"localhost\" } } Converting a complex case like the following example though, is a much more daunting task. { \"description\": \"Pipeline to parse Apache logs\", \"processors\": [ { \"grok\": { \"field\": \"message\", \"patterns\": [\"%{COMBINEDAPACHELOG}\"], \"on_failure\" : [ { \"set\" : { \"field\" : \"error\", \"value\" : \"field does not exist\" } }, { \"convert\": { \"field\" : \"client.ip\", \"type\": \"integer\" } } ] } }, { \"date\": { \"field\": \"timestamp\", \"target_field\": \"@timestamp\", \"formats\": [ \"dd/MMM/YYYY:HH:mm:ss Z\" ], \"locale\": \"en\" } }, { \"geoip\": { \"field\": \"clientip\", \"target_field\": \"geo\" } } ] } It requires correctly converting field names like into Logstash bracket notation , knowing the syntactic details of the Grok filter in Logstash and so on. Using the tool though, we easily arrive at: filter { grok { match => { \"message\" => \"%{COMBINEDAPACHELOG}\" } } if \"_grokparsefailure\" in [tags] { mutate { add_field => { \"error\" => \"field does not exist\" } } mutate { convert => { \"[client][ip]\" => \"integer\" } } } date { match => [ \"timestamp\", \"dd/MMM/YYYY:HH:mm:ss Z\" ] target => \"@timestamp\" locale => \"en\" } geoip { source => \"clientip\" target => \"geo\" } } output { elasticsearch { hosts => \"localhost\" } } Imagine going through all these steps by hand potentially for an ingest node config written by someone else a long time ago and without much prior Logstash experience. This tool enables you to do just that in a matter of minutes instead of hours, allowing you to quickly prototype Logstash in your environment without the need for a significant time investment. UsageThe tool is implemented entirely in Javascript and executed via Java's Nashorn scripting engine, making it at least as portable as Logstash itself. You can run it by using the wrapper script found at `bin/ingest-convert.sh`. For example: ➜ cat /tmp/ingest.json { \"description\": \"Pipeline to parse Apache logs\", \"processors\": [ { \"grok\": { \"field\": \"message\", \"patterns\": [\"%{COMBINEDAPACHELOG}\"], \"on_failure\" : [ { \"set\" : { \"field\" : \"error\", \"value\" : \"field does not exist\" } }, { \"convert\": { \"field\" : \"client.ip\", \"type\": \"integer\" } } ] } }, { \"date\": { \"field\": \"timestamp\", \"target_field\": \"@timestamp\", \"formats\": [ \"dd/MMM/YYYY:HH:mm:ss Z\" ], \"locale\": \"en\" } }, { \"geoip\": { \"field\": \"clientip\", \"target_field\": \"geo\" } } ] } ➜ bin/ingest-convert.sh --input=file:///tmp/ingest.json --output=file:///tmp/ingest.cfg ➜ cat /tmp/ingest.cfg filter { grok { match => { \"message\" => \"%{COMBINEDAPACHELOG}\" } } if \"_grokparsefailure\" in [tags] { mutate { add_field => { \"error\" => \"field does not exist\" } } mutate { convert => { \"[client][ip]\" => \"integer\" } } } date { match => [ \"timestamp\", \"dd/MMM/YYYY:HH:mm:ss Z\" ] target => \"@timestamp\" locale => \"en\" } geoip { source => \"clientip\" target => \"geo\" } } output { elasticsearch { hosts => \"localhost\" } } That's all there is to it. One thing to note is that we automatically added an output that assumes a local Elasticsearch host at the default address as t","locales":"","title":"Ingest Node to Logstash Config Converter"}
{"index":{}}
{"author":"Daniel Palay","category":"News","publish_date":"2017-08-23T00:00:00.000Z","url":"/blog/elastic-cause-awards-2-0","seo_title":"","content":" Recently, we celebrated . As I reflected on that milestone for the company, I decided to go back and re-read Shay’s announcement post – .In reading through the post, I recalled an anecdote –  shared at many points by each one of our founders – about the insightful moments they experienced when comments were still activated on the blog. What they found there profoundly changed their view on what was possible with our software. Empowering health workers in Rwanda and tracking malaria medicine in Cambodia…none of our founders could have anticipated that our software was solving humanitarian problems across the globe (and especially in developing nations).Fast forward 3 years and I can attest that we still are in awe and are truly humbled every time we hear of someone doing something good in the world with our software. Whether it’s something like the fine folks at Yale working to discover causes and cures for cancer or our friends at Giant Oak who broke up a major black market rhino horn trading operation, we couldn’t be more proud to have played a small part in the betterment of the human condition.These stories of people doing good in the world and for the world – that fill us with the proverbial warm fuzzies – were the basis for the establishment of the . We were thrilled to honor our first three Cause Award recipients who worked to combat human trafficking (IST Research), tracking the Ebola outbreak in Sierra Leone (eHealth Africa), and rooting out the potential deadly effects of school violence (NoSchoolViolence.org).Now, we are even more thrilled to announce the opening of the . If you are part of a project (or just know about one) where you are doing something good in or for the world (whether the scope is local, regional, or global) please take a moment to apply today.The application period is open now through December 15, 2017 and we will be announcing the final honorees at Elastic{ON} 2018 in San Francisco, February 26 – March 1. All honorees will be invited to present their project work on one of our stages., including all of the deadlines, application information, and details on what you receive as an Elastic Cause Awards honoree, . ","locales":"","title":"Elastic Cause Awards 2.0"}
{"index":{}}
{"author":"James Baiera","category":"Engineering","publish_date":"2017-08-22T00:00:00.000Z","url":"/blog/structured-streaming-elasticsearch-for-hadoop-6-0","seo_title":"","content":" Spark Structured Streaming Support Support for Spark Structured Streaming is coming to ES-Hadoop in 6.0.0 Last year (July 2016 to be exact) . With it came many new and interesting changes and improvements, but none as buzzworthy as the first look at Spark’s . Initially released as an experimental feature, now one year later, it is considered . I am excited to announce that we will be supporting Structured Streaming as part of our native Spark integration! What is Structured Streaming? is the a new stream processing model in Spark that combines the existing approach of executing in micro-batches with the same data transformation and optimization framework that users have come to love from the SparkSQL API’s. Structured Streaming’s programming model aims to fill the missing fourth quad of Spark API’s along RDDs, DStreams, and Datasets: Structured Streaming leverages the same Dataset API from SparkSQL, allowing for the same operations (with a few ) to be executed on bounded (batch) or unbounded (streaming) data sources. The new programming model also advertises through usage of disk-based checkpointing of input offsets, commit logs, and idempotent data sinks. Using SQL on a streaming data source introduces new patterns for performing aggregations. Structured Streaming provides a rich set of features for performing aggregations over , allowing for groupings of data to be materialized as data flows in. It’s execution model stores long running aggregations in memory, so if you have event data that arrives late it will be . Users can provide watermarks that inform the framework to discard extremely late data if there is little chance of encountering data for that window ever again. All of these features together make Spark Structured Streaming an enticing streaming technology for Spark users. Integrating with Elasticsearch Available now in is a Streaming Sink implementation that allows for sending data from a Structured Streaming job to an Elasticsearch cluster. This sink implementation is built with all from the ES-Hadoop connector, like automatic connection failover between Elasticsearch nodes and dynamic multi-resource writes. Under the hood Hooking into Spark as an extension through the SQL DataSource api, ES-Hadoop offers up an implementation of the new interface, allowing users to save a streaming Dataset to Elasticsearch. In order to uphold Structured Streaming’s exactly once processing semantics, we must make sure of the following: Users of ES-Hadoop can specify one of their fields to be used as the document’s ID, and Elasticsearch manages ID based writes consistently. We can’t know ahead of time what your documents’ IDs are, so it’s up to each user to ensure their streaming data contains an ID of some sort. With your help, we have requirement #2 satisfied, but what about requirement #1? Spark Structured Streaming ships with some internal log classes that help us out here. ES-Hadoop extends an internal metadata log implementation that persists committed batch id’s to HDFS. We have provided a simple commit protocol for these batch IDs: If it does not already exist, then each batch ID is initialized in the driver, and sent to the workers. Each worker responds back to the driver with a result for the opened transaction. This result contains if the task succeeded or failed, as well as some simple statistics about the task. The driver collects these transaction results, and if they are all satisfactory, the batch ID is committed to HDFS as a file which contains the serialized transaction results. You may be wondering where these logs are kept on the filesystem once you launch your job. Structured Streaming requires users to specify a checkpoint location when writing their streaming queries. ES-Hadoop scoops up whichever checkpoint configurations you chose to set and persists its commit log alongside the rest of Spark’s checkpoint data. As ","locales":"","title":"Introducing Spark Structured Streaming Support in ES-Hadoop 6.0"}
{"index":{}}
{"author":"Jim Goodwin","category":"Kurrently in Kibana","publish_date":"2017-08-22T00:00:00.000Z","url":"/blog/keeping-up-with-kibana-2017-8-21","seo_title":"","content":" Welcome to This is a weekly series of posts on new developments in the Kibana project and any related learning resources and events.Last week , these releases contain security fixes.We've been working on 5.6.0 and 6.0 releases and testing upgrades between them. We're also making the new K7 UI design real by building out the UI Framework and the new CSS model.Finally, we're working on a number of accessibility improvements for 6.0, watch for an upcoming blog post on the topic.Speaking of blog posts, take a look at the if you have a chance. ","locales":"","title":"Kibana: This week in Kibana for August 21, 2017"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2017-08-22T00:00:00.000Z","url":"/blog/brewing-in-beats-monitor-kafka-logs-with-filebeat","seo_title":"","content":" Welcome to ! With this weekly series, we're keeping you up to date with what's new in Beats, including the latest commits and releases. Filebeat: Kafka moduleBased on an older , this adds a new module in Filebeat for monitoring the logs. The module collects and parses the Kafka logs, extracts the most interesting information to store in Elasticsearch. The Kafka module was tested with logs from versions 2.11. It comes with a sample dashboard: This feature is currently scheduled to be released with 6.1. Custom template patternIn Beats we allow the user to specify a custom index naming scheme (e.g. ). It is important, however, that the mapping template is still applied when the index name is changed. This gives more flexibility over specifying the index pattern (e.g. ) in the template, by supporting the same syntax. With this change, the Beat also refuses to start if the indexed is changed but the template is not, to catch common configuration errors. This change will be included in 6.0. Repository: elastic/beatsAffecting all BeatsChanges in master: Changes in 6.0: MetricbeatChanges in master: Changes in 6.0: PacketbeatChanges in master: FilebeatChanges in master: InfrastructureChanges in master: DocumentationChanges in master: Changes in 5.5: Changes in 5.6: Changes in 6.0: New community BeatsChanges in master: ","locales":"","title":"Brewing in Beats: Monitor Kafka logs with Filebeat"}
{"index":{}}
{"author":"Jason Zucchetto","category":"Engineering","publish_date":"2017-08-21T00:00:00.000Z","url":"/blog/index-sorting-elasticsearch-6-0","seo_title":"","content":" In Elasticsearch 6.0 we’re introducing a new feature called Index Sorting. Users can now optimize Elasticsearch indexes to store documents on disk in a specific order. We’re very excited for Index Sorting, as it’s another useful tool in optimizing Elasticsearch performance! Through this article, we’ll dive into a number of areas: Index Sorting in LuceneLucene’s IndexSorterMany years ago, Lucene introduced an offline tool known as the . The IndexSorter copied a source index to a new destination index, and ordered the documents on disk based on a user specified order. At that time, because it was not possible to update the destination index directly, users of this feature had to re-build a sorted view every time new documents were added to the source index. The IndexSorter was the first attempt to provide a way to sort documents on disk, not at search time, but at index time. With index sorting, a new concept called “early termination” was introduced. Suppose for instance that you want to retrieve N documents sorted by date (date being a field in the index). If the index is sorted on disk by this date field it would be possible to “stop” the request after visiting the first N documents that match the query (since they are already in the order the user specified). This is what we call “early termination”. Early termination of a query can bring significant improvement to search response times, especially for sort-based queries, and led to the increased popularity of the IndexSorter tool among Lucene users. The static nature of the tool prevented its usage for indices with a lot of updates, which is why it was eventually replaced with a solution that allows incremental updates. Instead of doing a one-time sort of a static index, a new solution was proposed to sort documents at merge time. Lucene improvementsOriginally, Lucene indexed documents in the order they were received, and assigned each document an incremental (and internal) document id (assigned on a per segment basis). The first document indexed in a segment had a document id of 0, and so on. At search time, each segment is visited in document id order, to retrieve documents that match a user query. In order to retrieve the best N documents for a query, Lucene needs to visit every document matching the query across all segments. If the query matches millions of documents, retrieving only the best N would still require millions of documents to be visited. A Lucene index creates a new segment whenever a refresh is triggered. This new segment contains all the documents that were added after the last refresh. When the segment is flushed it becomes visible to the searcher and new documents can appear in search results. Because refreshes occur constantly, the number of segments can easily explode in an index. Segment merges happen in the background to limit the number of segments from growing too large. Merges are triggered based on a policy that selects segments eligible for merging, the selected segments are then merged in a new segment that replaces the old segments. By default, the segment merge process copies documents from different segments to a new segment based on their internal document ids. In order to replace the static tool (the IndexSorter mentioned above), a to allow index sorting for dynamic indices that reorders documents during the merge process based on a configurable order (the value of a field for instance). This new design was a huge step in the right direction, and allowed an index to be sorted on the fly and to use this information on a per-segment basis. Some segments are sorted (segments created by a merge) and some are not (the newly flushed segments). At merge time, the unsorted segments are first “sorted” and then merged with other sorted segments. This merge policy that lived in a module was then moved to a top-level option on the IndexWriterConfig to make index sorting a in Lucene. Thou","locales":"","title":"Introducing Index Sorting in Elasticsearch 6.0"}
{"index":{}}
{"author":"Rasmus Makwarth","category":"Engineering","publish_date":"2017-08-17T00:00:00.000Z","url":"/blog/starting-down-the-path-for-elastic-apm","seo_title":"","content":" A few months ago, , and today we’re really excited to take our first big step in adding APM to the Elastic Stack! Adding APM (Application Performance Monitoring) to the Elastic Stack is a natural next step in providing our users with end-to-end monitoring, from logging, to server-level metrics, to application-level metrics, all the way to the end-user experience in the browser or client. Elastic APM consists of three components - the agents, the server, and the UI: Today, we’ve open-sourced the along with agents for and ! Early testers welcome This is a pre-alpha release, and we welcome you to take it for an early spin. You can get started by following the README in the  repo. We’d love to get feedback as we work our way towards the alpha and beta releases, and you’re welcome to and join the discussion on . If you’d like to see us add support for your stack, please take a minute and . Dedicated APM UI is next Later this year we’ll give the UI a massive upgrade when we release a dedicated APM UI, similar to the intuitive and easy-to-use interface that is known from Opbeat today. The UI will be delivered as a Kibana plugin and will be included in the Beta release. Stay tuned! ","locales":"","title":"Starting Down the Path of APM for the Elastic Stack"}
{"index":{}}
{"author":"Jim Goodwin","category":"Releases","publish_date":"2017-08-17T00:00:00.000Z","url":"/blog/kibana-5-5-2-and-4-6-6-and-2-4-6-plugins-released","seo_title":"","content":" Hello, and welcome to the 5.5.2, 4.6.6 release of Kibana! We've also released the 2.4.6 versions of our reporting, graph, marvel, and shield plugins.  These releases contains some small bug fixes and important security fixes. Please see details below. Kibana 5.5.2 is available on our and on . When you’re finished reading, take a look at the complete for all the goodies. Kibana 4.6.6 is also available on our   under \"past releases\" and only contains the changes for the security update. Kibana 5.5.2 and Kibana 4.6.6 security update Kibana markdown parser Cross Site Scripting (XSS) error (ESA-2017-16) Kibana versions prior to 5.5.2 had a cross-site scripting (XSS) vulnerability in the markdown parser that could allow an attacker to obtain sensitive information from or perform destructive actions on behalf of other Kibana users. Affected Versions: All prior to 5.5.2 and 4.6.6 Solutions and Mitigations: Users should upgrade to Kibana version 5.5.2 or 4.6.6. Reporting impersonation error (ESA-2017-17) The Reporting feature in X-Pack in versions prior to 5.5.2 and standalone Reporting plugin versions versions prior to 2.4.6 had an impersonation vulnerability. A user with the reporting_user role could execute a report with the permissions of another reporting user, possibly gaining access to sensitive data. Affected Versions: All prior to 5.5.2 and 2.4.6 Solutions and Mitigations: Reporting users should upgrade to X-Pack version 5.5.2 or Reporting Plugin version 2.4.6. A mitigation for this issue is to remove the reporting_user role from any untrusted users of your Elastic Stack. CVE ID: CVE-2017-8446 Discover Management Visualization Dev Tools ","locales":"","title":"Kibana 5.5.2, 4.6.6, and 2.4.6 plugins released"}
{"index":{}}
{"author":"Tyler Hannan","category":"News","publish_date":"2017-08-17T00:00:00.000Z","url":"/blog/beta-release-of-elastic-cloud-hosted-elasticsearch-on-google-cloud-platform-gcp","seo_title":"Introducing Hosted Elasticsearch on Google Cloud Platform (GCP): Beta Release Now Available","content":" True to our promise of bringing Elastic Cloud to wherever our users are, we've expanded your options for hosting and managing Elasticsearch and Kibana with Elastic Cloud. In April, to deliver Elastic Cloud on Google Cloud Platform (GCP). Today, you can officially see, taste, touch, and experience it via a beta release.  Existing Elastic Cloud customers can get going with this new offering in just a few clicks via , while new users are welcome to give . Initially, Elastic Cloud was only available on Amazon Web Service (AWS). But it was always our intention to offer our service through more cloud providers like GCP. Doing so brings the Elastic products closer to your applications living on GCP, giving you the flexibility to run your projects and business the way you choose.There were other contributing factors for this expansion, too. GCP is one of the most innovative and open source friendly cloud platforms available today (). Plus, we've seen a growing demand from our community Like our offering on AWS, Elastic Cloud on GCP provides access to the latest versions of our open source software, X‑Pack features, and Elastic's support and consulting services. Upgrading, scaling, and extending your deployment on GCP and AWS can all be done from the same UI in Elastic Cloud. The (previously known as the ELK Stack) consists of Elasticsearch, Kibana, Beats, and Logstash. unlocks powerful features such as security, alerting, and monitoring to extend the Elastic Stack. Together, Elastic products solve use cases encompassing search, logging, metrics, and analytics for . We hope you enjoy this beta release. Your feedback is welcome. Reach out to us by submitting a ticket through the Elastic Cloud  or . We're excited to take another step toward giving you the offerings you need to make your projects successful. Want more details? Read on. A Few Frequently Asked Questions In general, the functionality is very much the same as what Elastic Cloud provides today on the existing non-GCP infrastructure. That said, there are two main differences between the beta and the GA version: Elastic will provide support using the same cloud standard support policy. However, during beta, our service may experience outages and issues and therefore we don't recommend to put production-critical workloads on it until it goes GA. We aim to make the GA available towards the end of the calendar year 2017. No, you will be able to keep using the same clusters you created during the beta period. Yes. All existing Elastic Cloud users are able to create clusters on GCP from the same user console. You can check the pricing on the . Please reach out us . Or, if you are a registered user you can also contact us through Elastic Cloud . Location matters. If you're running your application on GCP, you want you data close to it, both for performance and cost reasons. Having your Elasticsearch cluster residing on the same GCP region means that you have the lowest possible latency to it, and that you're not paying outbound traffic costs that you would have been paying if it was located somewhere outside of GCP. ","locales":"","title":"Beta Release of Elastic Cloud on Google Cloud Platform (GCP), More Options to Host Elasticsearch"}
{"index":{}}
{"author":"Josh Bressers","category":"Engineering","publish_date":"2017-08-16T00:00:00.000Z","url":"/blog/default-security-for-elasticsearch-and-the-elastic-stack","seo_title":"","content":" Version 6.0 of the Elastic Stack is set to be released soon. The and it's loaded with new and interesting features. The changes I'm most interested in, and most involved with, are changes to default security features. Elastic's adds a number of features to the Elastic Stack, two of those features that will see substantial changes in version 6.0 are authentication and TLS support for your cluster.The Way it WasIn the past when you installed X-Pack it would automatically create three users: , , and . All three users were created with the password of \"changeme\". The intent was you would change those passwords right away. There is even a setting called that could be set to disallow this password once your cluster was running. Default passwords aren't ideal in a security conscious environment, but they solved the difficult problem of how to bootstrap a password. Additionally a default cluster install would communicate between clients and nodes using unencrypted protocols over the network. Communications, including sensitive data, were not encrypted by default. A malicious user on the network could possibly intercept data as it traveled between nodes. The option existed to enable TLS encrypted communication between nodes and clients, but it was optional. This may have been acceptable behavior for a product in the past, today is different though. Security was often viewed as something you could deal with tomorrow. As history has taught us, tomorrow never comes. The expectations of the industry regarding default security in products is catching up to meet the current challenges. Today many organizations have security groups and often compliance is driving their security requirements. The days of default passwords and plaintext communications are no longer acceptable. If you pay attention to any news you are constantly hearing about companies dealing with security breaches, cyber attacks, and even malicious insiders. Our primary goal at Elastic is to help you stay off the front page because of an insecure cluster. We want to be an organization that understands the current state of security as well as where it's heading next, a trusted resource you can rely on instead of a vendor that has to be dragged into the present. Default Passwords No MoreI'm pleased to say there are no more default passwords for the Elastic Stack. This naturally creates an extra step when spinning up the cluster, but we've done our best to make the process simple and ensure it meets industry security expectations. We decided to solve this by adding what we're calling a bootstrap password. A bootstrap password can be auto generated or placed in the Elasticsearch keystore before the node is started. The bootstrap password is a temporary password that is used only until a real password has been set for the user. As soon as the cluster has started, you can use the command line tool to either set or generate strong passwords for the internal , , and users. This prevents a race condition between starting the cluster and setting the administrator password. The old model suffered from this problem, before the password was changed, even if it was changed quickly, there was an amount of time where an outside user could authenticate against the node. The details on the new password workflow can be found in the . TLS EverywhereBefore we discuss the TLS changes, it's important to set the stage by explaining how we, as an industry, got to where we are today. If you've never heard of something called or Zero Trust Networking I suggest you spend some time becoming familiar with these ideas as they represent the future of network design. The basic concept is that the world has changed drastically in the last few years and we've reached a point where security experts agree that your own network can no longer be trusted. That's right, the network you build with your own two hands should be considered hostile!","locales":"","title":"Default Security for Elasticsearch and the Elastic Stack"}
{"index":{}}
{"author":"Christoph Wurm","category":"Engineering","publish_date":"2017-08-16T00:00:00.000Z","url":"/blog/analyzing-network-packets-with-wireshark-elasticsearch-and-kibana","seo_title":"Analyzing network packets with Wireshark, Elasticsearch, and Kibana","content":" For network administrators and security analysts, one of the most important capabilities is packet capture and analysis. Being able to look into every single piece of metadata and payload that went over the wire provides very useful visibility and helps to monitor systems, debug issues, and detect anomalies and attackers. Packet capture can be ad hoc, used to debug a specific problem. In that case, only the traffic of a single application or a single server might be captured, and only for a specified period of time. Or it can be extensive, for example using an outside network tap to capture all traffic. While network traffic itself is sent in a binary format, each packet contains many different fields that using proper tools can be parsed out into numbers, text, timestamps, IP addresses, etc. All of this is data that can be stored in Elasticsearch and explored, searched and visualized in Kibana. Architecture Any data pipeline for network capture and analysis is composed of several steps: - Recording the packet traffic on a network. - Parsing out the different network protocols and fields. - Exploring the data in detail or in aggregate. In this blog post, I will show how to set up a pipeline using Wireshark and the Elastic Stack that can look like this: Packet capture Packetbeat There is already a tool in the Elastic Stack to index network data into Elasticsearch: . Packetbeat can be configured to capture network packets live as well as read packets from a capture file with the option. It can recognize and parse a number of application-level protocols such as HTTP, MySQL and DNS, as well as general flow information. However, it is not built for full packet capture and parsing of the myriad different protocols out in the world and is best used for monitoring specific applications. Especially its ability to match responses with their original requests and indexing the merged event is very useful if you’re looking at specific protocols. Wireshark/Tshark Wireshark is the most popular packet capture and analysis software, and open source. It can recognize . Its GUI is familiar to most network and security professionals. In addition to a GUI it provides the command-line utility to capture live traffic as well as read and parse capture files. As its output, can produce reports and statistics, but also parsed packet data in different text formats. One of the output formats supported by since version 2.2 () is a JSON format for the : Will do a live capture of packets on the network interface and output them in Elasticsearch Bulk API format into the file . Will read packets from capture file and output them as JSON for the Elasticsearch Bulk API format into the file . Importing from Wireshark/Tshark Elasticsearch Mapping Raw packet data contains an extraordinarily large amount of fields. As mentioned above Wireshark knows about 200,000 individual fields. Most likely, the vast majority of these fields will never be searched or aggregated on. Consequently, creating an index on all these fields is usually not the right thing to do. In fact, since a large number of fields can slow down both indexing and query speed . Also, the output of contains all field values as strings, regardless of whether the data is actually text or numbers including timestamps and IP addresses, for example. Without the right data types, you will not be able to perform type-specific operations on these fields (e.g. finding out the average packet length). To index numbers as numbers, timestamps as timestamps, etc. and to prevent an explosion of indexed fields, you should explicitly specify an Elasticsearch mapping. Here is an example: To then import the output of into Elasticsearch, you have several options: 1. 2. Filebeat Filebeat is very lightweight and can watch a set of files or directory for any new files ","locales":"","title":"Analyzing Network Packets with Wireshark, Elasticsearch, and Kibana"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2017-08-16T00:00:00.000Z","url":"/blog/brewing-in-beats-monitor-system-uptime-with-metricbeat","seo_title":"","content":" Welcome to ! With this weekly series, we're keeping you up to date with what's new in Beats, including the latest commits and releases. Metricbeat: iostat metrics This , contributed by , adds a number of new metrics to the diskio metricset, which provide the same functionality as the command. These are going to be super useful when investigating disk issues. These metrics are only available on Linux, and we plan to release them with version 6.1. Metricbeat: system uptime metricset With this , the system module in Metricbeat gets an “uptime” metricset. Like the name suggests, it exports the system uptime in milliseconds. It’s a simple addition, but it can be really helpful to detect system restarts and such. In the , this metricset is configured to send a document every 15m. The uptime metricset is available on all supported platforms. This feature is currently scheduled to be released with 6.1. Set default credentials for Kibana configuration Starting with Beats 6.0, the dashboards are loaded via a  Kibana API. While this improves the overall experience, it requires a bit more configuration because the Kibana endpoint needs to be configured as well. In a typical x-pack installation, the same username and password can be used for connecting to both Elasticsearch as for Kibana. So to reduce the number of required configuration changes, this copies the credentials configured in the Elasticsearch output to the Kibana configuration. This means that you won’t have to configure the same username and password twice. You can still provide different credentials if needed. This improvement will be included in 6.0. Other changes Repository: elastic/beats Affecting all Beats Changes in master: Metricbeat Changes in master: Heartbeat Changes in master: Testing Changes in master: Infrastructure Changes in master: Documentation Changes in master: Changes in 5.5: Changes in 6.0: ","locales":"","title":"Brewing in Beats: Monitor system Uptime with Metricbeat"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2017-08-15T00:00:00.000Z","url":"/blog/elasticsearch-5-5-2-released","seo_title":"Elasticsearch 5.5.2 released","content":" Today we are pleased to announce the release of , based on . Elastisearch 5.5.2 is the latest stable release, and is already available for deployment on , our Elasticsearch-as-a-service platform.Latest stable release in 5.x:Please , try it out, and let us know what you think on Twitter () or in our . You can report any problems on the . ","locales":"","title":"Elasticsearch 5.5.2 released"}
{"index":{}}
{"author":"Andrew Kroh","category":"Engineering","publish_date":"2017-08-15T00:00:00.000Z","url":"/blog/introducing-auditbeat-ship-linux-audit-logs-to-elasticsearch","seo_title":"","content":" Auditbeat is an exciting new addition to the Beats family for 6.0. Its purpose is to audit the activities of users and processes on your systems. Auditbeat is a beta project and currently has two main monitoring capabilities: file integrity monitoring and Linux audit framework monitoring. We expect to add more features as the project matures. Let’s take a closer look at its capabilities. Linux Audit Framework Monitoring Auditbeat receives events from the audit framework in the Linux kernel and sends them to Elasticsearch. This is similar to , but with some additional logic features that makes it really simple to get the data into Elasticsearch. It will automatically group related messages into a single event even if they arrive out of order from the kernel. It also parses and normalizes the messages so that the data can be delivered to Elasticsearch in a structured format. Let’s take a look at an example. What follows are three audit messages generated by the kernel and received by Auditbeat when a user unsuccessfully attempted to open the file. I configured Auditbeat with a rule for generating events when an syscall fails due to (permission denied). These messages are then turned into a single event that can be sent to Elasticsearch. { \"@timestamp\": \"2017-08-01T20:34:15.636Z\", \"audit\": { \"kernel\": { \"sequence\": 32796540, \"category\": \"audit-rule\", \"record_type\": \"syscall\", \"session\": \"15663\", \"result\": \"fail\", \"actor\": { \"primary\": \"admin\", \"secondary\": \"admin\", \"attrs\": { \"suid\": \"admin\", \"auid\": \"admin\", \"euid\": \"admin\", \"fsgid\": \"sysadmins\", \"gid\": \"sysadmins\", \"sgid\": \"sysadmins\", \"egid\": \"sysadmins\", \"fsuid\": \"admin\", \"uid\": \"admin\" } }, \"action\": \"opened-file\", \"thing\": { \"what\": \"file\", \"primary\": \"/etc/gshadow\", \"secondary\": \"138686\" }, \"how\": \"/bin/cat\", \"key\": \"open\", \"data\": { \"exe\": \"/bin/cat\", \"exit\": \"EACCES\", \"pid\": \"6654\", \"ppid\": \"6637\", \"syscall\": \"open\", \"tty\": \"pts0\", \"arch\": \"x86_64\", \"cwd\": \"/home/admin\", \"a2\": \"7fffdd0a8210\", \"a3\": \"7fffdd0a7c60\", \"comm\": \"cat\", \"a0\": \"7fffdd0a98e2\", \"a1\": \"0\" }, \"paths\": [ { \"item\": \"0\", \"name\": \"/etc/gshadow\", \"ouid\": \"root\", \"dev\": \"fc:00\", \"mode\": \"0100000\", \"nametype\": \"NORMAL\", \"ogid\": \"root\", \"rdev\": \"00:00\", \"inode\": \"138686\" } ] } } } This event answers the important questions of Plus Auditbeat retains all of the original syscall and the associated . Ingesting audit data from Linux has never been so painless! Check out the for more details on how it works and how to configure it. File Integrity Monitoring Another feature in this first release of Auditbeat is the ability to monitor files for changes. Security policies often have requirement for this type of monitoring. Auditbeat can watch a list of directories or files and send events in realtime to Elasticsearch when changes occur. The events contain file metadata and cryptographic hashes of the file contents. The file hashes can be useful in several ways. They can be used to identify whether a specific file is in compliance across a fleet of machines. Or the hashes could be cross-referenced with threat intelligence sources to identify potential malware. The setup is simple. You just need to specify the paths to the directories that you want Auditbeat to watch. This feature can be used on Linux, macOS, and Windows. Check out the for all the details about how it works and how to configure it. When a file is created, modified, or deleted in one of the watched directories Auditbeat generates an event like this. { \"@timestamp\": \"2017-08-01T15:24:07.217Z\", \"audit\": { \"file\": { \"action\": \"created\", \"type\": \"file\", \"path\": \"/etc/sudoers.d/superusers\", \"gid\": 0, \"uid\": 0, \"owner\": \"root\", \"group\": \"root\", \"inode\": \"196640\", \"size\": 42, \"mode\": \"0440\", \"mtime\": \"2017-08-01T15:24:07.215Z\", \"ctime\": \"2017-08-01T15:24:07.215Z\", \"atime\": \"2017-08-01T15:24:07.214Z\", \"hashed\": true, \"md5\": \"7944a620fcc4dc850fed30643e7a1401\", \"sha1\": \"a827","locales":"","title":"Introducing Auditbeat: Ship Linux Audit Logs to Elasticsearch and More"}
{"index":{}}
{"author":"João Duarte","category":"Engineering","publish_date":"2017-08-14T00:00:00.000Z","url":"/blog/logstash-multiple-pipelines","seo_title":"","content":" Being a central component of data flow between producers and consumers, it often happens that a single Logstash is responsible for driving multiple parallel streams of events.  The existence of these multiple isolated flows pose a few challenges for Logstash, since historically, each individual instance has supported a single pipeline, composed of an input, a filter, and an output stage. Users have found ways of implementing multiple isolated flows in a single pipeline, mainly through conditionals: tagging events early on in the input section and then creating conditional branches through the filters and outputs, applying different sets of plugins to different tags or event values.  This is a very common solution we often see in the community, but there are several pains users feel when implementing it: Conditional hell While implementing isolated multiple flows using conditionals works, it's easy to see how the existence of a single pipeline and single stages of processing makes the configuration extremely verbose and hard to manage as complexity increases. For a simple pipeline that houses two flows: The corresponding Logstash pipeline configuration is already riddled with primitives (marked in ) whose only purpose is to keep the flows separate: input { beats { port => 3444 } tcp { port => 4222 } } filter { dissect { ... } grok { ... } } output { elasticsearch { ... } tcp { ... } } Unfortunately, this is not the only issue with this solution.. Lack of congestion isolation If you're familiar with how Logstash works, you know that the output section of the pipeline receives a batch of event and will not move to the next batch until all events and gone through all of the outputs. This means that, for the pipeline above, if the TCP socket destination isn't reachable, Logstash won't process other batches of events, which in turn means that Elasticsearch won't receive events, and back pressure will be applied to both the TCP input and the Beats input. Different flows have different needs If the TCP-> Grok -> TCP data flow handles a very high volume of small messages and the Beats -> Dissect -> ES flow has large documents with less events per second, then it would be ideal to have the former flow with many workers and bigger batches, and the latter with less workers and smaller batches. With a single pipeline, the Filters+Output section will have a single set of parameters, and will process batches mixed with events from both flows. Also, if you're using configuration reloading, changing a grok pattern for the first flow will stop Beats from receiving data and sending to Elasticsearch. The current solution: Multiple Logstash Instances The problems described above can be solved by having multiple Logstash instances in the same machine, which can then be managed independently, but even this solution creates other problems: We're proud to announce that the solution to all of these issues will arrive in the upcoming Logstash 6.0, with the new Multiple Pipelines feature! Multiple Pipelines Multiple pipelines is the ability to execute, in a single instance of Logstash, one or more pipelines, by reading their definitions from a configuration file called `pipelines.yml`. This file lives in your configuration folder and looks something like this: - pipeline.id: apache pipeline.batch.size: 125 queue.type: persisted path.config: \"/path/to/config/apache.cfg\" queue.page_capacity: 50mb - pipeline.id: test pipeline.batch.size: 2 pipeline.batch.delay: 1 queue.type: memory config.string: \"input { tcp { port => 3333 } } output { stdout {} }\" This YAML file contains a list of hashes (or dictionaries), where each one represents a pipeline, and the keys and values are setting names for that pipeline. The values of settings which are omitted fall back to their default values (configurable in the already familiar `logstash.yml`), and only pipeline specific settings can be configured (i.e. setting `node.id` will throw an error)","locales":"","title":"Introducing Multiple Pipelines in Logstash"}
{"index":{}}
{"author":"Tim Roes","category":"Engineering","publish_date":"2017-08-10T00:00:00.000Z","url":"/blog/timelion-tutorial-from-zero-to-hero","seo_title":"Getting Started with Time Series Analysis in Kibana","content":" . Timelion is an visualization tool for time series in Kibana. Time series visualizations are visualizations, that analyze data in time order. Timelion can be used to draw two dimensional graphs, with the time drawn on the x-axis. What’s the advantage over just using plain bar or line visualizations? Timelion takes a different approach. Instead of using a visual editor to create charts, you define a graph by chaining functions together, using a timelion specific syntax. This sytnax enables some features, that classical point series charts don’t offer - like drawing data from different indices or data sources into one graph. This tutorial will first give a short introduction to the timelion UI in Kibana and will afterwards explain the timelion syntax and show several use cases, that you couldn’t or still cannot do with classical Kibana visualizations. The user interface You can access timelion from the main navigation on the left of the page. You can have multiple graphs on a timelion sheet. To add a new graph to the sheet use the link in the top menu bar. will create a completely new sheet. The input box on top of the window shows the expression for the currently selected graph. Click on a graph to select it for editing. All expressions you will see throughout this tutorial will be inserted into that textbox. Using the button in the menu, you can either store the whole timelion sheet with all its graphs or store the currently focused graph as a visualization, that you can place on any dashboard. The date range of the currently shown data can be influenced by the well-known date picker on the top right of the page. To set the scale of the x-axis, the selectbox beside the expression input is used. By default it’s set to “auto” which will automatically determine what a good scale would be depending on the selected time range. If you want to force e.g. one data point per day, you can set this to . Timelion expressions The simplest expression - that will also automatically be used for new graphs is the following: Timelion functions always start with a dot, followed by the function name, followed by parentheses containing all the parameters to the function (in this case just the asterisk). The (or if you are a fan of typing long words) function gathers data from Elasticsearch and draws it over time. If you don’t specify an index in the expression (we will see in a moment how you’re able to do this) all indexes of your Elasticsearch will be queried for data. You can change this default in the of Kibana by modifying the setting. By default the function will just count the number of documents, resulting in a graph showing the amount of documents over time. If you are entering that simple expression and only get a flat line, even though you selected a date range, which contains data: most likely your data doesn’t use as the name for the main time field. You can either change the default name via the setting in Advanced Settings, or use the parameter in the function to set it for an individual series. You will see how to set parameters in the next section. Function parameters Functions can have multiple parameters, and so does the function. Each parameter has a name, that you can use inside the parentheses to set its value. The parameters also have an order, which is shown by the autocompletion or the documentation (using the button in the top menu). If you don’t specify the parameter name, timelion assigns the values to the parameters in the order, they are listed in the documentation. The fist parameter of the function is the parameter (for query), which is a used to filter the data for this series. You can also explicitly reference this parameter by its name, and I would always recommend doing so as soon as you are passing more than one parameter to the function. The following two expressions are thus equivalent: Multiple parameters are s","locales":"","title":"Timelion Tutorial – From Zero to Hero"}
{"index":{}}
{"author":"Shane Connelly","category":"Engineering","publish_date":"2017-08-10T00:00:00.000Z","url":"/blog/elasticsearch-sequence-ids-6-0","seo_title":"","content":" What If...\"What if\" questions are fun. \"What if you had a time machine: when would you visit?\" \"What if you had one wish and it would come true: what would it be?\" They're the types of hypothetical questions you can ask at a dinner party and get insights about what interests and motives somebody has if barriers were broken down. A few years ago at Elastic, we asked ourselves a \"what if\" that we knew would lead to interesting insights: \"What if we had total ordering on index operations in Elasticsearch? What could we build?\" The answers were far ranging: All of this requires one little barrier to be broken down: adding sequence numbers to index operations. Easy: we just need to add a counter to every operation in the primary! So easy, we saw several of community members and employees trying their hand at it. But as we peeled back the layers of the onion, we than what it first appeared. Almost 6 years after we were first discussing how useful a Changes API would be, we still don't have one. What gives?! The purpose of this blog is to share what happened behind the scenes and give some insights into the answer for this question. In the last two years, we practically rewrote the replication logic from the ground up. We took the best things from known , while making sure we could still ensure the parallelism that makes Elasticsearch so fast: something many if not all traditional consensus algorithms can't do. We with distributed systems specialists and built a specification for . We added lots of and . This blog is necessarily technical, as we dig into some of the guts of how Elasticsearch does replication. However, we've aimed to make it accessible to a wider audience by explaining/defining/linking to some terminology, even if you may already understand it. First, let's dive into some of the challenges Elasticsearch deals with. ChallengesBefore we go much further, we have to talk a bit about our replication model and why it matters. In an Elasticsearch data index, data is split up into what are called \"shards\" which are basically sub-divisions of the index. You may have 5 primary shards (basically 5 sub-divisions of the index) and each primary shard may have any number of copies (called ) of that primary. But it's important that there's only 1 \"\" (often shortened to ) for each sub-division. The primary shard accepts (indexing operations are things like \"add documents\" or \"delete a document\") first and . It is therefore fairly straightforward to keep incrementing a counter and assign each operation a sequence number before it's being forwarded to the replicas. And as long as nobody ever restarts a server, you have a network with 100% uptime, your hardware doesn't fail, you don't have any long Java garbage collection events, and nobody ever upgrades the software, this straightforward approach actually works. But we live in the real world and it's when these assumptions change that Elasticsearch can end up in a \"failure\" mode and \"\" process. If they affect a node running a primary shard, it may require the primary to step down and for another replica to take its place. Since the change can come abruptly, it is possible that some ongoing indexing operations were not fully replicated yet. If you had 2 or more replicas, some of those operations may have reached one and not the other. Even worse, because Elasticsearch indexes documents concurrently (which is part of why Elasticsearch is so fast!), each of those replicas can have a different set of operations that do not exist in the other one. Even if you run with one replica (the default setting in Elasticsearch) there might trouble. If an old primary copy comes back and is added as a replica, it may contain operations that were never replicated to the new primary. All of these scenarios have one thing in common: the history of operations on shards can diverge upon primary failure and we need some way to fix it. Primary Terms & Sequence","locales":"","title":"Sequence IDs: Coming Soon to an Elasticsearch Cluster Near You"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2017-08-10T00:00:00.000Z","url":"/blog/brewing-in-beats-postgresql-module-in-filebeat","seo_title":"","content":" Welcome to ! With this weekly series, we're keeping you up to date with what's new in Beats, including the latest commits and releases. Filebeat module for PostgreSQLThe collects, parses, and visualizes the logs created by PostgreSQL instances. It can parse both the error logs and the slow logs, which in the case of PostgreSQL, go into the same set of files. We’ve now got support for PostgreSQL in Filebeat, Metricbeat, and Packetbeat, so you can monitor not only the logs but also its health by collecting periodically metrics from the SQL server and the network traffic exchanged with the PostgreSQL server. This module is currently scheduled to be released in version 6.1. Screenshots: New community Beat: CloudwatchlogbeatCreated by , harvests data from the AWS Cloudwatch Log Groups and ships them to Elasticsearch or Logstash. The log groups are periodically probed for new streams which are then polled for new events.The state of the beat is saved in a user-specified S3 bucket on a per-stream basis. This way, the Beat knows what is the last event that was harvested per stream and can resume its operation once restarted. Other changesRepository: elastic/beatsAffecting all BeatsChanges in master: MetricbeatChanges in master: Changes in 6.0: FilebeatChanges in 6.0: PackagingChanges in 6.0: DocumentationChanges in master: Changes in 6.0: ","locales":"","title":"Brewing in Beats: PostgreSQL module in Filebeat"}
{"index":{}}
{"author":"Suyog Rao","category":"Releases","publish_date":"2017-08-08T00:00:00.000Z","url":"/blog/logstash-6-0-0-beta1-released","seo_title":"","content":" We are excited to announce the released of beta1 for Logstash 6.0.0! As always, your feedback is super important to us, so please download this release and provide us any feedback you have! Talking about feedback, we hope you are familiar with our for pre-releases! You could win plenty of swag, and even a ticket to ElasticON 2018 when you provide feedback! This release is packed with exciting features! So without further ado, here are the highlights: Pipeline ViewerThe monitoring UI got a significant upgrade with this new feature. Users can now visualize their often complex pipeline configuration as a directed acyclic graph (DAG) representation. This UI provides a simple way to understand the overall pipeline topology, data flow, branching logic, and granular plugin level metrics. We overlay important metrics such as events per second, and time spent in milliseconds for each plugin in this view. Plus, there are visual indicators (colored labels) on the components when events spend extra time in certain plugins — this should draw your attention to the problem areas, providing an easy way to diagnose bottlenecks and optimize them.Centrally manage configurationsThis Configuration Management feature allows you to store and manage Logstash configurations remotely with Elasticsearch and Kibana. Logstash nodes can be configured to periodically poll new configuration updates from Elasticsearch and automatically apply changes without restarting the process. As part of this, we've also built a CRUD UI in the management section of Kibana to manage the Logstash configurations. This feature is part of the which can used for free for 30 days! This a transformational step for Logstash and makes managing a fleet of Logstash easier. If you manage a logging as a service platform using Logstash, this adds a level of self-service to your deployment. We are already exploring features like rollback and auditing on our short-term roadmap! Ingest to Logstash convertorEver wanted to migrate from Ingest Node to Logstash? Here's some of the reasons why you'd want to do that: We now have a CLI tool that takes an in JSON and produces a corresponding Logstash configuration DSL. You can then run this configuration natively in Logstash, make changes to it etc. $LS_HOME/bin/ingest-convert.sh --input file:///tmp/ingest/apache.json --output file:///tmp/ingest/apache.confFeedbackPlease , try it out, and let us know what you think on Twitter () or in our . You can report any problems on the . ","locales":"","title":"Logstash 6.0.0-beta1 released"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2017-08-08T00:00:00.000Z","url":"/blog/elasticsearch-6-0-0-beta1-released","seo_title":"Elasticsearch 6.0.0-beta1 released","content":" We are excited to announce the release of , based on . This is the third in a series of pre-6.0.0 releases designed to let you test out your application with the features and changes coming in 6.0.0, and to give us feedback about any problems that you encounter. Open a bug report today and become an . This is a beta release and is intended for testing purposes only. Indices created in this version . Upgrading 6.0.0-beta1 to any other version is not supported.Also see:You can read about all the changes in the release notes linked above, but there are some big changes worth mentioning below. ","locales":"","title":"Elasticsearch 6.0.0-beta1 released"}
{"index":{}}
{"author":"Matt Bargar","category":"Engineering","publish_date":"2017-08-09T00:00:00.000Z","url":"/blog/building-a-better-search-experience-in-kibana","seo_title":"","content":" Search is important. It’s kind of the reason why the Elastic Stack exists. It’s why a query bar sits atop almost every app in Kibana. Search is an integral part of the experience. For such a core part of Kibana, the current query bar is somewhat lacking. It gets the job done. Some power users can really make it sing. We’re missing some basics though. Autocomplete would be nice. Scripted field support would be good. Some dynamic, in page help wouldn’t hurt either, instead of a static link to the docs. ","locales":"","title":"Building a Better Search Experience in Kibana"}
{"index":{}}
{"author":"Tyler Hannan","category":"Releases","publish_date":"2017-08-08T00:00:00.000Z","url":"/blog/elastic-stack-6-0-0-beta1-released","seo_title":"","content":" 6.0.0-beta1 is available today! Before you get too excited, keep in mind that this is still a beta so don’t put it into production. There is no guarantee that any of the 6.0.0-beta versions will be compatible with other pre-releases, or the 6.0.0 GA. We strongly recommend that you keep this far, far away from production. And, since it is an beta, it is not available on Elastic Cloud. During the 5.0 release, we introduced the Elastic Pioneer Program and are continuing the with the 6.0 preview releases. Elasticsearch For more detailed information, and many other features, peruse the Elasticsearch . Kibana ‘Visualize’ all the features, and ‘Discover’ more information in the . Logstash For more information, grok the . Beats We don’t ‘let the beat drop’ but we drop the updates in a . ES-Hadoop We’ve reduced the details to some bullets, but the original map is in the . Get It Now! ","locales":"ja-jp","title":"Elastic Stack 6.0.0-beta1 released"}
{"index":{}}
{"author":"James Baiera","category":"Releases","publish_date":"2017-08-08T00:00:00.000Z","url":"/blog/eshadoop-6-0-0-beta1-released","seo_title":"","content":" ​I am excited to announce the release of Elasticsearch for Apache Hadoop (aka ES-Hadoop) built against Elasticsearch 6.0.0-beta1. : This is an beta release and is intended for purposes only. Crazy things might happen when running this code and indices created with this version . For the sake of your own sanity, we do not advise using this version in production. What’s new? Spark 2.2.0 and Stable Support for Spark Structured Streaming Spark 2.2.0 landed on July 11th and we spared no time in making sure we work on top of it. What’s with all the excitement? Why, has graduated to GA status! This means that we’re no longer treating our Structured Streaming integration in ES-Hadoop as an experimental integration as of this beta release. Please note that due to its experimental nature in prior versions, we will only be supporting our Structured Streaming integration on Spark versions 2.2.0 and above. Don’t fret though - This doesn’t impact our existing Spark integrations at all. Support for new Join Fields The days are numbered for Multi-typed indices in Elasticsearch. Users who work with Parent-Child based data need not worry about the future due to the advent of the new “join” field type in Elasticsearch. With this beta release, we are rolling out support for reading and writing data with this new field type. We’re excited to hear your feedback on this new feature! Multiple Mappings and Multiple Index Reads We took a long hard look at how we handle Elasticsearch mappings in the connector. After that long hard look we re-wrote a healthy chunk of code to fix an unhealthy bunch of problems. In this release you should no longer be bitten by common errors when reading from multiple indices (each with varying field types). The connector will also alert you when the indices you’re reading from have conflicting mappings in them. Check Out Our Bug Collection Nested Java Bean serialization problems, field exclusion problems on Pig and SparkSQL, partial document reads and serialization exceptions, all fixed in this release. Take a look at in this release! Feedback Now you might be wondering, “Why would I want to try a Beta Release? Aren’t these things normally riddled with bugs?” Well, yeah, sometimes. That’s why we need the help from all of you awesome early adopters! So, please, try this at home! You can ES-Hadoop 6.0.0-beta1, try it out, find out how it breaks, and let us know what you did on , , or in the . A crisp high five is waiting for all who participate! Not a huge fan of high fives? There’s always the instead! ","locales":"","title":"Elasticsearch for Apache Hadoop 6.0.0-beta1 released"}
{"index":{}}
{"author":"Tyler Hannan","category":"News","publish_date":"2017-08-08T00:00:00.000Z","url":"/blog/elastic-pioneer-program-6-0","seo_title":"","content":" Today, we released the beta1 of version 6.0 of the Elastic Stack and X-Pack. You can test the new goodness in the Elastic Stack today, by . We’re very excited about these upcoming version 6.0 preview releases, but we want to make sure they’re perfect (or as perfect as software gets) before they ship more broadly. Everyone who reports a legitimate bug on a 6.0 preview release of Elastic Stack will be recognized for the final release with a special thank you. If you find something particularly tricky, you could also earn a free ticket to . Why would I participate? The Pioneer Program during the 5.0 release helped us identify issues that we could only find with the assistance of the community – whether customers or open source users. The preview period is incredibly important. As much as we test the software before release, there are users with environments we haven’t conceived of, and folks using the Elastic Stack in ways we’ve never seen. We want to know the limits of our stack, and do everything we can to improve it before general release. We’re re-launching the Elastic Pioneer Program with the 6.0 release to encourage folks to try the latest version. How to participate To join the program, just try out the preview of any (preferably, every) part of the Elastic Stack, and open issues as you find them in the appropriate repo (, , , ) or forum (). When you open an issue, mention that you found the bug in 6.0.0-alpha1/2/beta1, and we’ll add a “Pioneer Program” label. While we appreciate the information, duplicate issues won’t enter you into the program. This is a preview release and is intended for testing purposes only. There is no guarantee that any of the 6.0.0-beta versions will be compatible with other previews, or the 6.0.0 GA. We strongly recommend that you keep this far, far away from production. And, since it is in beta, it is not available yet on Elastic Cloud. We hope you’ll join us in trying the preview versions of the Elastic Stack 6.0! ","locales":"","title":"Boldly Go - Elastic Pioneer Program"}
{"index":{}}
{"author":"Monica Sarbu","category":"Releases","publish_date":"2017-08-08T00:00:00.000Z","url":"/blog/beats-6-0-0-beta1-released","seo_title":"","content":" Today the Elastic stack 6.0 is leaving the Alpha phase behind and boldly enters the Beta. The Beta1 brings tons of improvements to your favorite Beat, be it Filebeat, Heartbeat, Metricbeat, Packetbeat, or Winlogbeat. Not only that, but we also have a surprise: the first public release of Auditbeat. Before going through the highlights, let’s get those handy links out of the way: Say Heya to AuditbeatIf you read the , you know that we have been working on harnessing the auditing framework of the Linux kernel. The audit framework unlocks tons of valuable data, including detection of short lived processes, network connections, unauthorized file accesses, and so on. Check out the talk from Elastic{ON} for an inspiring presentation on how the audit data and the Elastic stack are used against attackers. In Beta1, we’re doubling down on this approach by adding a new Beat to our collection: Auditbeat. Auditbeat works as an alternative to the auditd daemon, but one that is designed to integrate perfectly with the Elastic stack. If you have tried to write parsing rules for the auditd logs, you know it’s not exactly straightforward. Auditbeat not only does that for you, but it also automatically groups related events together in one document, so that everything is ready to visualize in Kibana. Because it’s built on top of libbeat, our mature framework for building Beats, it can send the events via secure and back-pressure sensitive connections to Logstash, Elasticsearch, and Kafka. In addition to collecting audit logs from the Linux kernel, Auditbeat also has a file integrity module. It watches files and directories for changes, and when a file changes it computes the MD5, SHA1, and SHA256 hashes and publishes them to Elasticsearch. The hashes can be compared against known malicious files, like it is done in this . This functionality is available on Windows, macOS, and Linux. New commands and configuration layoutWe rethought the way the Metricbeat and Filebeat modules are enabled and configured. Instead of one huge configuration file, we now have a directory with individual configuration files for each module. The Beats also get commands to list, enable or disable modules. For example: $ metricbeat modules list $ metricbeat modules enable redis $ metricbeat modules disable redis And these are not the only useful new commands. There are also commands to export the configuration, the Elasticsearch mapping template, to do a test fetch for a Metricbeat module, and even to test the connectivity with Logstash or Elasticsearch. Here is an example run for the command: $ filebeat test output elasticsearch: https://61fc3e1983ebd7b13e9d6098e4651345.us-east-1.aws.found.io:9243... parse url... OK connection... parse host... OK dns lookup... OK addresses: 54.235.139.222, 54.221.244.80, 54.243.122.128, 54.204.28.14, 54.221.223.25, 54.243.118.44, 54.243.142.98, 54.235.122.205 dial up... OK TLS... security: server's certificate chain verification is enabled handshake... OK TLS version: TLSv1.2 dial up... OK talk to server... OK version: 5.5.0 Add Docker metadata to the Docker logsThe processor, which we introduce in alpha2, got a little smarter. A little smarter, and a lot more useful. You might know that when using the default logging driver in Docker, the log files are placed under a path like . The processor can now extract the container ID from the file path and use it to query the Docker API for the metadata. This makes it . Internal pipeline refactoring6.0.0-beta1 also comes with a refreshed internal pipeline architecture. While this change is mostly internal, meant to simplify and improve the Beats overall architecture, it does have some visible effects. The new pipeline is asynchronous by default, meaning, for example, that while Filebeat is waiting for a network acknowledgement from Logstash/Elasticsearch, it continues to read and process lines from disk. We expect this to bring an increase in the maximum throughput. The Filebeat in","locales":"","title":"Beats 6.0.0-beta1 released"}
{"index":{}}
{"author":"Jim Goodwin","category":"Releases","publish_date":"2017-08-08T00:00:00.000Z","url":"/blog/kibana-6-0-0-beta1-released","seo_title":"","content":" This is the first beta release of Kibana 6.0.0. This release has a lot of new features including: Upgrade Assistant and Rolling Upgrade Support, Watcher UI for Threshold Based Alerts, an Experimental Kibana Query Language, Refactor of the Visualization Code, X-Pack Monitoring Email Notifications for Cluster Alert, Cluster Alert for X-Pack License Expiration, New Colors to Improve Accessibility, and Full Screen Mode for Dashboard. As well as over 111 enhancements and 64 bug fixes across Kibana and X-Pack, see the release notes for all of the details.Remember, these beta releases cannot be used in production they are for evaluation purposes only, they contain known bugs and are not feature complete. Kibana 6.0.0-beta1 is available on our  and please take a look at the complete . Please join our and help us improve the Elastic Stack and earn recognition for your efforts along with a cool gift package. To aid in migrating to version 6 of the Elastic stack, we are providing an upgrade assistant as part of X-Pack Basic (free) in 5.6.0. The upgrade assistant identifies to index and cluster settings and allows you to enable deprecation logging. Internal indices used by the stack will need to be reindexed, in addition to any indices created in 2.x. The assistant provides a one-click solution to these migrations.  After resolving deprecation issues and migrating indices, you are ready to upgrade your stack. Elasticsearch 5.6 supports rolling upgrades to 6.x, and Kibana 5.6 can run in tandem with an Elasticsearch cluster undergoing a rolling upgrade. Once the full Elasticsearch cluster has been upgraded, you can upgrade Kibana to 6.x. We've introduced a new UI for creating and editing alerts based on thresholds. It includes a builder experience with type-ahead suggestions and graphical feedback based on previewing the alert constraints. It supports sending alert messages with template values to the log, email, or slack. See the demonstration animation below for a quick look at the new functionality: In  we introduce an Experimental Kibana Query Language it is disabled by default and can be enabled through the Kibana configuration. Kibana currently provides four different search mechanisms with overlapping responsibilities: Exposing the Lucene query syntax and the query DSL to users creates a few problems. Since we don't control the query syntax we can't implement features that would require introspection into a user's query. This includes things like: We could solve these problems by building a model in Kibana to represent raw Elasticsearch queries, but there are other advantages to building our own query language: So, we hope you'll turn on the Kibana Query Language and give it a spin and send us feedback! When creating new visualizations, developers are no longer restricted to using just Angular as a rendering technology. The code now also enables developers to create custom editors that do not conform to the current sidebar-layout. Commonly used functionality - such as access to the query bar or time filter - is now also exposed on the visualization object. This avoids the need to import individual modules from inside Kibana. These changes are a first step in a longer term effort to provide a robust long-lived programming interface for building visualizations in Kibana.Cluster Alerts in Monitoring was added in the 5.4 release, but until now the alerts only appeared on the Overview page of the Monitoring app. This new feature allows you to receive email notifications when the alerts are triggered. To use it, go to the Advanced Settings page in Kibana Management, enter an email address for `xpack:defaultAdminEmail`, and click Save:The built-in alerts will send an email to that address when they initially trigger, and when they're resolved. Using this feature does require that your Elasticsearch nodes are configured for the ability to send emails from watches. If you haven't set that up yet, take a look at the","locales":"","title":"Kibana 6.0.0-beta1 is released"}
{"index":{}}
{"author":"Jim Goodwin","category":"Kurrently in Kibana","publish_date":"2017-08-08T00:00:00.000Z","url":"/blog/keeping-up-with-kibana-2017-8-7","seo_title":"","content":" Welcome to This is a weekly series of posts on new developments in the Kibana project and any related learning resources and events. Join us for a webinar on August 16 to discuss operational log and metric analytics. Register now: — elastic (@elastic) ","locales":"","title":"Keeping up with Kibana: This week in Kibana for August 7, 2017"}
{"index":{}}
{"author":"Sherry Ger","category":"Engineering","publish_date":"2017-08-07T00:00:00.000Z","url":"/blog/small-medium-or-large-scaling-elasticsearch-and-evolving-the-elastic-stack-to-fit","seo_title":"Scaling Elasticsearch, Kibana, Beats, and Logstash","content":" I and my colleagues here at Elastic support customers large and small, from Fortune 500 companies to startups. They have a wide range of use cases:  the size of their Elastic Stack deployments vary greatly too. In this post, we will focus on scaling the Elastic Stack to collect logs and metrics data and visualize them in Kibana. We will follow a pattern of ingest, store, and use. Modularity, flexibility, most of all, simplicity, are our main goals. We will examine a number of deployment topologies in increasing complexity based on our knowledge of the Elastic products and the experience of our customers. We will demonstrate how to expand from a minimalist pattern to multi-data centers logging platform. The Minimalist It all starts with exploring and visualizing logs, events, and metrics. Great, we got you covered! and are simple tools for a simple task. Output from Beats to Elasticsearch, the data is ready to visualize in Kibana. But wait, we can make this better with . We can parse log events and enrich the documents with custom fields. There are 120 predefined reusable patterns available to the Elasticsearch :  plus there are dozens of other ingest node processors to add new data fields or mutate existing ones. Together with , we are off collecting, parsing, and visualizing common log formats like Apache or Nginx HTTP server, or Syslog. As an added bonus, since we have Elasticsearch and Kibana set up, adding , , and to collect network and system stats and uptime, index them into Elasticsearch, and explore the data using Kibana is a cinch. To make it even easier, each of these Beat modules come with a . Include Elastic’s in the stack, we have encryption and secured transmission between Beats and the Elasticsearch cluster. Doing More Why stop at logs and metrics? What about other inputs, like Kafka, AWS Kinesis, Google Pub/Sub, TCP, UDP, JDBC, S3, SQS? Add Logstash to the stack. Besides being able to collect data from a variety of inputs, if we direct the Beats output through Logstash before reaching Elasticsearch, we can now do advanced like aggregate or such as translate, dns, JDBC streaming, or Elasticsearch, which provide the ability to gather data from multiple external sources into a single document. Furthermore, Logstash can send data to S3 or HDFS for archiving in addition to indexing into Elasticsearch. Safety First I know one of our goals here is simplicity. What if Elasticsearch or Logstash is overloaded? Or what if Logstash or its host is terminated abnormally? What to do when things go wrong will add complexity to our stack. Fortunately, with just what we have in the current pattern, there are a number of built-in resiliency mechanisms that provide some safety net. That said, this pattern does have limitations: Adding a Message Queue to the Stack To address the Logstash persistent queue limitations, we will need to introduce a queuing mechanism like to the stack. The added benefit of having a message broker is to decouple the indexing module from the collection module. The collection tier or the collectors, are the Beats and Logstash instances that sit between the inputs and the message queue. The indexing tier or the indexers are the Logstash instances between the message queue and Elasticsearch. We can add or subtract from each tier independently. This will be very valuable as more inputs and use cases feed into the stack. Centralized Logstash As more user groups and use cases become customers of our stack, we are evolving into a logging platform or logging as a service. How to manage configurations will be a key factor in selecting a deployment topology. In that, we see two common patterns, each with its own advantages and drawbacks. One pattern is to let each user group control their own Elasticsearch document schema and the configuration o","locales":"","title":"Small, Medium, or Large - Scaling Elasticsearch and Evolving the Elastic Stack to Fit"}
{"index":{}}
{"author":"Thomas Grabowski","category":"Engineering","publish_date":"2017-08-03T00:00:00.000Z","url":"/blog/sizing-machine-learning-with-elasticsearch","seo_title":"","content":" Many organizations have started using Elastic's machine learning for their , , and other projects that make use of anomaly detection for time series data.  One of the first questions we get from someone setting up machine learning is how to size their hardware and cluster most effectively. Sizing your environment for machine learning requires that you already have some idea of how to size Elasticsearch. To begin sizing for machine learning you should already have done your sizing calculations for data collection, mappings, and indexes.  We will not cover Elasticsearch sizing in this article, but there is an } that would be useful to review. Like many sizing questions, the sizing of hardware and cluster for machine learning depends on the use-case. There are many variables to consider when configuring machine learning.  We can guide you through some of the best practices, but ultimately sizing is something that needs to be customized for your unique situation. Questions that should be considered when sizing an environment for machine learning are: When you initially start working with machine learning, most of the time you don't know all the answers to these questions.  Testing configurations is usually an iterative process to get the answers you need for sizing the necessary resources.  It is a good idea to set up a lab environment first to try different configurations before moving to production. Sizing a lab to experiment with machine learningAcquire a sample data set for testingWhen setting up a lab environment you will need to start by collecting enough data to establish a model for machine learning. A good model for testing the subset should contain at least three weeks worth of data. When testing we recommend using actual data from one of your production systems rather than synthetic data. For machine learning testing, carve off a subset of data by a few devices over 3 or more weeks rather than pull an entire data set for a few days. Hardware resources for lab testingFor lab configurations you don't have to run dedicated machine learning (ML) nodes:  by default, any node with X-Pack installed will be able to run machine learning jobs. However, in production, we strongly recommend using dedicated ML nodes, as machine learning processing requires additional CPU and memory. Running a single job may use up to 4GB of memory with the default configuration in addition to what Elasticsearch utilizes for memory. If utilizing existing Elasticsearch data nodes in the lab there needs to be enough headroom available in both RAM and CPU. The amount of headroom that is required is wholly dependent on the volume and complexity of the data you expect machine learning to analyze and the type(s) of analysis chosen for that data. In general, the more data volume, the more CPU is required. The greater the complexity of the data/analysis, the more RAM required. There are many variables involved so it might be prudent to start \"simple\" with your machine learning configurations and move up in complexity as you watch your cluster performance along the way. Types of machine learning jobs and how they affect system resourcesSingle metric machine learning jobs consume the least hardware resources of the different types of jobs.  They can often consume less than 1mb of memory and a few seconds of compute time once per time bucket span because there are not many variables.  The machine learning job only needs to pull the metrics per bucket span.  If you are doing mostly single metric jobs, you will probably want to increase the job limit from the default limit of 10 (in the ). Multivariate machine learning jobs can be configured through the or advanced option within the Kibana interface.  These jobs can consume significantly more memory than jobs because of the number of variables they need to evaluate.  The machine learning algorithms model each selected metric for each unique term in the se","locales":"","title":"Sizing for Machine Learning with Elasticsearch"}
{"index":{}}
{"author":"Dale McDiarmid","category":"Engineering","publish_date":"2017-08-03T00:00:00.000Z","url":"/blog/integrating-elasticsearch-with-arcsight-siem-part-6","seo_title":"","content":"   Following on from our most recent , where we attempted to identify unusual processes using X-Pack Alerting, in this post we explore a more automated approach to the same challenge using machine learning.   The earlier post relied on effectively classifying any new process as unusual.  As highlighted in the closing footnote of that post, the approach does not represent an architecturally scalable or efficient means of identifying processes of interest -  rapidly leading to alert fatigue, unless other detection mechanisms and filtering are put in place e.g. restricting our query to a subset of servers of interest.  In practice, we can do .   X-Pack machine learning (ML) capabilities include automated anomaly detection, which allows us to spot occurrences of values of a field that are rare within a set of processes started on a server.  Using the partition field capabilities of Machine Learning, we effectively create an independent statistical model per server.  Our  ML job will use the to automatically gauge the level of rarity of a process simply by observing the relative frequencies of process names for that server over time. Processes that are truly rare will receive a high anomaly score, so rather than alerting on every new process, we can subsequently alert only on rare processes.   The ML Job we utilise below is well documented as an ML recipe .  Whilst the recipe example utilises auditd logs, captured with Filebeat, we adapt the configuration to the CEF data provided in the previous post. As a reminder, rare processes running on a host could be an indication of suspicious or malicious behavior.  For example an ftp process observed on a server that never runs ftp could be related to unauthorized access.  A rare process on a Windows system could likewise indicate the use of malware to steal credentials. Data OverviewAs a quick recap, our sample CEF data looks like: CEF:0|Unix|auditd||EXECVE|EXECVE|Low| eventId=30275 externalId=1737 rt=1495907409681 categorySignificance=/Informational categoryBehavior=/Execute/Response categoryDeviceGroup=/Operating System catdt=Operating System categoryOutcome=/Success categoryObject=/Host/Application/Service art=1495990987103 cat=EXECVE c6a4=fe80:0:0:0:5604:a6ff:fe32:b64 cs1Label=dev cs2Label=key cs3Label=success/res cs4Label=syscall cs5Label=subj cs6Label=terminal/tty cn2Label=ses cn3Label=uid c6a4Label=Agent IPv6 Address ahost=test-server-1 agt=192.168.0.12 av=7.3.0.7886.0 atz=Europe/London at=linux_auditd dtz=Europe/London deviceProcessName=auditd ad.argc=3 ad.a1=vim ad.a2=/etc/filebeat/filebeat.yml ad.a0=sudo aid=3xrP+T1wBABCAA5ZTdRz+fA\\=\\= Utilising the same auditd data subset as the watch in our previous post, we configure our ML job to process documents where the cat field contains the value EXECVE. The field ad.a0 represents the command issued, whilst ahost identifies the host from which the event originated. Finally rt, provides the start time of the process in epoch milliseconds. The Logstash , used to process this data with Logstash, maps the above fields to their standardised CEF form. The field is mapped to , to , to and the fields to as a concatenated string.  The latter is subsequently processed with a filter to produce the field Compared to the watch-based approach, ML requires larger datasets with which to build its statistical models.  We therefore provide a larger test set than the one we distributed with the previous post.  Fictitious anomalous processes have been injected into this dataset for purposes of example.  Instructions for obtaining this dataset, along with required Logstash configuration to ingest, can be found . Job ConfigurationAfter downloading the, we need to make a few minor modifications to utilise the fields above. Datafeed configuration { \"datafeed_id\": \"datafeed-unusual_process\", \"job_id\": \"unusual_process\", \"q","locales":"","title":"Integrating Elasticsearch with ArcSight SIEM - Part 6 - Detecting Unusual Processes with X-Pack Machine Learning"}
{"index":{}}
{"author":"Chad Pryor","category":"Engineering","publish_date":"2017-08-02T00:00:00.000Z","url":"/blog/getting-started-with-hosted-elasticsearch-v5-and-a-sample-dataset","seo_title":"Elastic Cloud Tutorial: Getting Started with a sample dataset","content":" Getting a environment up and running has never been easier. With Elastic Cloud, you can launch your cluster and start ingesting data in literally minutes. See how in This step-by-step set of instructions will walk you through setting up an Elastic Cloud account, creating and securing a Elasticsearch cluster, importing data, and visualizing it in Kibana. So, let's get started. Log into Elastic Cloud  Create your first hosted Elasticsearch cluster Enable KibanaNext, let's enable Kibana so we can configure cluster access and security: Secure your Cluster Next, let's configure cluster access and security, using the X-Pack security module, included with all Elastic Cloud accounts: You can update your passwords or add additional users using the same process. You may also use the new security API that is included in 5.5 by following the .Elasticsearch EndpointOnce you are logged into Kibana, you will first see the Discovery tab. However, there is no data to visualize. Next, we will work on ingesting data into Elasticsearch. Let's gather some information so we can be successful!   Import Data Now let’s get some data into our Elasticsearch cluster to see the Elastic Stack in action. If you don’t have a sample dataset handy, use one from the various data samples in our  I will be using the  and  (download your system version). To ingest the logs into our hosted Elasticsearch cluster, we will need to modify the elasticsearch output section of the . 1. Download the repository, and change to the directory that contains the  file. Be sure to replace hosts endpoint in the config with your own cluster endpoint (copied in the previous step) 2. Modify the username and password to the user account with write access configured Secure Elasticsearch section. I will be using our user which we added earlier: elasticsearch { hosts => \"https://e66e6e11692c749cc8e09f25e1af4efa.us-west-1.aws.found.io:9243/\" user => \"sa_admin\" password => \"s00p3rS3cr3t\" index => \"apache_elastic_example\" template => \"./apache_template.json\" template_name => \"apache_elastic_example\" template_overwrite => true } 3. Run the following command to index the data into Elasticsearch via Logstash: cat ../apache_logs | <Logstash_Install_Dir>/bin/logstash -f apache_logstash.conf 4. I find it useful to store the value of the ES_ENDPOINT in an environment variable, and this is the convention that I follow with some tests, below: export ES_ENDPOINT=https://somelongid.us-east-1.aws.found.io:9243 5. You can verify that your data exists in Elasticsearch by going to , where is the Elasticsearch endpoint URL. You should see the count as 10000.  Assuming that you set the environment variable above, you can do this using the curl command (make sure that you specify the user parameter for basic authentication for our https endpoint) %> curl --user sa_admin:s00p3rS3cr3t ${ES_ENDPOINT}/apache_elastic_example/_count {\"count\":10000,\"_shards\":{\"total\":5,\"successful\":5,\"failed\":0}} 6. You can verify the health of your cluster by going to . You should see your index listed along with its statistics: %> curl --user sa_admin:s00p3rS3cr3t ${ES_ENDPOINT}/_cat/indices health status index pri rep docs.count docs.deleted store.size pri.store.size yellow open .kibana 1 1 2 0 19.1kb 19.1kb yellow open apache_elastic_example 1 1 10000 0 7.3mb 7.3mb Visualize DataNow let's access your Kibana instance and continue with the example instructions to visualize our data: SummaryWe have successfully set up a cluster, imported sample data and looked at our first saved dashboard!  Now you have some sample Apache log data in Elasticsearch and you can begin to get some insight and more importantly value from your logs. You can continue exploring with other sample datasets from the  and the   or","locales":"","title":"Tutorial: Getting Started on Elastic Cloud with a Sample Dataset for hosted Elasticsearch and the Elastic Stack 5.x"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2017-08-03T00:00:00.000Z","url":"/blog/brewing-in-beats-graphite-module-in-metricbeat","seo_title":"","content":" Welcome to ! With this weekly series, we're keeping you up to date with what's new in Beats, including the latest commits and releases. Graphite module in MetricbeatThe graphite protocol is a very popular protocol for sending metrics data. While Logstash already has a graphite input, opens up use cases where an application sends its metrics over UDP to localhost, and Metricbeat ships them securely over the network to Elasticsearch / Logstash. - module: graphite metricsets: [\"server\"] enabled: true protocol: \"udp\" templates: - filter: \"test.*.bash.*\" # This would match metrics like test.localhost.bash.stats namespace: \"test\" template: \".host.shell.metric*\" # test.localhost.bash.stats would become metric=stats and tags host=localhost,shell=bash delimiter: \"_\" The configuration file allows patterns matching the graphite syntax and transforms the dotted notation in key-value pairs that fit the Elasticsearch data model. Here is an example: This module was created by the regular Beats contributor, . It’s currently planned for release in 6.1. Metricbeat support for Docker on WindowsDocker for Windows uses URLs like . This makes sure they are supported by our host validators, which was the only missing piece for the Docker module in Metricbeat to support Docker for Windows. This change will be included in 6.0.0-GA. Other changesRepository: elastic/beatsAffecting all BeatsChanges in master: MetricbeatChanges in master: FilebeatChanges in master: Changes in 5.6: AuditbeatChanges in master: PackagingChanges in master: DocumentationChanges in master: Changes in 5.5: Changes in 6.0: ","locales":"","title":"Brewing in Beats: Graphite module in Metricbeat"}
{"index":{}}
{"author":"Jim Goodwin","category":"Kurrently in Kibana","publish_date":"2017-07-31T00:00:00.000Z","url":"/blog/keeping-up-with-kibana-2017-7-31","seo_title":"","content":" Welcome to This is a weekly series of posts on new developments in the Kibana project and any related learning resources and events. ","locales":"","title":"Keeping up with Kibana: This week in Kibana for July 31, 2017"}
{"index":{}}
{"author":"Suyog Rao","category":"The Logstash Lines","publish_date":"2017-08-01T00:00:00.000Z","url":"/blog/logstash-lines-2017-08-01","seo_title":"","content":" Welcome back to The Logstash Lines! In these weekly posts, we'll share the latest happenings in the world of Logstash and its ecosystem.Benchmarking toolInspired by 's success (and design) for Elasticsearch, we've created a for Logstash. In the past, we've created similar tools that collect performance stats for LS, but they weren't based on the monitoring APIs introduced in 5.x. The new benchmarking tool just needs a JVM and is based on APIs built into LS. It also has a richer set of functionality. Users can run benchmarks on predefined configs and data sets like apache. You can perform a benchmarking via several ways: on a particular LS distribution version (5.4, 5.6 etc), on an installed LS, from a github hash/tag or on a branch. At the end of the run, it outputs stats like “throughput (events/sec)”, CPU usage, etc, to the terminal. This tool also has an option to ship performance stats directly to Elasticsearch. Internally, we plan to store nightly benchmark results to catch regressions. This is just the beginning though — there's plenty to do and polish up here. Next up, we want to add the ability to run benchmarks on custom configs and datasets, detailed documentation and more!Fixes in master and 6.0 ","locales":"","title":"Logstash Lines: Introducing a benchmarking tool for Logstash"}
{"index":{}}
{"author":"Carlos Pérez-Aradros","category":"Engineering","publish_date":"2017-07-31T00:00:00.000Z","url":"/blog/enrich-docker-logs-with-filebeat","seo_title":"Enriching logs with Docker metadata using Filebeat","content":" Docker, and containers in general, have certainly changed the way we deploy applications. With many benefits on scalability and reliability they also bring new challenges, and both the methodologies and tools we use need to be updated to the new ecosystem. Containers, unlike hosts, are ephemeral, a container can die in a host and trigger the creation of a new one in other. With this short lived instances of our applications we need the right data to track down these moving parts and keep up to speed with so many changes. As part of our push on Beats support of containers we recently implemented a new processor , that will released with 6.0.0 beta1. The idea is simple, yet really powerful. It enriches your logs and metrics with Docker , this way you gain full visibility into your infrastructure and applications. Let’s see it in action: Configuring FilebeatFilebeat can easily ship Docker logs, by default they are written by Docker under . As new containers are started, new files will be created to store their logs, following the same pattern, Filebeat can watch the entire directory and pick them as they appear. These would be the settings to ship Docker container logs to Elasticsearch and enrich them with the correct metadata. If you have , just edit filebeat.yml: filebeat.prospectors: - type: log paths: - '/var/lib/docker/containers/*/*.log' json.message_key: log json.keys_under_root: true processors: - add_docker_metadata: ~ output.elasticsearch: hosts: [\"elasticsearch:9200\"] Now to start shipping logs to Elasticsearch by running: $ ./filebeat -e -v Exploring logs in KibanaOnce logs start flowing into Elasticsearch, you can start watching them from Kibana interface, let’s have a look to one of them. This is one of the event reported by Filebeat, corresponding to a new log line in a NGINX server running on our Docker scenario: Thanks to  we not only get the log output but a series of fields enriching it, with useful context from Docker, like the container name, ID, Docker image, and labels! As an example, you may want to debug what’s going on in a specific container, you just need to filter your search results by your container name. That’s all! Simple things should be simple, and we strive to provide the best user experience, hiding the complexity where it belongs, in the code. Try it yourselfWe encourage everyone to join the testing effort and give this a try once beta1 is out! User feedback is important for us to shape features, so don’t hesitate to pass by and about this feature! ","locales":"","title":"Enriching logs with Docker metadata using Filebeat"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2017-07-25T00:00:00.000Z","url":"/blog/elasticsearch-5-5-1-and-2-4-6-released","seo_title":"Elasticsearch 5.5.1 and 2.4.6 released","content":" Today we are pleased to announce the release of , based on , and the bug fix release of Elasticsearch 2.4.6, the final release in the 2.x series. Elastisearch 5.5.1 is the latest stable release, and is already available for deployment on , our Elasticsearch-as-a-service platform.Latest stable release in 5.x:Latest stable release in 2.x:Please , try it out, and let us know what you think on Twitter () or in our . You can report any problems on the . ","locales":"","title":"Elasticsearch 5.5.1 and 2.4.6 released"}
{"index":{}}
{"author":"Jim Goodwin","category":"Releases","publish_date":"2017-07-25T00:00:00.000Z","url":"/blog/kibana-5-5-1-and-4-6-5-released","seo_title":"","content":" Hello, and welcome to the 5.5.1 and 4.6.5 release of Kibana!  These releases contains some small bug fixes and an important security fix. Please see details below. Kibana 5.5.1 is available on our and on . When you’re finished reading, take a look at the complete for all the goodies. Kibana 4.6.5 is also available on our   under \"past releases\" and only contains the changes for the security update. Kibana Node.js security flaw (ESA-2017-14) The version of Node.js shipped in all versions of Kibana prior to 5.5.1 contains a Denial of Service flaw in it's HashTable random seed. This flaw could allow a remote attacker to consume resources within Node.js preventing Kibana from servicing requests. All versions before 5.5.1 and 4.6.5 Administrators running Kibana in an environment with un-trusted users should upgrade to version 5.5.1 or 4.6.5. There is no workaround for this issue, the flaw can be triggered by an unauthenticated anonymous user. CVE-2017-11499 Visualization Design Platform Other Visualization ","locales":"","title":"Kibana 5.5.1 and 4.6.5 released"}
{"index":{}}
{"author":"Sophie Chang","category":"Engineering","publish_date":"2017-07-27T00:00:00.000Z","url":"/blog/explaining-the-bucket-span-in-machine-learning-for-elasticsearch","seo_title":"Bucket Spans for Elasticsearch and Machine Learning","content":" With the release of the Elastic Stack 5.5, machine learning (ML) is now a supported GA version. Along with this, we have introduced an experimental feature which allows the end-user to estimate the bucket span for an anomaly detection job. This is available when using the single or multi metric job creation wizards, but not when using the advanced configuration wizard. The bucket span estimator gives you the minimum viable bucket span based on the characteristics of a pre-viewed subset of your data. It’s a useful starting point for your analysis. However, once comfortable with the concepts of anomaly detection, then we urge you to delve a little deeper and make sure you have the right bucket span for your use case and your needs. Typically in our experience for machine data, bucket spans tend to be between 10 minutes and 1 hour, but it depends on the characteristics of the data. To explain more about bucket spans, and how to configure this appropriately, let me answer a few commonly asked questions. What is a bucket span?When analyzing data, we use the concept of a bucket to divide up a continuous stream of data into batches for processing. For example, if you were monitoring the average response time of a system, using a bucket span of 1 hour means that at the end of each hour we would calculate the average (mean) value of the last hour’s worth of data and compute the anomalousness of that average value compared to previous hours. Figure 1 shows a comparison of bucketed and non-bucketed data. By analyzing the data in buckets, we can more easily see significant changes, random fluctuations are smoothed, and there is a discrete range to model. As the time series data is gathered into buckets this also reduces the bandwidth and space required to process and store the data. This is important for online anomaly detection, especially in large scale environments. Why is the bucket span important?In machine learning terms, the bucket span and function define the feature we model. Choosing a shorter bucket span allows anomalies to be detected more quickly but at the risk of being too sensitive to natural variations or noise in the input data. Choosing too long a bucket span can mean that interesting anomalies are smoothed out. Figure 2 shows what the same data can look like when viewed at three different bucket spans. This chart plots the sum of a counter over time with coloured vertical lines denoting detected anomalies:  light blue is a warning, dark blue a minor, orange a major and red a critical anomaly. In the top most chart, a bucket span of 1 minute has been selected. This is as fine as the data rate, and the fluctuations in the data give rise to a very high background noise of anomalies. The periodic trend is not particularly visible. In the bottom chart, a bucket span of 1 hour has been selected. We can clearly see that the trend becomes significantly more visible with respect to the noise as a larger bucketing interval is used. However, there are noticeably fewer anomalies. Part of this reduction is due to the fact that anomalies are now only calculated hourly. However some anomalies have been hidden in the mass of data that exists in a larger bucket. Notice that the single important anomaly has a lower severity, and is only considered “major” rather than “critical”. For this data using the sum function, the use of a larger bucket has meant that the relative importance of the anomaly has been reduced in significance. In this example for this dataset, choosing a bucket span of 10 minutes provides an optimal balance of granularity, time to alert and prominence of the anomaly. How does the bucket span affect the time to alert?One might infer that (on average) an anomaly will be reported at least half a bucket span away from when it occurred. This is not true, but there are some caveat","locales":"","title":"Explaining the Bucket Span in Machine Learning for Elasticsearch"}
{"index":{}}
{"author":"Tim Vernum","category":"Engineering","publish_date":"2017-07-25T00:00:00.000Z","url":"/blog/strict-content-type-checking-for-elasticsearch-rest-requests","seo_title":"","content":" The Elasticsearch engineering team is busy working on features for Elasticsearch 6.0. One of the changes that is coming in Elasticsearch 6.0 is .        If you’re copying examples from our documentation, you’ll find that the the button automatically includes this option. ","locales":"","title":"Strict Content-Type Checking for Elasticsearch REST Requests"}
{"index":{}}
{"author":"Suyog Rao","category":"The Logstash Lines","publish_date":"2017-07-26T00:00:00.000Z","url":"/blog/logstash-lines-2017-07-26","seo_title":"","content":" Welcome back to The Logstash Lines! In these weekly posts, we'll share the latest happenings in the world of Logstash and its ecosystem.Similar to the Filebeat modules feature, Logstash can have its own modules now.  contain packaged Logstash configuration, Kibana dashboards and other meta files to ease the set up of the Elastic stack for certain use cases or data sources. The goal of these modules are to provide an end-end, 5 min getting started experience for a user exploring a data source without having to learn different parts of the stack.NetFlow is a network protocol developed by Cisco for collecting IP traffic information and monitoring network traffic. A large number of routers and switch vendors support exporting NetFlow packets via UDP. The Logstash Netflow module consists of a config that listens to UDP, parses all the netflow packets (multiple versions are handled) via the LS netflow codec. This module is based on the by our very own Solutions Architect, Rob Cowart.Here's is a screenshot of a Netflow dashboard from this module:Changes in 5.6 ","locales":"","title":"Logstash Lines: Introducing modules"}
{"index":{}}
{"author":"Jim Goodwin","category":"Kurrently in Kibana","publish_date":"2017-07-25T00:00:00.000Z","url":"/blog/keeping-up-with-kibana-2017-7-24","seo_title":"","content":" Welcome to This is a weekly series of posts on new developments in the Kibana project and any related learning resources and events. 5.5.1 and 4.6.5 released, includes an important security fix: — elastic (@elastic) New Visualizations in 5.x webinar starts in 30 min! Watch live and join the Q&A — elastic (@elastic) We've got a rather large two weeks worth of updates today. In 6.0, we’ve made a ton of progress toward our long-running goal of removing the raw proxies to Elasticsearch through the Kibana server.  We’ve removed the admin proxy entirely, which means all requests to things like the .kibana index or the .reporting indices has been removed to dedicate endpoints on the server.  The data proxy still exists, but we’ve restricted all operations on it to simply search and msearch requests. You'll note references in the Design sextion to K7, this is our project to significantly redesign and improve the Kibana user experience over the next major release or so. The first push will be to streamline and standardize the CSS model using our UI Framework. There is new experimental Kibana query language in 6.0 which has goals towards improving usability of queries wherever users need to interact with them. Also in 6.0 is a significant refactoring of the Visualize code base with the goal of making it simpler to implement new visualizations in a consistent reliable way. ","locales":"","title":"Keeping up with Kibana: This week in Kibana for July 24, 2017"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2017-07-24T00:00:00.000Z","url":"/blog/brewing-in-beats-test-command-for-metricbeat-modules","seo_title":"","content":" Welcome to ! With this weekly series, we're keeping you up to date with what's new in Beats, including the latest commits and releases. Test command for Metricbeat modules This adds a new test command for the Metricbeat modules. It goes over all the configured modules and executes a fetch for each, reporting the result back to the user, or any error that happened during the fetch. $ ./metricbeat test modules system... process_summary...OK result: { \"_namespace\": \"process.summary\", \"idle\": 0, \"running\": 1, \"sleeping\": 236, \"stopped\": 0, \"total\": 238, \"unknown\": 1, \"zombie\": 0 } nginx... stubstatus... error... ERROR error making http request: Get http://127.0.0.1/server-status: dial tcp 127.0.0.1:80: getsockopt: connection refused This functionality will be released in 6.0.0-beta1. Load dashboards via Kibana API in version 5.6 In the 5.x versions, the Beat loads the dashboards directly in the Elasticsearch index, and uses the script for that (available in the Beat package). Starting with 6.0 version, if the Kibana version is greater than 5.6, then the Beat uses the Kibana API to load the dashboards, otherwise it loads them directly to Elasticsearch. The adds the option in the 5.6 version to load the dashboards via the Kibana API from the script, if requested by the user. This would make it possible for Beats 5.6 to import the dashboards in the future Kibana 6.x versions. The default is still to insert the dashboards directly in Elasticsearch, but one can provide a Kibana URL, in which case the Kibana API is used. This functionality will be released only in 5.6. Other changes Repository: elastic/beats Affecting all Beats Changes in 5.x: Changes in master: Changes in 5.5: Metricbeat Changes in master: Filebeat Changes in 5.x: Changes in master: Winlogbeat Changes in master: Auditbeat Changes in master: Dashboards Changes in master: Machine learning jobs Changes in 5.x: Changes in master: Testing Changes in master: Changes in 5.5: Infrastructure Changes in master: Documentation Changes in 5.x: Changes in master: Changes in 5.5: Repository: elastic/go-libaudit Changes in master: ","locales":"","title":"Brewing in Beats: Test command for Metricbeat modules"}
{"index":{}}
{"author":"Jan Krynojewski","category":"User Stories","publish_date":"2017-07-24T00:00:00.000Z","url":"/blog/how-wirecard-uses-the-elastic-stack-to-monitor-transactions-and-analyze-errors","seo_title":"","content":" Since , Wirecard AG has developed into one of the world's leading, independent providers of outsourcing and white label solutions for electronic payments. Background & challengesAt Wirecard Technologies GmbH, the Service Delivery Acquiring Processing department is responsible for the smooth operation of the backend systems processing credit card payments. Our task is to take payments from e-commerce merchants, POS terminals and other channels, and deliver them to the networks of VISA, MasterCard, JCB and China UnionPay. Fast processing (i.e., short response times) and maximum service availability are of utmost importance. Because of the large number of distributed systems that are involved in processing each transaction, we had been relying on the central database cluster for our monitoring. This is where all the data and information that is relevant for our operations is stored. However, as the volume of transactions increased, it quickly became clear that this approach was not scalable. Each and every monitoring request led to an SQL query against the central tables. As a consequence, we constantly had to maintain the right balance between query frequency and system load, so that we could detect anomalies in the transaction processing quickly enough (such as unusually high numbers of error codes or slower run times), without compromising performanceFigure 1: Wirecard's previous monitoring processReal-time transaction monitoring with the Elastic StackIn the search for alternate solutions, we quickly discovered the Elastic Stack. Without any prior technical knowledge of the Elastic Stack and with a comparatively short PoC, our teams succeeded in showing that by combining Metricbeat, PacketBeat and FileBeat with Logstash, they were able to collect all the relevant data from our distributed systems, store it centrally in Elasticsearch, and then present it in Kibana — without having to send a single query to the database. Because we are using F5 BigIP load balancers in our infrastructure, we were able to leverage the BigIP's built-in high-speed logging functionality to filter out valuable information from the Application Layer 7, and stream the data in near real-time to Logstash's syslog input. There, we can transform the data by, for example, replacing IDs with terms, which greatly facilitates the visualization and interpretation of the data in Kibana. This whole setup is so efficient that there is no additional, measurable load on the transaction processing system. The setup of the Elastic Stack is comparatively simple: two central Logstash instances, each equipped with eight cores, receive the data from the load balancers and beat instances, and then pass on the data to the Elasticsearch cluster, consisting of three nodes. For our use case, this cluster size is ideal. It's a very low maintenance system, and yet it offers enough throughput to process our log and event data without any large delays. Our 24/7 Operations teams can now see all critical information through a central Kibana dashboard, and can monitor end-to-end processing of transactions in real time across system boundaries. Our Kibana dashboard shows us: Figure 2: Wirecard's Kibana dashboardUsing the X-Pack alerting feature, we were also able to implement granular checks, which are carried out every five seconds. These new checks, combined with the clear data display in Kibana, have significantly reduced our teams' response times when problems do occur. Thanks to Kibana's drill-down and filtering functionality, it is now much easier for us to analyze the root cause of errors, because all the information is stored centrally and can be easily correlated. Figure 3: Monitoring with the Elastic StackNext stepsOur positive experience with the Elastic Stack has convinced us to further expand this system and integrate more features. As a next step, we pla","locales":"de-de","title":"How Wirecard uses the Elastic Stack to monitor transactions & analyze errors"}
{"index":{}}
{"author":"Marty Messer","category":"Culture","publish_date":"2017-07-20T00:00:00.000Z","url":"/blog/it-depends","seo_title":"Elasticsearch Support and It Depends","content":" There are certain phrases that probably no one wants to hear. “I need you to work this Saturday”, “A raccoon is wrecking the kitchen”, “I love you, but . . .”, “Water is pouring from the ceiling.” None of these really make your day. But there is one phrase in the world of Technical Support that ranks up there with the worst: “It depends.”“It depends” usually comes as the answer to a question that often seems simple on its face (e.g. How many nodes should I have?), but belies a much more complex technical problem (e.g. Data volume? Query rate? Ingest rate? HA requirements? etc.). This phrase can create instant dread in the heart of the one asking. “It depends” evokes images of a mad scientist frantically pulling reagents off the shelf, dripping them into a flask between flashes of light, smoke, and thunder. It often signals to the asker that their question is too hard to explain to someone without a lesson in some dark arcana. Or it can mean the person simply doesn’t have the time to explain it. Or they don’t actually know the answer. But regardless, it’s ultimately not an answer—almost an excuse or deflection. It stinks, and everyone knows it.And herein lies the dilemma. At Elastic, “it depends” is a foundational part of how we talk, and therefore our culture. Almost every technical conversation seems to include these words somewhere along the way—between ourselves, between us and our customers, everywhere. How can such an ostensibly negative phrase be part of our very fabric? Elastic is a place of transparency: in thought, in speech, in code, built that way from our very foundations. We’re so empathetic, it can be overwhelming to new employees and new customers. How can this be?It’s simple, and it’s right in the title. That ellipsis is not just there for show. When we say “it depends” we say it as “it depends . . .” AND THEN WE GO ON TO TELL YOU WHAT IT DEPENDS ON. We spend the time. We take as much time as necessary for the asker to fully understand all the base knowledge required. We aren’t mad scientists:  we’re more like Bill, that Science Guy. We will delve into the arcana:  we’ll explain each reagent:  we’ll walk you through the process and distill it down until we arrive at that solution, and we’ll cancel meetings and stay up late (granted most of us are already in our pajamas—the beauty of a truly distributed company) to do it. Why? Because we ourselves are tinkerers and dreamers, students and scientists, explorers and waymakers. And we’re so passionate about what we do, our goal is for those coming to us for advice to be successful now and tomorrow when the next question arises. So prepare to drink deeply, because most importantly and most sincerely, we aim to make sure the asker knows what they should now DO (IOHBWIAWIO: In Our Humble But Well Informed And Well Intentioned Opinion), which is all they really wanted in the first place. Our culture doesn’t really lend itself to terse answers with no context. We don’t accept that title or position or tenure means anything when it comes to what we can learn from one another. When you ask an Elastician a question, and they start with “it depends…” hang on, shit is about to get real. ","locales":"","title":"It Depends..."}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2017-07-20T00:00:00.000Z","url":"/blog/brewing-in-beats-new-configuration-layout","seo_title":"","content":" Welcome to ! With this weekly series, we're keeping you up to date with what's new in Beats, including the latest commits and releases. Move to a modules.d layout for the configuration files This moves the per-module configuration in individual files inside a folder, and adds commands to list/enable/disable modules. As the number of modules in Metricbeat and Filebeat keeps growing, this makes the configuration more user friendly. Example commands: $ metricbeat modules list $ metricbeat modules enable redis $ metricbeat modules disable redis The all-in-one configuration file is still supported and included in the packages (as ). This functionality will be released in 6.0.0-beta1. Test command This adds a new command, which enables checking that the Elasticsearch and Logstash outputs are correctly configured, and which is extensible for future tests. We hope that this will simplify troubleshooting for many of our users. Here is an example output: $ filebeat test output elasticsearch: https://61fc3e1983ebd7b13e9d6098e4651345.us-east-1.aws.found.io:9243... parse url... OK connection... parse host... OK dns lookup... OK addresses: 54.235.139.222, 54.221.244.80, 54.243.122.128, 54.204.28.14, 54.221.223.25, 54.243.118.44, 54.243.142.98, 54.235.122.205 dial up... OK TLS... security: server's certificate chain verification is enabled handshake... OK TLS version: TLSv1.2 dial up... OK talk to server... OK version: 5.5.0 This functionality will be released in 6.0.0-beta1. Include Kibana dashboards in the individual Beats packages Before this , the sample dashboards were packaged in their own zip file, and the Beats (or the import_dashboards program) were downloading them on the fly at load time. The URL was computed automatically, so the download was in most cases transparent for the user. However, for non-released version, the URL had to be manually specified, which sometimes caused issues. It was also a source of issues in case of installations without Internet access. So we’ve decided to simplify things by including the dashboards in the Beats packages themselves. This functionality will be released in 6.0.0-beta1. Configuration reloading for Filebeat modules This adds support for live module configuration reloading, similar with what we have for the Metricbeat modules and Filebeat prospectors. A folder can be watched for configuration file changes, and the modules reloaded automatically in case a configuration is modified. This means one will be able to enable/disable modules (using the commands) without having to restart the Beat. This functionality will be released in 6.0.0-beta1. New Community Beat: Mqttbeat Created by , can act as an MQTT broker and index and received message into Elasticsearch. New Community Beat: Prometheusbeat Create by , allows forwarding the metrics from a Prometheus instance to Elasticsearch for long term storage and visualization. It works by opening an HTTP endpoint, compatible with the remote_write functionality of Prometheus. While we have support for scrapping Prometheus endpoints in Metricbeat, this is different in the sense that it is meant to be used side by side with the Prometheus server. Use UTC when computing index names Fixed a where index names that contain dates may be incorrect when the system timezone is not in UTC. Other changes Repository: elastic/beats Filebeat Changes in 5.x: Changes in master: Documentation Changes in 5.4: Changes in 5.3: Changes in master: Changes in 5.5: Affecting all Beats Changes in master: Auditbeat Changes in master: Testing Changes in master: Dashboards Changes in master: Packaging Changes in master: Winlogbeat Changes in master: Infrastructure Changes in master: Metricbeat Changes in master: Packetbeat Changes in master: ","locales":"","title":"Brewing in Beats: New configuration layout"}
{"index":{}}
{"author":"Colin Surprenant","category":"Engineering","publish_date":"2017-07-18T00:00:00.000Z","url":"/blog/logstash-persistent-queue","seo_title":"Logstash Persistent Queue","content":" In Logstash 5.1, persistent queue was introduced as a beta feature. As of Logstash 5.4, it has been officially promoted to a GA feature. Persistent Queue (PQ) has been a top feature request and is now recommended for production usage. It enables stronger resiliency with local disk persistence and allows Logstash to locally spool data to smoothly handle ingestion throughput spikes. Additional info on PQs is available in the documentation. Design What is PQ By default, Logstash uses in-memory queuing between the input and filter+output pipeline stages. If Logstash experiences a failure, the in-flight and in-memory data being processed will be lost. In order to protect against data loss during abnormal termination, Logstash introduced the persistent queue feature which stores input data on disk as an adaptive buffer, and uses pipeline acknowledgement back to the persistent queue upon processing completion. Any unacknowledged data (not fully processed and outputted) will be replayed upon restarting Logstash. Initial design goals Resiliency and Durability Failure Scenarios There are 3 types of failure that can affect Logstash. Durability PQ uses page files of size (by default ) to write the input data. The latest page is called the head page and is where the data is written in an append-only way. When the head page reaches it becomes a tail page which is immutable and a new head page is created to write new data.The durability characteristic of the head page is controlled using the setting. By default, PQ will force write data to disk at every 1024 received input events to reduce potential data loss upon OS level crash. Note that application and process level crashes or interrupted shutdowns should never result in data loss, regardless of the setting. If you want to protect against potential data loss for OS level crash you can set PQ for maximum durability using the setting, but note that this will result in a significant IO performance reduction. Performance The primary goal for these benchmarks is to show the impact of PQ on Logstash performance running on server-class high performance hardware using the default settings which we believe should be reasonable defaults for a majority of users. Obviously there are many tuning knobs in Logstash which can directly impact its performance so these benchmarks will vary depending on your specific configuration, settings and the hardware you run on. Persistent queue performance impact was measured using a custom script which polls and collects the Logstash stats API every second. The tests below were performed on Logstash 5.4.1 on an AWS EC2 c3.4xlarge instance using a typical Apache access logs processing configuration and a large enough sample logs file. The total runtime was 5 minutes with the first 30 seconds on each test ignored to account for the JVM warmup. The stats collecting script, config and logs file can be so you can run the benchmarks on your hardware and with your configuration and data. Apache configuration The benchmarks results show an approximately 10% performance penalty when using persistent queue in this configuration. Using in-memory queuing yielded an average of approximately 10600 events per seconds while using persistent queue approximately 9500 events per second. Usage Settings Specify to enable persistent queues. By default, persistent queues are disabled (default: ). The directory path where the data files will be stored. By default, the files are stored in . The maximum size of a queue page in bytes. The queue data consists of append-only files called “pages”. The default size is . The maximum number of events that are allowed in the queue. The default is (unlimited). The total capacity of the queue in number of bytes. The default is (1gb). Make sure the capacity of your disk drive is greater than the value you specify here. If both and are specified, Logstash uses whichever criteria is reached first. Checkpoints There a","locales":"","title":"Logstash Persistent Queue"}
{"index":{}}
{"author":"Daniel Palay","category":"User Stories","publish_date":"2017-07-18T00:00:00.000Z","url":"/blog/cbc-radio-canada-elastic-consulting","seo_title":"","content":" ***** What was the rationale behind redesigning the music products?The impetus of the project was to allow users to have seamless access to both editorial content and the music experience, which are managed by different teams in Montreal and Toronto, in either official language. Even though and are separate domains and sites, they have been designed and built by one integrated team with the goal of creating one unified user experience across both domains. How does the Elastic Stack fit into the project?  To maximize access and expose Canadians to the breadth of content created by the CBC Music and the ICI Musique teams, Elastic technology is used to provide results to user queries from both sites (wherever the initial query was launched from). A French search term will return results from icimusique.ca and its equivalent from cbcmusic.ca and vice-versa. Are there any other places where you are utilizing the Elastic Stack? Elastic also plays a less visible but important role in a module we call the DJ, which fetches songs on the fly, based on attributes and criteria, to create dynamic playlists that are different for every user. What consulting package did you utilize in the lead up to the project launch? We invested in the 4-day best practices and architecture offering, which gave the team the confidence that our approach was applying best practices and the solution was designed in a way to support future growth. How was your overall experience with your Elastic consultant?Team members mentioned that they appreciated the way that Saud (our assigned Elastic consultant) explained the ins and outs and the best practices for all of the Elastic Stack. The one-on-one time spent with Saud certainly allowed them to really dive deep into the exact problems we were trying to solve and, in the end, we were able to devise concrete solutions that will help well into the future.  Do you feel better equipped to proceed with the future of the redesigned offering because of the work you did during your Elastic consulting package? Yes, the time we spent with our Elastic consultant helped strengthen our knowledge of the Elastic Stack and as such provided reassurance that we are on the right track with regards to our decision to leverage the technology for the project in question. As we look towards the future, the team feels better equipped to make decisions for the project and when not possible, feel confident in discussing options with fellow developers on the project. .   ","locales":"","title":"Elastic Consulting: An Investment for Success at CBC/Radio-Canada"}
{"index":{}}
{"author":"Rich Collier","category":"Engineering","publish_date":"2017-07-17T00:00:00.000Z","url":"/blog/alerting-on-machine-learning-jobs-in-elasticsearch-v55","seo_title":"Alerting on Machine Learning Jobs in Elasticsearch","content":" Anomaly detection using Machine Learning (ML) in X-Pack is obviously extremely useful, but people often ask how to alert/notify on the anomalies. It should be of no surprise that this is accomplished with another X-Pack feature, Alerting (a.k.a. Watcher). Now, in version 5.5, ML can help you automatically create those watches to alert on detected anomalies! The interaction between ML and Alerting is extremely flexible given the capabilities of both products. This blog will focus on both the one-click “easy” approach to alerting on an ML job, but will also touch upon the details behind creating more advanced watches. Even so, this blog only scratches the surface of the full breadth of capabilities of both ML and Alerting. The Easy Way As you may know, there are currently 3 kinds of jobs that can be created in ML (Single metric, Multi-metric, and Advanced). In v5.5, both the Single and Multi-metric jobs have a “Create watch for real-time job” checkbox that appears after you create the job. This allows for the creation of a new watch on the job results as it continues to run in “real-time” with live indexed data: The user has two simple options: the over which the watch scans for recently created anomalies that the job might generate (back from when the watch is fired) and the minimum anomaly that should trigger the alert. Remember that anomalies are scored on a dynamic scale from 0 to 100, so “critical” anomalies are those in the top 25th percentile with a score above 75. By default, the is set equal to twice the defined for the job. This allows for any ingest, query, and processing delays (more on this later). In general, one shouldn’t shorten this time interval from the default. If your X-Pack Alerting setup has , then there will also be a option: Clicking on the button registers the watch, and gives you a link to edit the watch for further customization: If you want to edit the watch, clicking on the link will direct you to the Watcher UI: The Watcher UI is not only handy for editing the watch’s JSON definition, but you can also simulate/test the watch to see the output that the watch will return. The watch that ML creates for you always includes the logging action, even if you have chosen to use the email action. Modify this as necessary if you want to remove the logging action or if you want to have your watch initiate a . If you test the watch over a time range that you know there are anomalies (by temporarily modifying the in the input query for the watch), you could see something similar to the following in the : There you have it - a simple watch created for your ML job! The Advanced Way Perhaps you’re the type that isn’t satisfied with just the basics - you like to roll your sleeves up and customize things further. If so, read on! But, before we get too far along, let’s take a step back and first understand that ML’s results are “presented” at 3 different levels: Secondly, you have 2 ways that you can “get at” the results Which method (API or ES query) is preferable? There’s no “official” answer on this, but personally I like querying the ES indices directly because there’s more flexibility. Now, depending on what you’re trying to accomplish with the alerting, you’d choose the appropriate query at either the bucket level, the record level, or the influencer level. I’ll strongly recommend avoiding querying for anomalies at the record-level, especially for big, diverse data sets. Machine Learning is already smartly aggregating the results of anomaly records at the bucket and influencer levels so that the resulting information is rate-limited and easier to manage for big data. Over at our , I’ve created two example custom watches - one that queries for results at the bucket level, and another that gives detailed anomaly record information, but only if the agg","locales":"","title":"Alerting on Machine Learning Jobs in Elasticsearch v5.5"}
{"index":{}}
{"author":"Maxim Alferov","category":"User Stories","publish_date":"2017-07-12T00:00:00.000Z","url":"/blog/how-aerdata-improved-ocr-search-capability-using-elasticsearch","seo_title":"","content":" About AerDataAerData, a Boeing company, provides integrated software solutions for lease management, engine fleet planning, records scanning as well as technical and back office services for aircraft and engine operators, lessors and MROs (Maintenance, Repair and Overhaul). With a strong customer focus, AerData delivers a reliable and secure service to its clients using latest technologies and state of the art infrastructure. In 2002, AerData developed STREAM (Secure Technical Records for Electronic Asset Management), to deliver a solution for the management of hard copy technical records. This allows an airline, lessor or MRO, to scan, index and centralize all its records for sharing both within and outside its organization. This is achieved via a DMS (Document Management System), the Microsoft SQL Server database, which encompasses fast access searching and indexing of text and associated metadata. This can then be customized as per the client's specification via a powerful OCR (Optical Character Recognition) search tool. ChallengesThis solution worked satisfactorily for many years, but as we acquired more customers and amassed more data, the database became increasingly difficult to maintain. To stay ahead of the market and still achieve our SLAs (Service Level Agreements), additional resources were required to improve performance and functionality etc. We quickly realized that our current solution was no longer fit for purpose due to the following reasons: 1. SQL Server Full-Text search has a lack (or limited functionality) to: 2. Difficult to combine multiple filters. Very often users want to search for a combination of terms (e.g. aircraft serial number, date and some specific keyword in the documents). The more filters we have, the more joins we need, and when it comes to millions and millions of records, the query becomes unacceptably slow (from several seconds up to 3-5 minutes) 3. Due to the large amount of data, making backups and restoring them is a time-consuming process SpecificationFirstly, we put together a list of essential requirements as a baseline for choosing the right tools for the job. Below are some of the OCR project requirements: Considering these requirements and bearing in mind our very successful logging project using Elasticsearch with Kibana, this seemed a logical route to follow. Next, we defined the minimum amount of metadata that we needed to store in Elasticsearch, then we created field mapping utilizing search performance and disk space. As we progressed the process, we kept adding metadata definition. For these ongoing schema changes, we used the Elasticsearch dynamic template feature to version our schema/field mapping which helped to easily track down changes. As part of the search requirements, we created a custom tokenizer to search for full and partial keywords. This gave us the ability to perform whole word searches and search on a subset of characters (e.g. finding the word \"air\" in \"airline\"). Data migrationAt the beginning of the project, we had amassed a lot of data which needed to be indexed in Elasticsearch (millions and millions of records over multiple customers). The easiest approach was to take records individually and index them, but this simply wouldn't work as it took several days to index an average customer database. Thankfully Elasticsearch offered some which immediately improved the performance of the migration process. Data synchronizationAfter the initial indexing, we had to make sure that our data was up-to-date and synchronized. To achieve this, we implemented the following solution: There are two types of events: All events were re-tried at least three times, and in the event of any failure with exponential backoff, our system proved to be resilient to potential errors/outages. QueryingAs part of the business requirements, we needed a query that combined multiple metadata with full t","locales":"","title":"How AerData improved OCR search capability using Elasticsearch"}
{"index":{}}
{"author":"Ismael Hasan Romero","category":"Engineering","publish_date":"2017-07-12T00:00:00.000Z","url":"/blog/a-practical-introduction-to-elasticsearch","seo_title":"A Practical Introduction to Elasticsearch","content":" Why this post? I recently had the pleasure of teaching a Master’s class at the University of A Coruña, in the course Information Retrieval and Semantic Web. The focus of this lesson was to provide a general vision of Elasticsearch to the students so they would be enabled to start using Elasticsearch in the course assignments:  the attendees ranged from people already familiar with Lucene to people facing Information Retrieval concepts for the first time. Being a very late class (it started at 7:30PM) one of the challenges was to keep the attention of the students (or, in other words, to keep them from falling asleep!). There are two basic approaches to keep attention when teaching: bringing chocolate - which I forgot to do - and making the lesson as practical as possible. And this is what today’s post provides: we will go through the practical part of that same lesson. The goal is not to learn every single command or request in Elasticsearch (that is why we have documentation):  instead, the goal is that you experiment with the joy of using Elasticsearch without prior knowledge in a 30-60 minute guided tutorial. Just copy-paste every single request to see the results, and try to figure out the solution to the proposed questions. Who will benefit from this post? I will show basic features in Elastic to introduce some of the main concepts, sometimes introducing more technical or complex concepts, and will link the documentation for further reference (but keep this in mind: for further reference. You can just continue with the examples and leave the documentation for later). If you have not used Elasticsearch before and you want to see it in action - and also to be the director of the action - this is for you. If you are already experienced with Elasticsearch, take a look at the dataset we will be using: when a friend asks you what can you do with Elasticsearch it is easier to explain it with searches in Shakespeare’s plays! What we will and will not cover? We will start adding some documents, searching and removing them. After that, we will use a Shakespeare dataset to provide more insight on searches and aggregations. This is a hands-on “I want to start seeing it working right now” post. Note that we will not cover anything related to configuration or to best-practices in production deployments: so, use this information to get a bite of what Elasticsearch offers, a starting point to envision how it can fit your needs. Setup First of all, you need Elasticsearch. Follow the to download the latest version, install it and start it. Basically, you need a recent version of Java, download and install Elasticsearch for your Operating System, and finally start it with the default values - bin/elasticsearch. In this lesson we will use the latest available version at the moment, 5.5.0. Next, you need to communicate with Elasticsearch: this is done by issuing HTTP requests against the . Elastic is started by default in port 9200. To access, you can use the tool that best fits your expertise: there are command-line tools (like curl for Linux), web-browser REST plugins for Chrome or Firefox, or you can just install and use . Each request consists of a HTTP verb (GET, POST, PUT…), an URL endpoint and an optional body - in most of the cases, the body is a JSON object. As an example, and to confirm that Elasticsearch is started, let’s do a GET against the base URL to access to the basic endpoint (no body is needed): The response should look similar to the following. Since we did not configure anything, the name of our instance will be a random 7 letters string: Some basic examples We already have a clean Elasticsearch instance initialized and running. The first thing we are going to do is to add documents and to retrieve them. Documents in Elasticsearch are represented in JSON format. Also, documents are added to indices, and documents have a typ","locales":"","title":"A Practical Introduction to Elasticsearch"}
{"index":{}}
{"author":"Tyler Hannan","category":"Culture","publish_date":"2017-07-12T00:00:00.000Z","url":"/blog/on-net-neutrality","seo_title":"","content":" ‘It’s a series of tubes…’ It is amazing how a once uttered phrase can quickly become part of the modern lexicon. In this case, however, a previous statement about the internet has again become relevant. Today, a large number of companies – many of whose success is predicated upon the assumption of broadband connectivity – participate in to save net neutrality. Enough has been written, at quite some length, that the details on both sides of the argument are available in a variety of media. We have chosen to participate in the Day of Action. For those who have been Elastic users for some time, you may be aware of our approach to throttling requests to balance hardware resources during segment merging and other activities. However, the potential of throttled access to the Elastic Stack is disconcerting. Our users are doing important work with our products…whether powering a business, or , for profit or for the good of humanity. Our participation is a statement of support for the experience of users and for the broader impact on internet usage. While this day of action is in response to US legislation, there is a broader, global statement that should be made. As Elastic, with employees who live and work all around the globe, we believe in net neutrality. We, as a company, believe in internet access free from restriction imposed by organizations regardless of location. ","locales":"","title":"On Net Neutrality"}
{"index":{}}
{"author":"Thomas Grabowski","category":"Engineering","publish_date":"2017-07-11T00:00:00.000Z","url":"/blog/using-elasticsearch-and-machine-learning-for-it-operations","seo_title":"Using Machine Learning and Elasticsearch for IT Operations","content":" Effective management of IT Operations requires getting feedback about the activity and performance of the servers, applications, and network infrastructure, as well as any problems that may be occurring. The primary way to get this operational data is through collecting the metrics and log data that these systems produce. Up until now most operations teams depended on the expertise of the staff to search and report on the operational data, but now the operational staff can use machine learning to identify anomalies in their data to be more efficient in analyzing that data and become more effective in their jobs.In this post we explain how IT Operations staff can take advantage of machine learning with their operational data. First, we describe how machine learning can enhance current search, reporting, and alerting scenarios. Second, we illustrate how Elastic’s new machine learning feature can be integrated into regular IT Operations tasks. Third, we detail what type of operational data is best suited for machine learning in the Elastic Stack. Finally, we provide additional information for anyone to get started with Elastic’s machine learning.Machine learning enhances search, reporting, and alertingThe Elastic Stack is a great foundation for monitoring IT Operations logs and metrics. It includes an important set of tools operations teams need for their applications and machine data. Operations teams are always looking for tools to reduce mean time to repair (MTTR) and proactively discover potential problem areas. The Elastic Stack provides an easy to use interface for real time search, reporting, and analysis of streaming metrics and logs. Now with the addition of integrated machine learning, the Elastic Stack becomes even more useful for IT Operations.After metrics and logs are collected, organizations often begin by leveraging the power of Elasticsearch to query their data. There is a lot of value from being able to search for specific information in the operations data. For example, it can be very useful to search for a specific IP address in the operational data and follow its utilization of the application, but for large applications, it is not practical to review every single IP’s activity. Once the volume of data grows too large, most operations teams begin to see the difficulty in merely searching their data. Finding answers is contingent upon asking the right questions. Machine learning’s anomaly detection can help point out the right question to ask and reduce the difficulty in searching for data that operations staff doesn’t know exists.Aggregated operations data can be organized into report visualizations and dashboards, like those found in Kibana. Kibana dashboards are great for getting overviews of data, but they don’t always show the most important data at that time. For example, when creating a report for reviewing web server logs it is not difficult to create a visualization of http status codes over time, but it would be very difficult to create enough visualizations to review status codes per unique IP address. Machine learning can evaluate the data to show entities that are not acting in normal patterns.Up until now the most common way to proactively watch data was to use threshold or rules-based alerts. The downside of static thresholds is that they are often too rigid for dynamically changing data, and optimizing the setting of thresholds to avoid false positives is a tedious process fraught with the likelihood of delegitimizing the alert to the consumer of it. Machine learning allows alerts to be more dynamic by learning normal behavior models and alerting when data doesn’t fit the historical model.Operations organizations need their tools to be smarter so they can make quicker, better decisions in less time to reduce their MTTR and not waste time trying to se","locales":"","title":"Using Elasticsearch and Machine Learning for IT Operations"}
{"index":{}}
{"author":"Russ Cam","category":"Engineering","publish_date":"2017-07-10T00:00:00.000Z","url":"/blog/elasticsearch-windows-msi-installer-release","seo_title":"","content":" With the 5.5 release of the Elastic Stack, we're proud to announce the first beta release of the Windows Installer for Elasticsearch. This first release is the culmination of a collective effort to provide a native installation experience for Windows users getting started with Elasticsearch, in addition to providing a friendlier distribution for scripted installations. The Windows Installer is a beta release and as such is not yet recommended for production use. We are looking to GA the MSI as soon as possible.First came .zipSince the beginning, Elasticsearch and the rest of the Elastic Stack have , allowing developers to run Elasticsearch in production, irrespective of whether their environment of choice is a Linux distribution or a flavour of Windows Server. The humble ships both batch and shell scripts to start a single Elasticsearch node on Linux, Mac or Windows, with a sensible default configuration to get you started. This is great for getting started but we can do better on Windows! Here comes .msiFor those unfamiliar, MSIs (Microsoft Installers) or Windows Installers as they are also known, are packages used for the installation and maintenance of software on Windows. They're a mature technology for performing transactional installs and uninstalls of software, with the ability to easily manage upgrades to newer versions. In addition, Installer packages can be digitally signed to help authenticate the identity of the creator, providing confidence in the integrity of downloaded software. Graphical user interfaceMost importantly, the getting started experience for those new to Elasticsearch on Windows is much improved with a Windows Installer, providing a modern graphical user interface (GUI) to guide the user through the installation process: Each step within the process provides contextual help for each displayed input: And the interface exposes common configuration options, allowing changes to configuration files to be handled at installation time: Further, the installer takes care of installing Elasticsearch as a Windows Service: In contrast to the .zip package, the Windows service configuration included with the Windows Installer no longer relies on to host the JVM to allow running Elasticsearch as a Windows service. The benefits here are that the Windows service can be controlled with the more familiar , and changes to configuration no longer require the service to be reinstalled, as is the case sometimes with using the batch file included with the .zip distribution. Finally, common Elasticsearch plugins can be downloaded and installed at the same time as installing Elasticsearch: The interface offers to download and install commonly used and processor plugins, as well as offering our popular , for example. Command lineWindows Installers are great for providing installation GUIs but they are equally at home for silent scripted installations. All of the inputs provided within the GUI are also available as . Running the installer from the command line with the defaults is as simple as start /wait msiexec.exe /i elasticsearch-5.5.0.msi /qn And to uninstall start /wait msiexec.exe /x elasticsearch-5.5.0.msi /qn elasticsearch.exeThe Windows Installer package includes an executable that takes the place of the batch script that ships with the . Besides being digitally signed and supporting all of the that the batch script does, the executable also prints messages to standard output in colour, making it easier to see what's going on whilst developing against Elasticsearch: Furthermore, the executable does not require explicitly setting a environment variable pointing to the directory of the Java installation:  the executable can find an installation of Java to use from a number of different places. In order of precedence: The advantage of picking up the location of Java from the registry is that one no longer needs to remember to update environment variable after insta","locales":"","title":"Elastic :heart: Windows (aka Windows MSI Installer release)"}
{"index":{}}
{"author":"Clinton Gormley","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2017-07-10T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2017-07-10","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Elasticsearch had a first line of defence in place to prevent nodes from running out of disk space. Once a node has reached a certain disk-utilization the cluster tries to move shards away from nodes with little disk space to prevent catastrophic situations where nodes fail will no space left on their devices. However, moving shards around is not always possible and even if it is, indexing might fill up disks faster than shards can be relocated. , nodes will stop accepting write requests to indices that have one or more shards allocated on a node being tight on disk space. This should provide a safer failure mode for clusters than the current behavior, where resources may be exhausted before being detected. Users will need to manually set indices back to read/write once they have provisioned more disk space. The are closed! All Sequence Numbers related PRs slated for 6.0 are merged. We now have: We also have the infrastructure we need to start developing the cross data centre replication (xDCR) X-Pack feature. We will continue to use the new infrastructure to tackle more complex correctness problems: the roll back of unneeded operations in replicas and usage of sequence numbers for optimistic locking. Aggregations now which is similar to the query rewrite phase. This gives any aggregation the opportunity to rewrite itself into a simpler or more generic form, which increases the chance of the aggregation being cached. The first implementation of this rewrite is in the filter/filters aggregations where we now rewrite the filters meaning we can cache requests which use the filter/filters aggregation (as long as the underlying filter is cacheable). The docs for the upcoming Java High Level REST Client are in the workings. You can get a preview here: . Plan is to add the missing search docs and a specific page with examples on how to migrate from Java API to REST client. Cluster alerts (which use Watcher internally) were problematic for users with many clusters as they would run into the soft limit on the number of script compilations per minute. (Mustache templates, used by Watcher, are treated as scripts internally). Now, all the watches in cluster monitoring , as we removed custom search inputs per cluster, so that it does not matter how many different clusters are being monitored. This means, there is no tweaking of the compilations per minute settings necessary. This was uncovered by an SDH ticket of a customer who monitored several clusters. We had a about storing ids in binary form in the index. Elasticsearch accepts arbitrary strings as document identifiers, and up to now we used UTF8 encoding when indexing/storing them in the index. However, it is very common to have strings that represent a base64-encoded byte[] as ids (eg. autogenerated ids) or a number (eg. auto-increment ids coming from an external database whose content is replicated to Elasticsearch). In those cases, the UTF8 representation is respectively 33% and 2.4x larger than the original binary representation, so we had room for improvement. We just the internal representation of ids to try to detect when the id might be a base64-encoded byte[] or a stringified number, and use a more efficient representation in those cases. This encoding makes base64-encoded ids about 32% smaller and numeric ids about 2x smaller compared to today, very close to the size of the original binary representation of those ids. Beware that these savings will not automatically translate to significant reductions of the size of the index, given that Lucene performs prefix-compression of those ids in the terms dictionary, and LZ4 (or DEFLATE if using ) compression of those ids in stored fields","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2017-07-10"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2017-07-06T00:00:00.000Z","url":"/blog/elasticsearch-5-5-0-released","seo_title":"Elasticsearch 5.5.0 released","content":" Today we are pleased to announce the release of , based on . This is the latest stable release, and is already available for deployment on , our Elasticsearch-as-a-service platform. This release includes a security fix for users of Elasticsearch X-Pack Security. Latest stable release in 5.x: You can read about all the changes in the release notes linked above, but there are a few changes which are worth highlighting: ","locales":"","title":"Elasticsearch 5.5.0 released"}
{"index":{}}
{"author":"Jim Goodwin","category":"Releases","publish_date":"2017-07-06T00:00:00.000Z","url":"/blog/kibana-5-5-0-released","seo_title":"","content":" Hello, and welcome to the 5.5.0 release of Kibana!  This is another release that is filled with great new features as well as more than 40 bug fixes and more than 60 enhancements. The great new features include filtering in the Context View, an interactive Filter Editor in Discover, Cross Cluster search support, Dashboard cloning, Machine Learning support in Monitoring, a Grok debugger, and new Gauge and Region Map Visualizations! Kibana 5.5.0 is available on our and on . When you’re finished reading, take a look at the complete for all the goodies. Apply filters to the context query There is now a filter bar in the Context View allowing you to apply new filters to the query. It also copies the currently defined filters when switching in and out of Context View. New filters can be added from within the Context View using the icons in the expanded detail rows. Filter Editors We've added more helpful input controls for editing filters. When you edit a filter, you'll get drop-downs and text boxes with suggestions from your data instead of just a JSON editor. We've also added the ability to add custom filters without having to click on a visualization. Support Cross Cluster Search Index Patterns can now point to indices from remote clusters when using Elasticsearch's cross cluster search feature. After setting it up in Elasticsearch, just mention the remote cluster alias in the index name like so: remoteCluster:indexName. Now you can search across clusters without Tribe nodes!Grok DebuggerGrok Debugger is a new developer tool in X-Pack that will help you build and test grok patterns. It takes inputs for sample log lines and a grok pattern and will simulate the output of a document.X-Pack Monitoring now includes data from your Machine Learning Jobs! This information allows you to observe historical trends, such as which Elasticsearch node a specific job is running on, the number of processed documents, and the job's state over a set interval of time. To find your jobs, simply select the new \"Jobs\" tab when monitoring your Elasticsearch cluster. Future releases will dive even deeper with the integration between Monitoring and Machine Learning so that you can further understand the flow of data when working with Machine Learning. As the screenshot shows, one job was started without selecting the full index range. It also shows how jobs are balanced across nodes. The \"Jobs\" tab is at the Elasticsearch cluster level alongside the Overview, Indices, and Nodes.Clone Dashboards in View Mode Introducing the ability to easily clone dashboards in view mode. Just click the new button in the top navigation, and enter in a new dashboard name. Gauge Chart The gauge and goals charts are two new visualization types that allow people to track a metric and display the in context of a set of reference values. Gauges are useful to indicate how a metric compares to a range of threshold values, for example, to show whether a server load is within a normal range or instead has reach critical capacity. Goal visualizations are similar, but are primarily used to indicate how far a metric is removed from a certain target value. Also, Metric visualizations now support custom ranges and styling of text and background colors.Region Map Kibana now has the Region Map Visualization. These are thematic maps in which boundary vector shapes are colored using a gradient, with higher intensity colors indicating larger values and lower intensity colors indicating smaller values. These are also known as choropleth maps. In order to color these layers, users specify a terms aggregation that matches a field in the vector layer. Kibana offers two vector layers by default:  one for countries of the world and one for US Shapes. Users can also bring in their own vector layers by configuring the Kibana-configuration file to point to any GeoJson file that is hosted on a CORS-enabled server.You can now create static series, this allows for adding background colors and threshold mar","locales":"","title":"Kibana 5.5.0 released"}
{"index":{}}
{"author":"Bohyun Kim","category":"Releases","publish_date":"2017-07-06T00:00:00.000Z","url":"/blog/introducing-the-elastic-stack-monitoring-service","seo_title":"","content":" Today, we are incredibly excited to announce the Elastic Stack Monitoring Service. This service extends our commitment to improving product usability and quality of support by providing you with a dedicated monitoring cluster to host your Elastic Stack monitoring data. This service is available to our Gold and Platinum self-hosted customers at no additional charge. As our customer base grows, we continue to look for ways to provide the highest quality of support. We believe that the Elastic Stack Monitoring Service brings us further along the path of achieving this goal.Typically, when you open a support ticket for a cluster issue, the Elastic Support team needs to gather various cluster information to diagnose the problem. And, in some cases, the Elastic Support team needs to get a snapshot of your historical cluster monitoring data, and then manually restore it to diagnose the issue.Now, with the Elastic Stack Monitoring Service, our Elastic Support team has direct access to your historical monitoring data as well as other relevant cluster information. This streamlines the diagnostic process and allows us to jump right into the more in-depth questions.We recommend the setup of having a dedicated monitoring cluster as a best practice so that your production cluster can be freed from the monitoring workload and usage. With the Elastic Stack Monitoring Service, you will no longer need to create and manage a dedicated monitoring cluster on your own, which can greatly simplify your daily workflow and management.  By opting in for this service, the Elastic Support team will create an Elastic Cloud cluster, and then send you the instructions for configuring your production Elasticsearch cluster to send its monitoring data to the Elastic Cloud cluster. With this service, it’s as simple as logging into the hosted Kibana instance for both you and the Elastic Support team and voilà, a new monitoring journey awaits.Can’t wait to sign up for this exciting new service? Please get in touch with your Elastic Support team or account manager today to get started! ","locales":"","title":"Introducing the Elastic Stack Monitoring Service"}
{"index":{}}
{"author":"James Baiera","category":"Releases","publish_date":"2017-07-06T00:00:00.000Z","url":"/blog/es-hadoop-5-5-0-released","seo_title":"Elasticsearch for Apache Hadoop 5.5.0","content":" ​I am excited to announce the release of Elasticsearch for Apache Hadoop (aka ES-Hadoop) built against Elasticsearch 5.5.0. What’s new? Support for Kerberos in Elasticsearch HDFS Repository Plugin Back when 5.0 was released, the HDFS Repository was moved out of the ES-Hadoop project and into the Elasticsearch project. At this time, the plugin was re-written to work under Elasticsearch’s security model. Support for ‘Kerberized’ HDFS clusters was dropped due to the overwhelming work required to have the client gel with the installed security policy. With 5.5.0, that’s all a thing of the past, as we are announcing that This has also been backported 5.4 branch as of 5.4.1. New Delimiter for Index Formatting When specifying index or type names using the dynamic multi-resource feature, users can specify an optional date format when specifying a date field to be extracted. In certain cases, the colon character (:) was causing problems with parsing the resource template as a path when using a date format in the index name. To address this issue, in 5.5 and up we will accept both the colon character (:) and the pipe character (|) as delimiters for date formats. The colon character is considered deprecated in 5.5 and will be eventually removed in ES-Hadoop 6.0. TTL and Timestamps Support for TTL and Timestamp is being removed in ES 6.0. In ES-Hadoop 5.5 we will log warnings when are specified in the configuration at job start. In ES-Hadoop 6.0, those warnings will become runtime errors when executing against an Elasticsearch version at or above version 6.0. For the sake of backwards compatibility, these settings will remain in the project in order to support their use in Elasticsearch 5.x and below. Deprecations Hadoop 1.x Support for Hadoop 1.x in the Elasticsearch-Hadoop connector will be deprecated in 5.5, and completely removed in 6.0. This means that Elasticsearch-Hadoop 6.0 and above will no longer work against Hadoop 1.x (pre-YARN) based distributions in version 6.0. In striving to maintain the strongest integrations for the most popular Hadoop ecosystem components, we’re moving forward with only supporting Hadoop versions 2.2 and above, which vendors have been bundling in their distributions for years. Most vendors no longer support Hadoop 1.x, and projects like Spark will no longer depend on Hadoop 1.x libraries going forward. Users on a Hadoop 1.x based distribution are recommended to upgrade to a Hadoop 2.2 or higher based distribution for continued bug fixes and enhancements to ES-Hadoop. Elasticsearch on YARN Beta The Elasticsearch on YARN project (, ) will be deprecated in 5.5, and completely removed in 6.0. Elasticsearch on YARN was an experiment for deploying Elasticsearch on top of YARN, the resource management platform introduced in Hadoop 2.0. The project was never recommended for production use and has been in beta status since its inception. The core limitation is YARN’s lack of official support for long-running services, and there has unfortunately been no prioritized innovation in the Hadoop community on this front over the last few years. As this is a requirement for Elasticsearch to achieve production level stability on YARN, we are instead focusing our efforts in enhancing other areas of ES-Hadoop. For any questions about the above deprecations, feel free to join the discussion on our . ","locales":"","title":"Elasticsearch for Apache Hadoop 5.5.0"}
{"index":{}}
{"author":"Tyler Hannan","category":"Releases","publish_date":"2017-07-06T00:00:00.000Z","url":"/blog/elastic-stack-5-5-0-released","seo_title":"","content":" 5.5 is here! In 5.4 we announced the addition of machine learning features to X-Pack. This beta release was an opportunity to gain valuable input from users, at scale, to tweak the implementation of the product. We are super excited that, as of 5.5, machine learning is GA! If you aren’t, yet, familiar with it a great place to start is by and reading . Machine learning is not yet enabled on . Stay tuned for updates! All other features are available today. The hits keep on coming. With this release we are adding the for our customers. Put simply, we (Elastic) will manage a dedicated monitoring cluster – run in Elastic Cloud – to host your monitoring data. If you partner with our support team already, you can probably imagine how providing immediate access to your monitoring data will streamline the conversations when you open a support ticket for cluster issues. Elasticsearch For more detailed information, and many other features, peruse the Elasticsearch . Kibana ‘Visualize’ all the features, and ‘Discover’ more information in the . As part of the GA of machine learning, X-Pack monitoring now includes data from your machine learning jobs! Assigned nodes, number of processed documents, and a job’s state over time are all presented. We will continue to add more features in upcoming releases. Logstash For more information, grok the . Beats ES-Hadoop We’ve ‘Reduced’ the features to a few bullets, but the full ‘Map’ is in the . Get It Now! ","locales":"ja-jp","title":"Elastic Stack 5.5.0 Released"}
{"index":{}}
{"author":"Jim Goodwin","category":"Kurrently in Kibana","publish_date":"2017-07-05T00:00:00.000Z","url":"/blog/keeping-up-with-kibana-2017-7-3","seo_title":"","content":" Welcome to This is a weekly series of posts on new developments in the Kibana project and any related learning resources and events. 5.4.3 has been released! (along with the rest of the Elastic Stack) it includes a security fix: — elastic (@elastic) ","locales":"","title":"Keeping up with Kibana: This week in Kibana for July 3, 2017"}
{"index":{}}
{"author":"Jim Goodwin","category":"Kurrently in Kibana","publish_date":"2017-06-27T00:00:00.000Z","url":"/blog/keeping-up-with-kibana-2017-6-26","seo_title":"","content":" Welcome to This is a weekly series of posts on new developments in the Kibana project and any related learning resources and events.  Behold the optic powers of ’s Time Series Visual Builder. Check out dynamic features in our latest tutorial: — elastic (@elastic) ","locales":"","title":"Keeping up with Kibana: This week in Kibana for June 26, 2017"}
{"index":{}}
{"author":"Suyog Rao","category":"Releases","publish_date":"2017-07-06T00:00:00.000Z","url":"/blog/logstash-5-5-0-released","seo_title":"","content":" We are happy to announce that Logstash 5.5.0 has been released today with some great features! Please see the for details. You can download the binaries .Dead Letter QueuesWe are happy to introduce a popularly requested feature called dead letter queues. In message processing systems, a dead letter queue (DLQ) is a mechanism to route bad events to a different destination when they can't be processed. For Logstash, this means an ability to shunt poisoned or unsuccessful events in the running pipeline to a new destination for further processing. In Logstash's case, the new destination is a file based queue to stores these bad events, so you can re-process them. This feature allows the existing pipeline to continue processing events without getting stuck because of bad events. As an initial implementation, the Elasticsearch Output is the only plugin that uses this feature. To use this feature, you'll have to enable it in the settings using dead_letter_queue.enable: true. Once enabled,  events that fail to process due to mapping issues in the Elasticsearch Output are automatically moved to the DLQ on local disk. When it comes time for re-processing the data that’s been dead lettered, you simply spin up a separate LS pipeline with the new DLQ input plugin and the updated processing logic.We are planning on iteratively expanding the scope of the DLQ from the plugin side and here is welcomed!Grok DebuggerYou know about regular expressions — yep, regexes are hard! Grok makes using regexes easier in the context of extracting fields from unstructured data. It is one of our popular plugins! Even though Grok is much easier to work with than raw regexes, users often need help while constructing the grok expressions for their gnarly log data. Enter - a popular web UI to iteratively craft grok expressions. There are other excellent tools as well, like the . Over the years, we've got requests from users to provide an option like this in Kibana. Also, many users didn't want to send their log data to an external website for security reasons. So, there you have it — under Dev Tools, there's now a Grok Debugger UI in 5.5! Just on Kibana and it is free to use! Under the covers, the Grok Debugger uses the ingest node’s . Since ingest node’s Grok implementation and Logstash’s Grok plugin share the same Joni regular expression engine, any pattern you construct using this UI is portable across both products.This is just an initial version and we plan to add more features!GeoIP ASN enrichment recently introduced a free database (GeoIPLite2-ASN) that can be used to lookup ASN information from an IP address. ASN information is also available in the commercial GeoIP2-ISP database. Logstash now supports both these databases starting from GeoIP plugin version 4.2.0.Bug fixes5.5.0 has plenty of bug fixes in Logstash core and plugins. The have detailed information.FeedbackPlease , try it out, and let us know what you think on Twitter () or in our . You can report any problems on the . ","locales":"","title":"Logstash 5.5.0 released"}
{"index":{}}
{"author":"Dimitri Marx","category":"Engineering","publish_date":"2017-07-05T00:00:00.000Z","url":"/blog/the-future-of-data-big-fast-ubiquitous","seo_title":"","content":" It would be hard to find many modern businesses with diminishing volumes of data that they need to process. Most businesses now also face a requirement to process data faster and deliver more, actionable insights in near realtime. Innovative businesses are always drowning in data that they would love to be able to make sense of and extract valuable insights from. This challenge is unlikely to ever go away: as our ability to handle data increases, so does the human desire to explore and find new sources of data to search and analyse. As a part of this, large-scale data analysis is moving into more and more new areas of business. Senior executives, sales, marketing, customer relations, finance, operations, logistics and nearly all areas of the modern business, now have access to growing troves of data, from which can unlock valuable competitive advantage, improve existing business processes, and build new applications. Data analysis is moving from being the walled realm of the data scientist, to becoming everyday business tool. It is becoming so ubiquitous in modern business, that it has become normalised. This poses new challenges for the development of data analysis tools and software. Increasingly, the task is to make the highly complex technologies, simple and intuitive for an ever increasing number of new end users. Usability, not just scale, is become a vital attribute. Search at the CoreData analysis, whether or not we call it \"Big\", all boils down to the power of search. Individuals are looking to gain insights and knowledge from data. A decade ago, mentioning \"search\" to an average user, very few would have immediately grasped all the possibilities of what can be achieved with it. But, open source technologies like Elasticsearch have made it simple to map any new problem domain model to \"search\" and have made crossing this mental barrier much simpler. At Elastic, we have seen an explosion of use cases where our technologies are being used to power \"not your typical search use case.\" Our users keep on finding innovative ways to use Elasticsearch, and it is perhaps one of the hallmarks of a great open source project, that it allows users to reach a level of creativeness that they initially never even imagined. \"Big\" data is, almost by definition, heterogeneous. The name \"Elasticsearch\" is a reference to this flexible combination of free text search, structured search, and analytics. It should not matter if the data is your typical web page/word document, or to a degree, a location on Foursquare, a trade in a bank, a web server log, or a metric of sorts. All effectively are a combination of structured and unstructured data that people want to explore, search through, regardless of the shape or volume of the data. Although the data content itself is interesting, if search works, to a degree, the data becomes irrelevant. The Future of SearchIf we look at all the trends in enterprise data analysis over the last couple of decades, these have largely been driven by advances in search technology, to enable new, more powerful uses of search. The ability to search for correlations across ever more dimensions or facets of a dataset, to search unstructured data, or just searching ever greater volumes:  new data storage and indexing technologies have certainly played a part, but largely to enable new, more powerful forms of search. The latest data technologies such as analytics and machine learning, are essentially more sophisticated applications of search. Graph analytics allows the user to search for new connections in the data, independently of the need for structure in the underlying dataset. In a world where technology offers almost overwhelming search possibilities, this provides a faster, more powerful way to explore data and unlock important trends and relationships. Graph provides a form of meta-analysis, searching for what trends warrant deeper analysis or ongoing monitoring. Even machine l","locales":"","title":"The Future of Data: Big, Fast, Ubiquitous"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2017-07-04T00:00:00.000Z","url":"/blog/brewing-in-beats-file-integrity","seo_title":"","content":" Welcome to ! With this weekly series, we're keeping you up to date with what's new in Beats, including the latest commits and releases. Auditbeat: File integrity monitoringWith this , Auditbeat gets support for monitoring file and directory changes. When a file is changed, it will calculate the MD5, SHA1, and SHA256 hashes for the file. This functionality uses inotify (linux), fsevents (macos), and ReadDirectoryChangesW (windows) to watch for changes. The hashes can be compared against known malicious files like it is done in this . The configuration looks like this: auditbeat.modules: - module: audit metricsets: [file] file.paths: binaries: - /usr/bin - /usr/sbin This functionality will be included in 6.0.0-beta1. Metricbeat: Aerospike module Thanks to a community contribution by , Metricbeat now has a module for . is a distributed NoSQL database. This module will be released with 6.0.0-beta1. JSON logging It is now possible for the Beats to . This is following our own advice because we generally recommend writing the logs in JSON if you can control the format. Our JSON logs currently have only a few standard keys (e.g. timestamp, level, message), but we plan to extend it in the future. This functionality will be included in 6.0.0-beta1, for now off by default. Load dashboards using the Kibana API In the 5.x versions and older, Beats are importing the Kibana dashboards directly in Elasticsearch, in the Kibana index (). Every time the Beats are importing the dashboards, they are overwriting the .kibana index, which is not ideal, especially because Kibana uses the index to store its settings, along with the saved objects. With the removal of types in Elasticsearch, Kibana team is planning to change the format of the index in 6.0.0-beta1. As the change breaks the way Beats are importing the Kibana dashboards into Elasticsearch, and it requires the creation of new dashboards, we thought it’s a good time to make the dashboard loading more robust. Starting with this , we store two versions of the dashboards, one for 6.0 and above, and one for 5.x and below. At load time, the Beat checks the Elasticsearch/Kibana version and depending on that either uses a Kibana API to load the dashboards or insert in the index as before. This functionality will be present in 6.0.0-beta1. Other changesRepository: elastic/beatsAffecting all BeatsChanges in master: AuditbeatChanges in master: FilebeatChanges in 5.x: Changes in master: MetricbeatChanges in master: HeartbeatChanges in master: InfrastructureChanges in master: DocumentationChanges in master: Changes in 5.4: DashboardsChanges in master: PackagingChanges in master: Repository: elastic/go-libauditChanges in master: ","locales":"","title":"Brewing in Beats: File integrity monitoring with Auditbeat"}
{"index":{}}
{"author":"Clinton Gormley","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2017-07-03T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2017-07-03","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Up to 6.0 the primary purpose of the translog was to persist all operations that hadn't yet been persisted to Lucene. Once Lucene files were committed (i.e., fsync) we would clear the translog and start writing to it again (a.k.a ). While this behaviour is optimal from a storage perspective, it is not ideal for the new ops-based fast recovery introduced in 6.0. In order to bring a replica up to speed without copying segments, the primary needs to have all the required operations available in its translog. That meant that fast ops recoveries weren't possible during full restarts as we flush when we recover the primary, or if a node left the cluster long enough for the primary to flush while it was away. This now changes with the introduction of a translog retention policy - we will now keep translog files () even if the data they contain is already committed to lucene. This is also very important for the future xDCR feature where we need the ops to be streamed off cluster. Scripts in Elasticsearch are now run in a particular \"context\", which defines the return value type and the variables and functions that are enabled in a particular script, e.g. an update script should have access to , while a fast search script shouldn't. Now, , instead of them being restricted to just a hardcoded list of functions. This will allow Painless scripts to expose utility functions which are only useful in certain contexts. We will no longer be using the default password for internal users in Elasticsearch 6.0 with security. Instead, a cluster in production mode will start with no passwords, but will only accept requests to change the password for the user from localhost. This can be done via the API, but we provide a command line tool which will generate strong passwords for all internal users and output them to the console. Alternatively, the user can specify their own passwords using the same tool. For Docker, where this CLI tool is not accessible, we allow a temporary password to be set via an environment variable, which will cease to work as soon as the user changes the password via the API. Some users have long requested longer user and role names in security, which were previously limited to just 30 characters. The new limits are as follows: A valid username's length must be at least 1 and no more than 1024 characters. It may not contain leading or trailing whitespace. All characters in the name must be be alphanumeric (a-z, A-Z, 0-9), printable punctuation or symbols in the Basic Latin (ASCII) block, or the space character. Apache Lucene We're moving towards getting Lucene 7 released. The 7.0 and 7.x branches are expected to be cut . , which Solr relies on, and we are looking at the best way to get things fixed since it will be important for Lucene 7 to support Java 9. Want to watch Lucene-related content? There were 3 Lucene-related talks at Berlin Buzzwords this year: Mike has been exploring using , which results in a 50% update throughput increase in some cases. ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2017-07-03"}
{"index":{}}
{"author":"Jim Goodwin","category":"Releases","publish_date":"2017-06-27T00:00:00.000Z","url":"/blog/kibana-5-4-3-released","seo_title":"","content":" The 5.4.3 release contains an important security fix in X-Pack Security. Please read the details below. ","locales":"","title":"Kibana 5.4.3 released"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2017-06-27T00:00:00.000Z","url":"/blog/brewing-in-beats-auditbeat-for-linux-auditing","seo_title":"","content":" Welcome to ! With this weekly series, we're keeping you up to date with what's new in Beats, including the latest commits and releases. Auditbeat - The new Beat on the blockIf you have been following along, you know that Andrew for a while on harnessing the auditing framework of the Linux kernel, creating among other things a Metricbeat module that could be used as an alternative for the old daemon. We have now decided to double down on this approach and to this module to be its own Beat: Auditbeat. Making it a Beat, it brings more freedom in extensibility and configurability, as well as giving more exposure.  The first version of Auditbeat will be released as Beta in 6.0. Filebeat modules: Machine Learning jobs adds support for loading Xpack Machine Learning job configurations from the Filebeat modules. An example ML configuration is added to the Nginx Filebeat module. This sample applies ML anomaly detection on the response codes rates. If a job configuration with the same ID exists, it is not overwritten, because deleting jobs could potentially delete user data. The user should manually delete the jobs in the UI if they want to upgrade. We currently plan to include this in 5.6 and 6.0. Libbeat publisher refactoringWe have merged the of the libbeat publisher/outputs refactoring. This is a large change without visible effects, but it was required for implementing features like spooling to disk, or dynamically reloadable outputs. It simplifies the handling of batches by buffering them in a single place, which should make the Beats easier to configure and tune for performance. This refactoring introduces a limitation that we didn’t have before: only one output can be active at a given time. Prior to this, it was possible to have multiple outputs enabled, but only if they had different types (e.g. one ES and LS output worked, but two ES outputs didn’t). Add more Beat commands adds three more Beat commands: $ filebeat completion bash Outputs a Bash auto-complete file. This command is hidden, meaning that it doesn’t show in the output of . $ filebeat export template Exports the generated Elasticsearch mapping template file. $ filebeat export config Exports the evaluated configuration in YAML format. Useful for troubleshooting configuration issues. Other changes:Repository: elastic/beatsAffecting all BeatsChanges in 5.4: Changes in master: FilebeatChanges in master: MetricbeatChanges in master: Changes in 5.5: HeartbeatChanges in master: DocumentationChanges in 5.4: Changes in master: Changes in 5.5: InfrastructureChanges in master: ","locales":"","title":"Brewing in Beats: Auditbeat, for Linux auditing"}
{"index":{}}
{"author":"Alex Francoeur","category":"Engineering","publish_date":"2017-06-26T00:00:00.000Z","url":"/blog/kibanas-new-time-series-visual-builder-part-2","seo_title":"Kibana's New Time Series Visual Builder - Part 2","content":" Hard on the heels of our , Chris Cowan is back with another video show off how powerful and versatile this new UI is. If you'd like to follow along, go ahead and download to start ingesting metrics from your systems or services. You'll also want to make sure you have the latest version of installed.   In this video, you will learn to do the following: ","locales":"","title":"Kibana's New Time Series Visual Builder - Part 2"}
{"index":{}}
{"author":"Igor Kupczyński","category":"Engineering","publish_date":"2017-06-21T00:00:00.000Z","url":"/blog/automated-snapshots-in-elastic-cloud-enterprise","seo_title":"","content":" Elastic Cloud Enterprise supports automated snapshots to your own repository. This is a convenient way to back up your data and to make sure that any changes to your clusters are easily reversible.  Introduction Elasticsearch stores data, so it is important to back it up as a protection against catastrophic failures. You can configure your indices to use replicas so you can withstand a node or server failure, but this is not enough in the case of a datacenter-wide issue or an operator error. To make the back up process simple Elasticsearch offers the snapshot API. A snapshot represents a point-in-time version of an index or even of all the indices in the cluster, including internal ones like .security. Snapshots are incremental — that is, they store a delta between the latest snapshot and the current view of the index — to ensure that backups can be done in a fast and efficient manner. Snapshots can then be restored to a new, empty cluster or even to the same cluster using the restore API. Check the .  Clusters created with Elastic Cloud Enterprise have full support for the Elasticsearch snapshot and restore APIs. You can add repositories, take snapshots, and restore snapshots according to your needs. However, the process would be tedious when managing dozens or hundreds of clusters without support for automated snapshots.  Automated snapshots in Elastic Cloud Enterprise In Elastic Cloud Enterprise we want to make the process of managing multiple clusters as easy as possible. Therefore, you can create one or more , snapshot repositories and then link clusters created with ECE to them.  Using an ECE managed snapshot repository provides a couple of benefits over the classic manually managed repositories. Firstly, clusters linked to a shared repository will automatically take their snapshots every half an hour. Since the snapshots are incremental this process is usually very quick and doesn't affect the cluster’s performance in any significant way.  Moreover, Elastic Cloud Enterprise will take a snapshot before any changes are made to the cluster topology — e.g. a script settings change, up/down scale or addition of new nodes. This means that in the case of an unintended or destructive change a cluster can easily be restored to the previous state.  Elastic Cloud Enterprise will keep the 100 most recent snapshots for each cluster, which gives you the possibility to restore the state of the cluster for the next two days. Additionally, in the event that partial snapshots are present, the most recent full snapshot is always maintained.  Elasticsearch itself supports , such as AWS S3, Google Cloud Storage or even shared file systems. As of this article, Elastic Cloud Enterprise supports only automated snapshots to S3-compatible storage. You can also use custom AWS S3 repositories by adding them to your Elasticsearch cluster directly via the snapshot and restore API. Support for additional repositories, such as Google Cloud Storage and Microsoft Azure Storage, is planned for a future release. Step by step guide First lets configure the shared repository.  In this example we've added an S3 repository named . Now we can reference it from the clusters.  During new cluster creation one can select a shared repository to use.  Similarly, if you have existing clusters — created before the shared repository was added — you can link them as well. Go to a cluster, select Manage and then select a and click .  When the repository is linked, then all the plan changes, e.g. scaling the cluster or changing settings will include the additional step.  This way you can safely change your clusters, knowing that there is a point-in-time snapshot just before the plan change.  Additionally, an automated snapshot is performed every 30 minutes and the last 100 snapshots are kept — which gives you around 48 hours to restore from backup if anything goes wrong.  You can monitor the ","locales":"","title":"Automated Snapshots in Elastic Cloud Enterprise"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2017-06-20T00:00:00.000Z","url":"/blog/elasticsearch-5-4-2-released","seo_title":"Elasticsearch 5.4.2 released","content":" Today we are pleased to announce the bug fix release of , based on . This is the latest stable release, and is already available for deployment on , our Elasticsearch-as-a-service platform. Latest stable release in 5.x:You can read about all the changes in the release notes linked above, but there are a few changes which are worth highlighting:Please , try it out, and let us know what you think on Twitter () or in our . You can report any problems on the . ","locales":"","title":"Elasticsearch 5.4.2 released"}
{"index":{}}
{"author":"Jim Goodwin","category":"Kurrently in Kibana","publish_date":"2017-06-21T00:00:00.000Z","url":"/blog/keeping-up-with-kibana-2017-6-19","seo_title":"","content":" Welcome to This is a weekly series of posts on new developments in the Kibana project and any related learning resources and events.  Happy Tuesday! It's another release, 5.4.2 to be exact: — elastic (@elastic) ","locales":"","title":"Keeping up with Kibana: This week in Kibana for June 19, 2017"}
{"index":{}}
{"author":"Adrian McMichael","category":"User Stories","publish_date":"2017-06-29T00:00:00.000Z","url":"/blog/scaling-the-elastic-stack-in-a-microservices-architecture-rightmove","seo_title":"","content":" IntroductionRightmove is the UK's #1 Property Portal. In the process of helping people find the places they want to live, we serve 55 million requests a day and use Elasticsearch to power our searches and provide our teams with useful analytics to help support our applications. Since 2014, we have been moving over to a microservices architecture. Microservices means more application instances — which in turn means a lot more logs. The old ways of SSHing onto a server and using grep, awk, and sed to diagnose failures was already a pain when we had 90 plus servers to handle, so it was obvious to us that we needed something to pull all our logs together in one place. Fast forward to 2017 and we have over 50 microservices all sending their logs to our Elasticsearch cluster. In doing so, we needed a way to scale our configuration on both the hardware and application side of things. So how did we achieve this and what did we learn along the way? Rightmove's Search for a Better SearchIn 2014, we were faced with a problem when our existing search engine technology was having issues scaling to fit the needs of our ever growing user base. We decided at that point it was a good time to explore our options and we lined up the contenders to see which one would be the best fit in terms of scalability, performance and features. Elasticsearch quickly rose to the top as not only did it perform around 10 times faster than our old search engine on average, but it also seemed to be very easy to work with and easier for our developers to learn. The chance to overhaul our search was also one of the things that led us to think about splitting out our old monolithic web applications into a microservices architecture. Since then, we've begun the journey of moving other search-based features (like our sold prices and property email alerts) as well as our customer based reporting and request anomaly detection to Elasticsearch. We have also been able to consider new property search features that would have been much harder to release in our old search technology. Once we had these microservices and Elasticsearch in our estate, the success of an internal Hackathon led us to the Elastic Stack as a means of pulling in all our application metrics and logs — which has since greatly improved the productivity of our teams when measuring the success of feature releases and debugging and fixing application issues. Our Elastic Stack Setup We split our configuration into 3 parts: Installing and Configuring the Elastic StackHere are a few things to consider when configuring the Elastic Stack: Configuring LogstashWhen configuring Logstash to help scaling, we do the following: input { file { path => \"${microservice_logs_path}app.log\" type => \"${index_prefix}app_log\" codec => json { charset => \"ISO-8859-1\" } }   How we use our dataThe data collected from our logs is used for various purposes. We use a combination of Kibana, monitoring features, and alerting features with our data as well as some custom UI tools we've built on top of the Elastic Stack in house to help monitor applications. Figure 1 - Scraper and bot detection Figure 2 - Deployment History Figure 3 - Traffic Visualisation Figure 4 - Application AlertingKnowledge Sharing & Internal AdvocacyWe learned early on that it's not enough to build the tools for your teams:  you also need to educate and sell your teams on why they should want to use them. We did this by hosting events like internal workshops where we taught developers and business folks alike how to use Kibana to create visualizations. Adopting an opt-in approach means that anyone who is interested can learn, and often this allowed business and support team members to get a better idea of how to ask for the data they wanted from the application developers. Attending the , organised in our area for Elasticsearch and were a big part of what gave our d","locales":"","title":"Scaling the Elastic Stack in a Microservices Architecture @ Rightmove"}
{"index":{}}
{"author":"Shay Banon","category":"News","publish_date":"2017-06-22T00:00:00.000Z","url":"/blog/welcome-opbeat-to-the-elastic-family","seo_title":"Welcome Opbeat to the Elastic Family","content":" I am very excited to announce that Elastic is joining forces with Opbeat, an application performance monitoring (APM) company. I’ve been personally interested in the APM space for some time now. Since the early days of Elasticsearch, I have believed that powerful search is fundamental to solving all kinds of complex data problems. Our users (, , , and to name a few) have validated that core belief in the IT operations space by using Elastic products to gain real-time insights from infrastructure logs and metrics, at a petabyte scale. Application performance data is the natural next step in the journey, and joining forces with Opbeat will allow us to accelerate innovation in the APM space for our users. Opbeat has built a highly curated application-level monitoring experience, focusing on the JavaScript ecosystem, i.e. Node.js on the backend and AngularJS / React on the front end. By helping developers monitor the end-to-end performance impact of changes to their application code, Opbeat enables them to find and fix issues faster. Integrating metrics from Opbeat agents with the flexible data store of Elasticsearch and the visualization power of Kibana, will bring APM to a whole new level. The future Elastic APM solution will let developers analyze their infrastructure logs and metrics, alongside application performance data in a single dashboard.   I’d like to welcome the entire Opbeat team and community to the Elastic family. I’m thrilled that Opbeat founders, Rasmus Makwarth and Ron Cohen, and team are joining us to build a turnkey APM solution, that extends the open source into new territories. Now to Rasmus and Ron for their thoughts. At Opbeat, we set out to build a next-generation APM solution that was truly designed for developers. Modern organizations are increasingly adopting a DevOps culture, requiring developers to monitor their own code in production. This shift requires modern APM tools to provide instant and actionable insights to developers. Opbeat provides a highly curated UI and deep code integration that maps production application issues to the developer source code.     Over the past six months, we’ve been fortunate to get to know the people at Elastic, and are amazed by the team, the business, and the vision. And, equally important, Opbeat and Elastic were culturally a natural match from the very first day. We’ve been happy users of the Elastic Stack for monitoring our infrastructure metrics and logs, and are excited about being a part of the team that extends the Elastic Stack to application-level metrics. We think that APM is a perfect extension of the Elastic Stack. Proactive tracking with X-Pack alerting features, pre-built Kibana dashboards, longer data retention, smarter alerts with anomaly detection and machine learning - the integration possibilities are endless. As for the current service, nothing changes for now. We’re committed to operating the service until we launch the Elastic APM product. When that time comes, we’ll help existing Opbeat customers migrate. We’re thrilled to be working with such a great team, and being a part of Elastic will enable us to accelerate our path to building the best APM product. The road ahead is exciting. ","locales":"de-de,fr-fr","title":"Welcome Opbeat to the Elastic Family"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2017-06-21T00:00:00.000Z","url":"/blog/brewing-in-beats-add-docker-metadata-to-logs","seo_title":"","content":" Welcome to ! With this weekly series, we're keeping you up to date with what's new in Beats, including the latest commits and releases. Add Docker metadata to your Docker logsIn case you are using Filebeat to collect the logs from your Docker containers, this makes it simple to enhance the logs with the Docker metadata (container ID, name, image, labels). For that, you just need to configure the processor in Filebeat. processors: - add_docker_metadata: ~ When using the default logging driver (json driver) in Docker, the log files are placed under the path . Filebeat generates an event for each log line and places the name of the log file under the field. With this change, the extracts the container ID from the field, and queries the Docker API to fetch details about that Docker container. Install audit rules with MetricbeatThis adds the ability to have Metricbeat install audit rules to the kernel. Metricbeat supports adding both file watch rules () and syscall rules ( or ). The for specifying rules is the same as with . For example, both of these are supported. kernel.audit_rules: | -w /etc/passwd -p wa -k identity -a always,exit -F arch=b64 -S open -F exit=-EACCES -F key=access Add random startup delays for each metricsetWith this , Metricbeat starts each metricset at a slightly different time, via random delays. This avoids the problem, where sometimes thousands of Beats are inserting documents in , causing spikes in the ingestion rates. Other changesRepository: elastic/beatsAffecting all BeatsChanges in 5.4: Changes in master: MetricbeatChanges in 5.4: Changes in master: Changes in 5.5: DocumentationChanges in master: Changes in 5.4: Changes in 5.5: Repository: elastic/gosigarChanges in master: Repository: elastic/go-libauditChanges in master: ","locales":"","title":"Brewing in Beats: Add Docker metadata to logs"}
{"index":{}}
{"author":"Jim Goodwin","category":"Releases","publish_date":"2017-06-20T00:00:00.000Z","url":"/blog/kibana-5-4-2-released","seo_title":"","content":" The 5.4.2 release contains some minor fixes. Please read the details below. ","locales":"","title":"Kibana 5.4.2 released"}
{"index":{}}
{"author":"Renuka Hermon","category":"User Stories","publish_date":"2017-06-16T00:00:00.000Z","url":"/blog/addressing-human-security-issues-cause-award-honoree-ist-research","seo_title":"","content":" The Pulse Platform was initially conceived to interact with hard-to-reach populations through the technology they already have in their pocket: their cell phone. This initial vision for SMS-based population engagement expanded as the internet and social media became more widely available and emerged as the primary mechanism to recruit and exploit individuals across the globe. The Pulse Platform consists of hardware and software spanning user interface, collection, storage, processing, analysis, and visualization to provide a comprehensive data aggregation and analysis system. Pulse provides real-time information collection across the internet and from hard-to-reach populations. Digital internet monitoring is based on a web-scraping infrastructure, providing continuous capture of targeted selections of accounts, terms, content, and entire websites for prolonged periods of time. Active engagement involves direct communication with individuals in at-risk populations utilizing their mobile technology. Monitoring and engagement data are fused in a common database and custom analytics help identify bad actors and labor exploitation. The Elastic Stack’s ability to index, interpret, and monitor the overall enterprise has made us huge fans of Elastic products.  Pulse is a fully functional product that has been applied to analyze extensive internet data in support of the District Attorneys of New York and San Francisco to prosecute sex trafficking cases, to monitor and analyze ISIS recruitment activity, to interact with local populations in Afghanistan to understand teacher pay, in Uganda to identify potential for overseas labor trafficking, and in Liberia to support Ebola tracking.  We are currently using Elasticsearch 2.3.1, Kibana 4.5.4, and Logstash in our production environment. Our current solution is built in AWS (although we're in the process of looking at Elastic Cloud in the near future). All data nodes are fed through an Elastic Load Balancer that runs through an http auth proxy before hitting the cluster. We also have the master nodes on a load balancer that we have set up for visualizing the cluster with Kopf. We have a large pipeline that incorporates many different technologies including HDFS, HBase, Storm/Spark, Kafka, Zookeeper, and other open source technologies that ultimately feed Elasticsearch the real-time data that allows us to perform analyses on breaking events around the world. We currently utilize Kibana for all our data analysis, and incorporate all the different widgets, including geolocation maps, to help pinpoint epicenters correlating to data points, as well as graphs and pie charts to show comparisons between data points. We also utilize a Marvel [monitoring] cluster hosted in Elastic Cloud to gather health statistics on our AWS-based clusters. We chose a Marvel cluster in Elastic Cloud as a starting point for migrating our other clusters to Elastic Cloud. We currently have Logstash set up across our entire technology stack to gather logs in our Elasticsearch cluster should we need them to troubleshoot issues. We keep 7 days’ worth of logs for each technology we're using. We utilize Curator to automate the removal of data from our Elasticsearch cluster.  We are currently in the process of hardening the Pulse Platform to fully realize it as a truly global information collection tool. We will continue to do innovative research and development funded by DARPA and other innovative research organizations to maintain cutting-edge functionality. We are deploying the Pulse Platform for more countries and new customers every month, and will continue to support a variety of operations in support of enhancing human security. ","locales":"","title":"Addressing Human Security Issues: Cause Award Honoree IST Research"}
{"index":{}}
{"author":"Christian Dahlqvist","category":"Engineering","publish_date":"2017-06-19T00:00:00.000Z","url":"/blog/filebeat-modiles-access-logs-and-elasticsearch-storage-requirements","seo_title":"","content":" Elastic recently introduced, which are designed to make it extremely easy to ingest and gain insights from common log formats. These follow the principle that \"simple things should be simple\", and comes with preconfigured ingest pipelines and Kibana dashboards, which can be used out-of-the-box or as a starting point for custom dashboards. In this blog post, we wanted to explore the on-disk storage requirements of data indexed using Filebeat modules. We will use the Apache2 Filebeat module for this analysis. Ensuring that data on disk is stored as efficiently as possible is often important because this can have a significant impact on the required cluster size at scale and the number of documents each node can handle. We will start our analysis with default Filebeat options and then explore techniques to optimize on-disk storage, while also discussing the tradeoffs associated with these optimizations. The techniques described in this blog post are based on the guidelines provided in the We will show that the default settings that Filebeat modules come with save around 20% disk space compared to the Elasticsearch defaults, and that further savings of up towards 50% can be achieved with more aggressive tuning. What are Filebeat modules?Filebeat modules simplify the collection, parsing, and visualization of common log formats. Each module handle collection, processing and visualization of files related to a specific type of logs. The Apache2 module, used in this blog post, knows the default location of the logs for the operating system being used and is able to collect and parse both access and error logs with minimal configuration. If Apache logs are stored in a non-standard location, this can easily be configured. Data collected by the Filebeat module is forwarded to an Elasticsearch where a specialized ingest pipeline parses the logs before they are indexed into Elasticsearch. Each Filebeat module comes with optimized, standard mappings for Elasticsearch, designed to save space without sacrificing functionality. Once the data has been stored in Elasticsearch, it can be analyzed using the default Kibana dashboards that are provided as part of the module (see screenshot below) Enrichment of web access logsA raw Apache2 access log record will look like this when written to the log file: 85.214.196.224 - - [30/May/2014:09:31:13 -0500] \"GET /files/xdotool/docs/html/files.html HTTP/1.1\" 200 4465 \"http://semicomplete.com/files/xdotool/docs/html/osx__hacks_8h_source.html\" \"Mozilla/5.0 (Macintosh:  Intel Mac OS X 10_8_5) AppleWebKit/537.73.11 (KHTML, like Gecko) Version/6.1.1 Safari/537.73.11\" The default ingest pipeline used by the Filebeat Apache2 module further enriches the log before it is indexed into Elasticsearch as a  JSON document with the following data and structure: The portions highlighted in bold blue is data that has been added through enrichment. The geoip section contains location-related data based on the client IP address and the user_agent section contains information about the operating system and browser of the user. These new fields added via the enrichment process lets the user ask more interesting questions of the data such as, \"which city is most of my traffic coming from\" or  \"which device or browser is the most popular?\" But, the new fields also increase the size of each log record. The remaining fields represent metadata describing how and when the log entry was collected, and can be used to trace the origin. Benchmarking test setupFor these tests, we indexed a file containing 280000 sample Apache2 access log records using the Filebeat Apache2 module. As we do not want the test data to interfere with the default Filebeat indices, we defined a fixed index name 'filebeat-test' in the configuration instead of the standard time-based index. The configuration file used is shown below: # Filebeat Apache module filebe","locales":"","title":"Filebeat modules, access logs and Elasticsearch storage requirements"}
{"index":{}}
{"author":"Rory Hunter","category":"Engineering","publish_date":"2017-06-15T00:00:00.000Z","url":"/blog/viewing-activity-in-elastic-cloud-enterprise","seo_title":"","content":" is our hosted service that lets you deploy and manage Elasticsearch and Kibana with ease. Now brings that same proven platform to your own datacenter. Since ECE shares the same code, it benefits from all the experience that Elastic has gained for more than 2 years of running clusters at scale, from handling node failures through to a streamlined UI. At Elastic we manage thousands of clusters in our Cloud service using the same UI that's now part of ECE. Over time, we found that sometimes it was hard to tell what was happening at any given moment, simply due to the number of hosts, clusters and nodes involves. For example, if a host is starting to fail and it holds 50 nodes of 50 different clusters, all those nodes need to be migrated before we decommission the host. To help us understand the current state of the system, we developed the Activity Feed and Cluster Activity view, both of which feature in . Activity FeedWhen we're dealing with thousands of clusters, we can't simply display them all on a page and expect our software engineers and site reliability engineers (SREs) to deal with all the data - it simply doesn't scale. Instead, the UI shared by Elastic Cloud and ECE has the Activity Feed. It's a kind of an event log, showing what has happened recently to clusters and what actions are currently pending. Take this example: An SRE looking at this can tell at a glance that one cluster is in the process of being updated, one is being created, one has recently been stopped, and a number of other clusters have finished being created recently. Cluster ActivityWhen it comes to understanding why a cluster is in a particular state, sometimes our engineers need to know the backstory, and they don't want to go hunting through logs, trying to piece together what changed. This is where the Cluster Activity view comes in: This view lists all the attempted changes to a cluster, and what those changes were. It includes attempted changes that didn't succeed - for example, attempting to grow a cluster when there isn't enough disk space to accommodate it. This gives our SREs a complete history of a cluster. They can even drill right down and see diffs of the cluster's configuration, and how long each step of a reconfiguration took. Future ImprovementsWe're always looking for ways to streamline our UIs and help our SREs do their job. For example, we're trialling ways to make the cluster history more visual and quicker to digest. Find Out MoreLearn more about Elastic Cloud Enterprise at . You can also . ","locales":"","title":"Viewing Activity in Elastic Cloud Enterprise"}
{"index":{}}
{"author":"Dale McDiarmid","category":"Engineering","publish_date":"2017-06-14T00:00:00.000Z","url":"/blog/integrating-elasticsearch-with-arcsight-siem-part-5","seo_title":"","content":"  provide examples of setting up alerts for common security threats using the alerting features in X-Pack.  Following on from our most recent , where we attempted to identify a successful brute force login attack, we now push the limits of rule-based alerting by creating a watch to detect suspicious process execution on machines in your infrastructure. As a teaser, in our next post, we'll compare and contrast this rule-based approach with a new technique - using X-Pack machine learning capabilities to detect the same suspicious process activity. For this (our last) Watcher example, we utilise a dataset which contains process starts events for a specific server as produced by auditd. Whilst we distribute this dataset in CEF format, this data could easily be captured using the for those not using ArcSight. Our sample event data looks like: Auditd generates many event types and is hugely . Our sample dataset includes a wide range of server activity. However, for the purposes of our watch and this blog, we are interested only in events which represent a process starting. With respect to the CEF format, these can be identified by the cat field containing the value EXECVE. The field ad.a0 represents the command issued, whilst ahost indicates the source host from which the event originated. rt provides the start time of the process in epoch milliseconds. The used to process this data with Logstash maps the above fields to their standardised CEF form. The cat field is mapped to deviceEventCategory, rt to deviceReceiptTime, ahost to agentHost and the ad.* fields to ad.argc as a concatenated string. The latter is subsequently processed with a filter to produce the field ad.a0. The full dataset, logstash configuration and accompanying watch described below can be found . Given that Watcher is limited to alerting using a static rule, as defined by a query, we consider an unusual process to be one which is started on a server for the first time. This relies on the user constructing a field which acts as a signature and unique identifier. Whilst this makes for an interesting example, which can be adapted to other similar use cases (e.g., \"Alert me when a user logs into a server for the first time\"), it would practically result in a large number of false positives in large infrastructures. Analysts would be encouraged to limit the watch to either specific servers of interest or add additional rules to define \"unusual\". Another teaser: We'll describe how an X-Pack machine learning recipe can be used to extend the usefulness of this approach in our next post - for now, if you want to know what we mean by an ML recipe, check out this recent . Design When considering the design of any watch, security analysts should consider scalability and performance. A naive solution to the above problem might use the following approach: 1. A query consisting of:       a. An aggregation to identify all of the servers and all their respective processes that have ever been started ( i.e., two nested terms aggregations for server name and process name).       b. A second aggregation (sibling of a) which identifies the servers and their respective processes that have been started in the last N minutes. This aggregation would be identical to a, but include a filter to restrict the results to the required period. 2. A Painless script condition and transformation which perform the set complement logic on the results of 1a and 1b i.e. to identify processes started in the last N minutes (1b) which are not present in 1a. The condition would return true if any of the complements result in a set size > 0. The transformation would, however, need to identify all complete complements for alerting. With painless lambda functions these can be achieved efficiently in a single line. 3. The results of 2 used in action to alert the servers which have new processes. Whilst this approach only utilises a singl","locales":"","title":"Integrating Elasticsearch with ArcSight SIEM - Part 5"}
{"index":{}}
{"author":"Suyog Rao","category":"The Logstash Lines","publish_date":"2017-06-13T00:00:00.000Z","url":"/blog/logstash-lines-2017-06-13","seo_title":"","content":" Welcome back to The Logstash Lines! In these weekly posts, we'll share the latest happenings in the world of Logstash and its ecosystem. Last week we  6.0.0-alpha2 -- the headline feature was the ability to run multiple pipelines in a single instance. Ingest to Logstash conversion tool (6.0.0-beta1)We now have a CLI tool in Logstash that takes an  in JSON and produces a corresponding Logstash config DSL. You can then run this config natively in Logstash, make changes to it etc. There are a few processors that are available in ingest, but don't have a LS counterpart -- these will be skipped during conversion. Here are some reasons why users would want to migrate from ingest to LS: Example$LS_HOME/bin/ingest-convert.sh —input file:///tmp/ingest/apache.json —output file:///tmp/ingest/apache.conf Upgrade to JRuby 9KMaster branch now uses as the underlying Ruby engine — all our core + plugin tests pass! This change also brings in Ruby 2.x support which means we can now update a bunch of our gem dependencies to newer versions. The previous JRuby version we were using (1.7.x) was only compatible with an older Ruby engine. 1.7.x was also EOL'd last year. JRuby 9k is the latest stable release, so we'll get new features and bug fixes. Big shoutout to PH and Guy for leading t. Changes in 5.x Changes in 5.5 ","locales":"","title":"Logstash Lines: Ingest to Logstash converter, upgrade to JRuby 9k"}
{"index":{}}
{"author":"Jim Goodwin","category":"Kurrently in Kibana","publish_date":"2017-06-13T00:00:00.000Z","url":"/blog/keeping-up-with-kibana-2017-6-12","seo_title":"","content":" Welcome to This is a weekly series of posts on new developments in the Kibana project and any related learning resources and events. This post actually covers the period between May 22 and June 12. If you're using 5.3.0 or 5.4.0 - please upgrade to 's latest release, 5.3.3 and 5.4.1 — Bo-Hyun Kim (@ensheneer)       ","locales":"","title":"Keeping up with Kibana: This week in Kibana for June 12, 2017"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2017-06-13T00:00:00.000Z","url":"/blog/brewing-in-beats-introduce-subcommands","seo_title":"","content":" Welcome to ! With this weekly series, we're keeping you up to date with what's new in Beats, including the latest commits and releases. SubcommandsThis introduces subcommands to the command line of the Beats, based on the library. The benefit of subcommands is that they allow for specific flags for a given command, and also allow for one-time maintenance commands. Examples: $ filebeat help $ filebeat help setup $ filebeat version $ filebeat run -h $ filebeat run -e --modules=nginx $ filebeat setup -e --modules=nginx The command is also set as the root command, meaning that is equivalent with , so backwards compatibility is kept. The first real use case for subcommands is that we’re replacing the program with the command. This allows us to remove the program from the packages, significantly reducing their size. In the future we plan to have more commands, for example, for listing / enabling / disabling modules. The subcommands will be introduced in 6.0. Add TTL option to the Logstash outputThanks to this , by , the Logstash output can be configured with a time-to-live (TTL) option. When the TTL expires, the connection is closed and a new one created. This allows for a better distribution to Logstash instances behind a load balancer. Since connections from Filebeat to Logstash are sticky, when an instance joins the load balancer, it does not get an equal distribution. For example, if there are 4 instances behind a load balancer and 3 of them are rebooted, then all Filebeat connections will go to the single instance that was not rebooted. By specifying a TTL on the connection, there is an opportunity for the load balancer to distribute connections equally between the instances. This is currently scheduled to be released with 6.0. Other changesRepository: elastic/beatsAffecting all BeatsChanges in master: Changes in 5.5: PacketbeatChanges in master: FilebeatChanges in 5.x: Changes in master: DocumentationChanges in master: Changes in 5.4: Changes in 5.5: InfrastructureChanges in master: Repository: elastic/gosigarChanges in master: ","locales":"","title":"Brewing in Beats: Introducing subcommands"}
{"index":{}}
{"author":"Uri Cohen","category":"Releases","publish_date":"2017-06-08T00:00:00.000Z","url":"/blog/elastic-cloud-enterprise-1-0-1-released","seo_title":"Elastic Cloud Enterprise 1.0.1 released","content":" We are happy to announce the release of ECE 1.0.1. This version is minimal maintenance release that will ensure that clusters on new ECE installations default to version 5.4.1 of Elasticsearch and Kibana due to a few security vulnerabilities discovered in Kibana and Elasticsearch 5.4.0 (please refer to to learn more). If you previously installed version 1.0.0, you do not need to upgrade your ECE installation. You can for version 5.4.1 and then upgrade your clusters. You should upgrade the and clusters used by ECE and any existing 5.4.0 clusters that you may have created. There is currently no supported method for deleting Elastic Stack packs, but we recommend that you do not create any new 5.4.0 clusters. The installation process of version 1.0.1 is identical to that of version 1.0.0, please refer to for more details. ","locales":"","title":"Elastic Cloud Enterprise 1.0.1 released"}
{"index":{}}
{"author":"Nikolaj Richers","category":"Engineering","publish_date":"2017-06-07T00:00:00.000Z","url":"/blog/exploring-the-api-for-elastic-cloud-enterprise","seo_title":"Exploring the API for Elastic Cloud Enterprise","content":" With Elastic Cloud Enterprise, we also introduced a new RESTful API to help you get started on your own automation journey. In its current 1.0 form, the API already supports operations on many different parts of the Elastic Cloud Enterprise platform and on the Elastic Stack. In this blog post, we’ll walk you through some hands-on examples to show how you can use the API with Elasticsearch clusters and Kibana. To reproduce the examples described here, all you need is , installed on a host that meets the basic prerequisites. (Not sure how to get started? Try our .) The examples in this blog post all use HTTPS over port 12443, run against localhost. Instead of localhost, you can just as easily run these examples against a remote ECE host. Using HTTPS requires that you . Alternatively, you can specify the option to turn off certificate verification as shown in our examples or simply use HTTP over port 12400 until you get your TLS certificate sorted out.  A first API call: What Elasticsearch clusters are there? Let’s try a simple API call with a GET request to retrieve information about existing Elasticsearch clusters on our Elastic Cloud Enterprise installation: curl -k -X GET -u root:password https://localhost:12443/api/v1/clusters/elasticsearch { \"return_count\": 3, \"elasticsearch_clusters\": [{ \"cluster_name\": \"My First Cluster\", \"plan_info\": { \"healthy\": true, \"current\": { \"plan_attempt_id\": \"05e6a34d-365f-45be-bc22-22669909da1b\", \"attempt_end_time\": \"2017-05-09T19:22:31.706Z\", \"plan_attempt_log\": [], \"attempt_start_time\": \"2017-05-09T19:21:32.529Z\", \"healthy\": true }, \"history\": [] }, \"associated_kibana_clusters\": [], \"elasticsearch\": { \"healthy\": true, \"shard_info\": { \"healthy\": true, \"available_shards\": [{ \"instance_name\": \"instance-0000000000\", \"shard_count\": 1 }], \"unavailable_shards\": [{ \"instance_name\": \"instance-0000000000\", \"shard_count\": 0 }], \"unavailable_replicas\": [{ \"instance_name\": \"instance-0000000000\", \"replica_count\": 0 }] }, \"master_info\": { \"healthy\": true, \"masters\": [{ \"master_node_id\": \"77TjkuEdT_-IHGJZpV9klg\", \"instances\": [\"instance-0000000000\"] }], \"instances_with_no_master\": [] } }, \"links\": { }, \"healthy\": true, \"status\": \"stopped\", \"topology\": { \"healthy\": true, \"instances\": [{ \"disk\": { \"disk_space_used\": 0 }, \"maintenance_mode\": false, \"service_running\": true, \"healthy\": true, \"instance_name\": \"instance-0000000000\", \"service_version\": \"5.3.2\", \"allocator_id\": \"192.168.40.89\", \"zone\": \"ece-region-1a\", \"container_started\": true, \"memory\": { \"instance_capacity\": 1024, \"memory_pressure\": 12 } }] }, \"metadata\": { \"version\": 4, \"last_modified\": \"2017-05-09T19:22:31.698Z\", \"endpoint\": \"96fc5d151c6b4b15b6ab9107aad67cef.192.168.40.89.ip.es.io\" }, \"cluster_id\": \"96fc5d151c6b4b15b6ab9107aad67cef\" } ... ] } As you can see from the JSON output returned by the API, there are three clusters, one of which is called . There are two other clusters whose output we omitted to keep things short. They are the and the clusters that always get created with every installation of Elastic Cloud Enterprise. Take care when modifying these two clusters, as they are required for Elastic Cloud Enterprise to run smoothly. To keep your installation safe, some operations are not allowed on these clusters, but we do recommend that you . Some of the terminology in the JSON will be unfamiliar. For example, there is a reference to various things that include the term “plan,” such as and . A plan is just a cluster configuration that gets applied to a cluster. When you change a cluster (to resize it, for example), you are simply attempting to apply a new plan. For now, you can just follow along with our examples, but if you want to learn more about what the JSON output means, our RESTful API reference content is available in with descriptions. Create a first Elasticsearch cluster through the APIFor our API exploration, we’ll be us","locales":"","title":"Exploring the API for Elastic Cloud Enterprise"}
{"index":{}}
{"author":"Sonya Liberman","category":"User Stories","publish_date":"2017-06-07T00:00:00.000Z","url":"/blog/looking-at-content-recommendation-through-a-search-lens","seo_title":"Looking at Content Recommendation Through a Search Lens","content":" Outbrain is the world's leading content discovery platform, bringing personalized, relevant online, mobile and video content to audiences. Outbrain helps publishers engage, monetize and understand their audiences through data, and helps marketers to reach their most relevant audiences. Outbrain serves over 200 billion personalized content recommendations every month and reaches over 500 million unique monthly visitors from across the globe. In this article we would like to share how we reduced the problem of recommending content to a user, to a search problem for users' implied interests, and how this enabled us to solve some of our major scale bottlenecks, and to increase the complexity of our online prediction models. Jungle of Market Rules, Personalization, and ScaleOur content discovery engine finds content that is most likely to be relevant to each user's interests, as manifested in their browsing patterns. In addition to being personalized, Outbrain's recommendations must comply with a long list of market rules and restrictions. For articles that pass all market rules, a ranking function is applied to select the most relevant content for a given user. One simple example of a market rule is geographic targeting. This rule enables a marketer to target an audience from a specific location, namely to ensure its content is recommended to users from that location only. Outbrain publishers and marketers have hundreds of possible market rule configurations, and these must be enforced in real time at a 35K/sec throughput of requests on millions of potential content recommendations and at a latency of under 100 milliseconds per request. While increasing throughput is horizontally scalable and can be achieved by adding more serving machines, inventory size is much more difficult to scale, especially with a growing set of market rules. After several architecture design attempts, we realized that Elasticsearch, being a distributed scalable search platform, can provide us with the solution. Elasticsearch has three key properties for our serving-stack needs. First, it is able to score documents by their relevance to a query. Second, it is able to filter documents by certain attributes. Finally, it does this efficiently and at scale both in terms of throughput and inventory size. Therefore, by translating the problem of recommending content to a user into a search problem for users' implied interests, we can base our recommender system on Elasticsearch and handle the aforementioned scale challenges. Let's now describe how this translation is being done. Every Potential Recommendation is an Elasticsearch Document Every article in the inventory of our potential content recommendations is indexed as a separate Elasticsearch document. We index its title, other meta-data, and most importantly, a set of semantic features that will be used to determine its relevance to each potential user. Those features are generated offline using an NLP-based semantic engine, and include high-level categories, more specific topics, explicit entities appearing in the article, and so forth.   Recommendations and Search: Your user IS the queryThe next step after indexing our inventory of potential content recommendations, was to translate users' implied interests to an Elasticsearch query. Users at Outbrain have 'profiles' that represent their interests. These are generated based on the semantics of the content each user reads. We can then incorporate user interests into the Elasticsearch query such that all relevant content recommendations are retrieved. Specifically, we generated a 'should' query, so that any document matching at least one clause of the query is eligible to be returned to the user. Documents with more than one match will be scored higher. The 'boost' parameter can be used to specify that not all matches contribute equall","locales":"","title":"Looking at Content Recommendation Through a Search Lens"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2017-06-06T00:00:00.000Z","url":"/blog/elasticsearch-6-0-0-alpha2-released","seo_title":"Elasticsearch 6.0.0-alpha2 released","content":" We are excited to announce the release of , based on . This is the second in a series of pre-6.0.0 releases designed to let you test out your application with the features and changes coming in 6.0.0, and to give us feedback about any problems that you encounter. Open a bug report today and become an . This is an alpha release and is intended for testing purposes only. Indices created in this version . Upgrading 6.0.0-alpha2 to any other version is not supported.Also see:You can read about all the changes in the release notes linked above, but some notable changes include: ","locales":"","title":"Elasticsearch 6.0.0-alpha2 released"}
{"index":{}}
{"author":"Tyler Hannan","category":"Releases","publish_date":"2017-06-06T00:00:00.000Z","url":"/blog/elastic-stack-6-0-0-alpha2-released","seo_title":"","content":" The 2nd alpha release of 6.0.0 is available today! Before you get too excited, keep in mind that this is still an alpha so don’t put it into production. There is no guarantee that any of the 6.0.0-alpha versions will be compatible with other pre-releases, or the 6.0.0 GA. We strongly recommend that you keep this far, far away from production. And, since it is an alpha, it is not available on Elastic Cloud. During the 5.0 release, we introduced the Elastic Pioneer Program and will continue that same program with this release. What is the Elastic Pioneer Program? We’re very excited about these upcoming Elastic Stack version 6.0 releases, but we want to make sure they’re perfect (or as perfect as software gets) before they ship more broadly. Everyone who reports a legitimate bug on the pre-release of the software will be recognized for the wider release, and receive a special Elastic gift package as our thank you. How to participate To join the program, just try out the pre-release of any (preferably, every) part of the Elastic Stack, and open issues as you find them in the appropriate repo (, , , ) or forum (). When you open an issue, mention that you found the bug in 6.0.0-alpha1/alpha2/betaY, and we’ll add a “Pioneer Program” label. While we appreciate the information, duplicate issues won’t enter you into the program. Elasticsearch For more detailed information, and many other features, peruse the Elasticsearch . Kibana ‘Visualize’ all the features, and ‘Discover’ more information in the . Logstash For more information, grok the . Beats We don’t ‘let the beat drop’ but we drop the updates in a . ES-Hadoop We’ve reduced the details to some bullets, but the original map is in the . Get It Now! ","locales":"","title":"Elastic Stack 6.0.0-alpha2 released"}
{"index":{}}
{"author":"Monica Sarbu","category":"Releases","publish_date":"2017-06-06T00:00:00.000Z","url":"/blog/beats-6-0-0-alpha2-released","seo_title":"","content":" It's time for the second Alpha version of the upcoming 6.0 release of the Elastic stack. The Beats 6.0.0-alpha2 release comes with new features, some rethinking of old features, and bug fixes. This blog post covers the highlights. Quick links: Auditd alternative in Metricbeat, simpler to useAuditd is the userspace component to the Linux Auditing System. It can audit every syscall in the Linux Kernel, and write audit records to log files. For example, it writes a log line every time a user accesses a file, starts a process, or connects to a socket. It’s part of the Linux kernel starting with 2.6.14. The power of auditd is that by inspecting the audit log files, you can track security-relevant events, detect misuse or unauthorized activities.  The auditd log format is a bit challenging to parse, but the auditd Filebeat module (available starting with Filebeat 5.4) parses the individual log lines for you.  Another issue with Auditd is that can generate a lot of data, so you need to be specific on what you look for in order to make use of it. This is partly because the Linux Audit Framework sends multiple messages for a single auditable event. For example, a rename syscall causes the kernel to sent eight separate messages. Each message describes a different aspect of the activity that is occurring (the syscall itself, file paths, current working directory, process title). To be more valuable, these events need to be correlated together.  So, we decided to create our own version of auditd that is easier to use and better integrated with the Elastic stack. For this, we created the audit module in Metricbeat. It establishes a subscription to the kernel to receive the events as they occur. Messages for one event can be interleaved with messages from another event. The audit module buffers the messages in order to combine related messages into a single event even if they arrive interleaved or out of order.  The Linux kernel only supports a single subscriber to the audit events so the audit Metricbeat module cannot be used simultaneously with a service like auditd. Auditd should be disabled if the audit module is being used.  The audit module is based on library, developed by us. It comes with two sample applications: audit and auparse. The audit application registers to receive audit events from the kernel and outputs the data it receives to stdout. The auparse application parses the log files from the Linux auditd process or the output of the audit application and it combines related log messages that are a part of the same event.  To make use of the audit module, you need to install manually the audit rules to the Kernel. The audit rules are where you configure the activities that are audited. These rules are configured as either syscalls or files that should be monitored. For example you can track all syscalls or file system writes to . As a next step, we are planning to have the audit module load automatically the audit rules, to make it easier for the user. The audit module is a beta feature, and is subject to change. New processor: add_docker_metadataInspired by the processor, which we added with Alpha1, the processor queries the Docker API to enhance the events with the Docker name, image, and labels. To lookup the metadata, the processor currently needs the container ID to be present in one of the event fields. This is the case, for example, with the cgroups events created by the system module in Metricbeat. processors: - add_docker_metadata: match_fields: [\"system.process.cgroup.id\"] host: \"unix:///var/run/docker.sock\" TLS configuration for the connection with the Docker server is also possible. Optimized Metricbeat default configurationThis release introduces a new default , one that is better suited for the majority of metrics use cases. The new configuration takes advantage of the way of filtering processes, which we introduced in Alpha1, and it also uses the new way of generating the Elasticsearch mapping t","locales":"","title":"Beats 6.0.0-alpha2 released"}
{"index":{}}
{"author":"Clinton Gormley","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2017-06-06T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2017-06-05","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. — elastic (@elastic) With the removal of types, we had to come up with a way to support parent/child joins that didn't depend on having multiple mapping types in an index. Good progress has been made in 5.5 with the introduction of the , a within documents of the same index. Only . The join field , which defaults to . have been updated to work with the new join field. Existing 5.x parent/child indices will continue to work as before in 6.x. : Apache Lucene We expect to cut the branch . After several respins, it looks like . Lucene will soon allow you to index . This is useful if you want to use term frequencies to hold scoring signals. The only way that it could be done before was by using payloads, which come with a higher cost in terms of CPU usage and disk requirements. However this remains a very expert feature, which only works with the DOCS_AND_FREQS index options. The introduction of a dependency from the classification module on the sandbox module triggered a about when it is fine for modules to depend on each other. , where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2017-06-05"}
{"index":{}}
{"author":"Sarah Cecilia Griffin","category":"User Stories","publish_date":"2017-06-06T00:00:00.000Z","url":"/blog/dimensions-of-the-ballerina-in-the-age-of-technology","seo_title":"","content":" .) The following captures the perspective of ballerina and choreographer, Sarah Cecilia Griffin, who worked closely with the project’s designers, engineers, musician, and seamstress. *** I am a dancer who enjoys a challenge. I’ve spent most of my life perfecting the minutiae of the narrow and disciplined language of ballet, honing my skills in order to ask different questions and explore new possibilities on stage and off. Lately the question is: where does innovation originate in the creative process? The need to innovate looms large in the minds of dancers and choreographers in the contemporary dance landscape, and the concerted effort to create something truly “new” can push and stretch artists beyond our comfort zones. I find that the most groundbreaking work happens in the intersection points between disciplines—in the sharing of languages, skill sets, and perspectives. As a dancer I have worked with and learned from artists of varied disciplines and backgrounds to create collaborative performance art that feels completely new to all of us. Working on the creative team for Elastic’s “Ballerina” presentation was an experience unlike any other in the breadth and scope of cross-disciplinary collaboration. Last fall I was contacted by Issac Kelly (designer and engineer) with the conceptual skeleton of the project, and I was immediately intrigued. The Bay Area is a major hub for both arts and technology, and I have often wondered why the two don’t intersect more often. I’ve always been a nerd for science and technology, and I enjoy exploring dance and movement from new and different perspectives. For me, dancing exclusively for dancers, with dancers, and around dancers all the time creates something of a feedback loop in which innovation and risk-taking can stagnate. This project struck me as an opportunity to be a creative leader and collaborator in a completely new environment. I came into the process a few months after Issac, Kat Meler (art director), and Crystal Titus (costumer design) had started developing functional hardware and production design with the Elastic team. This would be my first choreographic commission, and it was helpful to start with some established concepts, parameters, and goals. At our first team meeting in Issac’s workshop I had a chance to test out the prototype accelerometer wristbands with Kibana and connect movement to visualization. It was a lot of fun to play around and see my movements projected into data points and graphs in real time. I experimented with motions in all planes, varying speed and size. I was most captivated by the way the software captured movement so dynamically. I could execute the same basic movement with a small shift of energy—fast vs. slow, smooth vs. sharp, large vs. small—and see a significant change in visualization. In early January I began sketching out choreography alone in a studio at ODC Dance Commons in San Francisco. I had four basic tasks to demonstrate: accelerometers on the hands, feet, and hips, and infrared heat map tracking. I chose to work with basic classical ballet vocabulary in order to create clear, recognizable, and repeatable movement sequences. Without the hardware and software on hand, I could only work from memory and imagination. I began by creating a gestural language as a basic framework from which to build more complex movement. These small gestures of the hands and feet on linear planes could essentially teach the audience to read the movements and visualizations together. I began to deconstruct the grammar of the ballet language my body speaks so fluently, distilling movements down to their basic planes of motion, acceleration, and g-force. These basic elements could then be combined and recombined with increasing complexity to tell a story about how movement can be seen. I didn’t have to create mind-blowing complex choreography:  I could use","locales":"","title":"Dimensions of the Ballerina in the Age of Technology"}
{"index":{}}
{"author":"Spencer Alger","category":"Releases","publish_date":"2017-06-06T00:00:00.000Z","url":"/blog/kibana-6-0-0-alpha2-released","seo_title":"","content":" We are excited to announce the immediate availability of Kibana 6.0.0-alpha2, which includes some of the most anticipated Kibana features of all time. Open a bug report today and become an . This is an alpha release and is intended for testing purposes only. Upgrading 6.0.0-alpha2 to any other version is not supported. CSV export Did someone say CSV export? We’re pretty sure we heard someone ask for CSV export. Just to be safe, we built CSV export. Search for the documents you want to export in the Discover app, and then export matching documents as a CSV file via the reporting menu. CSV export comes with X-Pack basic, which is our free license. Dashboard-only mode Dashboards are everything for some users. In the roles section of the Kibana management application you can now assign to roles, one of which is . When users with this mode access Kibana they will only see the dashboard application, and all other applications and editing controls will be hidden. We think this will provide a better experience to users who are given read-only access to Kibana via X-Pack security. Region maps The new region map visualization allows you to bucket your data by geographic region and color those regions based on a metric. Kibana provides region data for “World Countries” and “US States”, but you can also configure your own GeoJSON formatted data via config file. Gauge charts Looking to fill executive hearts with glee? With the new gauge visualization, metrics can communicate their exact value how great, normal, or bad they are! Draw the gauge with an arc, circle, or skip the fancy drawing and just use color to get the message across. There’s more where that came from In addition to all of the enhancements in 6.0.0-alpha2, we have more exciting features in the works for the next 6.0.0 pre-release. Stay tuned… In the meantime, we’d appreciate your feedback and reports of any problems you encounter with this new release. today, and don’t hesitate to reach out to us on , , or our . ","locales":"","title":"Kibana 6.0.0-alpha2 is released"}
{"index":{}}
{"author":"James Baiera","category":"Releases","publish_date":"2017-06-06T00:00:00.000Z","url":"/blog/eshadoop-6-0-0-alpha2-released","seo_title":"","content":" ​I am excited to announce the release of Elasticsearch for Apache Hadoop (aka ES-Hadoop) built against Elasticsearch 6.0.0-alpha2. : This is an alpha release and is intended for purposes only. Crazy things might happen when running this code and indices created with this version . For the sake of your own sanity, we do not advise using this version in production. What’s new? Alpha Support for Spark Structured Streaming Spark 2.0 saw the alpha release of , a new streaming framework that combines the query planning facilities from Spark SQL with the potential for exactly once processing of stream messages. Now, with ES-Hadoop 6.0.0-alpha2, we are proud to present our own Elasticsearch structured streaming sink for Apache Spark, complete with all features you’ve come to love from the connector, as well as backed by an internal HDFS-based commit log to help ensure correct message delivery. All of this is detailed further in our . Please remember though: Both Structured Streaming in Spark and this sink in ES-Hadoop are alpha features! There could still be bugs lurking out there. We advise against rolling this out to production without careful consideration. Squashing Bugs Parsing errors from index auto-creation, backwards compatibility errors with scroll id’s, missing support for timestamps in params and much more. Take a look at in this release! Feedback Now you might be wondering, “Why would I want to try an Alpha Release? Aren’t these things normally riddled with bugs?” Well, yeah, sometimes. Thats why we need the help from all of you awesome early adopters! So, please, try this at home! You can ES-Hadoop 6.0.0-alpha2, try it out, find out how it breaks, and let us know what you did on , , or in the . A crisp high five is waiting for all who participate! Not a huge fan of high fives? There’s always the instead! ","locales":"","title":"Elasticsearch for Apache Hadoop 6.0.0-alpha2 released"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2017-06-06T00:00:00.000Z","url":"/blog/brewing-in-beats-enrich-events-with-docker-metadata","seo_title":"","content":" Welcome to ! With this weekly series, we're keeping you up to date with what's new in Beats, including the latest commits and releases. New processor: add_docker_metadataInspired by the processor, this adds the processor to enhance the events with the Docker metadata like name, image, and labels. The processor requires that the container ID to be present in the event fields, and queries the Docker API to get the metadata for that container. For example, the cgroups events generated by the system module in Metricbeat contain the container ID under , and you can configure the processor as following to enrich the events generated by each Docker container: processors: - add_docker_metadata: match_fields: [\"system.process.cgroup.id\"] host: \"unix:///var/run/docker.sock\" In case you want to connect to Docker over TLS, you need to configure a client and a CA certificate. The processor is released in 6.0.0-alpha2. Filebeat: Redis moduleBuild upon the Redis prospector, which was merged the week before, we now with two filesets: A Kibana dashboard is included: We currently plan to release this module with 6.0. With this, Redis joins the select group of projects for which we have a Filebeat module to monitor its logs, a Metricbeat module to collect its metrics, and a Packetbeat module/protocol to parse its network traffic. Metricbeat: New Rabbitmq moduleThanks to , Metricbeat gets support for RabbitMQ. The module comes with the that collects metrics from RabbitMQ nodes by querying the HTTP API . The module is marked as experimental and it will be released with 6.0. Other changesRepository: elastic/beatsAffecting all BeatsChanges in master: FilebeatChanges in master: MetricbeatChanges in 5.x: Changes in master: InfrastructureChanges in master: DocumentationChanges in 5.4: Changes in 5.3: Changes in master: ","locales":"","title":"Brewing in Beats: Enrich events with Docker metadata"}
{"index":{}}
{"author":"Suyog Rao","category":"Releases","publish_date":"2017-06-06T00:00:00.000Z","url":"/blog/logstash-6-0-0-alpha2-released","seo_title":"","content":" We are pleased to announce our second alpha release for Logstash 6.0.0! You can find the here or head straight to to try the new features! Mo' pipelines mo' funWe've added support for running multiple, self-contained pipelines in the same Logstash instance (on the same JVM). Logstash configurations can be complex with multiple data flows built using conditionals. This feature will help simplify that — if your data flow and processing can be separated, you can run them in an isolated fashion as separate pipelines. This separation means that a blocked output in one pipeline won’t exert backpressure in the other pipeline.A new configuration file called file has been added to define multiple pipelines that can be run on the same instance. Users can dynamically add, modify and remove pipeline configurations. This feature also provides an option to run pipelines with their own run-time settings like workers size, batch size etc. This feature isn't complete yet — the monitoring UI in x-pack currently reports metrics from a single pipeline:  we're working on enhancing the UI to support multiple pipelines as well, so stay tuned for updates!Breaking Change: glob only *.conf in conf.d folderCurrently, when Logstash is installed and set up via package managers, it loads all files found in as configuration. This can be problematic if there are non-configuration files in this folder. For example, user makes a copy of a configuration file and names it with a extension which results in unexpected behavior. Starting from alpha2, Logstash will only glob files ending with extension in ().FeedbackWe would love for you to try 6.0.0-alpha2 and the new multi-pipeline feature! Let us know what you think! ","locales":"","title":"Logstash 6.0.0-alpha2 released"}
{"index":{}}
{"author":"Suyog Rao","category":"Engineering","publish_date":"2017-06-05T00:00:00.000Z","url":"/blog/logstash-lessons-handling-duplicates","seo_title":"Little Logstash Lessons: Handling Duplicates","content":" This post describes approaches for de-duplicating data in Elasticsearch using Logstash. Depending on your use case, duplicated content in Elasticsearch may not be acceptable. For example, if you are dealing with metrics, duplicated data in Elasticsearch may lead to incorrect aggregations and unnecessary alerts. Even for certain search use cases, duplicated data could lead to bad analysis and search results. Background: Elasticsearch Indexing Before we get to the de-duplication solutions, let's take a brief detour into Elasticsearch's indexing process. Elasticsearch provides a REST API for indexing your documents. You have a choice of providing an ID that uniquely represents your document or you can let Elasticsearch generate an ID for you. If you use the HTTP PUT verb with the index API, Elasticsearch expects you to supply an ID. If a document already exists with the same ID, Elasticsearch will replace the existing contents with the one you just provided — the last indexed document wins. If you use the POST verb, Elasticsearch will generate a new document with a new ID even if the content is already present in the corpus. For example, let's say you had just indexed a blog post a second ago, and you resend the same blog post using the POST verb, Elasticsearch creates another document with the same content, but a new ID. While Elasticsearch provides an explicit _update API which can be used as a potential workaround, we will be focusing this post on just the indexing API. Logstash's Elasticsearch output uses the indexing API and by default does not expect an ID to be supplied. Hence, it treats every single event as a separate document. However, there is an option where you can easily set a unique ID for every event in Logstash. Bring your own ID If your data source already has an ID, it is easy to set this as the document ID before indexing into Elasticsearch. For example, users of JDBC input could easily use the primary key from the source table as the Elasticsearch ID. Using the field reference syntax, it is straightforward to set the document ID in the output section: output { elasticsearch { hosts => \"example.com\" document_id => \"%{[upc_code]}\" } } where upc_code is a field in your data. This field may have come from a field in your structured log format or extracted using a grok filter. De-duplicating similar content As we mentioned earlier, duplicated content may not be acceptable in your use case. Using a concept called and the Logstash fingerprint filter, you can create a new string field called fingerprint that uniquely identifies the original event. The fingerprint filter can take one or more fields (message field is the default) in your original event as the source to create a consistent hash. Once these fingerprints are created, you can use this as the document ID in the downstream Elasticsearch Output. This way, Elasticsearch will only update or overwrite existing document contents after comparing the fingerprint, but will never duplicate them. If you would like to consider more fields for de-duplication, the concatenate_sources option is the way to go. The fingerprint filter has multiple algorithms you can choose to create this consistent hash. Please refer to the as each function varies in the strength of the hash and may require additional options. In the following example, we are using the MURMUR3 method to create a hash from the message field and set it in a metadata field. Metadata fields are not sent to outputs, so they provide an efficient way to store data temporarily while processing events in the pipeline. filter { fingerprint { source => \"message\" target => \"[@metadata][fingerprint]\" method => \"MURMUR3\" } } output { elasticsearch { hosts => \"example.com\" document_id => \"%{[@metadata][fingerprint]}\" } } If you are using any of the cryptographic hash functions algorithms (like SHA1, MD5), it is required to provide a key option. T","locales":"","title":"Little Logstash Lessons: Handling Duplicates"}
{"index":{}}
{"author":"Alex Francoeur","category":"Engineering","publish_date":"2017-06-01T00:00:00.000Z","url":"/blog/master-time-with-kibanas-new-time-series-visual-builder","seo_title":"Kibana Time Series Visual Builder","content":" We love visualizing data, especially time series data, which is why we created a new UI that specializes in manipulating and viewing it. Kibana's new time series visual builder combines the power of with an easy-to-use interface for analyzing and monitoring your data in as few clicks as possible. ","locales":"","title":"Master time with Kibana’s new time series visual builder"}
{"index":{}}
{"author":"Jim Goodwin","category":"Releases","publish_date":"2017-06-01T00:00:00.000Z","url":"/blog/kibana-5-4-1-and-5-3-3-released","seo_title":"","content":" The 5.4.1 and 5.3.3 releases contain important security fixes, and we recommend that you upgrade as soon as possible. Please read the details below. Kibana instances on Elastic Cloud for previous versions of 5.4 and 5.3 will be upgraded automatically.The time series visual builder that was released in 5.4.0 is vulnerable to a cross-site scripting attack (XSS), where a malicious user could embed HTML into markdown documents that could result in JavaScript being executed in other users' browsers. This could be abused to steal sensitive information or to perform destructive actions on behalf of other users. 5.4.1 fixes this vulnerability by no longer allowing HTML in markdown documents.  () ","locales":"","title":"Kibana 5.4.1 and 5.3.3 released"}
{"index":{}}
{"author":"Haley Eshagh","category":"Releases","publish_date":"2017-05-31T00:00:00.000Z","url":"/blog/manage-and-monitor-elasticsearch-clusters-at-scale-on-any-infrastructure","seo_title":"Centrally Manage, Monitor, and Provision Elasticsearch at Scale","content":" We are doing backflips and handstands about Elastic Cloud Enterprise (ECE) 1.0 officially becoming GA. This new product lets you provision, monitor, and orchestrate a fleet of Elasticsearch clusters and Kibana instances the way you want, in the environment you choose, from a single console. (And if you want to dive right in,  and enjoy. Also, you'll likely fancy our upcoming .) Leading up to today's launch, we've talked about , , and . Now, we'd like to take a closer look at the path that leads users to adopt a product like ECE. Many of our users share a familiar adoption story. Developer at Company X fiddles with a data-related problem, stumbles across Elasticsearch, downloads it, spins up a test cluster, ingests some data, finds success. Then a coworker hears about it, they add their data, and to support this the cluster has to get bigger. Soon enough Company X has dozens of nodes of Elasticsearch running in production supporting mission-critical functions. And that's just the start of it. Let's say that Company X is actually a bank. Their multi-node production cluster currently supports a logging use case for multiple applications, but now they're looking to power new applications for security analytics and transaction analysis. Plus, the marketing, human resources, and SRE teams have gotten wind of Elastic and have placed requests to either try it out or apply it to solve their own business problems. Now things are getting interesting. A problem rainbow has emerged, one that consists of the unpleasantries that naturally come with the hard problem of managing scale within a cluster: multiple use cases, multiple tenants, and more and more data. That said, these are the exciting kinds of problems to have, but challenging nonetheless. To explain, let's break this down by zeroing in on two (unintentionally existential) questions: 1) who are you and 2) what are you trying to do? To question number one: who are you? , you care about time-to-insight, quick responses, a custom experience, and expectations being met. But it's unwise to assume that all tenants are alike. They have different needs, habits, and requirements — each a potential source of friction and frustration. There are other considerations, too, like different backup policies that can incur extra work or infrastructure burden. Or some tenants might be building heavy-duty alerts that query a year's worth of data that bog down the cluster for other tenants. making all of the Elastic magic happen, you care about good user experience, added ROI to projects or divisions, and making sure your time isn't overwhelmed by putting out fires. And from this perspective there is another set of variables to account for. So what to do? The software works so well and offers so much promise to the organization, how should the production team proceed before the cluster (or possibly their spirit) crumbles? An option (actually, an eventuality): split the cluster. Breaking it up into smaller bits solves some of the pain points we've discussed, but it does come with a cost. There needs to be consideration for deployment and provisioning using configuration management or orchestration tools, managing different Elastic versions across each cluster (which gets tricky for releases before 5.x), supporting different SLAs and maybe even different types of infrastructure, and so on. It becomes a slippery slope: if one tenant gets their own cluster, others will want their own clusters, too. Managing the simple things at a smaller scale become more complex as they compound. Soon, the production team runs the risk of spending more time supporting clusters than they are on the work they originally set out to do. And now we have arrived. This is the type of challenge that ECE eats for breakfast. It makes addressing all of","locales":"de-de,fr-fr,ja-jp,ko-kr,zh-chs","title":"A New Elasticsearch Frontier: Elastic Cloud Enterprise 1.0 GA"}
{"index":{}}
{"author":"Barnaby Gray","category":"User Stories","publish_date":"2017-06-02T00:00:00.000Z","url":"/blog/kibana-baby-kick-counter-part-2","seo_title":"","content":" This is part 2 of 2 about using Elasticsearch and Kibana to track patterns in baby activity. Part 1 covers the hardware and setup for tracking baby kicks. Machine learningHaving collected about 6 weeks of baby kicking data, it's time to test the new toy in the Elastic Stack: . Machine learning is a feature of X-Pack -- which you can test out using a 30-day trial license -- and installing it was a straightforward case of following the . From the new 'Machine Learning' app in Kibana, I chose 'Create new job', and 'Create a single metric job': Pointing it towards my index, choosing aggregation 'Count' and then Create Job and you get a nice preview of the analysis scanning through the data and highlighting anomalies as it goes:    Once this is complete you can open up the Anomaly Explorer and run through events that it has identified as anomalous: It has detected that 12 kicks at 12am are unusual where no activity would be expected, and identified this purely from the implicit patterns without needing to be trained what to expect (ie. unsupervised machine learning) - neat! I did some reading up on the medical advice for expectant mothers monitoring baby activity, which as often is the case is a bit mixed depending on what you read. It used to be a simple rule of thumb: time how long it takes for 10 kicks to be felt (so called 'count to 10') and if this is within 2 hours, everything is fine. Apparently this is a bit of a broad generalisation and activity varies quite widely from baby to baby. What's emphasised now is it's important to spot changes in the normal pattern of activity for your particular baby - all babies being different. We wanted to spot the situation where activity drops from normal, which could be indicative of a problem. So I created another job with aggregation type 'Low count' of these anomalies. None were identified in our data - phew! :-) And just to test this theory I wanted to remove a period of data and see whether the anomaly detection would pick up this simulated drop in activity. To do this I could have manually deleted data, but instead I used the powerful and underrated index aliases feature to create an alias to my original data limited by a query excluding a gap of time on 1st May: echo '{ \"actions\" : [ { \"add\" : { \"index\" : \"bumps\", \"alias\" : \"bumpless\", \"filter\": { \"bool\": { \"must_not\": { \"range\": {\"timestamp\": {\"gte\": \"2017-05-01T01:15:00\", \"lt\": \"2017-05-02\"}} } } } } } ] }' | curl -XPOST -u elastic:changeme localhost:9200/_aliases And finally, running the same low count job on this flagged this period as 41x lower. Great - simulated anomaly correctly identified.   In conclusionThis simple example illustrates the machine learning module has great potential application in the field of health monitoring (with suitable caveats - I am not a doctor, etc!). ","locales":"","title":"Kibana baby kick counter - part 2"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2017-06-01T00:00:00.000Z","url":"/blog/elasticsearch-5-4-1-and-5-3-3-released","seo_title":"Elasticsearch 5.4.1 and 5.3.3 released","content":" Today we are pleased to announce bug fix releases of , based on , and , based on . Elasticsearch 5.4.1 is the latest stable release, and is already available for deployment on , our Elasticsearch-as-a-service platform. This release includes two security bug fixes — all users of X-Pack Security should upgrade. Latest stable release in 5.x: Bugfix release in 5.3: You can read about all the changes in the release notes linked above, but there are a few changes which are worth highlighting below. X-Pack Document Level Security and Aliases (ESA-2017-09)X-Pack Security versions prior to 5.4.1 and 5.3.3 did not always correctly apply Document Level Security to index aliases. This bug could allow a user with restricted permissions to view data they should not have access to when performing certain operations against an index alias. Affected versionsX-Pack Security 5.0.0 to 5.4.0 is affected by this flaw. Solution and MitigationsAll users of X-Pack security should upgrade to version 5.3.3 or 5.4.1. If you cannot upgrade, on the index will mitigate this bug. CVE-2017-8441 X-Pack Privilege Escalation (ESA-2017-06)This release fixes a privilege escalation bug in the  functionality. This bug prevents transitioning into the specified user specified in a  request. If a role has been created using a template that contains any of the  properties, the behavior of  will be incorrect. Additionally if the  user specified does not exist, the transition will not happen. Generally when using the  functionality a user will transition to a different user. This bug will cause the role query to execute as the user which authenticated to Elasticsearch, not the user specified via . This could result in a query returning incorrect or unexpected results. If you are not using  functionality or the  properties you are not affected by this issue. Affected versionsX-Pack Security 5.0.0 to 5.4.0 is affected by this flaw. Solution and MitigationsIf you are affected by this issue, we suggest you upgrade your Elastic Stack to version 5.4.1 If you are unable to upgrade, removing use of the  placeholder and ensuring the  setting cannot be modified by untrusted users is a valid solution.  CVE-2017-8438 Other important changes ConclusionPlease , try it out, and let us know what you think on Twitter () or in our . You can report any problems on the . ","locales":"","title":"Elasticsearch 5.4.1 and 5.3.3 released"}
{"index":{}}
{"author":"Tim Sullivan","category":"Engineering","publish_date":"2017-05-30T00:00:00.000Z","url":"/blog/elasticsearch-cluster-alerts-for-issues-from-xpack-monitoring","seo_title":"","content":" In future releases, we plan to provide alerts for X-Pack license expiration approaching, Elasticsearch shards approaching maximum size limits, CPU, memory, and disk utilization, and the holy grail: nodes joining and leaving the cluster. We know that one size does not fit all, we plan on working on customizable thresholds in the future.We are also actively working to enable E-mail configuration from Kibana for the built-in Watches. In the future, we will let you set notifications via Slack, HipChat, PagerDuty, Jira and Webhook integrations.When you're on vacation on a beautiful beach, you can fully enjoy your time and relax knowing that your Elastic Stack issues will be kept under close watch with Cluster Alerts in X-Pack monitoring.To try out this new feature, get started today with with a trial license, where you can take a full advantage of all . If you have any questions or requests, please let us know via our forum. ","locales":"","title":"Cluster Alerts for Elasticsearch Issues: Cluster Alerts in X-Pack Monitoring"}
{"index":{}}
{"author":"Dale McDiarmid","category":"Engineering","publish_date":"2017-05-30T00:00:00.000Z","url":"/blog/malware-analysis-wannacry-elastic-stack","seo_title":"Using Elasticsearch to Detect Signs of Ransomware like WannaCry","content":" Ransomware has occupied the news headlines in the past few weeks with the WannaCry infection significantly impacting . As of this writing, it is estimated that over 400,000 computers have been infected. In this blog we explore how the Elastic Stack can be used during the triage phase of a malware outbreak to identify potential infections within your organisation. The ability to quickly search through network and operating system events can enable the rapid identification of machines which have been compromised, given our knowledge of specific malware signatures. The Elastic Stack cannot prevent infection - that requires a combination of people, process, and other technology - or exhaustively identify new malware attack vectors, but it lets you gain rapid insight into your current situation. If your organisation was recently impacted by the rapid spread of WannaCry, or even if you would just like additional reassurance that no machines in your infrastructure are exhibiting signs of exploitation, this blog provides some quick triage techniques you can use if Packetbeat and Winlogbeat are running in your infrastructure. If you don’t have these installed, we provide some simple instructions to prepare yourself for the next time...maybe HaftaCry, GonnaCry, WillCry, SurelyCry? Simple SetupTo test these techniques, we simulated a WannaCry infection in our malware lab, by configuring two Windows 7 SP1 VMs (unpatched as distributed by the ) and a Windows 2012 R2 server VM (also unpatched). The latter was responsible for acting as domain controller, DNS, and DHCP server, as well as managing the SMB shares. Whilst the latter may not be required, we experienced issues (similar to those reported ) getting the malware to propagate when machines belonged only to a workgroup. A fourth Linux VM hosted our Elasticsearch and Kibana instances. This entire environment was hosted on an OSX host and configured with internal networking only i.e., no external access. We installed the following Elastic Stack components on each Windows VM: winlogbeat.event_logs: - name: Microsoft-Windows-Sysmon/Operational The capabilities of Sysmon extend far beyond this post. This tool is hugely configurable and is capable of monitoring a plethora of Windows system events such as process creations, network connections, and changes to file creation time. Our Sysmon configuration, heavily inspired by this excellent , with accompanying beat configurations have been provided for reference. WannaCry BehavioursWannaCrypt or WannaCry is an interesting combination of old-time worm and Ransomware, with infection occurring due to a SMBv1 vulnerability. For our purpose, we deliberately infect a machine and track its infection, thus producing signatures you can subsequently identify using Kibana capabilities. The specific behaviour of WannaCry varies depends on its . The well publicised kill switch, for example, is no longer present in more recent copies. We summarise its behaviour below. This isn’t an exhaustive analysis - others have performed comprehensive analysis at , and . Sites such as have allowed the community to identify a comprehensive set of signatures for each variant by uploading . After initial infection, the WannaCry ransomware may proceed to: The specific SMB exploit has been well publicised and has been . Aspects of the following assume unpatched systems. Applying the Elastic StackFor purposes of completeness we selected a variant of the malware which exhibits all of the above signatures. For those looking to replicate the following, this variant has a SHA256 hash of 24d004a104d4d54034dbcffc2a4b19a11f39008a575aa614ea04703480b1022c. Obtaining the this file is left to the user, who should take the typical precautions when dealing with such files. Detecting a WannaCry DownloadAs a first step we can utilise the Elastic Stack to identify instance","locales":"","title":"Detecting Signs of Ransomware: WannaCry and the Elastic Stack"}
{"index":{}}
{"author":"Clinton Gormley","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2017-05-29T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2017-05-29","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. — elastic (@elastic) It has been a long standing issue that replicas can fall out of sync when a primary shard fails. The differences are subtle and only relate to write operations that have been in-flight while the old primary failed. To date, these differences can only be resolved by doing a full file-based sync, which is slow and heavy. That heavy handed solution meant we have to wait for the next shard relocation to fix it, meaning that shard counts can be off for a quite a while. With the introduction of sequence numbers we now have a faster, ops based, synchronisation mechanism. Instead of aligning segment files (which can diverge quite a bit) we can identify and sync only the operations that need to be corrected. Since these are typically just a few operations, we can use this mechanism to re-sync the replicas with the new primary and make sure they are identical. This can be in real time and without stopping on-going indexing, which is pretty unique. Originally we thought that using the new ops based recovery logic to do a live background sync would only make it somewhere in the 6.x series but we're making good head way (, , , ) and version 6.0 will already have some of this functionality. Any operation that's present on the new primary will be replicated over to the replicas, making sure that's everything on the new primary is also on the replica. Of course, replicas can also have operations that are not present on the new primary, and this issue also needs to be addressed by allowing replicas to roll back those unacknowledged changes. This part will be coming during the 6.x life time and will not be part of 6.0. The new has landed in master. This allows users to run significant terms on text fields without using field data, which greatly reduces the heap usage for significant terms on fields. Instead, the aggregation works by re-analysing the source field for the collected documents (or a sample thereof). This agg also contains a feature for removing duplicated text (such as email signatures or inline ads in articles) from the analysis to stop them from skewing the results. Changes in 5.5: : , where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2017-05-29"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2017-05-29T00:00:00.000Z","url":"/blog/brewing-in-beats-read-redis-slow-logs-with-filebeat","seo_title":"","content":" Welcome to ! With this weekly series, we're keeping you up to date with what's new in Beats, including the latest commits and releases. Filebeat: Read slow logs from RedisThe is a system to log queries that exceeded a configured execution time. The slow logs are accumulated in memory, so no files are written on the disk. To read the slow logs, you can use the command, and it will return only the N most recent slow log entries. With this , a new prospector type is added to Filebeat that connects to Redis, retrieves the slow logs and clears the buffer so on the next poll only new events will be read. It’s inspired from the community Beat, . This is the first specialized prospector type in Filebeat, which opens the door for other interesting use cases in the future. With this configuration: - input_type: redis hosts: [\"localhost:6379\"] # How often to fetch the Redis slow logs #scan_frequency: 10s # Network type to be used for redis connection. Default: tcp #network: tcp … Filebeat generates an event for each Redis slow log: { \"@timestamp\": \"2017-05-16T06:27:17.000Z\", \"beat\": { \"hostname\": \"ruflin\", \"name\": \"ruflin\", \"read_timestamp\": \"2017-05-16T06:27:19.275Z\", \"version\": \"6.0.0-alpha2\" }, \"message\": \"SET hello world\", \"redis\": { \"slowlog\": { \"args\": [ \"world\" ], \"cmd\": \"SET\", \"duration\": { \"us\": 11 }, \"id\": 38, \"key\": \"hello\" } } } Where the parsed slow log is available under and the contains the full Redis command, including the arguments concatenated. This feature is experimental, and will be included in the 6.0.0 version. Repository: elastic/beatsFilebeatChanges in master: MetricbeatChanges in master: PacketbeatChanges in master: InfrastructureChanges in master: Changes in 5.4: DocumentationChanges in 5.4: Changes in 5.3: Changes in master: PackagingChanges in master: ","locales":"","title":"Brewing in Beats: Read Redis slow logs with Filebeat"}
{"index":{}}
{"author":"Alexander Reelsen","category":"Engineering","publish_date":"2017-05-24T00:00:00.000Z","url":"/blog/watching-the-watches-writing-debugging-and-testing-watches","seo_title":"","content":" TLDR:  This blog post will outline how to efficiently write, execute, and debug watches. This will be useful if you are writing your own watches or if you have to debug watches written by your colleagues. The correct workflowA watch is the unit of work for the alerting features in X-Pack. A watch contains information on where to get data from that could trigger the alert, a condition to trigger upon, which actions to take when that condition holds true, and how often the watch should be executed. Let’s say we want to add a watch to our system and see if it triggers how we want it to. Let's go with the most simple watch we can imagine: PUT _xpack/watcher/watch/my-watch { \"trigger\" : { \"schedule\" : { \"interval\": \"5m\" } }, \"input\" : { \"simple\" : { \"foo\": \"bar\" } }, \"actions\" : { \"log-me-action\" : { \"logging\" : { \"text\" : \"Payload is {{ctx.payload}}\" } } } } This is the most barebones watch. We have to define a , an and a single action. If you do not specify a condition, an condition will be used, so the actions are always executed. Now you have two choices. First, you could wait for the scheduled execution run to check if everything works as expected. This, however, would make your testing cycle viciously slow, so you should not do that. Instead you could use the to execute the watch manually like this: POST _xpack/watcher/watch/my-watch/_execute You will get a longish output that shows the execution path of the watch. In this case, it will simply return the configured input. However, a search or a webhook might return the HTTP response from the destination host. You will see information if the condition was met, and lastly you will see for each action, if it was executed and its return value. This entry is exactly the same as the one that is stored in the watch history, when the watch is triggered automatically. However this workflow still has a weakness: you have to execute two calls. First you store the watch then you execute the watch. In order to overcome this, the allows you to specify a watch like this PUT _xpack/watcher/watch/_execute { \"watch\" : { \"trigger\" : { \"schedule\" : { \"interval\": \"5m\" } }, \"input\" : { \"simple\" : { \"foo\": \"bar\" } }, \"actions\" : { \"log-me-action\" : { \"logging\" : { \"text\" : \"Payload is {{ctx.payload}}\" } } } } } When you check the Execute Watch API response, you will see that the field of the watch is referred to as because we supplied the watch with the request. This saves you an additional round-trip of creating a watch and executing it. But this is just the first step! Let's do a quick excursion into the watch history, and then we'll come back for more and better testing. A journey into the watch historyThe watch history is stored using time-based indices that are created automatically by Watcher on a daily basis. Whenever a watch is executed, a corresponding watch history entry is created for the execution. Let's examine a sample watch history for the above watch, when we ran the Execute Watch API. { \"_id\" : \"_inlined__7310b96e-b693-4092-b54f-c54cf178fde5-2017-03-06T13:13:35.467Z\", \"watch_record\" : { \"watch_id\" : \"_inlined_\", \"state\" : \"executed\", \"trigger_event\" : { \"type\" : \"manual\", \"triggered_time\" : \"2017-03-06T13:13:35.467Z\", \"manual\" : { \"schedule\" : { \"scheduled_time\" : \"2017-03-06T13:13:35.467Z\" } } }, \"input\" : { \"simple\" : { \"foo\" : \"bar\" } }, \"condition\" : { \"always\" : { } }, \"result\" : { \"execution_time\" : \"2017-03-06T13:13:35.467Z\", \"execution_duration\" : 2, \"input\" : { \"type\" : \"simple\", \"status\" : \"success\", \"payload\" : { \"foo\" : \"bar\" } }, \"condition\" : { \"type\" : \"always\", \"status\" : \"success\", \"met\" : true }, \"actions\" : [ { \"id\" : \"log-me-action\", \"type\" : \"logging\", \"status\" : \"success\", \"logging\" : { \"logged_text\" : \"Payload is {foo=bar}\" } } ] }, \"messages\" : [ ] } } The field indicates whether the watch was executed successfully. Usually you will see the state. What else could you see? The stat","locales":"","title":"Watching the watches: Writing, debugging and testing watches"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2017-05-24T00:00:00.000Z","url":"/blog/brewing-in-beats-enhance-kubernetes-module-in-metricbeat","seo_title":"","content":" Welcome to ! With this weekly series, we're keeping you up to date with what's new in Beats, including the latest commits and releases. — Monica Sarbu (@monicasarbu) New community Beat: Protologbeat Created by ,   intends to allow any application to easily log messages locally without writing them on disk, and indexes them into Elasticsearch. The application sends the log entries to a local UDP or TCP socket. Protologbeat is a replacement of the older , by the same author. It can act accept plain-text or JSON logs and also act as a syslog destination replacement. New community Beats: Varnishlogbeat and Varnishstatbeat Thanks to , there are two new community Beats created to monitor . reads log data from a Varnish Shared Memory file and collects stats data from a Varnish Shared Memory file and ships it to Elasticsearch. Both are using . Enhancements to the Kubernetes module in Metricbeat The metricset is added to the kubernetes module in Metricbeat. It extracts metrics from kube-state-metrics services, and adds info specially useful about: Also, thanks to , the metricset is added to the kubernetes module in Metricbeat. The events are polled from the Kubernetes API server and are sent to Elasticsearch. The Kubernetes module will be released in version 6.0. Output the generated Elasticsearch template into a file At startup, each Beat generates the Elasticsearch template from the fields.yml and loads it to Elasticsearch. With this , you can configure the name of the generated Elasticsearch template under the together with its version under . The version refers to Elasticsearch, and in case it’s not specified, it’s set to the Beat version. ./metricbeat -e -E setup.template.output_to_file.path=template.json To generate the template for the Elastisearch 2.x version, you can do: ./metricbeat -e -E setup.template.output_to_file.path=template.json -E setup.template.output_to_file.version=2.4.0 This new way of generating the templates will be released in 6.0. Other changes Repository: elastic/beats Affecting all Beats Changes in master: Metricbeat Changes in master: Winlogbeat Changes in master: Filebeat Changes in master: Infrastructure Changes in master: Documentation Changes in master: Changes in 5.4: ","locales":"","title":"Brewing in Beats: Kubernetes module enhancements"}
{"index":{}}
{"author":"Suyog Rao","category":"The Logstash Lines","publish_date":"2017-05-23T00:00:00.000Z","url":"/blog/logstash-lines-2017-05-23","seo_title":"","content":" Welcome back to The Logstash Lines! In these weekly posts, we'll share the latest happenings in the world of Logstash and its ecosystem. Logstash modules (5.x) We made good progress on this week. The first parts have landed on a feature branch which allows users to create modules as LS plugin gems. We've also added support to launch modules from the CLI and the ability to override configs using variables. A lot of this UX is consistent with the beats modules.Other changes in 5.5 Other changes in 6.0 There's also plenty of infrastructure and cleanup work on-going:Upgrade to JRuby 9k (In-progress, 6.0.0)Logstash currently uses JRuby 1.7.26 which was EOL'd last year. This week we to upgrade to JRuby 9k for LS 6.0.0. This will bring in Ruby 2.0 support as well. This a pretty significant change and we want to make sure all our core and plugins tests pass. ","locales":"","title":"Logstash Lines: Progress on Logstash modules, JRuby 9k and more"}
{"index":{}}
{"author":"Adrien Grand","category":"Engineering","publish_date":"2017-05-23T00:00:00.000Z","url":"/blog/elasticsearch-query-execution-order","seo_title":"","content":" We often get questions about the order in which filters are executed, whether filters get executed before or after queries, etc. Those are indeed important questions: the recipe for quickly executing a query is often related to running the cheap bits before the expensive ones. You might have heard or read in the past that filters are executed before queries. While this statement is a good way to convey the fact that we do not compute the score on documents that do not match filters, the truth is a bit more complicated. Also the phrasing of the question suggests that we iterate over all documents and check whether queries/filters match one by one, while again the truth is more subtle: our index structures can help efficiently skip documents that can't possibly match. How does a query/filter run by the way?I can't give you insights into the order in which queries get executed before explaining how query execution works. Queries and filters expose the following operations: This is a bit complex, but there are good reasons for all these operations to exist! The most important thing to notice at that stage is that matching documents happens in two phases. There is first an approximation which allows to efficiently iterate over a superset of the documents that match the query, those are the / operations. And then, there is a verification phase that aims at verifying whether the current document actually matches with the operation. The goal of this design is to make sure we only start running the costly bits after we reached agreement between all approximations, which should be cheap. Also, something interesting to note is that the only difference between a query and a filter is that we never call on filters. To better understand what those operations do, let me give you simple examples: Term queriesTerm queries are the most efficient queries that Elasticsearch supports: their matches are pre-computed in the inverted index structure. Disjunctions (a OR b OR ...) Since disjunctions are essentially about merging sorted iterators, we use a for efficiency. Conjunctions (a AND b AND ...) This is sometimes referred to as given how we alternatively advance clauses until we find common matches. Phrase queriesPhrase queries are interesting because they are the reason why we got the and operations in the first place. Phrase queries are essentially a conjunction, where we perform some additional operations on a per document basis in order to check whether they match or not. This also gives you some insights into why filters perform better than queries. In this case, not only can filters skip the score computation, but they can also stop iterating over positions as soon as one match is found, they do not need to count them all. Back to execution orderIf you read back the description of how conjunctions run, execution order for and is decided based on , and execution order for is decided based on . So if you search for , we will first look at index statistics to find which one of the terms is the rarest, iterate over documents that contain this term and check whether they contain other terms as well. Now a more complicated example: imagine that you search for and terms have the following index statistics: has a cost of so we will execute its approximation before which has a cost of . However once we reach agreement between both approximations, ie. documents that contain all 4 terms , , and , then we will call on first, since it has positions per document in total while has of them. As you can see, there is no simple answer to \"which query runs first\"! ConclusionHopefully this blog post gave you some insights into how query execution works and how Elasticsearch decides on which bits to execute first. Metadata from the inverted index like term frequencies and doc frequencies are not only useful for scoring, but also for figuring out the optimal execution order. Before I leave you, here are some freq","locales":"","title":"In which order are my Elasticsearch queries/filters executed?"}
{"index":{}}
{"author":"Shane Connelly","category":"Engineering","publish_date":"2017-05-22T00:00:00.000Z","url":"/blog/index-type-parent-child-join-now-future-in-elasticsearch","seo_title":"","content":" A year and a few months ago, , including \"when to pick which.\"  If you don't wish to read the full history, I'll give you the TL: DR: the conclusion was something like: Multiple types in the same index really shouldn't be used all that often and one of the few use cases for types is parent child relationships. Sadly, not everyone has stumbled on all of our (shameless plug!), which means a lot of our users have gone on using types for what they were never really intended for.  This raised a very good engineering question: should we continue this confusing \"type\" construct?  Should we just (continue) recommending against it?  Should we give the real use case of parent-child a proper first-class citizen in the Elasticsearch hierarchy?  The good news is we have a much less confusing future that we're moving forward on.  The bad news is this will be a completely breaking change which means you will need to make application-side modifications as soon as you can,  First, let's talk about why the change has come about. WhyElastic (the company behind Elasticsearch) has seen  WhatAs the one main use case derived from types vs indices is parent-child relationships, we have decided to supersede the \"type\" with a special field that stores the relationship between documents.  We feel this represents a much better feel of the data.  However, doing so is complicated, which gets us to the \"when\" element... WhenWe want the transition away from \"types\" to be as smooth as can be, so we're targeting long, multi-phase deprecation process to get us there.  At the time of this writing, the current engineering targets for these are something like the following: What does this mean for me?The answer to this question depends on what use case you're using the Elastic Stack for: Of course, we recommend people get ahead of whatever they can by adopting things like this early.  If you're looking to do so, you can use Kibana's Console / dev-tools to reindex any particular data moving data with a \"_type\" field into a \"type\" field. POST _reindex { \"source\": { \"index\": \"old\" }, \"dest\": { \"index\": \"new\" }, \"script\": { \"inline\": \"\"\" ctx._id = ctx._type + \"-\" + ctx._id:  ctx._source.type = ctx._type:  ctx._type = \"doc\":  \"\"\" } } This moves the special \"_type\" field over to \"type\" which you can then use in subsequent filtering, aggregations, etc. I have more questions!  Tell me more!Step 1: don't panic :)Step 2: please let us know on our We're actively looking for any particular problems this may cause with use case, so if you think you may have some, please talk to us! ","locales":"","title":"Indices, types, and parent / child: current status and upcoming changes in Elasticsearch"}
{"index":{}}
{"author":"Clinton Gormley","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2017-05-22T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2017-05-22","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. — Bruno Costa (@BrunoElo) In an arduous and heroic campaign, three brave knights (Sir Luca, Sir Tanguy, and Sir Christoph) have scythed their way through ferocious hordes of aggregations and have finally destroyed the last one standing. Victory! This means that the new REST client (which will replace the transport client) is close to landing. More remains to be done: the branch still has to be merged to master and then backported to 5.x, plus a lot of tests remain to be written, but the battle is close to being won! The REST client covers only the essential APIs for now (ping, info, index, bulk, get, exists, delete, update, search) but this list will be expanded over time. The latency of a search request depends on the slowest/busiest node involved in the search. The search thread pool in master to a new type () which behaves like the old type by default. It can, however, be configured to have a target response time (and min and max queue sizes) in which case queue sizes will be automatically adjusted based on response times. Slower nodes will have shorter queues, and so will reject search requests earlier than other nodes. These rejected requests will be forwarded to other nodes. This is the first step in the ability to work around degraded nodes. The next step is to figure out a way to set the target response time automatically. Changes in 5.5: Changes in master: Coming soon: Apache Lucene The release process for Lucene 6.6.0 has . This might be the last 6.x minor release before 7.0. There is a patch in progress that adds support for using points similarly to how range fields work. These bounding boxes can then be queried at search time and support the same relations as range fields: INTERSECTS, WITHIN, CONTAINS or CROSSES. Up to Lucene 6.x, norms had to store a scoring factor that combined the index-time boost with length normalization. However Lucene 7.0 will not support index-time boosts anymore, which gave us an opportunity to Actually we are no longer storing the normalization factor but the length directly, which we encode on a single byte while retaining 4 significant bits (and even more for small values). And at search time we keep a translation table between the encoded length, which only has 256 possible values, and the length normalization factor. This will give users who search short fields a much better experience since all lengths up to 40 now get encoded to a different byte, while the previous encoding would already quantize the length normalization factors for lengths of 3 and 4 to the same byte, which many users complained about over the years. Over the last months, there have been lots of efforts to create a highlighter to rule them all, called the unified highlighter. In particular, there are no features that the postings highlighter has and the unified highlighter doesn't, so we are considering . , where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2017-05-22"}
{"index":{}}
{"author":"","category":"User Stories","publish_date":"2017-05-22T00:00:00.000Z","url":"/blog/leveraging-elasticsearch-at-1worldsync-to-increase-product-sales","seo_title":"","content":" This article describes how came to introduce Elasticsearch and the Elastic Stack in our infrastructure. The goal was to provide a much faster search experience for our customers as well as a means to proactively take action if there are trends for production issues.Our Business1WorldSync is the leading provider of product content solutions, enabling more than 25,000 global companies in over 65 countries to share authentic, trusted content with customers and consumers. 1WorldSync provides solutions that meet the diverse needs of the industry through its technology platform and expert services empowering intelligent choices for purchases, wellness, and lifestyle decisions.We provide several products for our customers as SaaS. On the supplier side, we provide a Publisher product:  while on the retailer side, we have an Approval service. The Publisher product enables customers like Coca Cola, Unilever, etc. to administer their product information and distribute them worldwide to retailers like Edeka, Metro, Rewe, Walmart and others. The Approval product is used by retailers to get up-to-date metadata information about the products sold in their stores. They approve the metadata about the articles or reject the metadata due to missing information necessary for the retailers.Elasticsearch at the Core of our Search Engine As a customer would like to search for article information like title, description, logistics information, ingredients, there is naturally a need for a search functionality that helps the customer get the respective results for those queries. The product information, referred to as (trade) items, persists in a relational Oracle database. An item contains all the information belonging to that product. The nature of items demands a flexible schema, since not each and every product (chocolate, juice, sweets, ...) has the same kind of attributes. There are also composite attributes, like dimensions, which consist of e.g. value + unit: e.g. “length = 13 cm”). In fact, the GDSN defines more than 4,000 attributes for items. For more information about the GDSN take a look at our mother company .Therefore, we evaluated document oriented databases to support a deep attribute search (like the length, height, and width) as well as to speed up the search for products. In the German product information community, called FMCG, we have about 20 million products, each with at least hundreds of attributes — which are searchable. To make things a bit more interesting, these items can be organized in a hierarchical order, like a bottle is enclosed in a box which is packed on pallets. On top of that, product information is versioned. So, there is not only one version of a product but several.The search form looks like this: On the top (marked in orange) you can do a free text search for fields like title, description, dimensions. On the left side, you have a detailed search where you can specify types and values to search for. The result is a list of trade items which match the search criterias.The reason why we decided to use Elasticsearch as our core search engine is because we can store one item as one document (in a denormalized way) and its search is lightning fast. To store data with a flexible schema for the trade items is not an easy task in a relational database. We ran into performance issues for more complex searches while our amount of trade items rose. So, in summer 2015, we began to import the product information data to Elasticsearch to make it searchable. The performance increase due to the switch to Elasticsearch was immense (depending on the specified search up to 10-100 times faster!). So our pilot customer GS1 Australia was happy with the outcome of the new search. On top of the performance increase, we had the new feature to search in composite attributes, which are attribute values nested and composed together. (E.g. physical dimensions co","locales":"","title":"Leveraging Elasticsearch at 1WorldSync to increase product sales"}
{"index":{}}
{"author":"Jim Goodwin","category":"Kurrently in Kibana","publish_date":"2017-05-22T00:00:00.000Z","url":"/blog/keeping-up-with-kibana-2017-5-22","seo_title":"","content":" Welcome to This is a weekly series of posts on new developments in the Kibana project and any related learning resources and events. How does extend what you can do with & ? Let us explain: — elastic (@elastic) ","locales":"","title":"Keeping up with Kibana: This week in Kibana for May 22, 2017"}
{"index":{}}
{"author":"Henrik Nordvik","category":"Engineering","publish_date":"2017-05-18T00:00:00.000Z","url":"/blog/tracking-down-native-memory-leaks-in-elasticsearch","seo_title":"","content":" We've spent a lot of time lately tracking down in the Elastic Cloud service. One in particular hit small clusters with relatively little activity and needed quite a bit of digging to figure out. The following applies to version 5.1.1 of the Elastic Stack, and has been fixed in newer versions.  ","locales":"","title":"Tracking Down Native Memory Leaks in Elasticsearch"}
{"index":{}}
{"author":"Luisa Antonio","category":"News","publish_date":"2017-05-18T00:00:00.000Z","url":"/blog/announcing-the-grand-opening-of-the-online-elastic-store","seo_title":"Announcing the Grand Opening of the Online Elastic Store","content":" Until today, the only way to receive Elastic stickers, t-shirts, and other swag was to attend a meetup, see us at a conference, or attend Elastic{ON}. Now, you can get swag anywhere, anytime (unless you live in the future, we haven’t quite figured that one out quite yet). We are happy to announce that is now open for business. To celebrate, we’re offering 20% off Elastic socks for for a limited time. Not only are they custom-designed socks from , but the designs themselves are inspired by one of our open source projects, . Allow us to introduce our Line and Dot Graph socks, integrating with feet worldwide. We are humbled by the folks within the community who are genuinely excited to collect Elastic stickers and proudly display them on their laptops, skateboards, water bottles, and everywhere else, and we want to make it easier than ever for you to get those stickers. That is why we are offering our core Elastic stickers for free, with a small charge for shipping. We hope you enjoy browsing through . Stay tuned for new products and t-shirts in the future. We’ll be announcing promotions and specials through our social channels, so follow us on , , or . Visit today! ","locales":"","title":"Announcing the Grand Opening of the Online Elastic Store"}
{"index":{}}
{"author":"Samantha Dodge","category":"User Stories","publish_date":"2017-05-18T00:00:00.000Z","url":"/blog/using-elasticsearch-to-manage-a-supercomputers-hot-warm-and-cold-architecture","seo_title":"","content":" What were the drivers in making 2016 the hottest year on record? How might massive particle accelerators be replaced by desktop devices? What gravitational forces were at work in forming the early universe? As scientists around the world strive to answer these and other critical questions, many turn to a unique supercomputer for the computations, data analyses, and simulations crucial to their work: the (NERSC).NERSC is the primary scientific computing facility for the Office of Science in the U.S. Department of Energy. More than 6,000 users rely on NERSC to conduct data-intensive research that will help solve fundamental problems in science and engineering. To meet their needs, NERSC runs two massive petaflop systems: Edison (a Cray XC30 system) and Cori (a Cray XC40 system). According to a November 2016 review of the top 500 supercomputers in the world, Cori — which features a unique mix of more than 10,000 Haswell and Knight’s Landing nodes — is the fifth most powerful system in the world.To achieve their scientific goals, NERSC users — a group that includes more than one Nobel Prize recipient — must be able to access and work with their log and metric data when needed. That could be today, tomorrow, or a decade from now. This presents NERSC with a significant challenge: how can they manage all the data in an efficient way? At .“We’ve been told, ‘You can’t use Elasticsearch as a time series [metrics] database.’ But we do,” said Davis. “To do it, we built what we call a hot, warm, and cold architecture.\" Based on this Elasticsearch architecture, their unique data collection system organizes information according to whether the storage need is short term (a few days, possibly weeks), longer term (generally weeks or months), or very long term.“When we say long term, we mean forever,” said Davis. “When it gets onto the [High Performance Storage System (HPSS)], we never, ever delete anything out of that system.” Some of the information on the system is more than 30 years old. But that doesn’t mean it won’t be used: NERSC needs to store it, ensuring it can be retrieved when required.The hot storage nodes consist of a SSD-based system configured for speed, not space. The warm nodes — for storage of up to a year — add disk space but still allow relatively rapid retrieval. “The warm storage nodes we build using a [RAID] 5 array drive mixed with what’s called a [logical volume measurement (LVM)] cache. We combine the two together to give me an SSD that fronts the RAID 5 array,” said Davis.NERSC’s cold storage takes place on a GlusterFS. “We snapshot onto the Gluster, we take the data off the Gluster, put it in HPSS. If we need to restore, we pull from HPSS back onto the Gluster, then take that and restore it back in using Elastic[search].” Altogether, NERSC currently dedicates 90TBdisk space to Elasticsearch to manage their massive time series database. “The whole idea of the data collect…was to make sure that we had one location that had everything. One location, one access method, so that you weren’t doing a SQL query from here, a flat file look-up from here, and something else from there, and trying to get it all together. One place,” said Whitney.So exactly how do Davis, Whitney, and the rest of the team at NERSC use Elasticsearch and Curator 4 to manage their data? , including how they tag nodes, what goes into retrieving and restoring data, and how they handle their daily cold storage routine. ","locales":"","title":"Using Elasticsearch to Manage a Supercomputer’s Hot, Warm, and Cold Architecture"}
{"index":{}}
{"author":"Mike Paquette","category":"Engineering","publish_date":"2017-05-17T00:00:00.000Z","url":"/blog/using-machine-learning-and-elasticsearch-for-security-analytics-deep-dive","seo_title":"","content":" Introduction In our of our multi-part series on integrating Elasticsearch with ArcSight SIEM, where we used X-Pack alerting features to detect a successful brute force login attack, we hinted that we were excited about the pending arrival of our machine learning features in X-Pack. Well the time has come, and are here. Now we want to walk through what it means to use machine learning to detect anomalies, that are associated with cyber threat behaviors, in log data living in Elasticsearch. Math, not Magic Before we jump into this, it's probably a good idea to add some context. A common misperception in cybersecurity is that machine learning is a magic box of algorithms that you let loose on your data and they start producing nuggets of brilliant cyber insight for you. A more enlightened understanding of machine learning in cybersecurity sees it as an arsenal of \"algorithmic assistants\" to help the security team automate the analysis of security-relevant log data by looking for potentially incriminating anomalies and patterns -- but under the direction of human security experts. Threat Monitoring or Threat Hunting - A Role for Machine Learning X-Pack machine learning features can be used for interactive investigation of threat-related anomalies. An \"anomaly swimlane\" visualization in Kibana is often used as the starting point for threat hunting expeditions, but details about detected anomalies will transparently show the security analyst \"why\" the detected behavior was anomalous, how unusual it was, why it relates to the elementary attack behavior it attempts to detect, and which entities in the data were influential on the attack behavior. Because the X-Pack machine learning features are tightly integrated into the Elastic Stack, the alerting techniques we described in our Integrating Elasticsearch with ArcSight SIEM and posts can now be applied to a new source of insight, the machine learning results index, whose index pattern is called ml-anomalies-*. In this way, results produced by these algorithmic assistants can be used to trigger alerts for ongoing threat monitoring. Fig. 1 X-Pack machine learning features integrated with Elastic Stack Machine Learning \"Recipes\" for Threat Detection While threshold-based event notification is powerful, such as triggering a notification when a successful login is preceded by multiple unsuccessful logins, the ability to automate the detection of anomalous behavior without having to define specific data conditions simplifies the experience for the security analyst. That said, as we mentioned above, we're talking about mathematics, not magic, so the machine learning engine must be given its marching orders as settings in its job configuration. Since the engine can model any type of time-series data - numerical or categorical - the types of machine learning jobs that can be configured are unlimited. While this is flexible, it can be a bit too much for a security analyst who really just wants to find threats. Here we introduce the concept of machine learning \"recipes\" for security use cases. Recipes describe how to configure machine learning jobs, so that we can use automated anomaly detection to uncover elementary attack behaviors that can be difficult to detect using other means. Elementary attack behaviors include activities such as DNS tunneling, web data exfiltration, suspicious endpoint process execution, and more. Each recipe is contained in a short document that includes sections for theory of operation, description, and specific recipe steps for modeling and observing results. Recipe steps include feature selection, modeling approach, detection target, comparison set, candidate influencers, analysis time period, and results interpretation. We've introduced four machine learning security use case examples with the launch of V5.4. Available now in , each ","locales":"de-de,fr-fr,ja-jp,ko-kr,zh-chs","title":"Using Machine Learning and Elasticsearch for Security Analytics: A Deep Dive"}
{"index":{}}
{"author":"Phil Verdemato","category":"User Stories","publish_date":"2017-05-17T00:00:00.000Z","url":"/blog/discovering-new-uses-for-existing-drugs-with-scibite-and-elasticsearch","seo_title":"","content":" SciBite offers . We take text, in any format, and extract the scientific terminology using formal named entity recognition. It’s an incredibly fast solution that integrates seamlessly with existing systems. SaaS is available through its Java based RESTful API, with the following capabilities: We provide pluggable technology, so that scientists can integrate semantic enrichment exactly where they need it. Fast, lightweight and simple to use, we transform data by providing technologies that understand the scientific content they process. Our clients span a range of industries, but are predominantly from the life sciences sector, and it’s the pharmaceutical industry that we’ll focus on in this post. The problemAll drugs originally have an exclusive patent on them, meaning only the patent owner can manufacture and sell them. But all patents eventually come to an end, meaning other companies can make a generic — and redirect up to 90% of sales (according to . So what do you do if you’re a pharmaceutical company? Well, ideally, you create a new drug.  But creating a new drug is a costly process — an extremely costly process. First, there are the huge resources required to do preliminary research: which disease/syndrome? What drugs are already available? What do they target? Where is there an opportunity? Then comes the actual research, the different trial phases, and all the paperwork to achieve regulatory compliance.  We’re talking billions of dollars to get a brand new drug to market (more than $2.5 billion according to). So, how do you continue to do business without such a huge investment? What if you could find a different use for a drug that was already on the market? A lot of the groundwork will already have been done: you know it’s already been approved as safe for humans, so all you have to do is prove that it’s effective for a different issue or indication. Then you have a new patent and a new exclusivity. This process is much more efficient. There is a snag to all of this, though. Finding out all those links to other indications still requires significant research investment.  Imagine trawling through millions of scientific papers, looking for secondary effects of drugs.  Pretty painstaking, and because there are so many synonyms for the same indication, drug or gene/protein, a simple text search (much like when you use a well known search engine) will miss many relevant results, as it doesn’t understand the scientific meaning of all those different synonyms. This is where SciBite, using Elasticsearch, steps in. Looking at repurposing DipyridamoleDipyridamole is currently used for treating Angina, a Coronary Artery Disease. At Ariel University in Israel, they are looking at running Phase II trials on using the same drug for Dry Eye Disease, which can lead to loss of sight. Let’s see how they could have jumped from Angina to Dry Eye Disease if they’d used SciBite and Elasticsearch technology. Starting from the standpoint of having absolutely no knowledge about this drug, we first need to find out more. Let’s run it through DOCstore, our semantically enabled search engine. Q1) What are the major diseases (or indications) it treats? A simple search for “Dipyridamole” comes up with the most frequently mentioned disease terms. As we see on the right of the image, the most common of these are Coronary Artery Disease, Ischemia, Myocardial Infarction, Thrombosis, and Stroke. Quite different from Dry Eye Disease, aren’t they? Which brings us to: Q2) How on earth is Coronary Artery Disease linked to Dry Eye Disease? Drugs work by binding to molecular targets, or a protein. These proteins belong to a type or protein family, if you like. By doing this, a drug can alter how a cell behaves. Now, this relationship between a drug and a protein isn’t necessarily exclusive to a single disease. It can be rel","locales":"","title":"Discovering New Uses For Existing Drugs with SciBite and Elasticsearch"}
{"index":{}}
{"author":"Suyog Rao","category":"The Logstash Lines","publish_date":"2017-05-16T00:00:00.000Z","url":"/blog/logstash-lines-2017-05-16","seo_title":"","content":" Welcome back to The Logstash Lines! In these weekly posts, we'll share the latest happenings in the world of Logstash and its ecosystem. Feature: Dead Letter Queue (master and 5.5)The long awaited (DLQ) feature has been merged to master and 5.5. This adds the ability to shunt poisoned or unsuccessful events in the running pipeline to a local file-based store. Users can then read these dead events using a for further processing. This infrastructure is currently being used in ES output. Previously, any mapping issue would be logged and dropped to the ground. Right now, they get redirected to the DLQ. Users can then read these in another LS instance, choose to drop the field that cause the mapping error and re-index to ES. The DLQ input allows users to plug into the entire LS pipeline framework providing access to hundreds of filters and output destinations.While this first version provides an immediate solution to the existing data loss in ES output, we are evaluating other places in the pipeline that can use a DLQ. Once we ship the multi-pipeline support, the DLQ pipeline can coexist within the same production LS instance, which will make all of this very powerful.Feature: Grok Debugger (master and 5.5)An initial version of the Grok Debugger tool in Kibana has been merged. Users can debug or simulate field extraction using their log lines and grok patterns ala .Infra: Dockerize and Jenkinize integration testsCurrently, Logstash's integration test suite (called internally as , inspired from ) provides a ruby-based framework to standup services such as ES, Kafka, filebeat and run rspec tests against it. We build artifacts for every pull request, untar it, and run tests against the various services. We will now be for all the services which should provide resource isolation and allow us to test on multiple versions easily. Also, the tests will be moved to internal Jenkins from Travis. This change will allow us to run tests faster and be able to ssh into the build machine if there are issues.Other changes in master Other changes in 5.5 ","locales":"","title":"Logstash Lines: Grok debugger, Dead letter queues"}
{"index":{}}
{"author":"Tyler Langlois","category":"Engineering","publish_date":"2017-05-16T00:00:00.000Z","url":"/blog/puppet-state-of-the-union-may-2017","seo_title":"Puppet State of the Union: May 2017","content":" The Elastic Stack is a far-reaching ecosystem with many interconnected parts, and the are no exception when it comes to the configuration management side of the house. Today, we want to highlight some developments taking place in the Elastic Puppet ecosystem to help spread awareness of new features and upcoming changes.Modules RoadmapFirst, some housekeeping items that pertain to all of our official modules.Puppet 3The most prominent upcoming change is the deprecation of Puppet 3 (if you’re skimming this post, don’t panic! You still have time). , version 3 met its end in December 2016, which means that it is not only deprecated, but end-of-life, and past the cutoff date for any future updates. As systems operators ourselves, we recognize that keeping stride with software versions is one of the challenges that comes with the discipline, particularly when it comes to large enterprises.With that in mind, we’ll be keeping version 3.8 of Puppet in our list of supported versions for a period to ensure everyone has a window of time to work in, for those cases where upgrading is a challenge. Nevertheless, the Puppet ecosystem is moving on from version 3, and we can only hold on to old versions for so long. It’s for this reason that we will be removing support for Puppet 3 with the 6.x release of the Elastic Stack, which grants many months of leeway for those operations engineers with a Puppet upgrade still on their plate. Version 6 of the Elastic Stack is likely to arrive in late 2017, and will be prefaced with additional announcements as its release draws closer.New Forge NamespaceWhat open source project doesn’t like changing names every so often? now properly live under the “elastic” namespace where all future module updates and release will land. While any references to old modules under the namespace will continue to work, the modules there are no longer updated. We suggest updating any automation or tooling that references released modules from the Forge to this new namespace (for example, to ).Official Puppet Kibana ModuleOne of the most common requests we hear from Puppet users is for other parts of our stack to ship official modules. At Elastic{ON} 2017 we released a beta version of , which is now formally released at version 5.0 (major version synchronized with the rest of the Elastic Stack). The module supports everything you would expect from our Puppet modules, including Kibana plugin resources to make managing Kibana through Puppet a much easier experience.As always, we’re eager to hear from you regarding any bugs you've found or features you’d like to see and would encourage any bug reports or feature requests to be filed in the .Elasticsearch: Tasty New FeaturesThe official Elasticsearch module hasn’t stood still either. In addition to the steady march of bugfixes, supports a couple of useful new resource types: indices and pipelines.While the module already supported managing index templates (useful for auto-created indices from tools like Logstash or Beats), the type can help manage mappings and settings for indices in an Elasticsearch cluster. For example, to manage an Elasticsearch index called “logs” to retain only 1 replica per shard:elasticsearch::index { 'logs': settings => { 'index' => { 'number_of_replicas' => 1 } } }Likewise, are now supported as well. The ability to codify processing pipelines in Puppet can help centralize your workflow for processing logs and documents in Elasticsearch (see our ). For example, to create a pipeline that will automatically parse dates, the following Puppet resource could be used:elasticsearch::pipeline { ‘parsedate’: content => { 'description' => 'Parse the date field into @timestamp', 'processors' => [{ 'date' => { 'field' => 'date', 'formats' => ['dd/MM/yyyy hh:mm:ss'], 'timezone' => 'US/Pacific' } }] } }Logstash: All Aboard for Version 5Version 5 of Logstash brought huge improvements ","locales":"","title":"Puppet State of the Union: May 2017"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2017-05-15T00:00:00.000Z","url":"/blog/brewing-in-beats-auditd-alternative-in-metricbeat","seo_title":"","content":" Welcome to ! With this weekly series, we're keeping you up to date with what's new in Beats, including the latest commits and releases. We have decided to promote the auditd module in Metricbeat to be its own Beat: Auditbeat. Please read the for more details. Auditd alternative in Metricbeat, simpler to use Auditd is the userspace component to the Linux Auditing System. It can audit every syscall in the Linux Kernel, and write audit records to log files. For example, it writes a log line every time a user accesses a file, starts a process, or connects to a socket. It’s part of the Linux kernel starting with 2.6.14.  The power of auditd is that by inspecting the audit log files, you can track security-relevant events, detect misuse or unauthorized activities. The auditd log format is a bit challenging to parse, but the auditd Filebeat module (available starting with Filebeat 5.4) parses the individual log lines for you. Another issue with Auditd is that can generate a lot of data, so you need to be specific on what you look for in order to make use of it. This is partly because the Linux Audit Framework sends multiple messages for a single auditable event. For example, a syscall causes the kernel to sent eight separate messages. Each message describes a different aspect of the activity that is occurring (the syscall itself, file paths, current working directory, process title). To be more valuable, these events need to be correlated together. So, we decided to create our own version of that is easier to use and better integrated with the Elastic stack. This creates the audit module in Metricbeat. It establishes a subscription to the kernel to receive the events as they occur. Messages for one event can be interleaved with messages from another event. The audit module buffers the messages in order to combine related messages into a single event even if they arrive interleaved or out of order. The Linux kernel only supports a single subscriber to the audit events so the audit Metricbeat module cannot be used simultaneously with a service like . should be disabled if the audit module is being used.   The audit module is based on library, developed by us. It comes with two sample applications: audit and auparse. Audit registers to receive audit events from the kernel and outputs the data it receives to stdout. Auparse parses the log files from the Linux auditd process or the output of the audit example command and it combines related log messages that are a part of the same event. The audit module is not complete yet. Currently, you must manually install audit rules using . This will be part of the next 6.0 release. Filebeat: Add option to force end a multiline event This , submitted by , adds a new option to the multiline configuration, called , which receives a regular expression to match against the input lines. When there is a match, the current multiline event will be ended. This is useful for using multiline to capture events with “start” and “end” lines. For example: multiline: pattern: 'start' negate: true match: after flush_pattern: 'end' Elasticsearch mapping settings via the Beats configuration files As we’re now generating the mapping template at runtime in Beats, we can also allow for simple tuning via the Beats configuration file. With the , it’s possible to write: setup.template.settings: index.number_of_shards: 1 index.number_of_replicas: 1 This will be part of the 6.0 release. Other changes Repository: elastic/beats Affecting all Beats Changes in master: Filebeat Changes in 5.x: Changes in 5.4: Changes in master: Metricbeat Changes in 5.x: Changes in master: Packetbeat Changes in 5.x: Winlogbeat Changes in 5.x: Changes in 5.4: Infrastructure Changes in 5.x: Changes in master: Processors Changes in master: Documentation Changes in master: Changes in 5.4: Dashboards Changes in master: Repository: elastic/gosigar Changes in master: ","locales":"","title":"Brewing in Beats: Auditd alternative in Metricbeat"}
{"index":{}}
{"author":"Jim Goodwin","category":"Kurrently in Kibana","publish_date":"2017-05-10T00:00:00.000Z","url":"/blog/kurrently-kibana-2017-5-8","seo_title":"","content":" Welcome to This is a weekly series of posts on new developments in the Kibana project and any related learning resources and events. 5.4 released, including advanced charts: — elastic (@elastic) ","locales":"","title":"Kurrently in Kibana: This week in Kibana for May 8, 2017"}
{"index":{}}
{"author":"Suyog Rao","category":"","publish_date":"2017-05-09T00:00:00.000Z","url":"/blog/logstash-6-0-0-alpha1-released","seo_title":"","content":" We are pleased to announce our first preview release for Logstash 6.0.0! Yes, you read that right! Six-dot-oh! There’s so much work going into this release, I can’t wait to show off all the shiny new features. But for this alpha, it’s all about the right foundation in place to ship those new features. Read on! ","locales":"","title":"Logstash 6.0.0-alpha1 released"}
{"index":{}}
{"author":"Tyler Hannan","category":"Releases","publish_date":"2017-05-09T00:00:00.000Z","url":"/blog/elastic-stack-6-0-0-alpha1-released","seo_title":"","content":" Never one to rest after a major release, we are pleased to make the first public alpha of 6.0 available today. At each of the individual projects talked about their current roadmap and the plans for 6.0 and it is incredibly compelling to begin seeing that work come to fruition. Before you get too excited, keep in mind that this is still an alpha so don’t put it into production. There is no guarantee that any of the 6.0.0-alpha1 versions will be compatible with other pre-releases, or the 6.0.0 GA. We strongly recommend that you keep this far, far away from production. And, since it is an alpha, it is not available on Elastic Cloud. During the 5.0 release we introduced the Elastic Pioneer Program and will continue that same program with this release. What is the Elastic Pioneer Program? We’re very excited about these upcoming Elastic Stack Version 6.0 releases, but we want to make sure they’re perfect (or as perfect as software gets) before they ship more broadly. Everyone who reports a legitimate bug on the pre-release of the software will be recognized for the wider release, and receive a special Elastic gift package as our thank you. How to participate To join the program, just try out the pre-release of any (preferably, every) part of the Elastic Stack, and open issues as you find them in the appropriate repo (, , , ) or forum (). When you open an issue, mention that you found the bug in 6.0.0-alpha1/alphaN/betaY, and we’ll add a “Pioneer Program” label. While we appreciate the information, duplicate issues won’t enter you into the program. As a heads-up, and proof this is an early preview release, there are a few known issues you should be aware of. These are enumerated in each section below. Elasticsearch For more detailed information, and many other features, peruse the Elasticsearch . Kibana ‘Visualize’ all the features, and ‘Discover’ more information in the . Known Issues: Machine Learning was in 5.4. If you interested in finding anomalies in your time series data, please continue to test with that release. If you have watches that were created in an earlier version, this will cause all pages in Watcher UI to throw an error and be unusable. Logstash For more information, grok the . Beats We don’t ‘let the beat drop’ but we drop the updates in a . Known Issues On macOS the import_dashboards program might fail when unpacking the dashboards zip file. You can workaround it by running just before running the import_dashboards program. ES-Hadoop ES-Hadoop 6.0.0-alpha1 has also been released today. Get It Now! ","locales":"ja-jp","title":"Elastic Stack 6.0.0-alpha1 Released"}
{"index":{}}
{"author":"Monica Sarbu","category":"Releases","publish_date":"2017-05-09T00:00:00.000Z","url":"/blog/beats-6-0-0-alpha1-released","seo_title":"","content":" We’re starting our road towards the 6.0 release of the Elastic stack, with the first alpha version being released today. In the Beats team, we’ve been baking a lot of new features that we’re really excited to put out there for the first time. This blog post will cover some of the highlights. Quick links: Kubernetes monitoring With this Beats release we’re turning the Elastic stack into a perfect monitoring companion for Kubernetes. Both Filebeat and Metricbeat now have integrations with Kubernetes to collect pod logs and metrics, as well as enhance them with Kubernetes specific metadata. Metricbeat gets a , which works by interrogating periodically the kubelets. It gives you details about the running containers pods, like the CPU usage, memory usage, bytes exchanged over the network, about filesystem, or about the logs capacity. The provided sample Kibana dashboard shows you at a glance the monitoring status of your Kubernetes cluster. Filebeat, and other Beats as well, is now able to automatically attach Kubernetes specific metadata to the log lines by using a . By default, the Kubernetes processor adds the following metadata: Depending on the Beat, the Kubernetes processor uses slightly different logic to obtain this metadata. For example in Filebeat, it takes the path of the log file, extracts the container ID from it and uses it to retrieve metadata about the pod from which the log message originated. In Metricbeat, it uses the IP and port of the endpoints to resolve which containers are part of which pods. The Kubernetes support in Beats is a wonderful example of open source collaboration. , from Ebay, worked closely with the Elastic engineers to contribute upstream a lot of the code they had for monitoring their Kubernetes cluster with Beats. Vijay’s experience was essential to shape these features, so we’d like to thank him and Ebay on behalf of the Elastic community. More Metricbeat modulesBesides the Kubernetes module which we already mentioned, Metricbeat 6.0.0-alpha1 comes with a bunch of new modules: memcached, perfmon, dropwizard, HTTP, vSphere, Elasticsearch, and Kibana. An interesting one is the . We liked the idea of the , a community Beat created by . It offers a generic solution for collecting data periodically from multiple HTTP endpoints and indexing the responses in Elasticsearch.  Christian contributed it as a module in Metricbeat to be used for monitoring application that export their metrics over HTTP.  For example, if the HTTP endpoint returns the result in a JSON format, you can configure the json metricset from this module to index the contents of each JSON object into Elasticsearch. is another interesting community Beat, created by , that we adopted in Metricbeat as part of the Windows module. It comes with the to collect performance counters from Windows. It uses the PDH functions to collect performance data. Metricbeat - Report the top N processesOne of the most appreciated Metricbeat features is that it reports CPU and memory statistics about the individual processes, which gives you a lot of insight into what is taking up the resources. However, often hundreds of processes are running on each monitored system, and that tends to consume significant amounts of disk space on the Elasticsearch side. From the first days of Metricbeat, it was possible to filter the process statistics by the process names or other fields, which is useful when you know which of the processes you want to monitor in advance. With the 6.0.0-alpha1 release, we’re adding the option to only report the top N processes by CPU and/or memory. We found that this option strikes a good balance between the need to limit the amount of consumed disk space and the need to have granular enough metrics, so we’ve made it the default in the Metricbeat configuration. Dashboards using the Time Series Visual BuilderThe system module of Metricbeat gets two new Kibana dashboards, built using the new Time Series Visual Buil","locales":"","title":"Beats 6.0.0-alpha1 released"}
{"index":{}}
{"author":"Jim Goodwin","category":"Kurrently in Kibana","publish_date":"2017-05-15T00:00:00.000Z","url":"/blog/keeping-up-with-kibana-2017-5-15","seo_title":"","content":" Welcome to This is a weekly series of posts on new developments in the Kibana project and any related learning resources and events. Parse and display CSV data with Ingest Node, , and . — elastic (@elastic) ","locales":"","title":"Keeping up with Kibana: This week in Kibana for May 15, 2017"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2017-05-09T00:00:00.000Z","url":"/blog/elasticsearch-6-0-0-alpha1-released","seo_title":"Elasticsearch 6.0.0-alpha1 released","content":" We are excited to announce the release of , based on . This is the first in a series of pre-6.0.0 releases designed to let you test out your application with the features and changes coming in 6.0.0, and to give us feedback about any problems that you encounter. Open a bug report today and become an . This is an alpha release and is intended for testing purposes only. Indices created in this version . Upgrading 6.0.0-alpha1 to any other version is not supported.Elasticsearch 6.0.0-alpha1 is a departure from our previous major version upgrades. Elasticsearch 1, 2, and 5 were huge releases containing many new features and changes, and it required significant effort to upgrade between major versions. In Elasticsearch 6, we are trying to make this process much, much easier.Throughout the 5.x series, we have added new features and breaking changes to the (6.0) branch, then we have added backwards compatibility and backported these changes to 5.x minor versions. This means that we have been able to get more improvements into the hands of our users earlier, and that the differences between 6.0 and 5.x are smaller than in previous major versions.So, if the differences are small, why upgrade at all? Elasticsearch 6.0.0 will use Lucene 7, which brings benefits like and . Elasticsearch and X-Pack have a number of exciting new features lined up which will evolve during the 6.x series such as , secure settings, distributed Watch execution, and support for OAuth, SAML, and Kerberos. Plus there are the incremental improvements made in major releases to clean out cruft, improve APIs, and to improve performance and resilience. ","locales":"","title":"Elasticsearch 6.0.0-alpha1 released"}
{"index":{}}
{"author":"Jim Goodwin","category":"Releases","publish_date":"2017-05-09T00:00:00.000Z","url":"/blog/kibana-6-0-0-alpha1-released","seo_title":"","content":" This is the first alpha release of Kibana 6.0.0. This release doesn't have a lot of new features, it is mostly work in progress on compatibility with Elasticsearch 6.0.0. In future alpha and beta releases we'll be previewing some great new things with Dashboards, Tilemaps, Timelion expression editing, CSV Export, and more.Remember, these alpha releases cannot be used in production they are for evaluation purposes only, they contain known bugs and are not feature complete.Kibana 6.0.0-alpha1 is available on our  and please take a look at the complete . ","locales":"","title":"Kibana 6.0.0-alpha1 is released"}
{"index":{}}
{"author":"Davide Bortolami","category":"User Stories","publish_date":"2017-05-09T00:00:00.000Z","url":"/blog/computation-near-the-visualization-layer","seo_title":"","content":" At we use the to monitor the behaviour of industrial machinery. While we can run various analytical softwares on top of it, there is really no tool like to dig into the data quickly. Our mantra to keep the system light and usable is The Elastic Stack fits nicely into this philosophy, keeping the cloud provider’s bill down and allowing every person on our customer’s team (and ours) to play on the same dataset stored in Elasticsearch. We’ve been looking constantly to move more and more of the data analytics as close to the user and as far away from the database as possible. The introduction of in allowed us to perform more flexible computations on top of the queries, such as derivatives, cumulative sums and applying filters. The problem Engineers love math. They really do! We found that Timelion’s native syntax was a bit distant from the algebraic equations automation engineers were used to. Here’s how you currently do (x+10)*1.1 in Timelion The expression would get further complicated if two values needed to be compared through some kind of mathematical formula, as often is the case when physics is involved. The solution Fortunately, for every missing feature in there are endless expansion possibilities. The goal was to allow the previous expression to be written neatly, with all the useful information between quotes, as follows: This syntax also allows all the math to be concentrated in a single equation and combine data from different Elasticsearch queries neatly. Following this goal, we developed , a small plugin to enable equation parsing and math in , based on the library. It can do statistical operations, Boolean logic, deal with units of measurement and pretty much anything MathJS can do natively. Extending Timelion Timelion’s syntax is basically a chain of functions, the first of class provides the list of values (), each subsequent of class transforms those values. In our case Mathlion’s function accept as a mandatory input the string (for example) with the equation to parse provided by the user. The new functions needs to be wrapped in a NodeJS module and correctly exported in order to be installable as a plugin: Rashid Khan, original developer of Timelion and Kibana, maintains an Timelion plugin on GitHub. The use case In this example we have documents containing three numeric variables: By itself, only “power” is somewhat useful. What we’re really interested in is to calculate an indicator called (pf) that needs to be kept above 95% to avoid penalties in the electric bill. The math is the following: Where |S| is the , P is the active power, Q the reactive power, pf% the reactive power in percentage. Unfortunately, cannot natively do square roots. But we can with Mathlion. The resulting graph: We can also use a Boolean expression to filter our data, for example to show only values where we are using a lot of power (more than 70kW) and also where the power factor is critically low pf<95 Boolean conditions can have a value of either 0 or 1. We can easily multiply their result to our incoming data to zero it out (if the condition is ) or pass it along (if ). Since subsequent expressions can see the variables of the previous ones we can display graphs over graphs calculated on the same values. The graph is redrawn in grey only when pf<95, otherwise it displays a horizontal line that indicates the threshold. All native functions can be chained to function, for example to display the rate of change of the calculated power factor by appending Conclusions is an incredible tool that allows an expressive syntax to be applied to ’s queries outputs. For the edge cases and custom requirements that cannot be covered by itself it’s easy to write plug-in functions in NodeJS. Author bio is cofounder of (), an Italian startup specialized in industrial analytics and software for scientific instruments. ","locales":"","title":"Computation near the visualization layer"}
{"index":{}}
{"author":"Gabriel Moskovicz","category":"Engineering","publish_date":"2017-05-08T00:00:00.000Z","url":"/blog/indexing-csv-elasticsearch-ingest-node","seo_title":"","content":" The idea of this article is to go over the capabilities of some of the features of ,  which will be combined to parse a Comma Separated Value (CSV) file. We will go over what is an Ingest Node, what type of operations one can perform, and show a specific example starting from scratch to parse and display CSV data using the Elasticsearch and Kibana. For that we will use an of community issues from New York, NY. This CSV file was updated in October 2015 and It consists of which draws the complete railway station entrances and exits lists. The goal will be to use the Ingest feature of Elasticsearch in a cluster on  to parse the data into a structured json, index the data, and use Kibana to build a Map of New York City that includes all this railway stations. We will use the Ingest feature from Elasticsearch instead of Logstash as a way to remove the need of extra software/architecture setup for a simple problem that can be solved just with Elasticsearch. With all this in place, we will be able to visualize the data and answer some questions such as “Where can we find a station with Elevators?”, “Where are most of the Stations located?”, “Which is the most dense area?” among others. Our data will come from a text file, and will turn into insights. Cluster Setup To start, we are going to use a small Elastic Cloud cluster with 2GB of Memory and 48GB of Disk. We will download the CSV file with this data from the Export to CSV feature included in the following website: . We will use a Linux script that is composed with a simple loop to iterate through the CSV lines and send it to our cluster on Elastic Cloud. Elastic Cloud will give us an endpoint for our Elasticsearch instance. In addition to this, we need to enable Kibana to use the Developer tools and also to build the Dashboard. In order to be able to parse the file, we need to replace all double quotes with single quotes and delete the first line of the file (header) before processing it. This can be done with your prefered tool. Each entry should look like (please note the single quotes): BMT,4 Avenue,25th St,40.660397,-73.998091,R,,,,,,,,,,,Stair,YES,,YES,FULL,,FALSE,,FALSE,4th Ave,25th St,SE,40.660323,-73.997952,'(40.660397, -73.998091)','(40.660323, -73.997952)' Parsing to json In order to be able to search and build dashboards, we need to parse the plain text into a structured json. For this, we will send the data to elasticsearch using the following script: while read f1 do curl -XPOST 'https://XXXXXXX.us-east-1.aws.found.io:9243/subway_info_v1/station' -H \"Content-Type: application/json\" -u elastic:XXXX -d \"{ \\\"station\\\": \\\"$f1\\\" }\" done < NYC_Transit_Subway_Entrance_And_Exit_Data.csv This script will read the file (named NYC_Transit_Subway_Entrance_And_Exit_Data.csv), line by line, and send the following initial json to Elasticsearch: { \"station\": \"BMT,4 Avenue,59th St,40.641362,-74.017881,N,R,,,,,,,,,,Stair,YES,,YES,NONE,,FALSE,,TRUE,4th Ave,60th St,SW,40.640682,-74.018857,'(40.641362, -74.017881)','(40.640682, -74.018857)'\" } The json contains just one field “station”, with a single line. Once the json is sent to Elasticsearch, we need to take the information and break the station field into multiple fields, each containing a single value of the unstructured line. It is highly recommended to use the  from Elasticsearch to play and develop the pipeline before actually creating it. Initially you should just start with a document and an empty pipeline: POST _ingest/pipeline/_simulate { \"pipeline\": {}, \"docs\": [ { \"station\": \"BMT,4 Avenue,59th St,40.641362,-74.017881,N,R,,,,,,,,,,Stair,YES,,YES,NONE,,FALSE,,TRUE,4th Ave,60th St,SW,40.640682,-74.018857,'(40.641362, -74.017881)','(40.640682, -74.018857)'\" } ] } There are many processors available to process the lines, so you should consider all of them to choose which to use. In this case we will simply use the which allows us to easily define a simple pa","locales":"","title":"Indexing your CSV files with Elasticsearch Ingest Node"}
{"index":{}}
{"author":"Clinton Gormley","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2017-05-08T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2017-05-08","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. — David Pilato🇫🇷🇪🇺 (@dadoonet) Apache Lucene The discussion about releasing Lucene 7 and suggests that we target sometime in June. It's also an opportunity for everyone to mention changes they would like to see in, it will soon be too late to include new changes into that release! We are about to so you can expect to see it released within a couple weeks. Having an iterator API would allow us to perform more aggressive compression, eg. by compressing multiple values together. This could be especially effective for time-series since values are expected to be correlated with time, which itself is correlated with index order. This needs to be explored as it could also have a performance impact so we need to find a fine trade-off. , where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2017-05-08"}
{"index":{}}
{"author":"Tyler Hannan","category":"Releases","publish_date":"2017-05-04T00:00:00.000Z","url":"/blog/elastic-stack-5-4-0-released","seo_title":"","content":" May the 4th be with you. 5.4 is, in fact, the release that you have been looking for. It is, again, a substantive release with features and functionality in nearly all areas of the Elastic Stack. And, as per usual, it is available – right now – on . The headlining feature of this release is the addition of machine learning features (in beta) to X-Pack. Machine learning means many things to many people, but in this initial release, we focused on making it easy to detect anomalies in time series data. For many of our users, it has become impractical to spot infrastructure problems, cyber attacks, or business issues by only eyeballing dashboards or creating rules. features automatically model the normal behavior of your time series data to detect what isn’t. Note that machine learning is not yet enabled on Elastic Cloud. Stay tuned for updates! We can’t overstate our excitement about this automated anomaly detection capability in X-Pack.. … but wait, there’s more. Elasticsearch For more detailed information, and many other features, peruse the Elasticsearch . Kibana ‘Visualize’ all the features, and ‘Discover’ more information in the . Hear, hear! The Watcher UI is here! Accessible from Kibana’s Management app, it’s now easy to create, manage and track your watches. Easily acknowledge alerting watches or deactivate them during known service periods. You can even test your watches right in the UI. Use the new cluster alerts feature in to proactively detect issues in your Elastic Stack. Cluster alerts will display in the top-level view of the Monitoring app in Kibana. If your cluster status is red, for example, you will see a prominent notification and information about the error. Logstash For more information, grok the . Beats We don’t ‘let the beat drop’ but we drop the updates in a . ES-Hadoop ES-Hadoop 5.4.0 has also been released today. Get It Now! ","locales":"ja-jp","title":"Elastic Stack 5.4.0 Released"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2017-05-08T00:00:00.000Z","url":"/blog/brewing-in-beats-vsphere-module-in-metricbeat","seo_title":"","content":" Welcome to ! With this weekly series, we're keeping you up to date with what's new in Beats, including the latest commits and releases. Vsphere module in Metricbeat Thanks to , Metricbeat has a new for monitoring . It uses the library to collect metrics from any Vmware SDK URL (ESXi/VCenter). This library is built for and tested against ESXi and vCenter 5.5, 6.0 and 6.5. Make kubernetes indexers pluggable The libbeat processor that adds metadata by calling the Kubernetes APIs needs a way to extract the container ID from the events. The way to do that depends on the Beat. For Filebeat, for example, it’s extracted from the file path. For Metricbeat, it’s obtained based on the IP and port of the endpoint. This makes this step pluggable, so each Beat can provide it’s own custom implementation. This improvement is based on a community contribution from the same . Move template loading configuration under setup Each Beat loads the Elasticsearch mapping template at startup. The template is autogenerated from the , that defines the mapping of each exported field. The path to the , the name of the template and other options are defined in the Elasticsearch output configuration under . This moves the template loading logic from the Elasticsearch output into a separate package that is configured under the # A template is used to set the mapping in Elasticsearch # By default template loading is enabled and the template is loaded. # These settings can be adjusted to load your own template or overwrite existing ones. # Set to false to disable template loading. setup.template.enabled: true # Template name. By default the template name is filebeat. # The version of the beat will always be appended to the given name # so the final name is filebeat-%{[beat.version]}. setup.template.name: \"filebeat\" # Path to fields.yml file to generate the template setup.template.fields: \"${path.config}/fields.yml\" # Overwrite existing template setup.template.overwrite: false Kubernetes dashboard With this , the is renamed to kubernetes. A new sample dashboard is created and added to the list of sample Metricbeat dashboards. Other changes Repository: elastic/beats Affecting all Beats Changes in 5.x: Metricbeat Changes in master: Heartbeat Changes in master: Filebeat Changes in master: Infrastructure Changes in master: Documentation Changes in master: Changes in 5.3: Changes in 5.4: Dashboards Changes in master: Packaging Changes in master: ","locales":"","title":"Brewing in Beats: Vsphere module in Metricbeat"}
{"index":{}}
{"author":"Suyog Rao","category":"Releases","publish_date":"2017-05-04T00:00:00.000Z","url":"/blog/logstash-5-4-0-released","seo_title":"","content":" We are happy to announce that Logstash 5.4.0 has been released today! Please see the for details. You can download the binaries . Persistent Queue is out of BetaBack in 5.1.0, we introduced a called persistent queues to improve the durability of Logstash. We received plenty of feedback from the beta program that allowed us to fix bugs and make improvements. Today, we are announcing that persistent queue is a fully supported feature in Logstash! The goals for this release are: Persistent Queue is still disabled by default. You can enable it by changing in the logstash.yml file. We've also updated our to include new architecture recommendations using persistent queues. Bug fixes5.4.0 has plenty of bug fixes in Logstash core and plugins. The has details of bugs fixed. ","locales":"","title":"Logstash 5.4.0 released"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2017-05-04T00:00:00.000Z","url":"/blog/elasticsearch-5-4-0-released","seo_title":"Elasticsearch 5.4.0 released","content":" Today we are pleased to announce the release of , based on . This is the latest stable release.Latest stable release in 5.x:You can read about all the changes in the release notes linked above, but there are a few changes which are worth highlighting below. ","locales":"","title":"Elasticsearch 5.4.0 released"}
{"index":{}}
{"author":"Jim Goodwin","category":"Releases","publish_date":"2017-05-04T00:00:00.000Z","url":"/blog/kibana-5-4-0-released","seo_title":"","content":" Hello, and welcome to the 5.4.0 release of Kibana!  The 5.3.0 release was big and 5.4.0 is even bigger. We have amazing new features like the Time Series Visual Builder, new Create Visualization UX, View/Edit mode for Dashboards, and Derivative and Moving Average for Visualizations. We also have lots of bug fixes and UX improvements. Kibana 5.4.0 is available on our and on . When you’re finished reading, take a look at the complete for all the goodies. We've redesigned the \"Create Visualization\" wizard. We've grouped the different visualization types into buckets and created unique icons for each visualization type, making it easier for you to find the type of visualization you're looking for. We've introduced  and  modes for your Dashboards. Now when you first open a dashboard, you'll be in  mode.In order to make edits and save them, you'll have to enter  mode by clicking the  button in the top navigation. Having these modes allows us to optimize the experience for users whose job is primarily to view the dashboards. This means no more distracting borders and edit controls on mouse over, and no more accidental changes. You'll also be able to discard edits by clicking . Saving a dashboard will automatically bring you back into  mode. Time Series Visual Builder provides a specialized user interface for working with time series data. It allows you  to build up analytics based on time-series in a visual way and uses pipeline aggregations in Elasticsearch to fully distribute all of the computation. While support for some of the pipeline aggregations is available in traditional Visualize charts, with Time Series Visual Builder, we wanted to provide a curated experience focused specifically on working with time series data. Assuming that you're visualizing a time-series simplifies the user interface and saves users many steps when building up these types of charts. Time Series Visual Builder also adds some unique features, such as confidence bands and annotations, specific to the time-series use case. Time Series Visual Builder is marked “experimental” in Kibana 5.4, which means that we may change or modify this feature in the future, and do not guarantee backward compatibility of charts built using this visualization type until it becomes available in GA. To see a great demo of Time Series Visual Builder have a look at this presentation from Elastic{ON} 2017:  . Event Context most commonly comes up when you're tracking a system problem through thousands of log messages. The idea is that once you’ve filtered down on a number of events (for example error messages) , you'll want to see some of the events around each of those in order to see what happened before and after the error. The feature in 5.4 allows user to see global context, and we are working to allow you to further filter this context down (for instance to logs coming from a specific server) in a future PR:  Kibana now uses the geo-centroid of the results to place results on a map. The result is a more natural looking map with fewer visual artifacts. It avoids the \"grided\"-look from earlier versions. The map now also supports panning and zooming with touchscreens, as well as bug fixes that improve stability. Visualizations like line, bar, and area charts now support most of Elasticsearch's pipeline aggregations, including most of the parent and sibling aggregations. These aggregations give you a new view into your data. For example, the moving average aggregation allows you to smooth over outlier data points over time and the derivative aggregation enables you to display rate of change. In Visualize, the ability for users to customize and style charts has been greatly improved. Users can now overlay multiple chart types on a single plot, use horizontal layouts, and modify the styling of axes and grid-lines. For more about these improvements check out \"\" The X-Pack Watcher UI allows you to do basic operations on watches. The watch list sc","locales":"","title":"Kibana 5.4.0 released"}
{"index":{}}
{"author":"Monica Sarbu","category":"Releases","publish_date":"2017-05-04T00:00:00.000Z","url":"/blog/beats-5-4-0-released","seo_title":"","content":" Today we are pleased to announce that Beats 5.4.0 was released. This is the latest stable version, and it comes with support for gathering JMX metrics from your Java applications, and Filebeat modules for collecting auditd and authentication logs. Quick links: Gather JMX metrics from your Java applicationsThe Metricbeat 5.4 release comes with the Jolokia module to monitor Java applications. provides REST-like access to JMX with JSON over HTTP, and allows easy access to JMX MBeans from everywhere. Here is an example of the configuration file: - module: jolokia metricsets: [\"jmx\"] enabled: true period: 1s hosts: [\"localhost:8778\"] namespace: \"metrics\" jmx.mappings: - mbean: 'java.lang:type=Runtime' attributes: - attr: Uptime field: uptime - mbean: 'java.lang:type=GarbageCollector,name=ConcurrentMarkSweep' attributes: - attr: CollectionTime field: gc.cms_collection_time - attr: CollectionCount field: gc.cms_collection_count - mbean: 'java.lang:type=Memory' attributes: - attr: HeapMemoryUsage field: memory.heap_usage - attr: NonHeapMemoryUsage field: memory.non_heap_usage The jmx metricset collects metrics from each Jolokia instance that is defined in the mappings section. To define an instance, you specify an MBean ObjectName and an array of attributes. Each attribute specifies the name of the attribute to collect and the event field where the value is exported. Linux auditd logs Linux Audit has been part of the Kernel since 2.6.14. It consists of a kernel component to hook and monitor syscalls and a userspace daemon to log these syscall events. Auditd can, for example, write a log line every time a user accesses a file, starts a process, or connects to a socket. In Filebeat 5.4, we’ve introduced the auditd Filebeat module to collect, parse, and visualize the logs created by auditd. The auditd module parses the audit event type, unix epoch time, audit event counter, and the arbitrary key/value pairs that follow. It also gives you the Geo location of the audit event addresses for remote logins. Linux System authentication logsWith the addition of the fileset in the system module, Filebeat 5.4 brings the ability to parse the system authentication logs. System authentication logs are typically available on Linux systems under or for Centos/Redhat, and they contain logs for things like: Monitoring authentication failures is useful for intrusion detection. For example, every time user logs in via ssh, the authentication log records details like: the user’s IP address, the GeoIP information, the status of the ssh login, and whether the user used a public key or password. When a sudo command is executed, the authentication logs contain information about the sudo command, the username and the password of the target user, and the execution status. Also, when a new user is created, you can get details like the username, the user ID, the group ID where the user was created, or the home folder of the user. For more details about these logs, see the blog post. FeedbackIf you want to make use of the new features added in Beats 5.4.0, please , install it, and let us know what you think on Twitter () or in our . ","locales":"","title":"Beats 5.4.0 released"}
{"index":{}}
{"author":"Drew Raines","category":"Releases","publish_date":"2017-05-04T00:00:00.000Z","url":"/blog/docker-base-centos7","seo_title":"CentOS: Elastic Common Docker Base OS","content":" In the recent of the Elastic Stack, we standardized on as the base for our official Docker images (, , , ). Over time, as we’ve rolled out Docker support for more of our products, we have used images that met a narrow window of concerns at any given point instead of maintaining a cohesive story across the whole product line. Using a single OS base, we hope, will bring our stack alignment that will improve the overall experience. We’ve found that CentOS provides that foundation – great support for our products and friendliness to our users. We realize the most radical departure will be for those of you who have come to depend on our base. We considered the cost and feel like it’s time to move on. Don’t get us wrong: we greatly appreciate the good work over at Alpine. A minimalistic distribution with a focus on security and simplicity is, and has been, right up our alley. In practice, however, two things became barriers. The first is actually our fault: some of our software has heavy dependencies. Two of our flagship products depend on the JVM, which weighs down any container image and Alpine is no different. We aren’t the typical consumer of the 5MB base image:  ours at best are still at least 150MB. The most common benefit of Alpine is a small image, and for our stack, we usually don’t get to see it. By comparison, the CentOS 7 base image is 70MB. While that’s a far cry from 5MB, it’s comparable to a lightweight network install, and has many batteries included that we had to install anyway, like bash. Once we add our stuff, we’re only looking at about a 2x increase in CentOS over Alpine. Moreover, a nice property of the layered approach to storage is that the more common the base, the more those layers get reused. For a host using more than one of our images, it probably won’t even see a 2x increase overall, not to mention fewer bits to transfer over the network. The other barrier is that our community is struggling with issues that are outside of our control. Underneath the magic of Docker is a very complex layering of software that still has to work in concert, and sometimes it doesn’t. The combination of all those pieces also makes diagnosing these issues complicated. One illustrates this very well. implementation of , a very basic syscall Elasticsearch uses through the , in cases where many layers of or filesystems exist for a running container. We’ve seen countless other issues surface in our CI environment, and while most are solvable with a hack here and there, we wonder how much benefit there is with the hacks. In an already complex distributed system, libc complications can be devastating, and most people would likely pay more megabytes for a working system. CentOS uses the more venerable GNU libc. While certainly from decades of accretion, glibc has also had time to find and fix bugs in more esoteric parts of the codebase that may only get exposed in, for example, . We know that Alpine ships glibc-based packages, but using a second-class libc in a niche OS seemed like a step further away from stability rather than one toward it. In the end, Docker does a really nice job of getting the base layer out of your way once you have a buildable image. If we can help you get there more consistently, it will be a better experience all around. But as always, please head over to if you are struggling, or open an issue if you find a bug (, , , ). If you are brand new to Elastic, or haven’t had a chance to use our Docker images yet, we provide a simple path to . You only need Docker installed! Special thanks to Toby McLaughlin and Dimitrios Liappis for their tireless efforts on our Docker front. You can tell you all about it from . ","locales":"","title":"New Common Docker Base OS: CentOS 7"}
{"index":{}}
{"author":"Steve Dodson","category":"News","publish_date":"2017-05-04T00:00:00.000Z","url":"/blog/introducing-machine-learning-for-the-elastic-stack","seo_title":"","content":" Today we’re proud to announce the first release of machine learning features for the Elastic Stack, available via X-Pack. Joining Elastic has been like jumping on a rocket ship, but after 7 crazy months we are excited that the is now fully integrated into the Elastic Stack, and we are really excited about getting feedback from users. Note: Before you get too excited, keep in mind that this functionality is marked beta in version 5.4.0. Machine Learning Our goal is to empower users with tools to get value and insights from their Elasticsearch data, and we view machine learning as a natural extension to the search and analytics capabilities in Elasticsearch. For example, Elasticsearch allows you to search for transactions for user ‘steve’ in real time across huge volumes of data, or use aggregations and visualisations to show the top ten selling products or trends in transactions over time. Now, with machine learning you can go deeper and ask questions like “Have any of my services changed behaviour?” or “Are there any unusual processes running on my hosts?” These questions require behavioural models of hosts or services that can be automatically built from data using machine learning techniques. However, machine learning is currently one of the most overloaded terms in the software industry, as fundamentally it is used to describe a broad range of algorithms and methods for data driven prediction, decision making, and modelling. It’s therefore important to cut through the noise and describe specifically what we are doing. Time Series Anomaly Detection Today, the machine learning features of X-Pack are focused on providing “Time Series Anomaly Detection” capabilities using unsupervised machine learning. Over time we plan to add more machine learning capabilities, but for now we are focused on providing added value to users storing time series data such as log files, application and performance metrics, network flows, or financial/transaction data in Elasticsearch. Example 1 - Automatically Alert on Unusual Changes in a Key Performance Indicator Value The most straightforward use case of this technology is to identify when a metric value or event rate deviates from its normal behaviour. For example, has the response time of my service increased significantly, or are the expected number of website visitors significantly different from normal at this time of day? Traditionally, rules, thresholds or simple statistical methods are used for this type of analysis. Unfortunately, these simple methods are rarely effective on real-world data, as they often rely on invalid statistical assumptions (e.g. Gaussian distributions), don’t allow for trends (long term or periodic) or are brittle to changes in signal. Therefore, the first entry point into the machine learning features is a Single Metric job that allows you to investigate how the product learns what's normal and identifies anomalies on univariate time series data. If the anomalies you find are useful, you can run this analysis continually in real time and alert when an anomaly occurs. Whilst this seems like a relatively simple use case, the backend of the product contains a large of amount of complex unsupervised machine learning algorithms and statistical modelling so we are robust and accurate to arbitrary signals. The implementation is also optimised to run natively in an Elasticsearch cluster, so millions of events can be analysed in seconds. Example 2 -Automatically track 1000s of metrics The machine learning product can scale to 100,000s of metrics and log files, and so the next step is to analyse multiple metrics together. These could be multiple related metrics on a host, performance metrics from a database or application, or multiple log files from multiple hosts. In this case we can simply partition the analysis and aggregate the results into one pane of glass showing overall","locales":"de-de,fr-fr,ja-jp,ko-kr,zh-chs","title":"Introducing Machine Learning for the Elastic Stack"}
{"index":{}}
{"author":"Russ Cam","category":"Engineering","publish_date":"2017-05-03T00:00:00.000Z","url":"/blog/elasticsearch-and-kibana-deployments-on-azure","seo_title":"","content":" We've about how easy it is to spin up an Elasticsearch cluster on Azure, using our . The Marketplace is a fantastic way to try Elasticsearch on Azure since all the hard work of provisioning resources and wiring them together is taken care of, leaving you just the simple task of providing the inputs to the step-by-step wizard. A few minutes later and Voila! you have a cluster! Once trying the Marketplace offering however, you're most likely going to want to integrate deployments into your build automation pipeline of choice, and this is where comes in:  using the Azure CLI or PowerShell SDKs, it's easy to target a specific release of the template for reliable deployments. What's more, the template accepts many additional parameters to configure features such as snapshot and restore to an Azure storage account, Transport Layer Security, load balancing and SSL offload with Application Gateway, as well as being able to configure the number and size of disks to attach to each data node. The latter is particularly useful when you have a relatively small data set, so your storage demands are not large, but require virtual machines with sufficient RAM and CPU for your running workload. For this post, let's explore a few different scenarios. Deploying from the command lineYou have a number of tooling options when it comes to deploying from the command line: These options are available to you whether you’re running on Windows, Linux or OSX. Cross-platform FTW! Using a Service PrincipalTo perform automated deployments using Azure PowerShell, we want a non-interactive login, so that has access to deploy to our target subscription is one way to achieve this. Once a Service Principal is configured, it can be used to log in and select the subscription into which we wish to deploy $clientID = \"<client id>\" $clientSecret = \"<client secret>\" $secureClientSecret = $clientSecret | ConvertTo-SecureString -AsPlainText -Force $credential = new-object -typename System.Management.Automation.PSCredential ` -argumentlist $clientID, $secureClientSecret Add-AzureRmAccount -Credential $credential -Tenant \"<tenant id>\" -ServicePrincipal Select-AzureRmSubscription -SubscriptionId \"<subscription id>\" if all is well, then we should see something like the following in standard output Environment : AzureCloud Account : <account id> TenantId : <tenant id> SubscriptionId : <subscription id> SubscriptionName : <subscription name> CurrentStorageAccount : <storage account, if configured> Account : <account id> Environment : AzureCloud Subscription : <subscription id> Tenant : <tenant id> Now we're ready to automate! Simple deploymentThe following diagram provides an overview of the Azure resources that the Elastic ARM template can deploy:The template accepts to configure deployed resources, with many of them optional. This allows a number of different topologies to be deployed, letting you decide whether to:  the template is able to deploy many different versions of Elasticsearch and Kibana, which can be controlled through the   parameter, deploying the latest version of Elasticsearch and Kibana if unspecified. For template version 5.1.2, this is Elasticsearch and Kibana 5.3.1. We're looking to align the template version with the Elastic stack version in a future release.The simplest deployment possible is one where we supply arguments only for We can target git commit, branch or tag within the repository by using its identifier in the template URI and base URL. The following example uses as the version of the template to use, creating a resource group \"simple-deployment\" in the Australia Southeast region to deploy the cluster into $templateVersion = \"5.1.2\" $templateUrl = \"https://raw.githubusercontent.com/elastic/azure-marketplace/$templateVersion/src\" $mainTemplate = \"$templateUrl/mainTemplate.json\" $location = \"Australia Southeast\" $resourceGroup = \"simple-deployment\" $name = \"elasticsearch\" $templateParameters = @{ \"","locales":"","title":"Elasticsearch and Kibana Deployments on Azure"}
{"index":{}}
{"author":"Dale McDiarmid","category":"Engineering","publish_date":"2017-05-03T00:00:00.000Z","url":"/blog/operational-analytics-with-elasticsearch-at-elasticon-2017-part-2","seo_title":"","content":" This blog post continues the description of the Elastic{ON} Operational Analytics demo, describing how the georeferenced WiFi data collected in the was visualised using a custom tile map of Pier 48, the Elastic{ON} venue. Whilst using this demo as an example, this same approach could potentially be reapplied to visualise any geo-enriched metrics on a custom map. For example, are you using to capture and ingest geocoded metrics for each server in your 100,000 sq-ft data centre? Read on and use the following approach to plot server metrics over a floor plan of the data centre. BackgroundIn the , we described the approach used to collect wireless network usage metrics at Elastic{ON} 2017. Using a combination of Packetbeat, SNMP and Ingest Node all DNS and HTTP traffic was encoded with a coordinates describing the approximate location of the source before being indexed into Elasticsearch. More specifically, the locations referred to Ruckus WiFi access points that the device generating a packet was likely connected to. Similarly, sentiment data collected from consoles was also geo-enriched with the location of each terminal. Now that we had geo-enriched data, we needed to address the challenge of plotting attendee activity and sentiment over a floor plan of the Elastic{ON} venue. From a Kibana visualization perspective, this represented the largest technical challenge. By default, the Kibana visualization utilises the to render a world map over which Elasticsearch data points can be plotted. However, this visualization also allows the user to specify any compliant map server as the data source. This would obviously require us to host a WMS service to serve the tiles of our floor plan. A WMS tile service effectively serves images of a predetermined size (256x256) for the coordinates and zoom level requested. These “tiles” can either be pre-generated and served from a datasource or generated dynamically (typically a combination of both with heavy caching involved). We therefore had two fundamental choices: Multiple approaches were considered beyond the scope of this blog. Our final choices were largely motivated by the desire for a simple and repeatable process, limited time and the relative small volume of data that would be involved - the comparative size of the data would be tiny in comparison to the challenge of a full global tile service with multiple zoom levels! provides an open source, feature rich and accessible means of providing a WMS service. For newbies to the domain, the simple setup and UI were decisive. Considering the above criteria and choice of WMS service our data sources were limited to: Whilst the former represented the simplest process, early experiments resulted in issues with more complex granular images and zoom levels. The hockey rink represented a simplified set of black lines - floor plans with text and labels are more detailed. As the user zoomed, the image would be sampled to generate the tiles resulting in poor images. We considered using different source images for different zoom levels - this would complicate the process as well as making it image dependent and harder to replicate for future readers. The vectorised approach we describe below fulfilled all our requirements - simple, repeatable and more importantly decent results at all zoom levels! By providing GeoServer with a vector representation, it is able to more effectively sample the data at different zoom levels. More importantly, the vector could consist of “layers”, each representing a different feature e.g. chairs, which could in turn be configured to only be drawn at specific zoom levels e.g. don’t show text at higher zoom levels. On settling on the vectorised source we describe the process assuming the user has either an svg or vector based pdf of the desired image. Courtesy of our design team we started with the following. Pre-requisites ","locales":"","title":"Operational Analytics with Elasticsearch at Elastic{ON} 2017 - Part 2"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2017-05-02T00:00:00.000Z","url":"/blog/brewing-in-beats-http-module-in-metricbeat","seo_title":"","content":" Welcome to ! With this weekly series, we're keeping you up to date with what's new in Beats, including the latest commits and releases. New community Beat: Rsbeat is built on top of libbeat infrastructure to collect the Redis slow logs and ship them to Elasticsearch directly or via Logstash. The slow logs are accumulated by Redis in memory, so no files are written with information about the slow command executions. Rsbeat interrogates Redis to retrieve the slow logs. Metricbeat: new http module Thanks to , the creator of the successful , Metricbeat has a generic module for collecting data periodically from multiple HTTP endpoints, and indexing the result in Elasticsearch. The http module is especially useful when no dedicated Metricbeat module is available, but there is a HTTP based monitoring endpoint. In case the HTTP endpoint returns the result in a json format, you can configure the metricset to export all the JSON fields under a configured namespace field as shown in the following example: { \"@timestamp\": \"2017-05-01T13:00:24.745Z\", \"http\": { \"http_json_namespace\": { \"date\": \"05-01-2017\", \"milliseconds_since_epoch\": 1493643625474.000000, \"time\": \"01:00:25 PM\" } }, ... } Dashboards using the Time Series Visual Builder The system module of Metricbeat gets two , built using the new Time Series Visual Builder that will be added with the new Kibana 5.4 release. You can watch a demo of the time series visual builder during the . Metricbeat: report the topN processes adds the option to only report on the top N processes by CPU and/or memory. It is useful because storing metrics about each and every process from every host can be quite expensive from a storage point of view. Previously it was possible to filter processes by name, which was useful if one knew in advance which are the most interesting processes. This adds a new option which should be quite convenient in practice, because the number of per-process documents gets limited while still allowing to display the top processes. Add Alibaba Cloud metadata Thanks to the , the processor enhances each event with metadata.  This metadata is only available when VPC is selected as the network type of the instance. Other changes Repository: elastic/beats Affecting all Beats Changes in 5.4: Changes in master: Processors Changes in master: Filebeat Changes in master: Winlogbeat Changes in 5.x: Metricbeat Changes in master: Heartbeat Changes in master: Infrastructure Changes in master: Changes in 5.3: Changes in 5.4: Documentation Changes in 5.4: Changes in 5.3: Changes in master: Dashboards Changes in master: ","locales":"","title":"Brewing in Beats: Http module in Metricbeat"}
{"index":{}}
{"author":"Jim Goodwin","category":"Kurrently in Kibana","publish_date":"2017-05-02T00:00:00.000Z","url":"/blog/kurrently-kibana-2017-5-1","seo_title":"","content":" Welcome to This is a weekly series of posts on new developments in the Kibana project and any related learning resources and events. Did you know 's timelion can resample and interpolate sparse time series? Now you do: — Rashid Khan (@rashidkpc) ","locales":"","title":"Kurrently in Kibana: This week in Kibana for May 1, 2017"}
{"index":{}}
{"author":"Saskia Vola","category":"User Stories","publish_date":"2017-05-02T00:00:00.000Z","url":"/blog/text-classification-made-easy-with-elasticsearch","seo_title":"","content":" is widely used as a search and analytics engine. Its capabilities as a text mining API are not as well known. In the following article I'd like to show how text classification can be done with Elasticsearch. With a background in computational linguistics and several years of business experience as a freelancer in the field of text mining, I got the chance to implement and test the following techniques in different scenarios. When I stumbled across Elasticsearch for the first time, I was fascinated by its ease of use, speed and configuration options. Every time I worked with it, I found an even simpler way to achieve what I was used to solve with traditional Natural Language Processing (NLP) tools and techniques. At some point I realized that it can solve out of the box that I was trained to implement from scratch. Most NLP tasks start with a standard preprocessing pipeline: Some NLP tasks such as syntactic parsing require deep linguistic analysis. For this kind of tasks Elasticsearch doesn't provide the ideal architecture and data format out of the box. That is, for tasks that go beyond token-level, custom plugins accessing the full text need to be written or used. But tasks such as classification, clustering, keyword extraction, measuring similarity etc. only require a normalized and possibly weighted Bag of Words representation of a given document. Steps 1 and 2 can be solved with the (before 5.0 ) in Elasticsearch. Raw text extraction for these plugins is based on , which works on the most common data formats (HTML/PDF/Word etc.). Steps 4 to 6 are solved with the out of the box. Sample-Mapping: { \"properties\":{ \"content\":{ \"type\":\"text\", \"analyzer\":\"german\" } } } If the mapping type for a given field is \"text\" (before 5.0: \"analyzed string\") and the analyzer is set to one of the languages natively supported by Elasticsearch, tokenization, stemming and stopword removal will be performed automatically at index time. So no custom code and no other tool is required to get from any kind of document supported by Apache Tika to a Bag of Words representation. The language analyzers can also be called via REST API, when Elasticsearch is running. curl -XGET \"http://localhost:9200/_analyze?analyzer=english\" -d' { \"text\" : \"This is a test.\" }' { \"tokens\":[ { \"token\":\"test\", \"start_offset\":10, \"end_offset\":14, \"type\":\"<ALPHANUM>\", \"position\":3 } ] } The non-Elasticsearch approach looks like this: Gathering the text with custom code, document parsing by hand or with the Tika library, using a traditional NLP library or API like , , , or anything else which has been developed in some research department. However, tools developed at research departments are usually not very useful for an enterprise context. Very often the data formats are proprietary, the tools need to be compiled and executed on the command line, and the results are very often simply piped to standard out. REST APIs are an exception. With the Elasticsearch language analyzers, on the other hand, you only need to configure your mapping and index the data. The pre-processing happens automatically at index time. Traditional approach to text classificationText classification is a task traditionally solved with supervised machine learning. The input to train a model is a set of labelled documents. The minimal representation of this would be a JSON document with 2 fields: \"content\" and \"category\" Traditionally, text classification can be solved with a tool like , , , etc. Creating the modelsMost machine learning algorithms require a representation of the data. The feature space is usually something like the 10,000 most important words of a given dataset. How can the importance of a word be measured? Usually with . This is a formula that has been invented in the 70ies of the last century. TF-IDF is a weight that scores a term within a given document relative to the rest of the dataset. If a term in a document has a high TF-IDF","locales":"de-de","title":"Text Classification made easy with Elasticsearch"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2017-04-27T00:00:00.000Z","url":"/blog/elasticsearch-5-3-2-and-2-4-5-released","seo_title":"Elasticsearch 5.3.2 and 2.4.5 released","content":" Today we are pleased to announce the bugfix release of , based on , and , based on . Both versions is already available for deployment on , our Elasticsearch-as-a-service platform. All users of stored templates should upgrade to 5.3.2.Latest stable release in 5.x:Latest stable release in legacy 2.x:You can read the full changes list in the links above, but the main reasons for the 5.3.2 release are:Please , try it out, and let us know what you think on Twitter () or in our . You can report any problems on the . ","locales":"","title":"Elasticsearch 5.3.2 and Elasticsearch 2.4.5 released"}
{"index":{}}
{"author":"Pier-Hugues Pellerin","category":"Engineering","publish_date":"2017-04-27T00:00:00.000Z","url":"/blog/archiving-your-event-stream-with-logstash","seo_title":"","content":" Logstash is a swiss army knife for event ingestion and transformation. You can configure it to stream and transform multiple data sources into Elasticsearch. You can then visualize this data easily with Kibana.  Logstash can also send to more outputs than just Elasticsearch -- we support over 70 outputs. In this post, we will be talking about the which allows you to store your events to an S3 bucket for long term archival or for batch processing at a later time. Logstash supports multiple outputs:  so you can send data to Elasticsearch as well as archive it to S3. Using conditional logic, you can even route certain streams of data you want sent to S3. If you want more control on the created S3 structure you can use event data and the field reference syntax to dynamic create the S3 prefixes. We will go in more details in the following examples. A \"Hello World\" S3 archival Before we dive into a complex example, a simple way to archive your data to S3. Just define a S3 block in the output section of your configuration, and all your data will be archived. input { beats { port => 5544 } } output { s3 { access_key_id => \"SECRET\" secret_access_key => \"SECRET\" bucket => \"mybucket\" codec => json_lines } elasticsearch { } } The  option controls how your data will be serialized in the S3 item. Controlling data archived to S3 Maybe you only need to send some of your data to S3. You can use our  like you do in the filter section to select a subset of your events. You can also use this approach to send events to different buckets for different events. This could mean you have 2 separate S3 output blocks differing in bucket names.  input { beats { port => 5544 tags => [\"edge_firewall\"] } } output { elasticsearch { } # Archive only some events to your s3bucket if \"edge_firewall\" in [tags] { s3 { access_key_id => \"SECRET\" secret_access_key => \"SECRET\" bucket => \"edge_firewall\" codec => json_lines } } } Here, only events tagged will be routed to S3. Dynamically choosing prefixes One of the popular feature requests in the S3 output was the ability to use field references or values in the event to control the prefix paths for the uploaded documents. This is great for a multi-tenant deployment where you want to separate paths for each tenant on S3. To be able to support that feature we had to do a complete rewrite of the plugin. Other recent changes include a move to an asynchronous upload model and be able to use the latest version of the AWS-SDK. In the example below, we see how to dynamically choose your S3 destination. Beats by default sends the and the , so we can use these event values to create the prefixes on your s3 bucket. input { beats { port => 5044 codec => json } } output { s3 { access_key_id => \"SECRET\" secret_access_key => \"SECRET\" bucket => \"mybucket\" codec => json_lines prefix => \"/%{[beat][name]}/%{[beat][hostname]}/%{customerID}\" } }The beats inputs will receive events with different values for fields , , . These fields can now be used in the option to configure the created path. For minimizing the collisions on the remote server we only allow you to set the prefix dynamically and we will generate the base name of the file for you. Assuming the events will come from multiple clients, the previous configuration will generate a structure similar to this on your S3 bucket: If you expand the directory tree, it will look similar to the one shown below -- each **field reference** is a directory. Under the hood When you use S3 output, data is buffered to disk locally before uploading to S3. Each unique dynamic prefix will generate a file on disk with the matching events, and this file will be periodically and asynchronously uploaded to S3. This is controlled by the  option. By default, it will look for the size and the creation time of the file. These values can be changed by using the  and the  option. Each file on disk will respect the rotation options independently of other files","locales":"","title":"Archiving your event stream with Logstash"}
{"index":{}}
{"author":"Rashid Khan","category":"Engineering","publish_date":"2017-04-26T00:00:00.000Z","url":"/blog/sparse-timeseries-and-timelion","seo_title":"","content":" Ok, you’re getting data once per minute, and you’ve dropped into Timelion and you’ve got this stupid looking chart. It’s calculating an interval of less than 1 minute–1 second in this case–because you’re only looking at 15 minutes of data. So Timelion says “Ah, I can show you a nice high resolution” chart. You only get data every minute, but Timelion is asking Elasticsearch for per-second data. So once every 60 seconds, there is a big number, and no documents in between. The is 0, zip, zilch, nada, nothing for 59 out of 60 seconds, so timelion plots that big fat zero. But as far as you’re concerned, it isn’t 0, its just that you only reported at the end of the minute. We need to tell timelion “If there are no documents in the bucket, the number isn’t 0, it’s just not known yet, aka, null”. The keyword in that statement: . Timelion does . This says: Ask Elasticsearch for a count of documents, over time, in which the field is present: . If that count equals () 0, set the bucket to , otherwise, set the bucket to the value of , that is, our original expression from the first chart. Since there are now a bunch of buckets, I’ll need to set this chart to for the moment, as lines won’t connect over null buckets…we’ll get to that in a moment. It might not look like it, but this is progress. The chart on the left doesn’t have all the zero values of the chart on the right, but we need to connect the dots by filling in the values. Timelion’s could do this for us: Oooo, it lines up nicely with our original. Oh, that looks good right? . It’s actually very wrong. We’ve invented bytes that didn’t happen! Look what happens when we apply to total up all these bytes: We can’t just interpolate like that. Drawing those straight lines between the points is a lie, and not your usual little-line-chart lie:  its a big fat everything-is-totally-wrong-now lie. No, what we need is to distribute that total of bytes every 60 seconds, into the other 59 buckets. We can use for that: Now the chart on the left doesn’t look as good, but looks aren’t everything… and don’t judge a book by its cover… and a bird in the hand. Hey, leave that bird alone. Where were we? Oh right, charts. The chart on right reveals that what we’ve done is actually quite accurate. The teal line now represent the per-second data, while the red continues to be per minute. The cumulative sum of the post- series now tracks the original quite well and setting the interval drop down does a nice job of always showing us the data at the requested resolution, even if it has to correct it because you don’t have frequent enough data. But what if you ? No matter what interval is selected? We can now employ . Below we can see that that chart’s y-axis extents and shape stay about the same as we change the interval. This is because we’re scaling the chart to always represent a 1 second rate. So there you go. Now you know how to normalize sparse data, as well as bring it back to a per interval rate, and hopefully you can see how you might use , and in other problems too. If you need more ideas, check out this blog post about Timelion conditionals: ","locales":"","title":"Normalizing sparse time series with Timelion"}
{"index":{}}
{"author":"A.J. Angus","category":"Engineering","publish_date":"2017-04-28T00:00:00.000Z","url":"/blog/supercharging-elastic-operations-with-salesforce-and-the-elastic-stack","seo_title":"Supercharging Operations with Salesforce and Elasticsearch","content":" At Elastic, we are continually finding ways to demonstrate the use of the Elastic Stack in new and novel ways. Within our Operations team, we have set goals to create efforts that allow us to improve our customer experience using the Elastic Stack. We recently did an Elastic{ON} talk on how we . This post will describe how we have improved our support experience using Salesforce Community Cloud and , our hosted Elasticsearch offering.ChallengeOur goal here was to create a framework within Salesforce where we could easily bring in data from an Elastic Cloud cluster. We wanted a reusable framework and connection to Elastic Cloud that could be used over and over again with minimal effort after the initial framework was complete. Best Prototype ResourcesThe best resources available here for you to try a similar approach is to have: For our use case we set up a 3 node cluster with 2GB memory and 48GB storage in Elastic Cloud. It is pretty simple to setup and provision once you have sized your memory and storage needs. For us, our data storage needs were small and we are only doing reads from the cluster, so the smallest cluster worked for us. Our Use CaseLast June we migrated our support portal from Zendesk to Salesforce Community Cloud. It was a long process but it set us up to with the infrastructure needed to bring the Elastic Stack into our support and customer experience flow. Fast forward to now, our goal was to use Elasticsearch to power a recommendations engine for knowledge base articles in our support portal. Our portal is a Community Cloud template with customized Salesforce Lightning components. Our goal with Elasticsearch was two-fold: The final solution looks like this:Detailed ExplanationFor a detailed technical explanation of how to combine Salesforce and the Elastic Stack in your own environment, please see our (Project Xavier). What’s Next?Now that we have launched our first version of this integration, we have big plans on how to add more features that will help us serve you and all of our customers better. As an example, in Case Submission, we are adding more content sources (blogs, all product documentation) to make it easier to find relevant content in the portal. In addition, we are working to index all of our cases and other support interactions into Elasticsearch to add more magical features in other parts of our support experience. Stay tuned. P.S. Seeking Talented EngineersDoes this sound like an interesting challenge that you want to be part of? ! ","locales":"","title":"Supercharging Elastic Operations with Salesforce and the Elastic Stack"}
{"index":{}}
{"author":"Renuka Hermon","category":"User Stories","publish_date":"2017-04-27T00:00:00.000Z","url":"/blog/behavior-correlation-using-elasticsearch-cause-award-honoree-noschoolviolence","seo_title":"Behavior Correlation using Elasticsearch: Cause Award Honoree NoSchoolViolence.org","content":" The intersection of sociology and technology is where Paul Privateer envisions the future of his work. Privateer, who holds a PhD in informatics and narrative theory, is the founder of (NSV), an organization on a mission to reduce school violence by identifying predictive behavioral patterns in adolescents. NSV was one of our at . Using Elasticsearch, the NSV team created The Lantern: a tool that helps researchers, parents, school administrators, and mental health professionals make connections between specific observable behaviors and potential violence. Rather than focusing on strategies solely related to coping with school violence, this app gives community members the resources to proactively help at risk adolescents before they act out. The beginnings of this project stem from personal exposure. After experiencing student tragedies first-hand as a graduate-level professor, Privateer grew wary of hearing the response, “There was no way we could have known.” Was there really no way to predict any of these cases? Wondering how he could reach out to students before a tragedy, Privateer began making mental notes of shared behaviors like slipping grades, antisocial tendencies, unjustified hostility, and more. What if he could find a pattern that would serve as an alert to parents and teachers? He knew he was onto something when he discovered that “the CDC, the World Health Organization, Department of Justice, FBI, the UN, all recognize 12 [risk factors of violence] all the way from cyber-bullying, sexual harassment, weapons use, fighting, drug use,” Privateer explained. The discovery of this correlation compelled Privateer to take action. He assembled a small team to start researching these connections further, combing through police state records, FBI records, and academic journals. Over time he became worried about how they would identify relationships. “If it’s SQL…in the old form of relational, how are we going to even begin to deal with that? That would take eons to be able to look through all of those grids and start looking at those graphs and then correlating the implications of it.” For the project to be valuable, the NSV team had to find a realistic way to interact with their data. Based on a recommendation from his student, Privateer started using Elasticsearch - and the seemingly untamable database became a beacon of hope. By putting Elasticsearch on top of a Python flask tool, The Lantern app can structure the correlating frequency of a specific term to the possible form of violence. Users of the search function can type in a string, and have the frequency of the associated type of violence returned. Their research became accessible and their app could give concerned parents and administrators direct insight regarding these behavioral correlations. The forecast for Privateer and his team involves a more sophisticated use of Elasticsearch. Going beyond single use case searches, he sees the technology being used to create pattern frameworks that help “create a much broader intervention program that deals with children in a much more precise and accurate way.” As the tools grow more robust, the scope of their focus will likely expand into other areas including health issues and academic performance. ","locales":"","title":"Behavior Correlation using Elasticsearch: Cause Award Honoree NoSchoolViolence.org"}
{"index":{}}
{"author":"Tyler Fontaine","category":"News","publish_date":"2017-04-26T00:00:00.000Z","url":"/blog/announcing-support-for-elastic-cloud-hosted-elasticsearch-standard","seo_title":"Announcing Support for Elastic Cloud Standard, Our Hosted Elasticsearch Offering","content":" Starting May 1, 2017, we will be moving support for our customers from the public Discuss forum to a ticket-based system backed by a team of dedicated Elastic Cloud engineers. Before diving into the details, we’d like to thank all our customers for helping us evolve and enhance our Elastic Cloud service with your feedback. Everything we do, we do for you. When we launched Elastic Cloud in July 2015, we decided to offer support for Standard via an open forum. Our experience in community building is deep, but we knew that offering a forum around a service like Elastic Cloud would be a learning opportunity. And we learned a lot! As we worked with you, we learned about the kind of help you needed, the size and shape of your questions and concerns, and ultimately how you wished to be treated. You spoke. We listened. We thank you for your feedback. We feel this move from open forum support to a ticket-based system is the best path forward toward providing the quality of services our growing customer base deserves. Here’s why we chose to move to a private system: Here’s how it will work: On Monday, May 1, 2017, emails will go out with support portal login instructions to all current Elastic Cloud standard customers with active clusters. This will allow you to log into the support portal, where you can see and track both current and archived cases. If you had a cluster with us before, don’t worry - if you decide to give us a try again, your accounts will be provisioned as soon as you start up a new cluster. But if you have questions in the meantime, you can always . Every day is humbling at Elastic, as our assumptions are challenged and replaced with new information. And working with you makes all the difference. On the note of listening to our customer base, we’ve just launched . This means you can pay for Elastic Cloud through AWS, so ","locales":"","title":"Announcing Support for Elastic Cloud Standard, Our Hosted Elasticsearch Offering"}
{"index":{}}
{"author":"Emily Mosher","category":"User Stories","publish_date":"2017-04-25T00:00:00.000Z","url":"/blog/eyes-in-the-sky-geohazard-monitoring-with-terradue-and-elasticsearch","seo_title":"","content":" Landslides, earthquakes, ground settlement, and floods. Earth is an unquiet planet, and sometimes that’s trouble for the life that inhabits it, including humans. But it’s hard to predict natural disasters. The best way to protect life and infrastructure from geohazards is to prepare in advance. Post-disaster studies show that mitigation efforts pay off, but scientists need the best data they can get to analyze risk. One place they get this good data is from .Terradue is a European Space Agency (ESA) spin-off company that specializes in massive processing of Earth science data using Elastic. Emmanuel Mathot, a technical leader at Terradue, about their Elasticsearch-powered query engine that helps monitor geohazards from space. Since last year, Terradue has done even more exciting work that Mathot discussed in his March 8, 2017 .Earth observation satellites orbit the planet 24/7 taking fantastically precise radar images of its surface. “This [satellite] instrument is able to deliver data we use to track movements down to a few millimeters,” said Emmanuel Mathot. It can even or measure how fast individual buildings settle into the ground, like .As the satellites orbit, they take pictures of the planet’s entire surface once every six days. All the images and metadata about satellite position, velocity, and orbital track get indexed into Terradue’s Elasticsearch query engine, Mathot explained. This engine provides the best data to monitor ground movement along a fault or near a volcano, or to map flood plains, among other uses. It does this by selecting the most appropriate images to compare to each other by minimizing some parameters within the metadata.One good illustration of this involves earthquakes, specifically that struck central Italy in August of 2016. To create the most accurate picture of earthquake-related ground movement, researchers compared the satellite’s first post-event image of the disaster area with the best image taken in the months before the earthquake. There are typically around 50 of these pre-event images to pick from and which one is “best” depends on many factors.To do so, Terradue uses Elasticsearch to compare the satellite’s position when it took the post-event picture to its position when it took each the pre-event image. It also analyzes the time span between when the images were taken, the satellite’s orbital track and velocity, and other metadata. The query engine compares all of the possible images and selects the best one, and it can do it in about ten seconds.“Without this information in Elasticsearch, in our catalog,” said Mathot, “It’s not possible. The user has to [make] assumptions.” With these accurate pictures, scientists can locate the fault lines that previously went unnoticed. With the earthquake in Italy, the pictures confirmed Italian scientists’ field observations that the ground had moved a whopping 70 centimeters. The query engine images offer more knowledge than just ground displacement — it can even delineate water movement if you ask the data the right questions. Nigerian scientists have partnered with ESA and Terradue to map flooding along the Niger River in Africa. “The impact of the floods on the people and the culture is really critical, and understanding and forecasting the intensity and extent is key to reduce the impact of the flooding,” said Mathot. To find patterns, they need an accurate picture of what the river looks like during the rainy and dry seasons over several years.The trouble is, liquid water doesn’t show up in images the way the ground and buildings do. But scientists figured out that when the satellite images a patch of ground, rougher surfaces bounce back higher intensity microwave backscatter profiles, while smooth surfaces — like water — bounce back low intensity backscatter data, all of which is indexed into Elast","locales":"","title":"Eyes in the Sky: Geohazard Monitoring with Terradue and Elasticsearch"}
{"index":{}}
{"author":"Suyog Rao","category":"The Logstash Lines","publish_date":"2017-04-25T00:00:00.000Z","url":"/blog/logstash-lines-2017-04-25","seo_title":"","content":" Welcome back to The Logstash Lines! In these weekly posts, we'll share the latest happenings in the world of Logstash and its ecosystem. This week we released 5.3.1 which fixed a couple of important bug fixes. Details in the . Changes in 5.4.0 ","locales":"","title":"Logstash Lines: GeoIP ASN data, Persistent Queue fixes"}
{"index":{}}
{"author":"Luca Cavanna","category":"Engineering","publish_date":"2017-04-25T00:00:00.000Z","url":"/blog/tribe-nodes-and-cross-cluster-search-the-future-of-federated-search-in-elasticsearch","seo_title":"","content":" Elasticsearch has a powerful that allows it to search against all indices on  the local cluster. We recently released Elasticsearch 5.3.0 including a new functionality called that allows users to compose searches not just across local indices but also across clusters. This means that one can search against data that belongs to other remote clusters. Historically, the Tribe Node was used when searches needed to span multiple clusters, yet it works very differently. In this blog post, we will cover why we implemented Cross-Cluster Search, how it works and compares to the Tribe Node, and why we are convinced it is a step in the right direction for the future of federated search using Elasticsearch.Keep it simpleWhen we sat down to try and redesign the next-generation Tribe Node, we re-evaluated the problems that it was trying to solve. The goal was to make federated search possible without all the limitations that the Tribe Node provides, and without adding a specific API for it, so that maintaining such feature would also be easier. We realized that some of the features that the Tribe Node offers besides federated search are commodities. The Tribe Node supports many Elasticsearch APIs allowing, for instance, to retrieve the cluster state or nodes stats via a Tribe Node, which will return the information collected from all the remote clusters and merged into a single view. Merging information coming from different sources is nothing complicated though, and is easily performed on the client side by sending multiple requests to the different clusters. The hard problem that must be addressed on the server side is federated search. It involves a distributed scatter and gather to be executed on nodes belonging to multiple clusters as well as the result merging and reduction requiring internal knowledge. That is why we decided to focus on solving this specific problem in a sustainable and robust way by adding support for Cross-Cluster Search to the existing _search API.Search API DetourThe _search API allows Elasticsearch to execute searches, queries, aggregations, suggestions, and more against multiple indices, each one composed by one or more shards. When a Client sends a search request to an Elasticsearch cluster, the node that receives the request acts as the coordinating node for the whole execution of the request. It identifies which indices, shards, and nodes the search has to be executed against. While executing, all data nodes holding a shard that is queried receive requests in parallel, then each node executes the query phase locally and sends the results back to the coordinating node. The coordinating node waits for all shards to respond in order to reduce the results down to the top-N hits that need to be fetched from the shards. A second execution phase then fetches the top-N documents from the shards in order to return the results back to the Client.How Cross-Cluster Search WorksSince Elasticsearch 5.3.0, it is possible to register remote clusters through the under the namespace. Each cluster is identified by a cluster alias and a list of seed nodes that are used to discover other nodes belonging to the remote cluster, as follows:PUT _cluster/settings { \"persistent\": { \"search\": { \"remote\": { \"cluster_one\": { \"seeds\": [\"remote_node_one:9300\"] }, \"cluster_two\": { \"seeds\": [\"remote_node_two:9300\"] } } } } }Once one or more remote clusters are registered, it is possible to execute search requests against their indices using the _search API of the cluster the remotes are configured on. In contrast to a so-called local index, remote indices must be disambiguated with their corresponding configured cluster alias separated by a colon (e.g. ).Whenever a search request expands to an index on a remote cluster, the coordinating node resolves the shards of these indices on the remote cluster by sending one _sear","locales":"","title":"Tribe Nodes & Cross-Cluster Search: The Future of Federated Search in Elasticsearch"}
{"index":{}}
{"author":"Jim Goodwin","category":"Kurrently in Kibana","publish_date":"2017-04-24T00:00:00.000Z","url":"/blog/kurrently-kibana-2017-4-24","seo_title":"","content":" Welcome to This is a weekly series of posts on new developments in the Kibana project and any related learning resources and events. 5.3.1 released — elastic (@elastic) ","locales":"","title":"Kurrently in Kibana: This week in Kibana for April 24, 2017"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2017-04-24T00:00:00.000Z","url":"/blog/brewing-in-beats-dropwizard-module-in-metricbeat","seo_title":"","content":" Welcome to ! With this weekly series, we're keeping you up to date with what's new in Beats, including the latest commits and releases. Add support for dropwizard in Metricbeat Thanks to the , Metricbeat has a new module for the metrics. The collector metricset fetches Dropwizard metrics by querying the HTTP servlets. If you have a dropwizard project, then you can use the following configuration to automatically report the metrics to Elasticsearch: - module: dropwizard metricsets: [\"collector\"] enabled: true period: 10s hosts: [\"localhost:8080\"] metrics_path: /metrics/metrics Enhance Prometheus module in Metricbeat The current Prometheus collector implementation separates each metric into a separate event, that can generate a lot of data. Thanks to the same , there is a to group metrics under the same MetricFamily. This enables histograms and summaries to be included in the same Elasticsearch document, reducing the amount of documents and making querying easier. Add Icinga module in Filebeat Thanks to , Filebeat is capable of processing the main, debug and startup log of the daemon. Add support for Tencent Cloud The processor enhances each event with Cloud metadata. Currently it has support for Amazon Elastic Compute Cloud (EC2), Digital Ocean and Google Compute Engine (GCE). However, a lot of companies in China prefer to use Tencent Cloud and AliCloud. Thanks to , processor has also for them. Other changes Repository: elastic/beats Affecting all Beats Changes in master: Filebeat Changes in 5.4: Changes in master: Metricbeat Changes in master: Heartbeat Changes in master: Infrastructure Changes in master: Documentation Changes in 5.4: Changes in 5.3: Changes in master: ","locales":"","title":"Brewing in Beats: Dropwizard module in Metricbeat"}
{"index":{}}
{"author":"Emily Mosher","category":"User Stories","publish_date":"2017-04-21T00:00:00.000Z","url":"/blog/fighting-eboloa-with-elasticsearch-cause-award-honoree-ehealth-africa","seo_title":"","content":" \"We hit the ground behind. Behind the virus. It was taking one, sometimes two weeks between a suspected case being identified and getting a laboratory confirmation to start contact traces. At the speed Ebola works, we were getting further and further behind all the time.We needed to identify suspected cases, get them quarantined get lab results, and get contact tracing to figure out who the they had contact with over the last several days. That needed to happen in 24 hours. In order to do that we really needed visibility. We only saw the people showing up to the hospitals. We needed to broaden that picture.\"\"What we were able to do was expand these health call lines that had mostly been dormant in the countries. They were reactivated early on in the response, but were really kind of limited to one or two people on phones, writing down stuff mostly on paper. And we still weren’t getting enough information. And we weren’t getting that true picture of the outbreak.So we built a call center application. We have our own software team and we do a lot of work with open source tools. But we looked around and we needed a tool that was going to allow us to record data at a central location with all of those calls from around the region and we needed to distribute that data to the district centers. The centers that were spread around the country where they would get the data, respond to the alerts, and then add in the follow up alerts and the rest of the data. We needed to build a solution.\"\"We stumbled upon Elasticsearch after a bunch of late night Google queries and realized it was something that we could try to dramatically improve the way that we were searching our data because the database was growing so quickly. We constantly had to refactor and rewrite the application because we had no idea the volume this response was going to take on.\"\"Elasticsearch allowed us to immediately index all the different facets of the call center. And as we were adding new data elements in, new parts of the response, we could quickly update the indexes and get new reports based on the new elements. Kibana let us set up these visualizations and snapshot in time reports so that every 24 hours our response teams met in the morning and evening, they’d get reports on all aspects of the response in real time so they could see a picture — the whole picture — of the country for the first time in terms of what was going on.\"\"Every single person that called this call center, 24 hours later they got a call back. 'Hey, you reported a death, you reported a sickness. Did anyone come? Did anyone show up? If they did show up, did they do these things?'We created a feedback loop for the actual response [teams] themselves so they could quickly iterate through and improve the situation. We went from a two-week response window down to a 24-hour response window in two to three weeks just by constantly reiterating and running through the data and making sure the people making these decisions had access to the real time information.\"\"Responding to public health emergencies is all about time. And every minute, every second, every day that goes by where you don’t have visibility on the response is time you lose and it’s lives lost. So, the call center application, the data team, all the people in the responding agencies are really hooked into this application and used this data to drive decision making and drive Ebola out of these three countries.\" ","locales":"","title":"Fighting Ebola with Elasticsearch: Cause Award Honoree eHealth Africa"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2017-04-20T00:00:00.000Z","url":"/blog/elasticsearch-5-3-1-released","seo_title":"Elasticsearch 5.3.1 released","content":" Today we are pleased to announce the bugfix release of , based on . It is already available for deployment on , our Elasticsearch-as-a-service platform. All users of 5.3.0 should upgrade.Latest stable release in 5.x: ","locales":"","title":"Elasticsearch 5.3.1 released"}
{"index":{}}
{"author":"Giuseppe Valente","category":"Engineering","publish_date":"2017-04-20T00:00:00.000Z","url":"/blog/monitor-and-optimize-golang-application-by-using-elastic-stack","seo_title":"","content":" The Elastic Stack can be easily leveraged to monitor Go applications. It allows to do things like analyzing memory usage (memory leaks anyone?), performing long-term monitoring, tuning and capturing diagnostics. Beats in particular, the lightweight data shippers in the Stack, are designed to sit alongside the applications and are a natural fit for this kind of monitoring. Metricbeat is a Beat specialized in shipping service and/or server metrics, but also happens to be written in Go. It ships in a relatively small package (only about 10MB), and does not bring any additional dependencies with it. While its CPU overhead and memory footprint are also very light, it ships with modules for a variety of services such as: If the service you’re looking for is not listed, don’t worry: Metricbeat is extensible and (and this post is proof of that!). We’d like to introduce you to the Golang Module for Metricbeat. It has merged into the master branch of , and is expected to be released in version 6.0. Sneak preview Here’s a Kibana Dashboard that captures the potential of the Golang Module for Metricbeat: The top panel shows a summary of the heap usage, which gives us a general idea of ​​Go’s memory usage and GC status: The middle panel has three charts in it with a breakdown of: represents both memory in use and not yet reclaimed, while is obviously the size of objects that are active. accounts for objects that have been allocated but can be reclaimed as free memory. The two charts in the bottom panel are the GC time and GC count statistics. represents the percentage of CPU time spent on GC. The greater the value, the more frequently GC occurs, in other words more time wasted on GC. The trend seems upward and pretty steep, but the range of values is between 0.41% and 0.52% so not too worrisome. Normally the GC ratio warns an optimization in the code when it goes into the single digits. Memory leaks With this information we will be able to know in much detail about Go’s memory usage, distribution and GC implementation. For instance if we wanted to analyze whether there is a memory leak, we could check if the memory usage and heap memory allocation are somewhat stable. If for example and are clearly rising, it could be due to a memory leak. Historical information gives us great granularity in analyzing memory usage and GC patterns across different versions, or even commits! Great, now how do I get it? expvar First things first, we need to enable Go’s . is a package in Go’s standard library that exposes internal variables or statistics. Its usage is very simple, it’s basically just a matter of importing the package in the application. It will automatically detect and register to an existing HTTP server: import _ \"expvar\" If no HTTP server is available, the code below allows us to start one on port 6060: func metricsHandler(w http.ResponseWriter, r *http.Request) { w.Header().Set(\"Content-Type\", \"application/json:  charset=utf-8\") first := true report := func(key string, value interface{}) { if !first { fmt.Fprintf(w, \",\\n\") } first = false if str, ok := value.(string):  ok { fmt.Fprintf(w, \"%q: %q\", key, str) } else { fmt.Fprintf(w, \"%q: %v\", key, value) } } fmt.Fprintf(w, \"{\\n\") expvar.Do(func(kv expvar.KeyValue) { report(kv.Key, kv.Value) }) fmt.Fprintf(w, \"\\n}\\n\") } func main() { mux := http.NewServeMux() mux.HandleFunc(\"/debug/vars\", metricsHandler) endpoint := http.ListenAndServe(\"localhost:6060\", mux) } The path registered by default is , we can access it at . It will expose data in JSON format, by default provides Go’s but of course we can also register our own variables. Go Metricbeat! Now that we have an application with , we can use Metricbeat to get this information into Elasticsearch. The installation of Metricbeat is very simple, it’s just a matter of . Before starting Metricbeat we just need modify the configuration file () to enable the module: ","locales":"zh-chs","title":"Tuning Go Apps with Metricbeat"}
{"index":{}}
{"author":"Mark Walkom","category":"Engineering","publish_date":"2017-04-19T00:00:00.000Z","url":"/blog/geoip-in-the-elastic-stack","seo_title":"GeoIP in the Elastic Stack - Elasticsearch, Logstash, Ingest API","content":" Intro ","locales":"","title":"GeoIP in the Elastic Stack"}
{"index":{}}
{"author":"Jordan Sissel","category":"Engineering","publish_date":"2017-04-20T00:00:00.000Z","url":"/blog/log4j-input-logstash","seo_title":"","content":" Earlier this week, an was released detailing an object deserialization security flaw in the way Apache Log4j version 2 processes input data (). This flaw would give a remote attacker the ability to execute code of their choosing within the JVM process listening for Log4j events. ","locales":"","title":"The future of Log4j input in Logstash"}
{"index":{}}
{"author":"Court Ewing","category":"Releases","publish_date":"2017-04-20T00:00:00.000Z","url":"/blog/kibana-5-3-1-released","seo_title":"","content":" Kibana version 5.3.1 is released, and it includes a couple of important bug fixes and a fix for an open redirect vulnerability in X-Pack security. Shield version 2.4.5 for Kibana has also been released with a fix for the same vulnerability. With X-Pack installed, Kibana versions before 5.3.1 have an open redirect vulnerability on the login page that would enable an attacker to craft a link that redirects to an arbitrary website. Shield versions in Kibana prior to 2.4.5 are also vulnerable. . Users are encouraged to update immediately. Other bug fixes in 5.3.1 ","locales":"","title":"Kibana 5.3.1 released"}
{"index":{}}
{"author":"Emily Mosher","category":"User Stories","publish_date":"2017-04-19T00:00:00.000Z","url":"/blog/stretching-the-elastic-stack-to-fit-the-very-large-and-complicated-human-genome","seo_title":"Stretching the Elastic Stack to Fit the (Very Large and Complicated) Human Genome","content":" The Human Genome Project was supposed to cure all the diseases. At least, that's what the general public thought back in 2001 when the results of the project were first shared. But scientists suspected all along that uncovering the root of all human disease would be a bit more complicated — and they were right. The data could be collected, but making use of it was going to take years of grueling and tedious work. Daniel Myung and Bhasker Bokuri are two scientific computing experts at on March 9, 2017 about how their innovative approach to managing genomic data with the Elastic Stack opens new vantages for geneticists. Using the Elastic Stack, scientists can see their data in new ways that uncover safer, more effective treatments for diseases like cancer, Alzheimer’s, and diabetes. “The pharmaceutical industry still has a pretty low batting average,” said Myung. “There’s all this work and all this effort. It’s pretty expensive and there’s still this low hit rate of [a product idea] actually being safe and effective.” They want to use genetic data to better predict in the discovery phase how a drug will affect the body long before expensive trials. The goal is to speed the production of effective treatments and make drugs cheaper for all. They want to use genetic data to better predict in the discovery phase how a drug will affect the body long before expensive trials.As sequencing costs have fallen from tens of millions of dollars to around $1,000 to sequence a whole genome, thousands of studies have been pouring in from academia, public sources, and consortium efforts among multiple pharmaceutical companies. The consortium efforts are the source of most of the de-identified, aggregate data in Merck's database. The problem Myung and Bokuri face is that the data is unwieldy. There is so much of it, it’s in multiple formats from diverse sources, and the methods that have produced this 13 years-and-counting’s worth of data change every few years. Previous methods of handling this data, as Myung and Bokuri described, were tedious, manual, and required a significant amount of expert integration. It could take two days to add a single field. Myung and Bokuri have used Elastic products to make a pipeline that ingests and harmonizes the diverse data into a rapidly searchable database and creates a universal coordinate system for genetic variants. The size and the weird shape of their data fit well into Elastic. The size and the weird shape of their data fit well into Elastic.They approach it like a weather map, but instead of using location, temperature, humidity, pressure, and wind speed data to make weather predictions, they use genetic variant locations, epigenetic effects, phenotype expression, GWAS, and eQTL data to help researchers predict the efficacy of a drug. It’s not perfect, just like weather forecasts aren’t perfect, but it presents search results in hours that used to take weeks. And it helps researchers find better places to aim their questions and hopefully raise that pharmaceutical batting average. Interestingly, this method works in retrospect. Myung displayed a graph from the genetic database of the gene targeted by the successful cholesterol drug Lipitor — a known pharmaceutical home run.The graph displayed some interesting effects. If geneticists had run a search on this gene using the new Elastic database, they would have seen these effects and recommended this gene as a target for new medications. The database is promising, but it’s not done yet. “This is a start,” said Myung. “We’ve put something in a database and you can search things now.” Myung and Bokuri are still looking for ways to support the geneticists who make those data-driven decisions. In the future they hope to a","locales":"","title":"Stretching the Elastic Stack to Fit the (Very Large and Complicated) Human Genome"}
{"index":{}}
{"author":"Clinton Gormley","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2017-04-18T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2017-04-18","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. — Andrea De Pasquale (@a_de_pasquale) Changes in 5.x: Changes in master: Coming soon: Apache Lucene A is , which triggered a discussion about whether we should still get 6.5.1 out and work on getting 6.5.2 released short afterwards, or whether we should wait for the bug to be fixed before building a new release candidate for 6.5.1. Weak consensus seems to be to wait. Elasticsearch master has been so that we can start verifying what impact it has for us, especially in terms of disk footprint and performance given changes around sparse norms and doc values. The should pick up this change as of tomorrow. , where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2017-04-18"}
{"index":{}}
{"author":"CJ Cenizal","category":"Engineering","publish_date":"2017-04-19T00:00:00.000Z","url":"/blog/componentizing-the-kibana-ui-writing-javascript-with-react","seo_title":"Componentizing the Kibana UI, part 2: JavaScript with React","content":" Components in JavaScript This series is about how the Kibana team is componentizing the Kibana UI. In the , I outlined how we’ve been applying this process to Kibana’s CSS. But components are more than just CSS – they’re JavaScript, too. We went looking for a JS library to help us, and when we came upon React we found it to be a natural fit! React is a library for building user interfaces with JavaScript, open-sourced by Facebook in 2015. In React . This is the same fundamental principle which drives our componentization process. According to the , React allows you to “build encapsulated components that manage their own state, then compose them to make complex UIs.” Let’s explore these ideas in the context of our UI Framework. Encapsulation lets us hide complexity Engineers should be able to wield a UI Framework component like an artist wields a brush. You should be able to apply it to a medium and, with a little skill, expect a positive result. As a corollary, an engineer who’s using a component shouldn’t have to worry about how the component works underneath the hood. Implementation details should be encapsulated, hidden from view. This includes JavaScripty things like state and performance optimizations, but it also means that entire of our stack become invisible. The very markup and CSS that make up our components become implementation details, which are encapsulated by our React components. For example, take a look at how we converted our Button component to React ( if you’re interested in the details). Here’s what the component looked like originally: We just wrapped this markup inside of a React component, like so: And now engineers can use this component like this: By relegating the actual CSS and markup to mere implementation details, we gain three big advantages: Componentization means composition Good components can be defined by many traits. They should be modular, reusable, and ideally adhere to the . But great components are also . This means that many components can be thought of as simple containers, into which any other component or number of components can be injected. In the in this series, I raised the example of a Panel component. This component is just a box with a title. And like any box, it can contain literally anything: text, buttons, a table, or even other Panels. A component is when it exhibits this level of flexibility. The process of componentizing our user interfaces largely consists of figuring out how to compose them out of components. When we look at a complex user interface that needs to be broken apart into components, we start by looking for the containers. And typically, these are the UI elements that become we can extract into components in our UI Framework. By following this simple process, we end up with an ecosystem of components which can be reused and composed together to create complex UIs. React emphasizes composition as a first-class concept. It encourages component developers to design components to be stateless and functional. This results in components which accept dependencies, most notably the children dependency. React supports composition by allowing you to provide components as the children dependencies of other components. The React team has written a terrific article on how they designed React with composition in mind, entitled . If you’re getting started with React, I highly recommend you check it out. I should mention that I also love stateless, functional React components because they’re so easy to demonstrate in the UI Framework. In fact, I copied the above code snippet from our UI Framework documentation site. Unlike an AngularJS directive, which would require creating an Angular app, registering the directive to it, and creating a template in which to declare an instance of the directive, there’s very ","locales":"","title":"Componentizing the Kibana UI, Part 2: Writing JavaScript with React"}
{"index":{}}
{"author":"Haley Eshagh","category":"News","publish_date":"2017-04-18T00:00:00.000Z","url":"/blog/hosted-elasticsearch-services-roundup-elastic-cloud-and-amazon-elasticsearch-service","seo_title":"Hosted Elasticsearch: Elastic Cloud and Amazon Elasticsearch Service","content":" Our product, which runs Elasticsearch and Kibana as a service, has become popular. But there's been some confusion. Elastic Cloud and the Amazon Elasticsearch Service are different offerings, and neither is the same as . To be very clear, the Amazon Elasticsearch Service is not related to Elastic. Elastic does not partner with, participate in, or support, the Amazon Elasticsearch Service. Instead, we offer Elastic Cloud, our hosted Elasticsearch, Kibana, and X-Pack service that runs on AWS (and ). We also support a variety of customers who self-manage their own clusters on AWS. And, again, Elastic Cloud is not the same as the Amazon Elasticsearch Service. In fact, we believe Elastic Cloud brings unique value to mission-critical applications, validating the importance of our offering. We'd like to explain why. Features That Delight with the Projects You Love, Hosted When we talk about Elastic Cloud, we often dive straight into the details about our open source products in the Elastic Stack — Elasticsearch, Kibana, Beats, and Logstash. While they're fantastic, we'd like to start from a different angle: . X-Pack is an extension that bundles together security, alerting, monitoring, reporting, and graph exploration capabilities that extend what you can do with the Elastic Stack. Furthermore, Elastic Cloud is the only offering that ships with free features like and the (with zoom levels you can dial up to 18). Access to these features opens up brave new worlds and possibilities with Elasticsearch and Kibana. And X-Pack features, along with the open source products, get better and better with every release. This is why we take pride in being able to offer the latest versions of Elasticsearch, Kibana, and X-Pack on Elastic Cloud the day a release drops. When Elasticsearch 5.4 or 6.0 or 10.3 ships, you can count on Elastic Cloud having the option to upgrade to it at the same time. Similarly, if there is a security vulnerability, we patch user clusters as soon as the fix is released.   Our ability to get you the latest and greatest the moment it's available is critical to your success. We also know there's more to it than that, whether you need to make a change to your elasticsearch.yml (specific settings are whitelisted in Elastic Cloud), click a single button to upgrade your cluster to the most recent version, restore from an automated snapshot that occurs every 30 minutes, scale your cluster by moving a slider, or, recently, file a ticket in the support portal provided all Elastic Cloud customers. All of this matters because it makes good sense, but also because you've us it matters. And we will always strive to honor that and listen to you. There Is No Compression Algorithm for Experience As an Elastic Cloud user, you are, in a way, benefiting from the wisdom of the relative ancients. Elastic Cloud has been around since 2012 (known as Found in its early life). Amazon's Elasticsearch Service was introduced in 2015. Operational excellence is not something to take lightly or undervalue. There is a lot of work that goes into ensuring that the lights stay on and customers are getting a high level of service. There is no compression algorithm for experience. (Amazon, of course, has been offering services via AWS since 2006. In the context of managing and supporting hosted Elasticsearch, however, our team has a few years head start.) Elastic Cloud is a mature platform. It supports customers like IBM, Fandango, Activision Blizzard, Unilever, Shopify, and more. They put their trust in us to keep their mission-critical systems firing on all cylinders. To make sure that we are providing the uptime our Elastic Cloud premium customers require, we now provide an SLA for Elastic Cloud. And, we do so by measuring cluster-level availability","locales":"de-de,fr-fr,ko-kr,zh-chs","title":"Hosted Elasticsearch Services Roundup: Elastic Cloud and Amazon Elasticsearch Service"}
{"index":{}}
{"author":"Jim Goodwin","category":"Kurrently in Kibana","publish_date":"2017-04-18T00:00:00.000Z","url":"/blog/kurrently-kibana-2017-4-17","seo_title":"","content":" Welcome to This is a weekly series of posts on new developments in the Kibana project and any related learning resources and events. ","locales":"","title":"Kurrently in Kibana: This week in Kibana for April 17, 2017"}
{"index":{}}
{"author":"Kenny Ye","category":"User Stories","publish_date":"2017-04-14T00:00:00.000Z","url":"/blog/elasticsearch-as-android-malware-research-platform","seo_title":"","content":" Our team is responsible for mobile threat research at Trend Micro. Daily work of team members consists of mobile application analysis, virus signature creation, and vulnerability research. Virus signature is the key output which is then integrated with Trend Micro's mobile product to protect our customers from mobile ransomware, banking Trojans, and other types of threats. As of 31 January 2017, we have collected more than 40 million Android applications and of that 20 million have been judged as malware or PUA (Potentially Unwanted Applications). We sourced tens of thousands of APK (Android application package) files each day. How to quickly and effectively find malware in so many applications is a big challenge to the team. Beginning in the second half of 2014, we introduced the Elastic Stack into our backend system. ELK and since that point, the Elastic Stack has a played a very important role for us. We use it to: The following blog will focus on our efforts concerning #4. The Architecture Each Android application will pass a number of analyzers after it enters our system. In turn, each analyzer extracts some attributes from the application. We send these attributes to MongoDB using the hash of the application as the key. Then other modules can fetch them quickly. But if we want to get all the applications which have some specific attributes, MongoDB cannot help here because it is not possible to create index for all the attributes. With Elasticsearch, however, we can easily meet this requirement with all attributes synced and indexed directly into our Elasticsearch 2.4 deployment. In each cluster, one document represents one application. We use the SHA256 hash as the document id. Finally, a web interface facilitates the search and aggregation queries against data in these clusters. The Use CasesCase 1: Hunt malware with similar package To introduce this case, let me provide some background information first. Each Android application is assigned a package name. For instance, we can get the Facebook application on Google Play store with this URL: with \"com.facebook.katana\"as the package name. But package name is not exclusive as different applications can use the same package name as long as they are not installed on the same Android device. With that background in mind, let's look at a specific example of how malware authors can exploit this. Adobe ceased  in 2012 and from then on, the official Flash Player application was no longer available on Google Play. Now, let's dig a little further. Although Flash player has been abandoned by the Android platform, some mobile users still want it to watch the videos in Flash format. Malware authors take advantage of this and create malicious applications with the same or similar package names as the original Flash Player. Careless mobile users are easily deceived to install these malware apps. To verify this, let us check our cluster with below query. { \"size\": 0, \"aggs\": { \"package\": { \"filter\": { \"fuzzy\": { \"PackageName.raw\": { \"value\": \"com.adobe.flashplayer\" } } }, \"aggs\": { \"count\": { \"terms\": { \"field\": \"PackageName.raw\" } } } } } } The fuzzy query is to get all the applications with package name similar to \"com.adobe.flashplayer\" then terms aggregation categorizes them. The output is very interesting. \"con.adobe.flash.player\", \"com.adobe.flashplyer\", \"com.Adobes.flashplayer\" can you distinguish them? [ ... { \"key\": \"con.adobe.flash.player\", \"doc_count\": 6 }, { \"key\": \"com.adobe.flash.player\", \"doc_count\": 4 }, { \"key\": \"com.adobe.flashplayere\", \"doc_count\": 4 }, { \"key\": \"com.adobe.flashplyer\", \"doc_count\": 4 }, { \"key\": \"com.Adobes.flashplayer\", \"doc_count\": 3 }, { \"key\": \"com.adobe.flashplayer2\", \"doc_count\": 3 }, { \"key\": \"com.adobe.ftashplayer\", \"doc_count\": 3 }, ... ] Below is the set of these similar package names. Case 2: Hunt malware with key stringBelow is a piece of reversed Java code from a malicious applicat","locales":"","title":"Elasticsearch as an Android Malware Research Platform"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2017-04-18T00:00:00.000Z","url":"/blog/brewing-in-beats-enrich-events-with-kubernetes-metadata","seo_title":"","content":" Welcome to ! With this weekly series, we're keeping you up to date with what's new in Beats, including the latest commits and releases. Enrich each event with Kubernetes metadataThanks to , there is a to add support for Kubernetes as a processor for Beats. The Kubernetes processor allows Beats to enrich events with metadata coming from the Kubernetes Pod from which the event originated. By default, the Kubernetes processor adds the following metadata: Depending on the Beat, the  Kubernetes processor can add additional information. For example for Filebeat, it takes the fields, extracts the container ID, and uses it to retrieve metadata about the Pod from which the log message originated. This will be released as Beta in 6.0. Monitor Elasticsearch with MetricbeatA new module is added to Metricbeat for monitoring Elasticsearch. It exports metrics about: The Elasticsearch module in Metricbeat exports only a few basic metrics, and for a better monitoring experience, we recommend you to use. This will be released as Beta in 6.0. New community Beat: Kafkabeat is built on top of libbeat infrastructure to read the streaming events stored in Kafka and send them to Elasticsearch. An option would be to use Kafkabeat in the following scenario: Filebeat >> Kafka >> Kafkabeat >> Elasticsearch. Other changesRepository: elastic/beatsAffecting all BeatsChanges in master: MetricbeatChanges in 5.x: Changes in master: PacketbeatChanges in 5.x: WinlogbeatChanges in 5.x: FilebeatChanges in 5.x: PackagingChanges in 5.x: InfrastructureChanges in master: DocumentationChanges in 5.0: Changes in 5.3: Repository: elastic/gosigarChanges in master: ","locales":"","title":"Brewing in Beats: Enrich events with Kubernetes metadata"}
{"index":{}}
{"author":"Jim Goodwin","category":"Kurrently in Kibana","publish_date":"2017-04-13T00:00:00.000Z","url":"/blog/kurrently-kibana-2017-4-10","seo_title":"","content":" Welcome to This is a weekly series of posts on new developments in the Kibana project and any related learning resources and events. A colorful look at visualizations today, and what's coming with point series in : — elastic (@elastic) Even though this search has over a million hits, only 500 are retrieved and there is no indication that they are limited. We now show the actual total number of hits. ","locales":"","title":"Kurrently in Kibana: This week in Kibana for April 10, 2017"}
{"index":{}}
{"author":"Clinton Gormley","category":"Engineering","publish_date":"2017-04-11T00:00:00.000Z","url":"/blog/multi-data-path-bug-in-elasticsearch-5-3-0","seo_title":"Multi data path bug in Elasticsearch 5.3.0","content":" If you use a custom data path with Elasticsearch 5.3.0, you may be subject to a bug which could cause data loss unless properly handled. The bug is triggered as follows: The command line setting is used to tell Elasticsearch which default data path to use unless is configured either in the config file or on the command line. The bug occurs because , when specified as an array, is merged with instead of replacing it. ","locales":"","title":"Multi data path bug in Elasticsearch 5.3.0"}
{"index":{}}
{"author":"Dale McDiarmid","category":"Engineering","publish_date":"2017-04-11T00:00:00.000Z","url":"/blog/operational-analytics-with-elasticsearch-at-elasticon-2017-part-1","seo_title":"Operational Analytics at Elastic{ON}","content":" Each year at Elastic{ON}, we use demos to illustrate the capabilities of the Elastic Stack, hoping to both inspire attendees as well as provide discussion points around new features, use cases and common technical challenges. This year we had 4 demo booths, focussed on search, security analytics, operational analytics and Elastic Cloud Enterprise. This blog series focuses on the operational analytics demo, separating the discussion over two blog posts — the first post revolves around the \"Architecture and Data Input\", while the second focuses on \"Visualizations (inc. custom Tile Maps)\" and \"Final Thoughts\". This is the first post in the blog series, expanding on the architecture and data ingestion pipeline. Overview But before we jump in, a little context on this demo. For anyone that has ever tried building an analytics demo, you know that finding a compelling dataset for a demo is a daunting task. And that was the case when we set out to build an Operational Analytics demo for Elastic{ON}. And then we realised, \"Hey, we are at Elastic{ON}, an event that will generate plenty of real time operational data that could be used in so many ways to improve the attendee experience.\" A demo that was connected to the event itself also let us share realtime insights with attendees during the course of the 3 days. Post analysis of the dataset would also allow us to improve the experience for next year by addressing common marketing questions such as — what's the right mix of business and technical talks? Which talks should go in which halls based on anticipated interest? To achieve this we focused on two data sources Sentiment data — understanding audience feedback The latter represented the least technical challenge. We decided to deploy 8 , placing these at the entrance to each stage for session feedback. Attendees were encouraged to tap these on their way out to let us know how they liked the talk. Ingesting this data into Elasticsearch involved a simple script which periodically polled the vendor-supplied API for changes. We'll return to insights from this sentiment capture in future blog posts, but here's a sneak preview. Elastic{ON} — Sentiment AnalysisNetwork data — Crowd movement & engagement Background Using network analysis we wanted to identify crowd activity hotspots and general movement of attendees, as well as confirming a few hypotheses regards attendee engagement. Capturing this traffic had many more moving parts and was technically more involved that polling data from the sentiment consoles. For the rest of this post, we will therefore focus on the data capture for this aspect of the demo. With no knowledge of the WiFi infrastructure at Pier 48, we reached out to the vendor for the intended network configuration, after identifying the following requirements: The vendor, , planned to deploy around 35 (AP), located strategically throughout the venue to provide complete Wifi coverage. These would be coordinated by a , connected to a main Juniper switch responsible for all external routing. Brown Pelican provided us with the mac address and location of the access points ahead of time. Packet data Capturing all packet data was relatively straightforward and would allow us to solve requirements 1 and 2. We installed a small server with 2 interfaces next to the Juniper. The first interface, configured in promiscuous mode, was connected to the Juniper management interface and configured to receive a copy of all outbound packet data passing through the switch — effectively a network TAP. Packetbeat, running on this server, was configured to listen on this interface and send all data to an Elastic Cloud instance over the second interface connected to the main network. Sensitive to privacy concerns, we limited traffic collection to DNS requests on port 53 and http traffic on ports 80, 8080","locales":"","title":"Operational Analytics with Elasticsearch at Elastic{ON} 2017 - Part 1"}
{"index":{}}
{"author":"Vianney Bajart","category":"User Stories","publish_date":"2017-04-11T00:00:00.000Z","url":"/blog/software-asset-management-with-elasticsearch-at-red-mint-network","seo_title":"","content":" When your company has reached a critical size with an IT infrastructure that includes a substantial number of workstations and servers, managing your software assets and getting the real inventory picture can sometimes be a pain. By the time an audit comes around (e.g. on demand from Microsoft), it might be too late if you don’t have the analytics to output accurate history data. How many users for a specific software application? What is the usage time per user? Do you have more running instances than the number of purchased licenses allow? Is anyone running illegal copies of the software? Or conversely, have you purchased more licenses than you actually need? If you can’t produce this information, you might find yourself exposed to excess costs due to the paying of unused licenses, billing adjustments, and legal fees. We’ll explain how we enable smart, and pragmatic software asset management with and SDN (Software Defined Networking) telemetry. Network as a data source Fortunately, a lot of software is quite verbose. It constantly contacts its vendor’s servers for various purposes such as authorization requests, update checks, telemetry, or access to cloud services. This generates network traffic that can be dissected by a VNF (Virtual Network Function) in the Internet Service Provider’s infrastructure explored in Elasticsearch. Knowing the network packet’s source (where the software is installed), destination, and timestamp, makes it possible to know when a software is up and running just by detecting meaningful events on the network link. For instance, we’ve noticed that when or CC are running, they open a TLS-encrypted session with : { \"@timestamp\": \"2017-03-06T12:42:05.230651762+01:00\", \"track_id\": \"fa9b4f9539a18daeb7578e47ea2fb0b6544ea527\", \"type\": \"track\", \"track\": { \"sni\": \"ans.oobesaas.adobe.com\", \"appname\": \"SSL\", \"host_hmac\": \"71b5e991310e75d05c3e242ab1fc86a5dce6f3e6\", ... }, ... } Here, we use the anonymized field to identify the data source (computer, or tablet, etc.) and we use the of the certificate extracted from the TLS handshake to identify the destination. shows this exchange being performed every 9 minutes: That’s enough information to determine the number of users on the network and the usage times in a given time window! Computing the number of users In this example, we want to compute the number of users day by day in the previous week. First, we filter the data to select only those objects that match the relevant SNI and requested date range. The is used to define a relevant range (from to ). Then, we use the aggregation to gather data on daily buckets. Finally, computing the number of users is a straightforward matter using the aggregation on the field. Let’s query Elasticsearch to get the number of Adobe software users for each day of the previous week: { \"size\": 0, \"timeout\": \"1s\", \"aggs\": { \"telemetry\": { \"filter\": { \"bool\": { \"must\": [ { \"range\": { \"@timestamp\": {\"gte\": \"now-7d/d\", \"lte\": \"now/d\"} } } ], \"should\": [ {\"match\": {\"track.sni\": \"ans.oobesaas.adobe.com\"}} ], \"minimum_should_match\": 1 } }, \"aggs\": { \"time_slot\": { \"date_histogram\": { \"field\": \"@timestamp\", \"interval\": \"day\" }, \"aggs\": { \"user_count\": {\"cardinality\": {\"field\": \"track.host_hmac\"}} } } } } } } Computing usage times To compute usage times, a Time-To-Live value must be defined. If no signal is seen during this time period, the software is considered inactive. In the case of a periodic signal, the TTL must be slightly longer than the signal period to avoid glitches due to network latency or host slowdowns. The example below uses a 10-minutes TTL (longer than the observed 9-minute TTL in Adobe®). Let’s use the aggregation to compute the usage time per user and return the average value. This type of aggregation can be used to implement a Map/Reduce model with a combination of four scripts written in the language. init ","locales":"fr-fr","title":"Software Asset Management with Elasticsearch @ Red Mint Network"}
{"index":{}}
{"author":"Haley Eshagh","category":"User Stories","publish_date":"2017-04-10T00:00:00.000Z","url":"/blog/elasticsearch-siem-architecture-of-a-nonprofit-security-at-the-nature-conservancy","seo_title":"SIEM Architecture with Elasticsearch at a Nonprofit","content":" With 4,000 employees distributed across 70 countries, Nick Waringa and Daniel Shirer of (TNC) are not scientists. They are information technology and security experts, who spend their days thinking about ways to effectively understand the activity happening across their networks and systems.They presented how they're finding success with Elastic products at . TNC is a science-driven organization dedicated to conservation efforts worldwide, Waringa explained, all of which generate a multitude of data. And while data about endangered flying squirrels, fishery management, forest conservation, and strip mining plans might inform a relatively low risk profile for TNC, data about donations, donors, and protected land certainly increases it. With offices and outposts all around the world, often in areas with low bandwidth and limited or satellite connectivity, Waringa and Shirer’s challenge is growing. “We have over 200 field offices on-WAN, we have 250 offices that are off-WAN, and we have 533 home offices,” explained Waringa. And with limitations around centralization, directory services, and domain management, “How do you tie a user to a computer and understand the behaviors going on?” Waringa asked. From users unknowingly accessing a bad host to attacks on their network or even data exfiltration, Waringa and Shirer are tasked with protecting TNC’s sensitive information. With a background in SIEM, Waringa has seen his fair share of security vendor solutions. “They would bring in a lot of resources that were generalized, but they would not help you understand your network at the end of the day.” And that’s ultimately their goal: a complete and relevant picture of what was happening — from network paths and creation to client behavior. That’s where Elastic Stack, X-Pack, and Elastic support services offered them capabilities and flexibility they were looking for. Shirer described the architecture for their network security sensors in their field offices. Each is running Snort (for threat intelligence) and Bro IDS (their network flight recorder), Shirer explained. “[Bro is] identifying different types of data out of the network stream like HTTP, DNS, DHTTP — any files it finds, flying squirrel data, you name it,” he said. That data is then shipped by combination of , , and a called Unifiedbeat, which handles Snort Unified 2 logging, to Logstash for pre-processing before reaching Elasticsearch and Kibana for analysis and visualization. They use a similar flow for some of their syslog sources. Shirer dove deeper into their hardware and software design decisions — including their use of consumer gaming hardware — and how they configured it for their distributed and remote workforce. He highlighted their use of the Logstash sleep, translate, and dissect plugins, in addition to their use of Kibana heatmaps. A happy outcome to their work, Waringa noted, is a strengthening of their relationship with the operations team, which is interested in adding their data to the Elastic Stack. It opens up valuable opportunities with operational metrics for both teams. “It gives us security visibility and context,” Waringa explained. “It also gives the security guys the logs they need and...the operations guys what they’re looking for.”Waringa closed with possibilities for expanding their use of the Elastic Stack. “We have tons of data that’s not IT, ‘geeky data’ that we could be doing things with,” he said. With preserves all over the world, could TNC deploy sensors into the field and collect the data that’s relayed via drone flybys into Elastic? Could they do micro-level climate change monitoring on a scale not seen before? Could they improve other aspects of the business and use to help better inform TNC spending decisions? ","locales":"","title":"The Elasticsearch SIEM Architecture of a Nonprofit: Security at The Nature Conservancy"}
{"index":{}}
{"author":"Clinton Gormley","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2017-04-10T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2017-04-10","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. — elastic (@elastic) Elasticsearch has gained the ability to . This is the next step towards reducing intermediate memory consumption during search execution that will allow us to related to the number of shards that are involved in the execution of a single search request. Ultimately this will allow search executions against large number of shards or in other words hitting the entire cluster with sustainable memory consumption on the coordinating node. After , the _field_stats API has been deprecated in favour of a more lightweight API that only exposes unique field names across indices/types, and whether they are and/or , called the API. One important bit of the _field_stats API was the ability to return the min/max values of fields alongside filtering capabilities so that Kibana could figure out which time-based indices a query would match based on its time filter. This was important as including too many indices in a search request could cause memory issues since Elasticsearch would have to hold all shard responses in memory on the coordinating node. In order to work around this issue, we introduced of shard responses in order to keep memory usage on the coordinating node under control. As a consequence, Kibana should now be able to query all indices all the time, and Elasticsearch will make sure to execute things efficiently. A common misunderstanding is that this change will make queries more costly since they now have to go to all shards. However it was already the case with the API. Now, queries that have a time filter that does not match any documents will return instantly, so we do not expect any performance regression. Rally gained support for modelling of operations. With this change you can model Poisson processes with Rally out of the box. Poisson processes are often used to model arrival processes of independent actors (think coffee shops, telephone hotlines and most importantly for us: Web services) with a defined mean \"arrival rate\" (i.e. throughput). They play a central role in . Changes in 5.x: Changes in master: Coming soon: Apache Lucene With , we want to make sure that we are not using Lucene in ways that won't be allowed anymore in Lucene 7. This will also give Lucene 7 more integration tests before it gets released. For the record, you can read about what Lucene 7 will bring at . , where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2017-04-10"}
{"index":{}}
{"author":"Adrien Grand","category":"Engineering","publish_date":"2017-04-10T00:00:00.000Z","url":"/blog/better-query-planning-for-range-queries-in-elasticsearch","seo_title":"Query Planning for Range Queries in Elasticsearch","content":" If you are a frequent reader of this blog, you probably know that a lot of effort has already been put into . This time we are going to talk about recent improvements to the specific yet common case of range queries when they are used in conjunctions, ie. when they are ed with other queries. Why are conjunctions different?The way conjunctions work is by iterating over matches from the most selective clause and verifying whether other required clauses match too. This means that we need two operations to be fast in order for conjunctions to be fast as well: Ranges have been problematic so far because they could only iterate over all matches efficiently, not verify individual documents. Numerics are indexed using a tree structure, called \"points\", that is organized by : it can help you find matching documents for a given range of value, but if you want to verify whether a particular document matches, you have no choice but to compute the set of matching documents and test whether it contains your document. Said otherwise, even if you only need to verify a few documents, you need to evaluate all documents that match the range! There must be a better wayGood news is that in a majority of cases, Elasticsearch also has doc values enabled on numeric fields. Doc values are a per-field lookup structure that associates each document with the set of values it contains. This is great for sorting or aggregations, but we could also use them in order to verify matches: this is the right data-structure for it. Should we just use doc values all the time to execute ranges? No. Doc values are a form of columnar storage, not an indexed structure. If you want to evaluate all documents that match a range with doc values, there is no other choice but to perform a linear scan and verify every document, which is slow. In summary here is what a better query plan for range queries would look like: As a consequence, we introduced a new mechanism that allows queries to know whether they will be used for sequential access (iterating over all matches) or random access (verifying that some documents match) as well as a query wrapper called that wraps a query that is good at iterating over matches and a query that is good at verifying matches. Then delegates to the appropriate one depending on how it is used. This query wrapper will be used transparently by Elasticsearch on numeric fields that are both indexed and have doc values, which is the default. Something that is interesting to notice here is that this query planning optimization does not only depend on the fields that are used and their cardinalities, it goes further and estimates the total number of matches for each node of the query tree in order to make good decisions. This means that taking a query and slightly changing the range of values might completely change how the query is executed under the hood. BenchmarksObviously all this can't be perfect. It relies on the fact that the cost of iterating over all matches or verifying individual documents is the same for all queries, so while the theory fits nicely, it could be that practical results are disappointing. Let's see how this works in practice: For this benchmark, I took a 10M documents subset of Wikipedia with a body field that contains the text of the article and a numeric field that stores the last time the article was updated. A query that has a term query that matches 10,000 documents (0.1% of the index) is intersected with ranges that match various numbers of documents. On the X axis is the number of documents that the range matches, and on the Y axis is the time it took to run the query. Then I performed 3 measurements: once using an index structure all the time (\"Points\"), once using doc values all the time (\"Doc values\") and once using the new (\"Points + Doc values\"). When reading this chart, beware that it uses a logar","locales":"","title":"Better Query Planning for Range Queries in Elasticsearch"}
{"index":{}}
{"author":"Aaron Mildenstein","category":"Releases","publish_date":"2017-04-10T00:00:00.000Z","url":"/blog/curator_5_release","seo_title":"","content":" I am excited to announce the release of ! Let’s dive right in with the changes, shall we? Breaking Changes There’s really only two breaking changes to be aware of: So, why does Curator 5 only work with Elasticsearch 5.x? On one hand, reverse-compatibility is hard. Another difficulty is that sometimes a new feature doesn’t work with an older version of Elasticsearch, but that feature is in the docs, even with a big warning that says, “this feature doesn’t work with version X.” As a result, new users can have a bad experience, as they struggle to make something work for hours, and then ask for help in the only to learn that the feature will not work for them due to a version mismatch. To save everyone time and aggravation, Curator is trying to get on the unified release schedule (though it’s still a few versions behind). What’s the same? One of the nice things is that the configuration format remains unchanged. You are free to use and exactly as before, without having to change any configuration. The change from Curator 3 to Curator 4 was a jarring one for many users. The improvements were effective, however, and there hasn’t been a need to change the configuration syntax. Instant upgrade win! What’s new? Now this is where the exciting part comes! New features! Reindex Perhaps the biggest new feature in Curator 5 is the addition of the . The is powerful. This is just an example of a simple, local reindex with manually selected indices. “But,” I hear you say, “this is Curator! It should support filtered index selection!” And you’re absolutely right. That’s accomplished like this, with the placeholder: This example will reindex all of the daily Logstash indices from March 2017 into a monthly index. You can add all kinds of extra processing to these reindexing operations. Curator should support all possible configurations, save one only, and that is manual slicing (which is likely to be a pretty rare need, since automatic slicing is available). Want to reindex through an ingest pipeline? No problem? Reindex from remote? Oh, that’s the best part! This example will pull from with the provided credentials you have started the local node with the following setting in the file. If the setting was not present when the Elasticsearch node was started, it means that the node must be restarted after this setting has been added (it cannot be done dynamically): You whitelist remote nodes in order to be able to reindex from remote. In this case, would likely be the same IP or host name as in the reindex . Curator will test for the presence of the index, and if the task successfully completes, but that index is not found, it will log an error guessing that whitelisting is not set up properly. “But,” I hear you say again, “what if I want to use Curator’s index filters to select indices on the remote side?” : This example will reindex all of the daily Logstash indices from March 2017 from into a monthly index on the cluster. Generally speaking, the Curator should be able to perform a remote reindex from any version of Elasticsearch, 1.4 and newer. Strictly speaking, the Reindex API in Elasticsearch is able to reindex from older clusters, but Curator cannot be used to facilitate this due to Curator’s dependency on changes released in 1.4. However, there is a known bug with Elasticsearch 5.3.0 not being able to reindex from remote clusters older than 2.0. The patch will be available in Elasticsearch 5.3.1. Earlier versions of Elasticsearch 5.x do not suffer from this bug. There is a ton of documentation regarding what can be put in a , which has even more examples than this. Rollover Lots of you have been asking for , and here it is! The conditions are described in the . Read more in the . Date Math in Many users were eager to be able to create indices in Curator, but were unable to create indices with a future timestamp in the index name. Credit for this actually goes to the Elasticsearch team. For example, ","locales":"","title":"Announcing Curator 5"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2017-04-10T00:00:00.000Z","url":"/blog/brewing-in-beats-monitor-kubernetes-with-metricbeat","seo_title":"","content":" Welcome to ! With this weekly series, we're keeping you up to date with what's new in Beats, including the latest commits and releases. Monitor Kubernetes with Metricbeat Thanks to , who worked closely with Carlos, there is a to add support for Kubernetes monitoring in Metricbeat. The new module interrogates the kubelets, with more modules planned for the other aspects of Kubernetes. The kubelet is the primary “node agent” that runs on each node in Kubernetes. It gives you details about the running containers and the available pods like the CPU usage, memory usage, bytes exchanged over the network, about filesystem or about the logs capacity. To get it started, you just need to configure kubelet endpoint and enabled the metricsets you are interested in: - module: kubelet metricsets: [\"node\",\"container\",\"volume\",\"pod\",\"system\"] hosts: [\"localhost:10255\"] enabled: true The kubelet module is planned to be released in 6.0. Add processor for exporting timezone The processor can be used to add the timezone to events. This can be used to inform downstream systems on how the timestamp fields should be interpreted. The timezone is added in the field, currently not configurable. processors: - add_locale: Add DNS dashboards in Packetbeat We added two new Kibana dashboards to monitor the DNS queries with Packetbeat. An interesting one is about DNS tunneling that was added with this . Audit logs in Gosigar Gosigar, which is our library for getting operating specific data, is getting support to get and the logs from the Linux audit framework. This is an alternative to the Filebeat module we added last week. Having access to the audit framework from Go code opens a lot of interesting possibilities, for example, capturing short lived processes and connections in Metricbeat. Other changes in the elastic/beats repository Affecting all Beats Changes in master: Heartbeat Changes in master: Winlogbeat Changes in master: Metricbeat Changes in master: Packetbeat Changes in master: Filebeat Changes in 5.3: Changes in master: Infrastructure Changes in master: Documentation Changes in 5.3: Changes in master: ","locales":"","title":"Brewing in Beats: Monitor Kubernetes with Metricbeat"}
{"index":{}}
{"author":"Shay Banon","category":"News","publish_date":"2017-04-06T00:00:00.000Z","url":"/blog/elastic-google-partner-to-deliver-elastic-cloud-on-gcp","seo_title":"","content":" Heya Today we are announcing a significant step forward for our users. A few years ago, we joined forces with the best hosted and managed Elasticsearch service today known as the Elastic Cloud. We've always had the that Elastic Cloud could be run on multiple cloud platforms or within a customer's enterprise. I'm thrilled to announce a new partnership with Google to offer on to give developers even more choice. We chose to extend Elastic Cloud to GCP because it is one of the most innovative () and open source friendly cloud platforms, and we see growing demand from our community. Today, developers have the ability to use the and on GCP with a self-managed deployment. With this partnership, users will have the option to launch a customized and managed cluster via Elastic Cloud on GCP with a few simple clicks and settings. This offering will be available in the second half of 2017, and users who run Elastic Cloud on GCP will have access to the latest versions of our open source software, X-Pack features, and Elastic's support services. Until then, about this exciting new offering. ","locales":"de-de,fr-fr,ja-jp,ko-kr","title":"Elastic + Google Partner to Deliver Elastic Cloud on GCP"}
{"index":{}}
{"author":"Dean Prosenica","category":"Culture","publish_date":"2017-04-05T00:00:00.000Z","url":"/blog/the-official-xpack-basketball-team-they-came-they-saw-they-clustered","seo_title":"The Official X-Pack Basketball Team: They Came, They Saw, They Clustered","content":" Some say that their basketball is really just a cluster, and that being distributed across three offices makes them highly available, all we know is they’re called the X-Pack.  It is difficult to say when the seed was first planted, maybe a dream one restless London night, probably during a second attempt at a first round interview, but more than likely over a shared beverage and a lot of hope. The Laurel and Hardy like duo of Ryan and Dean had one desire, to bring Elastic to the courts. At the mention of the 32nd Baros International Tournament in Rotterdam, the two began deliberating the inception of X-Pack. Less than a 24 second shot clock later it was decided and the call went out to the rest of the Elastic family, “Ballers needed, Rotterdam calling”.  The Elasticians of Amsterdam, Paris, and London came together in spectacular fashion to form Elastic’s first basketball team, representing eleven nations across four continents. But none of this would have been possible without the silent partners across the pond dreaming that same dream, the team would be forever grateful.  X-Pack: After an intense pre-tournament training session, the team felt they were ready to undertake any challenge. The plan was simple, enjoy it and bring home the silverware, preferably the kind that divided into three with ease. This was no fairytale, the first night had heartbreak written all over it. Taking a little longer to get up and running than your typical Elastic POC, it was clear to all that something was off. Possibly tired from travelling, X-Pack dropped their first two games by the narrowest of margins, severely hurting their chances of qualifying. With only one game left that night, it would seem impossible to gain momentum if they failed to get this next win. Coach Liubinskas had a plan. With 26 years of experience under his belt (probably more than the rest of the team combined) and two games in, he knew the strengths of each player and with some careful configuration they were ready to match up with any other team.  Possibly facing their biggest challenge yet, is not what you would have guessed by the look in their eyes. Bolstered by the words of Coach Liubs, Christelle found ease crossing defenders and with a swift euro-step finding her way to the hoop, leaving a few sore ankles in her wake. This alone may not have worried their opponents but they had no answer to the speed of Sam on the fast break (sometimes even the ball struggled to keep up) running the floor like a man possessed, he put up a series of points to round off an impressive offensive display. They were now ready to show their strength on both ends of the court. An impressive defensive team performance, led by Theresa hurling herself at the ball and positioning herself perfectly to rack up offensive charges for the other team (at the cost of a few bumps and bruises, some even inflicted by team members that shall remain anonymous). This allowed X-Pack to close out the game and pick up their first win of the tournament. Not a moment too late, making the tally one win from three on opening night, this would have to do. On the recommendations of Coach, the team got an early night, plenty of hydration, and rest to maximize the chances of making the playoffs (the truth, the whole truth, and nothing but the truth).  Reinvigorated from a good night’s rest (seriously sticking with it), X-Pack hit the courts with one goal in mind. The following three games would see them continue the momentum of the day before, and starting to see a little flair emerging. Starting with isolation plays utilizing the shake n’ bake skills of Elena, left her defender on skates as she (literally) danced her way to the bucket, ensuring everyone knew she was “a professional basketball player”. Now, some might know her as","locales":"","title":"The Official X-Pack Basketball Team: They Came, They Saw, They Clustered"}
{"index":{}}
{"author":"Peter Pišljar","category":"Engineering","publish_date":"2017-04-04T00:00:00.000Z","url":"/blog/awesome-new-kibana-visualizations-heatmap-and-point-series","seo_title":"New Kibana Visualizations: Heatmap and Point Series","content":" With we started our journey of adding new visualizations to Kibana. But we did not stop there. In 5.2 we introduced Heatmap visualization, and in 5.4 we plan to bring in much-requested enhancements for point series visualizations (e.g. line, area, bar) supporting multiple Y axes, mixed charts, and other awesome stuff. Let’s look at some of these additions from a use case perspective. Heatmap ChartHeatmap charts allow you to plot individual bucket values as a color. Normally, in a chart a color would represent a single metric. 404 errors get one color and 200 responses get another. With heatmap, the color represents the aggregation value of a bucket and not to which series it belongs to. Let’s start with a very simple use case. Imagine we want to plot a chart representing visitors to our website. We want to see how many visitors we are getting per top 5 operating systems. One way to do this would be to use a pie chart. But we want to see this over the course of days. We could go with vertical bar chart where each bar would represent a day and it would be split in 5 sections representing operating systems. But a heatmap would probably represent this better. Select Date Histogram aggregation for your X-Axis aggregation and select Term aggregation on the machine.os field for your Y-Axis aggregation. Heatmap chart offers quite a few ways to customize it. You can choose among predefined color schemes or create your own by overwriting existing colors in the legend. It’s possible to define custom ranges for your color buckets and you can show labels on the buckets, just to name a few of them. Its best to take a look on your own and dive in. Point Series ChartsWith Point Series charts we mean the Area, Line and Vertical Bar charts. And with 5.4 we will have added another one to the group - Horizontal bar chart. We also added tons of options to them to make these chart types  far more powerful. Though the 5.4 release hasn’t shipped yet, you can already see these changes in master and Kibana nightly snapshot builds, and we want your feedback! Multiple Y AxesWhenever you are trying to plot two metrics on the same charts which have values in different ranges, this feature is a must have. Let’s start by creating a Vertical Bar Chart. Select Date Histogram for your bucket aggregation and add another metric aggregation next to the Count which is provided by default in X-Axis. In this example I added Average of machine.ram. Clicking on the play button will produce a chart where you can't really see the Count as it’s too small in comparison with Average machine.ram. But now you have another tab in visualize editor called 'Metrics & Axes'. Clicking it you will find the list of metrics. Select Average machine.ram and in the Value Axis drop down choose New Axis. Another Y Axis was just added and clicking on the play button again you will see much nicer chart. Each value axes offers you plenty of options to look into, so do not be afraid to click around or look to the documentation for explanation of what each of them does. Multiple chart types per plotIn the past many have gone thru the frustration of setting up a line chart, just to find out that it would look better like a bar. The only way to do it was to reconstruct your whole chart from scratch again. No more! Now you can easily change the type of chart used to plot each metric. This doesn't just allow you to switch from line to bar and vice versa, but allows you to mix different charts as well. If we continue with the example from above, changing the Average machine.ram metric type to line produces a chart that is actually easy to read now. Horizontal (bar) chartAnother cool thing you can do with your charts is position your axes where ever you find them suitable. And by changing your X-Axis position to be on the left you actually get a horizontal chart. Yup, it\\xE2","locales":"","title":"New Kibana Visualizations: Heatmap and Point Series"}
{"index":{}}
{"author":"Suyog Rao","category":"The Logstash Lines","publish_date":"2017-04-04T00:00:00.000Z","url":"/blog/logstash-lines-2017-04-04","seo_title":"","content":" Welcome back to The Logstash Lines! In these weekly posts, we'll share the latest happenings in the world of Logstash and its ecosystem.This week we merged the backend code which adds support to convert the existing Logstash config into a graph based representation. This new config representation in JSON can then be used to draw the pipeline visually, along with its metrics in the Kibana. The project is also called or LIR internally and can form the foundation for many new features (in addition to the visualizer). For example, we can now start thinking about a UI config editor, rewriting the pipeline execution in Java, new config language, etc. Stay tuned for updates in this area. If you are interested, you can also check our Elastic{ON} 17  for live demos in this area! ","locales":"","title":"Logstash Lines: Foundation to pipeline viz, HTTP proxy fixes"}
{"index":{}}
{"author":"Dale McDiarmid","category":"Engineering","publish_date":"2017-04-04T00:00:00.000Z","url":"/blog/integrating-elasticsearch-with-arcsight-siem-part-4","seo_title":"Integrating Elasticsearch with ArcSight SIEM - Part 4","content":"  provide examples of setting up alerts for common security threats using the alerting features in X-Pack. Following on from our , where we attempted to identify a successful login from a remote IP address, we introduce a more complex requirement to identify a sequence of events on the same authentication logs.  As you will see below, the watch can get quite complex very quickly. It is for this reason, in part, that we are so excited about the arrival of . Whilst threshold-based event notification is powerful, such as triggering a notification after a successful login if there were successful logins previously, the ability to identify anomalous behaviour automatically without having to define specific conditions simplifies the experience of the user and reduces the need to create complex rules. As we roll out machine learning broadly, we will provide similar examples to this post. Detecting a Successful Brute Force Login This examples uses the same dataset and fields as the previous post. To recap: CEF:0|Unix|Unix|5.0|cowrie.login.success|login attempt [root/!root] succeeded|Unknown|destinationUserName=root externalId=15 startTime=Nov 15 2016 19:22:29 destinationHostName=elastic_honeypot destinationAddress=192.168.20.2 deviceReceiptTime=Nov 15 2016 19:22:29 deviceTimeZone=Z transportProtocol=TCP applicationProtocol=SSHv2 destinationServiceName=sshd devicePayloadId=15 message=login attempt [root/!root] succeeded categoryOutcome=success categoryBehaviour=cowrie.login.success sourceTranslatedAddress=192.168.1.105 sourceAddress=192.168.1.105 deviceDirection=1 cs1=0 cs1Label=isError cs2=SSHService ssh-userauth on HoneyPotSSHTransport,2,192.168.1.105 cs2Label=system cs3=!root cs3Label=password cs4=111f70f0 cs4Label=session The values of and for our field represent a failed and successful login respectively. The fields and indicate the target server and logged in user respectively. Note that while our example data is from a single server, the provided watch could be applied to a larger infrastructure with potentially hundreds of servers. Finally, the indicates the time at which the event occurred. This field will be be parsed by our Logstash config file into the field. Last month’s watch, which alerted on successful logins, would likely result in a significant number of false alarms requiring investigation if we relied on this technique to identify potential risks for investigation. We therefore refine our alert to look for a specific suspicious sequence where “N failed logins attempts are followed by a successful login within an X minute window”. This should give us a relatively reasonable indication that a brute force attack has occurred and been successful. For the purposes of example, we assume X to be 5 minutes. To identify this discrete set of events with X-Pack (using the watcher API) we’ll need to use a combination of Elasticsearch aggregations and Painless scripting.   Using the same iterative process as described previously, we first identify the input and query to capture the relevant data before evaluating the response with a condition to determine whether the watch fires an action. Next we extract the data points of interest through a transformation before finally taking action. Step 1 - Set Up Watch Trigger and Watch InputFor a user based analysis of events, we’ll need a summary of the failed and successful logins per user. The following Elasticsearch query provides the appropriate summary: GET cef-*/syslog/_search { \"query\": { \"bool\": { \"filter\": [ { \"terms\": { \"categoryBehaviour\": [ \"cowrie.login.success\", \"cowrie.login.failed\" ] } }, { \"exists\": { \"field\": \"destinationUserName\" } } ] } }, \"aggs\": { \"users\": { \"terms\": { \"field\": \"destinationUserName\", \"size\": 1500, \"min_doc_count\": 4 }, \"aggs\": { \"times\": { \"terms\": { \"field\": \"@timestamp\", \"size\": 15000, \"order\": { \"_term\": \"asc\" } }, \"ag","locales":"","title":"Integrating Elasticsearch with ArcSight SIEM - Part 4"}
{"index":{}}
{"author":"Rashmi Kulkarni","category":"Culture","publish_date":"2017-04-03T00:00:00.000Z","url":"/blog/django-girls-silicon-valley-and-elastic-a-coachs-perspective","seo_title":"","content":" Meeting and coaching a diverse group of fantastic women at the office on March 5 was a truly enriching experience for me as a coach. Thirteen Elastic coaches from around the world came to work with nearly two dozen attendees. The atmosphere was very welcoming, and everyone was exceptionally motivated and eager to learn. Prior to this event, I had an opportunity to give a group talk about the to another set of diverse participants via pre-recorded video conference. It was very well received and boosted my confidence to get involved more in this organization. Since Elastic was hosting this time, I was all the more excited. For me, the most rewarding moments were when the Django Girls' faces lit up in excitement with each success. It's a feeling programmers are very familiar with: the moment something you've never done before in code starts working and your enthusiasm spikes. Getting to watch those \"aha!\" moments was the only reward the coaches needed! This one-day workshop was organized by our awesome developer relations team, and , and a member of our fantastic events marketing team, . They took care of the innumerable tasks required to ensure a successful event. They processed participant applications, recruited sponsors and mentors, set up a pre-event installation party to meet and greet the participants, and made sure there was food, drink, and swag aplenty to keep everyone energized during the workshop. Kudos to Michelle, Elyssa, and Luisa — your seamless organization skills deserve an entire blog post! workshops are designed to give women an amazing first coding experience and inspire them to fall in love with programming. Of course there are many men who would benefit from the very same tutorials, and everyone is welcome, but Django Girls is primarily about encouraging more women to build the skills needed to pursue technical roles. The percentage of women in computer-science-related fields is regretfully small, and that can be intimidating for women who want to start coding. The Django Girls project is dedicated to changing that. The workshop tutorial enables aspiring female programmers to build their and develop skills in Python, Django, HTML, and CSS. Attendees work in small groups and are coached by professionals who work with these technologies on a regular basis. For this event, the coaches met their teams via video conference for a pre-workshop \"installation party\" to get everyone set up with the tools they needed for the big day. The installation party helped both the coaches and the attendees to get to know each other personally. The rapport was already built, and both parties knew what to expect. It made breakfast more celebratory, because we could greet each other by name and recognized familiar, friendly faces.   Despite the huge amount of information that the participants were introduced to, they all made fantastic progress. They built a simple blog application and deployed it as a web app on . The buzz in the room was brilliant, and by the end of the day, everyone had a serious sense of accomplishment — the coaches couldn't have been more proud of their teams!   The women who participated in the workshop had a wide variety of experiences and professional backgrounds. We had two mother-daughter duos in the group — what better place than a Django Girls workshop to introduce your daughter to programming for the first time! Other participants included lab technicians, technical writers, students, and women interested in making a career change. Throughout the day, the coaches gave a variety of inspiring . They shared insightful views and personal stories about many topics. Talks ranged from how the coaches got involved in their careers in the tech industry to primers on topics like technology convergence or automation and messages on motivation, perseverance, and imposter syndrome. One of the strongest themes of the Django Girls w","locales":"","title":"Django Girls Silicon Valley & Elastic - A Coach's Perspective"}
{"index":{}}
{"author":"Clinton Gormley","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2017-04-03T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2017-04-03","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. — elastic (@elastic) Apache Lucene We'll likely be . There are chances it will be ready soon enough for Elasticsearch 5.4.0. This is similar to. In short, queries that reference IndexReaders (either directly or transitively) are problematic because queries might stay in the cache long after the index that they have been run against has been closed. This makes these queries hold references to the in-memory data-structures that Lucene maintains on top of the data that sits on disk and which can normally be reclaimed once a segment is removed. A fix for this bug will be in Lucene 6.5.1. , where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2017-04-03"}
{"index":{}}
{"author":"Daniel Mitterdorfer","category":"Engineering","publish_date":"2017-03-30T00:00:00.000Z","url":"/blog/honey-you-have-changed-quite-a-bit","seo_title":"","content":" Change is an inevitable part of life, and it is everywhere: people in our lives, our relationships, our environment — and JVM defaults. And the latter can bite you if you are not very careful. The other day we had a very interesting support case: A customer has upgraded a cluster from 1.7 to 2.4, and they experienced very high CPU usage after some time and on some of the nodes in a large cluster. This is definitely one of the tougher issues to debug. Our team analyzed piles of log files, stared at diagnostic dumps, visualized shard balances across the cluster, and reviewed the cluster configuration. Then it finally hit us: The startup scripts in their environment set the JVM option . Now you may wonder what this JVM option does. Glad you’ve asked! Let me take you on a short detour into the guts of the JVM. When a Java applications starts, the JVM will run it in interpreted mode. After some time, the JVM detects that some methods are called very often and will compile them to native machine code that is optimized for the current platform the application is running on. This machine code needs to be stored somewhere in memory by the JVM, and this part of memory is called the code cache. So the JVM option sets the size of the code cache to 64 MB. The JVM has not one but two JIT compilers: C1, which does only basic optimizations but can compile very quickly, and C2, which optimizes heavily but takes up more system resources. Which one should we choose? The JVM engineers had a trick up their sleeve: The JVM will use them both! This feature is called tiered compilation. So each method will be first run by the interpreter, then if it is invoked often enough, it will get compiled by C1 and finally, if it is called even more often, by C2. On Java 7, tiered compilation was off by default, and the default code cache size is 48 MB. This was a good default because depending on the underlying hardware, the JVM would choose whether to use C1 or C2, but it would not use both of them. In Java 8, tiered compilation was turned on by default, and, consequently, the default reserved code cache size was increased to 256 MB. This is necessary, because when methods get first compiled by C1 and then by C2, a lot more native code is produced. Back to our story: Originally our customer ran on Java 7, so they actually the reserved code cache size from 48 MB to 64 MB with this setting. However, when they migrated to Java 8, they did not change this flag but tiered compilation was enabled, which lead to a significant decrease from the default of 256 MB. Consequently, depending on the methods that got compiled on the respective nodes, the code cache went full. And when this happens, the JVM will compile no more new methods but instead will only run them in the interpreter. And this results in vastly reduced performance that we have also seen on the cluster nodes where the code cache was full. So we just had to remove this flag from the startup script and everything went back to normal. We also recommend not to change this setting as the default value is sufficient for Elasticsearch. Morale of the story: While we continuously strive to reduce the complexity within Elasticsearch, it is an application that is running on top of the JVM. So it is not sufficient to understand only Elasticsearch but you need to look at all levels: from hardware, to the operating system, the JVM, and up until the application layer. These complex systems change all the time and a seemingly simple change of a JVM flag can turn into a significant performance problem down the road. ","locales":"","title":"Honey, you have changed quite a bit lately, haven't you?"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2017-03-28T00:00:00.000Z","url":"/blog/elasticsearch-5-3-0-released","seo_title":"Elasticsearch 5.3.0 released","content":" : See Today we are pleased to announce the release of , based on . It is already available for deployment on , our Elasticsearch-as-a-service platform. Latest stable release in 5.x:This is a bumper release with some very cool new features, and a Security bug fix: ","locales":"","title":"Elasticsearch 5.3.0 released"}
{"index":{}}
{"author":"Uri Cohen","category":"News","publish_date":"2017-03-30T00:00:00.000Z","url":"/blog/get-hosted-elasticsearch-on-your-aws-bill-with-elastic-cloud","seo_title":"Official Hosted Elasticsearch AWS Marketplace Billing Now Available","content":" As we began working on  (), we did so based on a simple realization: our users wanted the ability to deploy the Elastic Stack without the requirement of locating and provisioning hardware. They want to spin up Elasticsearch clusters on demand, use Kibana to visualize and understand your data, scale dynamically, enjoy the latest and greatest Elastic Stack and X-Pack versions, and know that our team is there making sure the lights are on. It’s the Elastic Stack wherever and whenever you want it. And, for today, it is hosted on Amazon. In the process of supporting Elastic Cloud, we heard some common feedback: 'Amazon already has my credit card number.' Or, sometimes, 'I have to use my Amazon credentials for payment.' Say heya to Elastic Cloud, the official ! Now you can pick between having your bill through Elastic or AWS. If you’ve been using AWS, you’re probably already familiar with the AWS Marketplace. It contains thousands of Amazon Machine Image (AMI) listings from various vendors, and allows users easy access to vendor software directly from their AWS console using their AWS account for billing. However, not all software can be consumed as AMIs, specifically not software that’s provided as a service. So there was a whole class of applications and services that AWS users couldn’t consume from their AWS account. Among them, Elastic Cloud. Last November, AWS announced a new marketplace service, called SaaS subscriptions. It enables users to discover and subscribe to a range of SaaS and API products using their AWS account, right from the AWS console. With this marketplace, AWS users can subscribe on the AWS Marketplace and consume each service directly through the seller’s website or API, receiving a single bill showing all their metered usage for all consumed AWS and marketplace services. This, in turn, greatly simplifies the procurement and payment process with a single bill from AWS. The for Elastic Cloud provides an integrated billing model where you can purchase the official hosted Elasticsearch service through the marketplace, and pay with your AWS account bill. You can pay with with the payment information you already have stored and, perhaps, approved by your corporate procurement team while enjoying the newest releases and the full capabilities of X-Pack including monitoring, alerting, security, Graph, and more. Given the current constraints of the marketplace, we have taken the breadth of options available to you for deployment and consolidated them to 8 variants based on size and region. We look forward to your feedback on the process of signing up and provisioning.  through the AWS Marketplace now or try . ","locales":"ja-jp","title":"Get Hosted Elasticsearch on Your AWS bill with Elastic Cloud"}
{"index":{}}
{"author":"Tyler Hannan","category":"Releases","publish_date":"2017-03-28T00:00:00.000Z","url":"/blog/elastic-stack-5-3-0-released","seo_title":"","content":" Say ‘Heya!’ to 5.3.0 It is, again, a substantive release with features and functionality in nearly all areas of the Elastic Stack. And, as per usual, it is available – right now – on . Bonus time: Encryption-at-rest has been a highly requested feature for our customers wrestling with compliance regulations or strict internal policies. After many moons of testing and evaluating options, we’re happy to announce that we now encryption at rest via dm-crypt! The key factors in selecting this approach were performance, stability, and reliability - in our benchmarks, we observed a mere 2-5% performance overhead, and our comprehensive test suite will be run continuously with encryption-at-rest enabled. For now, this support applies to self-hosted clusters, not to Elastic Cloud, but stay tuned, encryption fans. … and now, back to your regularly scheduled program: Elasticsearch For more detailed information, and many other features, peruse the Elasticsearch . Kibana ‘Visualize’ all the features, and ‘Discover’ more information in the . When we introduced the Elastic Tile Service, we supported 10 levels of zoom. Now, X-Pack Basic users can zoom to level 18. to get started. Logstash Many highly requested features make an appearance in this release. For more information, grok the . In 5.2 we released the monitoring UI for Logstash. In 5.3, we’ve added cgroup information and persistent queue stats And, of course, the features are included in the X-Pack basic license for free, so to get started. Beats We don’t ‘let the beat drop’ but we drop the updates in a . ES-Hadoop ES-Hadoop 5.3.0 has also been released today. This version adds unicode support for index and type names and custom HTTP header support. Get It Now! ","locales":"de-de,fr-fr,ja-jp,ko-kr","title":"Elastic Stack 5.3.0 Released"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2017-04-04T00:00:00.000Z","url":"/blog/brewing-in-beats-collecting-auditd-logs","seo_title":"","content":" Welcome to With this weekly series, we're keeping you up to date with what's new in Beats, including the latest commits and releases. Filebeat module for auditd logsThe fileset is added to the system module of Filebeat to be able to parse the Linux auditd logs. It parses the audit event type, unix epoch time, audit event counter, and the arbitrary key/value pairs that follow. It also gives you the Geo location of the audit event addresses in case of remote logins. This is currently merged in master only (6.0). Collect performance counters from WindowsThe community (more precisely, ) added the in Metricbeat with the `perfmon` metricset to collect performance counters from Windows. It uses the to collect performance data. The module is a migration of the into Metricbeat. Currently, this is merged in master and planned for 6.0. Moving to govendorFor a while now we weren’t happy with the tool we used to manage the Go dependencies, to the point that most of us preffered doing the vendoring work manually. As we were waiting for a new standard tool to emerge, we avoided switching tools. All this changed with a community by , which showed us that govendor actually fits our needs and workflow much better. All changes in the beats repositoriesLibbeat (All beats)Changes in 5.x: Changes in master: FilebeatChanges in master: MetricbeatChanges in 5.x: Changes in master: PacketbeatChanges in master: DocumentationChanges in 5.3: Changes in master: PackagingChanges in 5.3: InfrastructureChanges in master: ","locales":"","title":"Brewing in Beats: Collecting auditd logs"}
{"index":{}}
{"author":"Jim Goodwin","category":"Kurrently in Kibana","publish_date":"2017-04-03T00:00:00.000Z","url":"/blog/kurrently-kibana-2017-4-3","seo_title":"","content":"Welcome to This is a weekly series of posts on new developments in the Kibana project and any related learning resources and events.","locales":"","title":"Kurrently in Kibana: This week in Kibana for April 3, 2017"}
{"index":{}}
{"author":"Aaron Mildenstein","category":"Engineering","publish_date":"2017-03-30T00:00:00.000Z","url":"/blog/logstash_lesson_elasticsearch_mapping","seo_title":"","content":" In , you configured Logstash to parse your data and send it to Elasticsearch.  Elasticsearch does a terrific job of guessing what your data types are, and how to handle them.  But what if you know you’re going to have a lot of data, and you want to tune Elasticsearch to store that data as efficiently as possible?  This has a lot of benefits, including reduced storage requirements, but it can also help reduce memory requirements for aggregations, and other large and complex queries.  The way to do this is with a mapping template. ","locales":"","title":"Logstash Lesson: Elasticsearch Mapping"}
{"index":{}}
{"author":"Court Ewing","category":"Releases","publish_date":"2017-03-28T00:00:00.000Z","url":"/blog/kibana-5-3-0-released","seo_title":"","content":" Spring time is upon us! The birds are chirping, the flowers are blooming, and Kibana awakes from its two month slumber with the release of version 5.3.0. Unless you’re in the southern hemisphere, where winter is coming. In any case, Kibana 5.3.0 for everyone! With 20 enhancements, 13 bug fixes, and an untold amount of improvements under the hood of Kibana 5.3.0, there has never been a better time to upgrade. Kibana 5.3.0 is available on our and on . When you’re finished reading, take a look at the complete for all the goodies. Top hits aggregation If you find yourself asking questions like “how do I visualize the most recent number for each bucket?” or “what’s the peak CPU usage for each server in the last day?”, then the top hits metric is for you. It’s available in most visualizations, but check out this example with heatmap: Thanks to github user for all of your hard work on this feature! Timepicker improvements Hello, fellow time traveler! Sick of opening the ole timepicker whenever you want to move forward or backward in time? We hear ya, which is why we added some fancy time-navigation buttons in the header to do just that. Every click is precious, so when you need to use the timepicker, it’ll now close automatically after you select a new time. Full screen visualizations on dashboard The dashboard is a great way to gain insights from many different visualizations, but you sacrifice real estate for each individual visualization to make it happen. In Kibana 5.3.0, you can now quickly expand any single visualization to take up the whole dashboard area temporarily when you want to examine it more closely. No more manually navigating to visualize just to get the information you need! SSL passphrases, and other configuration treasures We added the ability to use passphrase protected certificates in your SSL configuration. And while we were at it, we threw in the ability to configure certificate authorities, the supported “SSL” protocols, and the available cipher suites. Kibana’s defaults here are pretty darn secure, but if you want to tweak them even further, why should we stop you? And more Kibana 5.3.0 is a big release, and we encourage you to read the to do it justice. You can grab the release right now from our . Don’t see a feature that you want in this release? Head on over to our and file a feature request! Have feedback for us? We’d love to hear it on or on our . ","locales":"","title":"Kibana 5.3.0 released"}
{"index":{}}
{"author":"Monica Sarbu","category":"Releases","publish_date":"2017-03-28T00:00:00.000Z","url":"/blog/beats-5-3-0-released","seo_title":"","content":" Today we are pleased to announce that Beats 5.3.0 was released. This is the latest stable version and it comes with Filebeat modules in Beta and dynamic configuration reloading for modules. Quick links: Filebeat modules Up until this release, being able to successfully index logs, such as Apache access logs, required a variety of skills. The user was required to: For common log formats, such as access logs, MySQL slow logs, or syslog messages, we found that our users frequently encountered similar issues when building the grok patterns, and they ended up building similar Kibana dashboards. This caused a lot of duplicated effort in our community that we felt could be reduced by sharing the grok patterns, Elasticsearch templates, and Kibana dashboards for specific services with the entire community. Filebeat 5.3 comes with modules, similar to the modules in Metricbeat or Packetbeat. A typical module is composed of one or more filesets, one for each log type, that contain Ingest Node pipelines, Elasticsearch templates, Filebeat configuration files, and Kibana dashboards. Thus, monitoring the logs of common services gets as simple as running a single command, without the need to even know what grok is. In the future, a module could also include machine learning configurations for anomaly detection and also Watcher configurations for alerts. For the start, the Filebeat 5.3 release includes Beta modules for Apache2, MySQL, Nginx, and System. But that’s not all! You can easily create your own Filebeat module to add support for another service. The exciting thing is that you don’t have to write any code for it, because modules contain only configuration. Following the model of the metricset generator in Metricbeat, we created a generator for Filebeat filesets. It creates templates for most of the required files of a fileset, so most of the work for creating a new fileset is done for you. If we convinced you, and let us know what you think! Dynamic configuration reloading for modules Filebeat 5.3 release comes with dynamic configuration reloading for modules, which makes it possible to change any module configuration on the fly without restarting the Beat. With this feature, Metricbeat is able to dynamically reload its modules, Filebeat is able to reload the prospectors, and Heartbeat is able to reload the list of monitored targets. You just need to define a configuration directory (by default for Metricbeat) where new configuration files can be added, removed, or modified, and the Beat will automatically update its running configuration, and start or stop new modules accordingly. This feature is especially useful in container environments where one Metricbeat instance is used to monitor dynamic services in other containers on the same host, and the set of containers changes dynamically. Dynamic configuration reloading for modules is a beta feature in Filebeat and experimental in Metricbeat and Heartbeat.  Export environment variables used to start a process Metricbeat 5.3 is able to capture and index the environment variables that were used to start a process. This is a useful feature especially for monitoring Docker containers, where environment variables are often used for configuration. To avoid leaking sensitive data, by default, no environment variables are captured You can specify the list of environment variables to capture by configuring the option under the system module . For example: metricbeat.modules: - module: system metricsets: [process] process.env.whitelist: ['USER', 'PATH'] This feature is available on FreeBSD, Linux and macOS. Feedback If you want to make use of the new features added in Beats 5.3.0, please , install it, and let us know what you think on Twitter () or in our . ","locales":"","title":"Beats 5.3.0 released"}
{"index":{}}
{"author":"Suyog Rao","category":"Releases","publish_date":"2017-03-28T00:00:00.000Z","url":"/blog/logstash-5-3-0-released","seo_title":"","content":" We are pleased to announce the release of Logstash 5.3.0. If you can't wait to get your hands on it, head straight to our page. You can also view the release notes.Persistent QueuesWe've made a few important enhancements and resiliency improvements to the persistent queue feature. Among them: While the persistent queue feature is still in beta for 5.3.0, we've incorporated feedback from our users and fixed important bugs reported during this phase. We have a couple of features yet to complete before we can remove the beta tag off of this feature. Please stay tuned for our updates, and your feedback is very welcome! X-Pack MonitoringAfter releasing monitoring UI for Logstash in 5.2 as part of the X-Pack basic license, we've got lots of feedback from users. In this release, we've fixed a few defects and added more graphs! Version 5.2 added the ability to collect and report on cgroup information in the monitoring API. These stats are useful when you are running Logstash in a container. Version 5.3 brings new charts that show cgroup information in comparison to the regular machine level CPU. There are also new graphs under the “Advanced” tab to show how many times the container is being throttled, duration, etc.  Under the “Advanced” tab, a new graph shows the number of events queued on disk over time. This provides a better understanding of the event lag during ingestion. PluginsTo add to the awesomeness, this release is also packed with tons of plugin goodies. Here’s some highlights: New Plugins Besides the JDBC filter, four other new plugins have joined the family with great contributions from our community! FeedbackWe are super excited for this release of Logstash and look forward to your feedback. You can reach us at our, open issues in your or twitter(). Happy 'stashing! ","locales":"","title":"Logstash 5.3.0 released"}
{"index":{}}
{"author":"Adam Leadbetter","category":"User Stories","publish_date":"2017-03-28T00:00:00.000Z","url":"/blog/processing-marine-environmental-observations-with-logstash-at-the-marine-institute","seo_title":"","content":" The Marine Institute is the State agency responsible for marine research, technology development and innovation in Ireland. We carry out environmental, fisheries, and aquaculture surveys and monitoring programmes to meet Ireland's national and international legal requirements. We provide scientific and technical advice to Government to help inform policy and to support the sustainable development of Ireland's marine resource. We aim to safeguard Ireland's unique marine heritage through research and environmental monitoring. Our research, strategic funding programmes, and national marine research platforms support the development of Ireland's maritime economy.As a professional data manager for over a decade, all of which has been spent curating marine environmental data, I have spent countless hours reformatting raw data outputs from field instruments into standard data formats. That practice is just about sustainable when data are collected in a discrete way – from research vessels or short- to medium-term deployments of moored devices in the ocean. However, a new model for converting these raw outputs was needed when we deployed a subsea observatory in Galway Bay, tethered to the shore by fibre-optic cable and returning measurements including temperature, salinity, and current velocity every second. For us at the Marine Institute, that's where the Elastic Stack and, in particular, Logstash came in. Treating the incoming raw data sources like conductivity-temperature-depth sensors or fluorometers as application logs, we pipe this data through Logstash, using the Grok and Mutate filters to produce a structured, fully standardized data format. An example Grok filter for a conductivity-temperature-depth-sensor is: 2016-11-11T00:00:22.136Z|I-OCEAN7-304-XXXX| 27.55 11.930 35.666 30.855 1492.0759 00:07:48.89M IDRONAUT_OCEAN7_304 %{NUMBER:pressure:float}%{SPACE}%{NUMBER:temperature:float}%{SPACE}%{NUMBER:conductivity:float}%{SPACE}%{NUMBER:salinity:float}%{SPACE}%{NUMBER:sound_velocity:float}%{SPACE}%{TIME:raw_time} Which parses a raw instrument output to (for example): { \"message\" => \"2016-11-11T00:00:22.136Z|I-OCEAN7-304-XXXX| 27.55 11.930 35.666 30.855 1492.0759 00:07:48.89M\\r\", \"timestamp\" => \"2016-11-11T00:00:22.136Z\", \"instrument\" => \"I-OCEAN7-304-XXXX\", \"pressure\" => 27.55, \"temperature\" => 11.93, \"conductivity\" => 35.666, \"salinity\" => 30.855, \"sound_velocity\" => 1492.0759, \"raw_time\" => \"00:07:48.89\" } Once into this JSON semi-standard format, the Mutate filter can be applied to create a full-standard output. uuid { target => \"id\" } mutate { add_field => { \"[featureOfInterest][href]\" => \"http://linked.marine.ie/feature/exampleURI\" \"[member][0][type]\" => \"Measurement\" \"[member][0][procedure][href]\" => \"http://vocab.nerc.ac.uk/collection/L22/current/TOOL0861/\" \"[member][0][observedProperty][href]\" => \"http://vocab.nerc.ac.uk/collection/P01/current/TEMPPR01/\" \"[member][0][result][uom]\" => \"http://vocab.nerc.ac.uk/collection/P06/current/UPAA/\" \"[member][1][type]\" => \"Measurement\" \"[member][1][procedure][href]\" => \"http://vocab.nerc.ac.uk/collection/L22/current/TOOL0861/\" \"[member][1][observedProperty][href]\" => \"http://vocab.nerc.ac.uk/collection/P01/current/PSALCU01/\" \"[member][1][result][uom]\" => \"http://vocab.nerc.ac.uk/collection/P06/current/UUUU/\" \"[member][2][type]\" => \"Measurement\" \"[member][2][procedure][href]\" => \"http://vocab.nerc.ac.uk/collection/L22/current/TOOL0861/\" \"[member][2][observedProperty][href]\" => \"http://vocab.nerc.ac.uk/collection/P07/current/CFSN0330/\" \"[member][2][result][uom]\" => \"http://vocab.nerc.ac.uk/collection/P06/current/UPDB/\" } rename => { \"timestamp\" => \"[phenomenonTime][instant]\" \"temperature\" => \"[member][0][result][value]\" \"salinity\" => \"[member][1][result][value]\" \"pressure\" => \"[member][2][result][value]\" } remove_field => [\"@timestamp\",\"@version\",\"message\", \"host\"","locales":"","title":"Processing Marine Environmental Observations with Logstash @ The Marine Institute"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2017-03-28T00:00:00.000Z","url":"/blog/brewing-in-beats-monitor-memcached-with-metricbeat","seo_title":"","content":" Welcome to With this weekly series, we're keeping you up to date with what's new in Beats, including the latest commits and releases. This month the number of Go developers in the Beats team increases with 2. We are pleased to have and joining the rhythm of Beats. Metricbeat: Add support for memcached Metricbeat is now able to monitor the overall status of memcached by using the metricset of the memcached module. It gives you information like how many SET vs GET commands were, or how many connections are established vs total. New community Beat: Icingabeat ships events and states from Icinga 2 to Elasticsearch or Logstash. You can read more about introducing Icingabeat . New community Beat: Gpfsbeat collects GPFS metric and quota information, and index it into Elasticsearch. The General Parallel File System (GPFS) is a high-performance clustered file system developed by IBM. New community Beat: Nvidiagpubeat uses NVIDIA System Management Interface (nvidia-smi) to monitor NVIDIA GPU devices and can ingest metrics into Elasticsearch cluster. nvidia-smi is a command line utility, based on top of the NVIDIA Management Library (NVML), intended to aid in the management and monitoring of NVIDIA GPU devices. Nvidiagpubeat with help of nvidia-smi allows administrators to query GPU device state. It is targeted at the TeslaTM, GRIDTM, QuadroTM and Titan X product, though limited support is also available on other NVIDIA GPUs. Other changes since last update Filebeat Documentation All Beats All Beats Packetbeat Filebeat Metricbeat Documentation ","locales":"","title":"Brewing in Beats: Monitor memcached with Metricbeat"}
{"index":{}}
{"author":"Clinton Gormley","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2017-03-27T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2017-03-27","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. — Zachary Tong (@ZacharyTong) Changes in 5.x: Apache Lucene The vote for the release candidate has . In particular, this release will bring faster range and geo queries to Elasticsearch, as well as improvements to the way query parsers deal with token graphs, see below. You can read more about the release highlights at . Some analysis components, like shingles with multiple sizes, can generate very dense graphs. This is an issue for query parsing, since we need to enumerate all paths in order to generate a query. rather than out-of-memory errors. Mike McCandless wrote a blog post that describes . Watch This Space , where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2017-03-27"}
{"index":{}}
{"author":"Guilherme Melo e Maranhão","category":"User Stories","publish_date":"2017-03-27T00:00:00.000Z","url":"/blog/searching-for-needle-in-haystack","seo_title":"","content":" In 2010, we deployed in production the first version of a system that would change the way documents were managed in the State of Goiás Justice Prosecutor's Office, Brazil. It was an Electronic Document Manager named Atena (Athens), the \"Goddess of Wisdom\", which was responsible for registering all the Citizen Claims as well as the Judicial, Extrajudicial, and Administrative processes, and their procedural steps, engaging them in workflows, among many other functionalities related to the management of documents within a judicial government institution. Each process has an average of 7 related documents, which totalized, in the end of 2015, an amount of 15,626,816 processes. The global search functionality that was developed was unable to retrieve data in a satisfied manner. It used a PostgreSQL Full Text Search, which enabled phonetic matching, fuzzy search, ranking etc. However, the results weren't effective and also contributing to resource overhead, as the text search indexes (Gin) were much larger than the other PostgreSQL indexes. We defined some denormalized tables to store data redundantly and use them for search purpose. Although it improves performance, it was taking up more than 25% of disk space, in comparison to the other database tables. Besides all these issues, there wasn’t enough available documentation for setting and tuning the PostgreSQL searches. The users then began to face an annoying situation: the processes (and their documents) simply couldn't be found using it: precision, relevance, and performance issues were far from ideal. Slowly, the functionality became useless and, in the meantime, the complaints increased exponentially! Getting to workSomething needed to be done. How could we search among all this data in such a way that the users could find exactly (or at least, very close to) the information they wanted? It was like \"searching for needle in haystack\". That's when Elasticsearch came on the scene! It was set with 4 nodes, being all master and data, having 32GB RAM, 16 CPUs, 3 SSD disks (30GB for data and 150GB for log), each. Due to technology issues, we needed to build a fresh new system. It was named Delfos and the first step was to design the index structure: which metadata would be relevant? Which filters should be applied? Which fields should be analyzed? After a couple of meetings among users and team members, we decided for just one type, the process. The index mapping was defined with 56 fields that included objects (and their inner fields), strings, integers, longs, boolean and dates. We defined a single analyzer containing the filters: lowercase, asciifolding (our vocabulary had a lot of accent marks) and a customized stopword filter to the brazilian language. This analyzer was applied to 23 fields, which would be the searchable ones. The SearchThe search phase design was the most painful. We defined two types of queries: a should term query and a query string. Just one of them is executed according to the user search term: (1) query: defined for the exact processes' number, documents' number and some other numerical fields (a simple regular expression would verify the search term length and its type), which is shown below:  bool:{ should: [ { term: { process_number: { value: query, boost: 2.0 } } }, { term: { reference_number: { value: query } } }, { term: { 'document.number': { value: query } } }, { term: { 'court_hearing.number': { value: query } } }, { term: { 'session_of_jury.number': { value: query } } } ], minimum_should_match: 1 } (2) a : defined for the full-text search. Boost levels were specified for each searchable field. This feature was extremely important for our use case, as we had 23 searchable fields and some of them were much more used for querying than others, i.e., the name of a person involved in a specific process and the process's subject were much more required than the name of the process's creator department. We also set","locales":"","title":"Searching for needle in haystack"}
{"index":{}}
{"author":"Blerim Sheqa","category":"User Stories","publish_date":"2017-03-24T00:00:00.000Z","url":"/blog/no-more-endless-searches-for-bugs-or-misbehaviours-introducing-icingabeat","seo_title":"","content":" What’s the first thing that comes to your mind when you think about failing services? The adrenaline flowing through your body while you try to stay calm and find the root cause of the issue. The anger caused by not being able to find even a single hint that could help you. The disbelief when you finally found each piece of the domino bricks that fell and lead to exactly this problem. The relief when you fix it and relax all the muscles you have been tensing for the past twenty minutes. Introduction to Icinga is a tool that actively monitors your hosts and services. The results of these checks can be stored in a database and are used to decide who will get notified about which problem. Instead of creating static configuration, you add rules that will create your checks dynamically based on the characteristics of your hosts. By using the RESTful API changes can be made automatically and from remote. The support for zones enables you to distribute your monitoring environment and scale it as your infrastructure grows. IcingabeatThe Icinga API allows modifications of the running configuration and also publishes lots of information about almost everything. is a Beat that fetches data from the Icinga API and forwards it to Logstash or Elasticsearch. It supports two modes that run independently and collect various information. Collecting Icinga events is one of the features of Icingabeat. Events are results of your service checks, notifications that have been sent, triggered downtimes or created comments. There are plenty of other event types. Basically, an event is something that happened inside Icinga. The Icinga API transports these events through an HTTP stream. Every user with sufficient permissions can read them. Users can also be granted to see only some of these events. Types and filters let you configure exactly which events you want to store in Elasticsearch. The status poller is the second feature of Icingabeat. It runs independently from the event stream and polls periodically status information about the Icinga daemon itself. This information gives you an insight about how your monitoring environment is performing. Correlating Logs with Monitoring DataWhen you are notified about a problem, there’s a high chance that you probably won’t know what’s the exact cause of the issue just by reading the notification. The typical procedure of finding the root cause of a failing service includes searching for answers in the logs. Monitoring data and logging data are two powerful sources full of information about current activities in your environment. Storing your monitoring data at the same place as your logging data, in Elasticsearch, allows you to perform searches on both sources and view the results in one dashboard. You can search for a specific host and see in a timeline which errors were logged and what services were affected by this at the same time. You can find out who was notified about the issue and when they started responding to it. When collecting log events with Filebeat, they will be stored in a slightly different structure. Therefore, it makes sense to create separate visualisations for each purpose. One table could show all authentication logs and another list the failing services detected by Icinga. Afterwards you can combine the visualizations in one dashboard and search for hosts. The Filebeat data, you would query like this type:log AND source:”/var/log/auth.log” For Icingabeat: type:icingabeat.event.checkresult AND NOT check_result.state:0 Being notified about problems in your infrastructure is mandatory and helps you to keep the business up and running. Receiving false positives too often, however, will make you less attentive for those notifications. Furthermore, too many false positives will make your monitoring useless for you and everyone who uses it. Finding out which services cause the m","locales":"","title":"No More Endless Searches for Bugs or Misbehaviours: Introducing Icingabeat"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2017-03-04T00:00:00.000Z","url":"/blog/brewing-in-beats-filebeat-modules-polishing","seo_title":"","content":" Welcome to With this weekly series, we're keeping you up to date with what's new in Beats, including the latest commits and releases. Brewing in Beats: Profiling Go Programs — elastic (@elastic) Meet the Beats team at Elastic{ON} 2017 The Beats team is gathering in San Francisco to participate to the (Mar 7 - Mar 9). If you are there as well, just drop by the AMA booth to ask questions, to tell us about how you are using Beats or just to share the feedback with us. There are three talks about Beats: Hope to see you there! Filebeat modules: Improve error handling in case of missing plugins Some of the existing Filebeat modules depend on one or more Elasticsearch Ingest node plugins ( and/or ). We’ve now the Filebeat displays in case one of the required plugins are not available. The way it works is that the modules declare which plugins are required in their file and Filebeat queries the API to check that they are available. Filebeat All Beats Metricbeat Documentation Winlogbeat Dashboards xbeats Dashboards All Beats Documentation ","locales":"","title":"Brewing in Beats: Filebeat modules polishing"}
{"index":{}}
{"author":"Haley Eshagh","category":"Culture","publish_date":"2017-03-23T00:00:00.000Z","url":"/blog/on-indexing-heartbeats-and-ballet-into-elasticsearch","seo_title":"On Indexing Heartbeats and Ballet into Elasticsearch","content":" Nobody expects a ballerina to open a tech conference keynote. But that's just what we did. On March 7, 2017, we opened Elastic{ON} with a ballet performance shipping biometric and audio data into the Elastic Stack for visualization. This is a look at the effort that went into making it happen. An Unconventional Idea A little over a year ago, we pitched an idea. It went a something like this: The brief continued on to describe other types of data capture and visualization using a variety of sensors related to a ballet performance. The idea was to demonstrate a creative and unexpected use of the Elastic Stack that pushed the boundaries of what people expect. We casually circulated the concept to our peers to get their reactions. Dig it? Don't get it? Not true to Elastic? The majority response was positive. Eventually it made its way to Shay, creator of Elasticsearch and CTO. He had two comments: What we didn't hear was, \"No.\" So we got to work. The Proto-BallerinaIn a way, what became known as the Ballerina Project was made possible because of a popsicle, a hairdryer, and a warm Saturday afternoon. We teamed up with a local Elastic community member we knew who had the right kind of software, hardware, and design chops to pull something like this off. Presented with the idea, he worked out a few ways to explore the viability of the project. One of them was by building , using inexpensive parts bought off the internet and the Elastic Stack. Only, the weather in Oakland, California in June isn't spectacularly that interesting. So he improvised and pushed the temperature readings to extremes with a popsicle and a hairdryer, resulting in a Kibana line graph that roller-coastered hot and cold faster than most of us had seen before. While a temperature sensor the size of a cell phone built in a garage on a weekend was a long way from a wired-up ballerina, we learned that it was possible. All that was left was to do it. Dancing in a Crowd of \"Data Thingies\"The industry of \"data thingies\" is vast and expanding. A multitude of products offer actionable insights and worthwhile value extraction from the growing oceans of data. Technology promises to help us to find meaning in data by connecting the dots to make the best decisions possible. The trouble is, saying you work in \"data\" today is on par with saying you worked in \"computers\" a few decades ago: people tune out, eyes glaze over. Often, it's interpreted as you do something complicated that only some people will understand. To an extent, this is true. Every collision event at the Large Hadron Collider results in one million bytes (1 MB) of data — there are about 600 million events per second. That's a lot of data. Most of us wouldn't know how to start sifting through it to find something meaningful. So, perhaps we do leave that to the particle physicists to sort out. But there are many instances in which you don't have to be an expert. Every 140-character post to the Twittersphere is a data point — each is a record or a log of something that happened at a point in time. Looking across many of them will start to reveal a story, a trend, a pattern, or a series of unexpected connections. The same holds true for hundreds of failed Facebook login attempts (you're being hacked), or monthly toothpaste purchases from Amazon (you fear cavities), or electricity consumption in our homes (you care about energy efficiency). Indexing heartbeats or g-forces during a ballerina's pirouette isn't all that different from analyzing electron collisions, monitoring disk space, or tracking CPU usage. Everything is a data point. And the Elastic Stack gives you an incredibly powerful and flexible platform to explore data and find the stories within it.Elastic, the company, was founded by those who are at their core inquisitive, inventive, and curious. Our users are no different. They tinker, pl","locales":"","title":"On Indexing Heartbeats and Ballet into Elasticsearch"}
{"index":{}}
{"author":"Alex Brasetvik","category":"Engineering","publish_date":"2017-03-03T00:00:00.000Z","url":"/blog/memory-issues-well-remember","seo_title":"Memory issues we'll remember","content":" Elastic Cloud is on the tail end of eliminating a mix of memory issues that has caused problems for a lot of low-memory nodes, and in some rare cases even large nodes. Following the memory problems, we experienced connectivity issues on a handful of servers in eu-west-1 that affected any cluster with at least one node on these impacted servers. ","locales":"","title":"Memory Issues We'll Remember"}
{"index":{}}
{"author":"Court Ewing","category":"Releases","publish_date":"2017-02-28T00:00:00.000Z","url":"/blog/kibana-5-2-2-released","seo_title":"","content":" Today we’re releasing Kibana version 5.2.2, which includes fixes for two visualization regressions, and it is already available for deployment on , our Elasticsearch-as-a-service platform. There are a handful of bug fixes in this release, but the two important changes are fixes for regressions with visualizations: ","locales":"","title":"Kibana 5.2.2 released"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2017-02-28T00:00:00.000Z","url":"/blog/elasticsearch-5-2-2-released","seo_title":"Elasticsearch 5.2.2 released","content":" Today we are pleased to announce the release of , based on . It is already available for deployment on , our Elasticsearch-as-a-service platform. All users should upgrade. Latest stable release in 5.x: Most of the bug fixes in this release are fairly minor, but a few fixes deserve special mention: Please , try it out, and let us know what you think on Twitter () or in our . You can report any problems on the . ","locales":"","title":"Elasticsearch 5.2.2 released"}
{"index":{}}
{"author":"Haley Eshagh","category":"Culture","publish_date":"2017-03-22T00:00:00.000Z","url":"/blog/a-snapshot-of-the-elastic-timeline-and-how-it-all-came-together","seo_title":"A Timeline of Elasticsearch, Kibana, Logstash, Beats, and X-Pack Development","content":" When attendees turned the corner into the airy fishing-pier-turned-conference-venue for Elastic{ON} 2017, most eyes immediately banked left. They looked up. They looked down. Then they looked far away. And then they walked forward, and paused, and pointed, and stared, and stepped back, and pointed some more. It was exactly what we had hoped for. What attendees saw was a massive black wall approximately the length of a blue whale. On it, in white, was nine years of Elastic history going back to the early days (long before there ever was a company) to the latest 5.2 release in January 2017. The infographic is split into four sections: software downloads per month, product releases, user group formation, and selected boundary-pushing events at the company and beyond. All dates and numbers were quadruple checked using the best sources we could find. The source Illustrator file is painfully accurate (there are 700+ guidelines to prove it!) and a marvel all its own. To be honest, we didn’t really know how it would turn out in the final environment. We didn’t have the tools to really gauge what the final product might look like. Would the empty spaces feel too empty? Would it be as readable as we’d hoped? Would the scale play out nicely? Would the dark magic required to print something like this work in our favor? It’s been said that luck is only as good as the preparation that goes into it. Well, we got lucky. What struck us most is how it became a conversation piece or a visual to get lost in between sessions. Attendees with neon stickers. They . And there were different ways of looking at the timeline. Up close it tells one story, and from far away, it tells another. It's reminiscent of : in the beginning, there wasn’t much, but as time passed, more and more happened. An internal favorite is perhaps the product slice of the timeline. The design was inspired by subway transportation maps. You can see how far back some products go and watch the release cadence evolve from a seemingly random pattern to a more ordered one with Elasticsearch 2.0 in October 2015 and then a tighter integration across all product lines with 5.0 a year later. There’s plenty more to be said about this infographic. For now, have a look yourself and tell us what you see on Twitter using . ","locales":"","title":"A Snapshot of the Elastic Timeline & How It All Came Together"}
{"index":{}}
{"author":"António Sargento","category":"User Stories","publish_date":"2017-03-15T00:00:00.000Z","url":"/blog/found-it-document-retrieval-at-arquo-with-elasticsearch-and-elastic-cloud","seo_title":"","content":" In this post, I would like to share my experiences of using and on an project and the main reasons why Elasticsearch was the right choice for us. ARQUO is a cloud document and business process management platform that helps Enterprises and service providers quickly and efficiently implement document processing projects (mail-room, purchase order workflow or expenses management). Document Storage & RetrievalOne of the most important components of this platform is the digital archive. For a digital archive like ours one of the major requirements is to be able to store information (documents or digital assets) like invoices, purchase orders, photos, audio or video files and retrieve them when necessary. Document retrieval can be challenging if the information about the digital assets (metadata) is not correctly organised and indexed. In order to help our customers search useful information on those documents stored and find what they're looking for we needed a search engine to help us cope with that task. A search engine that can index and organise the document metadata as well as if necessary the document content ( information). Elasticsearch and Elastic Cloud to the RescueIn the past, we used on other projects (on-premise, without SaaS requirements). But for ARQUO, the requirements are not the same and we needed to find another tool that provided us with a distributed, multi-tenant, full-text search engine with a interface and schema-free documents. In addition it was a plus for us if that tool was offered as part of a SaaS model since at ARQUO we prefer to use it in a managed service fashion rather than deploying it ourselves. This is when we started looking at Elasticsearch since it matched with all our requirements and Elastic Cloud, originally known as Found, was just starting their managed Elasticsearch offering providing clustering, security and management capabilities, as well as simple cluster provision and management. Features that come with Elasticsearch like the full-text search capabilities, filtered queries, custom aggregations and the responsiveness are all important for us because we work with unstructured data. These capabilities allow our customers to search across the data faster and because the information is more organized through the aggregation feature it will also help them to find the required information more easily. The available tooling is also very important for data analysis and management. We signed up for the Elasticsearch hosted service as early adopters and beta-testers with our first cluster created in May 2013, and until now we are happy customers, first with Found and now with Elastic. The Future is BrightFor the near future we expect Elastic to be evolving continuously by adding new capabilities and new features, which will allow us to integrate them into our architecture. One of the Elastic capabilities that we would like to make use of in the near term is the log analysis. We are aiming to improve the way our event logs are being handled by using Logstash and Kibana. We have no reason to change to another service provider, even though there are many other players out there (Qbox or AWS Elasticsearch, for instance). Elastic Cloud has proven to be the best alternative for us since it's always running on the latest Elasticsearch version, it's got a good set of tools, a compelling model for a good price and a great pool of experts. Speaking of which, I would like to leave a couple of final words for the support guys:Bio: ","locales":"","title":"Found it! Document Retrieval at ARQUO with Elasticsearch and Elastic Cloud"}
{"index":{}}
{"author":"Uri Cohen","category":"Releases","publish_date":"2017-03-02T00:00:00.000Z","url":"/blog/elastic-cloud-enterprise-beta","seo_title":"Elastic Cloud Enterprise Beta","content":" When we the public alpha of Elastic Cloud Enterprise, I took the opportunity to discuss what it is and who it was for. Today, as we announce beta availability, let’s discuss what problems Elastic Cloud Enterprise can solve for you. As a reminder, Elastic Cloud Enterprise is our Elastic Cloud product, hosted and managed by you. Multiple clusters of Elasticsearch, Kibana, and all the features of X-Pack are orchestrated from a single console. But why does this matter? If you are offering logging, or monitoring, or search as a service to your organization, or if you are managing multiple use cases for the Elastic Stack across your company, you face a few common considerations. There is no time better than a public release to reiterate the value of ECE, the motivation behind it, and answer a few common questions. How do I streamline operations?Management, monitoring, and snapshotting are common operational tasks that every person running the Elastic Stack  has to consider. As your cluster count scales, the potential for complexity can scale as well. Automation is helpful but achieving all these tasks through a web based console or a REST API makes the process immensely simpler. And, as your project succeeds or your data volumes grow, you can easily scale clusters (up or down) in this same interface. And, as new versions of the Elastic Stack are released you can upgrade clusters to the latest with one-click upgrades. Where do I host it?Each organization has an affinity, a preference, for a particular provider or a particular hardware vendor. Elastic Cloud Enterprise lets you choose where you run the Elastic Stack whether on physical hardware, in your private cloud, or in the public cloud. Your organization’s preference or business need can shape your hosting strategy. What use cases is it best for?Simplify how you handle a variety of use cases across your organization. From fraud detection to website search, IT log analysis, and more. Perhaps you have a centralized function that provides and supports all Elastic Stack deployments across the entirety of your organization. Or, perhaps you have one giant cluster that serves multiple teams and use cases. Elastic Cloud Enterprise enables a cluster per team, per app, per use case and simplifies maintenance, ensures security, all while enforcing better tenant isolation. Logging as a service. Monitoring as a service. Search as a service. You decide. Whatever your users need of Elasticsearch you can deliver. Imagine providing your internal logging or monitoring users and endpoint without the risk of misconfiguration of their application causing issues in other running services. Elastic Cloud Enterprise takes care of all of the hard work of offering an Elastic Stack service. Provisioning, placement, resource management, security, upgrades, monitoring, remediation, and more enable the deployment of an external (or internal) service offering. How do I make sure all my clusters are secure? Elastic Cloud Enterprise automatically turns on X-Pack for every managed cluster, enabling security features like authentication, role-based access control, and encryption to protect your data. Is it the same software that runs Elastic Cloud?Put simply, yes. Put slightly more complex, Elastic Cloud predates Elastic Cloud Enterprise. As we treat our cloud offering as a product we continue to converge the code base so that the product you will run is the same that we use in our production environment. It is the majority of the same code with the exclusion of billing, synchronization into our business systems, etc. How can I use it?We’ve been actively working on this product for quite a period of time, tested with interested customers, and this is the second public release to experiment with. A poor user experience is a bug. And your feedback about where the bugs lie, what is missing for your desired workflow, and whether it makes you happy are instrumental in ensuring ","locales":"","title":"Elastic Cloud Enterprise Beta"}
{"index":{}}
{"author":"Kristina Frost","category":"Culture","publish_date":"2017-03-01T00:00:00.000Z","url":"/blog/celebrating-international-womens-day-at-elasticon-17","seo_title":"Celebrating International Women's Day at Elastic{ON}17","content":" Elastic{ON} is nearly upon us, and we're tremendously excited about bringing our dynamic community of engineers and innovators together for . I'm writing this blog on the of my two year anniversary here at Elastic, (and, as another co-worker pointed out on an internal thread, the day of the movement, raising awareness for a cause that profoundly impacts women and which is also near and dear to my heart). This upcoming Elastic{ON} is our third my third. I'm reflecting about how far we've come as a company in that time, particularly and especially in regards to our thinking about diversity and about . Back in 2016, when we launched the first , it was part of a collective goal to make our events inclusive and affirming, to progress together on the path of building a more diverse community and a more diverse company. Elastic's , current CTO, and forever co-founder, Shay Banon, wrote about this recently in regards to his perspectives on . He said: \"I want to make it clear: a diverse society is a resilient society. A diverse company is a resilient company. The more diverse we are, let it be ethnicity, religion, gender, sexual orientation, socio-economic background, geographic location, or any other aspect, will make us stronger, more humane, and much much more successful.\" This year's expands on last year's theme of connecting and inspiring women in the Elastic community and will include a candid panel discussion with outstanding women in technology as well as perspective from Shay. As a side note, if you are coming to the conference, reading this and asking yourself, , the answer to that question is \"absolutely yes, and please do!\" The breakfast is on March 8, which also happens to be . International Women's Day has been celebrated in a variety of forms all over the world since the 1900s, and was officially recognized by the United Nations in 1975. Elastic is a distributed company with employees in 34 countries, and celebrating the rise of women all around the world is important to us. We know there are workplace issues that can disproportionately impact female employees. We are committed to pay equity and doing our best to end the cycle of pay inequality by focusing on the position and experience, not prior salaries, for incoming hires. We also support a truly flexible work environment and are reviewing our leave policies, including parental leave. We embrace and honor the social, economic, cultural, and political achievements of women, because visibility and awareness help drive positive change. We invite you to join us as we build a workplace, an event, an open source community, and a technology industry that is more diverse. Joining us at the Women's Breakfast at Elastic{ON} is just one of the ways you can in the spirit of this movement, but there are plenty of other opportunities for you to help us celebrate diversity and social justice at Elastic{ON}, and I'd like to highlight a few of them here: I like to think of companies and people alike as the holders of lanterns and flashlights. Organizations and individuals have a choice about where they can invest attention and time, and what they can put into focus: in other words, where they direct the light that they carry. Next week, next month, next year, next decade we will continue to direct our collective energies into a community that is innovative and resilient:  into a community that encourages equality and which looks upon the world with brilliant minds and compassionate hearts. Because we are open source, because the very nature of our business invites collaboration with you, our users, we hope that you will continue to join us and help all of us together write a vision for software and technology that embraces the biggest and brightest parts of our collective humanity.Please join with us. Whoever and wherever you are, we would love to have you. ","locales":"","title":"Celebrating International Women's Day at Elastic{ON}<sup>17</sup>"}
{"index":{}}
{"author":"Tudor Golubenco","category":"Engineering","publish_date":"2017-02-28T00:00:00.000Z","url":"/blog/grokking-the-linux-authorization-logs","seo_title":"","content":" The authorization logs, which are usually found under either (for Debian based systems) or under (for RedHat based system), contain lots of interesting security related information like failed and successful SSH logins, sudo attempts, or user and group creation. Let’s have a closer at these logs and see how we can parse and visualize them. The authorization log files are typically created by the local Syslog server, which routes the and facilities to them. This means that any program running on the system can write to these logs files via the syslog protocol. It also means that the format used by the authorization logs is the syslog format, but the message part of them is different based on which program is writing each particular line. SSsssH, scanning attacks Let’s start by looking at a few log lines created by the SSH server: As you can see, the log format is not super consistent, but with a little bit of we can parse it into fields. The following pattern will match any of the first four lines: While this pattern is matching the last line from the sample above: If you’re doing this on a machine that has the SSH server open to the Internet and listening on the default port, you are likely seeing a ton of failed attempts. For example, here is from a single random server: As you can see, there’s a continuous stream of scanning and brute force attacks, creating over 3000 failed login attempts per hour to this server. Since we’ve got the IP addresses of each of these failed attempts, we can use GeoIP information to put them on a map: A large number of attempts are originating from China, but there’s actually a fair amount coming from US and Europe as well. Typically such a brute force attack would attempt try a lot of username and password combinations. The passwords tried are not in the logs, but we can look at what are the most commonly tried usernames: As expected, is the most targeted account. After that, there is a long list of usernames that tend to be common on Linux systems, like , , , , , and common first names like , , , and so on. While these attacks might seem basic and with low chances of succeeding, remember that it is enough for one user on the system to have a weak password and it will likely be exploited eventually. Methods of mitigating this risk include disabling password authentication for SSH, disabling logging in as over SSH, and changing the default SSH port. Speaking of password authentication, after parsing the logs this way, it is easy to check whether there were any successful logins using passwords when you expect only logins using private keys: Sudo make me a sandwich Another set of logs that you might be interested in are the logs created by the command. Any command executed with , as well as failed attempts, are logged to the authorization logs. Here are some samples: The following Grok pattern is able to match both the successful sudo commands and the failed ones. In case of errors, the error is stored in the field. Armed with this, we can easily see which are the most common commands executed over sudo: Which users execute commands with sudo, and when: And what errors were reported by sudo: This data can be valuable for detecting security breaches before they escalate, since if an attacker gets hold of an unprivileged account, it’s likely they will try first to get superuser rights. No users, no problems Another thing to keep an eye on, in the authorization logs, is the creation of new users and groups. Users are often created automatically when installing packages, and it’s important to check that they are created with the right privileges. This is what the log lines created by the and programs look like: And here are the Grok patterns for them: With this, you can get an overview of the new users and when they were created: Besides checking that no unexpected users have been created, it’s important here to check that users created for particu","locales":"","title":"Grokking the Linux authorization logs"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2017-02-28T00:00:00.000Z","url":"/blog/brewing-in-beats-profiling-go-programs","seo_title":"","content":" Welcome to With this weekly series, we're keeping you up to date with what's new in Beats, including the latest commits and releases. Metricbeat: Support for profiling Go programs We added the module in Metricbeat to export profiling metrics useful to do a long-term performance analysis and easily find performance issues such as memory leaks. It comes with two metricsets: and . The metricset can collect the variables exposed via expvar except the memory usage metrics that are gathered by the metricset. The golang module is experimental and we are planning to release it with 6.0. Fileset for the Linux authorization logs auth This adds a new fileset under the Filebeat system module: . It parses the authorization logs (typically or ) and creates events for things like: A sample dashboard is included for each of the events above. For example, for the SSH logins: This new fileset is scheduled to be released with version 5.4. Introduce Beat version in the Elasticsearch index and template By default, each Beat creates an index pattern every day of format , and loads at startup (if doesn’t exist already) the latest template. This means that during upgrades, the new template cannot be applied until the next day, when the index changes. the Beat version is added to the Elasticsearch index and template. The advantage is that an index always has the correct template applied based on the Beat version, and it helps in case different versions of Beats are running in parallel. Now, the index where the Beat dumps the data is by default. This should not have any effects on the Kibana dashboards as still applies to all data. As this is a big impact change, it will be available in the 6.0 release. Documentation All Beats Filebeat, Winlogbeat All Beats Documentation Packetbeat Infrastructure ","locales":"","title":"Brewing in Beats: Profiling Go Programs"}
{"index":{}}
{"author":"Suyog Rao","category":"The Logstash Lines","publish_date":"2017-02-28T00:00:00.000Z","url":"/blog/logstash-lines-2017-02-28","seo_title":"","content":" Welcome back to The Logstash Lines! In these weekly posts, we'll share the latest happenings in the world of Logstash and its ecosystem.Changes in 5.4.0Changes in 5.3.0Elastic{ON} 2017If you are coming to , our team would love to meet you! Please come see us at the AMA booth. We have plenty of new features and product updates that we can't wait to unravel! ","locales":"","title":"Logstash Lines: bug fixes, better environment variable support and more"}
{"index":{}}
{"author":"Clinton Gormley","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2017-02-27T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2017-02-27","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Learn how uses for near real time retail analytics at . Register now: — elastic (@elastic) In the last 2 weeks we several parts of our search layer to allow for better testablity as well as to add new features. With the addition of a couple of weeks ago the ability to search across many many shards became a much higher priority. With we added the first step towards removing the artificial soft-limit of 1000 shards per search request. We added the ability to reduce shard results in batches (by default 512 shards at once) to free up resources as soon as possible to prevent the high memory consumption that the soft limit was added to prevent. At this point only aggregations are reduced in batches but is already and is expected to come in version 5.4. This week we saw a two day round-the-clock debugging spree around a . We've been chasing reports of clusters not accepting requests due to the request circuit breaker before but couldn't pin them down. Last Monday, Jason Bryan signalled that his private cloud cluster displayed the same symptoms. Restarting the cluster made it accessible again but plotting the request circuit breaker value over time clearly showed a leak. Something was incrementing it with a few MB every 2.5 minutes. When those MBs consumed 70% of nodes memory, no request to the node could be made. Some more hours and late into the night we correlated it to the snapshotting logic in Cloud. That was puzzling as cloud only snapshots every 30 minutes. More digging led to the discovery that the cloud call to list the snapshots times out, breaks the connection and then immediately tries another time. That explained the faster cycle but we still had no clue what was happening on the Elasticsearch side. Since it was our own cluster, Jason Tedor built a custom jar with some debugging logic and the results were surprising - it wasn't snapshot related at all. If the client closed a connection to the REST layer before Elasticsearch could respond, we would fail to mark the request resources as freed. The snapshot listing call's only fault was being slow... the cluster had 426 snapshots in it, each with many indices. S3 was just slow to read (>5m some times). This issue and will be part of the imminent 5.2.2 release. We deployed the build candidate to Jason's cluster and confirmed that the leak is gone. Thanks again to Jason Bryan and the cloud team for working with us and jumping through hoops to get this resolved. Rally got a which indexes a subset of a Stackoverflow dump using nested documents. It runs nested queries, nested aggregations, nested sorts, as well as simple queries that do not leverage the nested structure, which still have to mask nested docs, so hopefully we should be better informed of performance improvements and regressions related to the use of nested documents in the future. Lucene's , exposed as the , is a powerful auto-suggest implementation, differentiated because it respects deleted documents and can apply filters. It also supports an analyzer to normalize the different ways users type what are in fact the same suggestion which . However, it was , which is a big limitation for uses cases such as suggesting author names from your index when prolific authors may written many documents. Under the hood, the suggester builds a per-segment , where each path is first the analyzed suggestion string, followed by a vInt encoding of the document ID. This means that the FST has already done the hard part for deduplication: all duplicates will share a single path through up until the document ID, at which point it will branch out to all the many documents with that suggestion. We've now so that in Lucene 6.5.0, Elast","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2017-02-27"}
{"index":{}}
{"author":"Eduardo Dalcin","category":"User Stories","publish_date":"2017-02-23T00:00:00.000Z","url":"/blog/providing-extinction-risk-assessments-for-biodiversity-with-elasticsearch","seo_title":"","content":" The National Center for Plant Conservation (CNCFlora) is an official agency of the ministry of the environment in Brazil, hosted at the Rio de Janeiro Botanical Garden. It is dedicated to realizing the risk assessment of Brazilian flora and coordinating the conservation efforts. Currently, Brazil has listed 46,113 species of flora, but even with a continuous and dedicated effort of the CNCFlora, since 2010, only about 11% of these were evaluated for the risk of extinction. Extinction risk assessment is the first step in the conservation of a species and should provide a scientific and objective assessment of the likelihood of a species becoming extinct at one time if the circumstances in which the species is found remains. The assessment of the risk of extinction of all known species is a global challenge agreed by signatories to the Convention on Biological Diversity (CBD) through the target 2 of the Global Strategy for Plant Conservation (GSPC). The remaining large knowledge gap about the conservation status of our flora, together with the challenge of evaluating it completely by 2020, shows that the use of technologies that allow rapid assessment of the risk of extinction supporting trained professionals to make decisions on the final categorization of species is essential. A little help from technologyThe risk assessment is a methodological process defined by the International Union for Conservation of Nature (IUCN) and adopted by many countries, such as Brazil. In this methodology, a set of data and information about the species are compiled and scrutinized by analysts, together with information about the occurrence of and potential threats to the remaining populations. Part of this methodology uses the \"occurrence records\", which comes mainly from herbarium collections, formed by specimens of dried plants mounted on a sheet of cardboard accompanied by a label with the species name and additional data relating to that sample. These data represent the occurrence of a biological specimen in time and space and are the primary source of data for studies on biodiversity and conservation. Thus, the methodology uses latitude and longitude of all the occurrence records to a species to calculates the Area of Occupancy, the Extension of Occurrence and Subpopulations. To help the process of assessing the risk of extinction of all the Brazilian flora, we developed a tool called \"Rapid Risk Assessment Application\" - RRAPP (details here: https://goo.gl/DrQiIB), which proceed these spatial calculi and, based on a specific criteria, categorize the risk of the extinction of each species of the Brazilian flora. Indexing on Elasticsearch and exploring on KibanaDue to resource constraints on infrastructure and personnel, a tool that is simple and operates efficiently was essential. Therefore, Elasticsearch was the best choice. The taxonomic information - the name of the species, the occurrence spatial data and analysis results are all indexed on Elasticsearch. A combination of queries of taxonomy (names) and occurrences (points) provide data for the assessment that saves the result (analysis) back into Elasticsearch. Aggregations are used to extract reports and enable overview and statistics of the whole dataset or parts of it on demand while keeping overhead minimal. With a big dataset that contains diverse data that do not always fit nicely into a standard vocabulary, we also need to rapidly iterate on the index-explore cycle, so using Kibana was only natural for this. With Kibana we can have a quick overview of the data as it is analyzed, perform changes and identify bias and problems to act upon. Here we can view the general result of analysis while they run. We can also preview the occurrence point distribution and see that there is a huge concentration at the center (0 latitude and longitude) due to data quality problems. Another","locales":"","title":"Providing extinction risk assessments for biodiversity with Elasticsearch"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2017-02-20T00:00:00.000Z","url":"/blog/brewing-in-beats-add-support-for-jolokia-lmx","seo_title":"","content":" Welcome to the weekly ! With this weekly series, we are keeping you up to date with all the changes in Beats, including the latest commits, releases and other learning resources. Metricbeat: Add support for Jolokia/JMXYou can use the new to gather arbitrary JMX metrics from your Java applications. Below is an example configuration: - module: jolokia metricsets: [\"jmx\"] enabled: true period: 1s hosts: [\"localhost:8778\"] namespace: \"metrics\" jmx.mappings: - mbean: 'java.lang:type=Runtime' attributes: - attr: Uptime field: uptime - mbean: 'java.lang:type=GarbageCollector,name=ConcurrentMarkSweep' attributes: - attr: CollectionTime field: gc.cms_collection_time - attr: CollectionCount field: gc.cms_collection_count - mbean: 'java.lang:type=Memory' attributes: - attr: HeapMemoryUsage field: memory.heap_usage - attr: NonHeapMemoryUsage field: memory.non_heap_usage For each mbean, you can define the attributes that should be fetched. The field defines the field name where the event will be placed. The namespace defines the metricset namespace. Central monitoring for Beats With the growing number of your deployed Beats, it comes the need to start monitoring the health of the Beats in a central point. Each Beat now a series of health metrics, and a subset of them to x-pack monitoring to be later visualized. By default, the monitoring data is sent to the same Elasticsearch cluster, as configured under the outputs. New community Beat: Mcqbeat reads periodically the status of queues from , a simple queue service over Memcache, and stores it into Elasticsearch. Other features and fixesAll Beats Filebeat ","locales":"","title":"Brewing in Beats: Monitor JMX metrics with Metricbeat"}
{"index":{}}
{"author":"Robbie Ogburn","category":"Engineering","publish_date":"2017-02-23T00:00:00.000Z","url":"/blog/user-impersonation-with-x-pack-integrating-third-party-auth-with-kibana","seo_title":"","content":" X-Pack Security is a fantastic way to secure your cluster, providing both authentication and authorization via RBAC. The authentication aspect is managed by services known as realms. Security ships with a handful of that give you the flexibility to choose how you identify your users. For many use cases, this works extremely well -- want to manage everything internally to Elasticsearch? The Native realm is your go-to. Already using LDAP within your organization? We’ve got you covered with the LDAP realm! There’s even an Active Directory realm if you’re a Windows shop.But what if you want to use X-Pack Security with an authentication service not covered by one of these built-in realms? You have a couple options: In this blog, I’ll walk you through a working example of the second option! We can use an authentication system like Google Sign-In to protect Kibana and translate my Google account name to a Native realm user in X-Pack Security:  from there, we let Security handle authorization.To do so, I’ll be using Bitly’s to handle the Google authentication layer and to pass the necessary* headers to Kibana. My overall architecture will look something like this: *Important Note: The above configuration will only work in Kibana 5.x or later. This is because we’ve implemented an additional way to trigger a login event by passing basic auth headers (rather than entering credentials in the Security UI login screen). K 5.x also allows you to whitelist headers, which allows us to pass our special “run as” header.Install Elasticsearch + Kibana (with X-Pack for both)First we’ll need to install an instance of Elasticsearch and Kibana (5.2+ preferred). You can pick whichever installation method you’d like, but I personally find the zipped distributions to be easiest for testing. We’ll also need to install X-Pack for both of them: Edit the config/kibana.ymlUncomment the property and add the header to the white list. This allows Kibana to pass our “run as” header to Elasticsearch from Nginx.elasticsearch.requestHeadersWhitelist: [ es-security-runas-user, authorization ]If you’re using Kibana 5.2+, add the following property to allow Monitoring to function properly:xpack.monitoring.elasticsearch.requestHeadersWhitelist: [ es-security-runas-user, authorization ]If you’re following along with a Kibana version prior to 5.2, you’ll want to disable Monitoring as the necessary headers won’t be passed:xpack.monitoring.enabled: falseStart Elasticsearch and Kibana$ bin/elasticsearch $ bin/kibanaLoad Sample Data and Prepare KibanaOur Elasticsearch instance is mostly empty at this point (there are some system indices for Monitoring and Security, but nothing really tangible to experiment with). Let’s grab some from the Kibana docs: You can use the built-in superuser account for the different API calls. I’d recommend loading all three data types (accounts, shakespeare, and logs) as it will help tie into the Security roles we’ll be defining later on. Once the data has been ingested, log into Kibana and define your index patterns: Create Our X-Pack Security Users and RolesWe’ll need to create a Native realm user to associate with our Google account. If my account was user1@elastic.co, I’d create a Native realm user called . Let’s create a user who can read the and indices, but has no access to our indices.curl -u elastic:changeme -XPOST \"http://localhost:9200/_xpack/security/role/shakespeare_bank_read\" -H 'Content-Type: application/json' -d' { \"indices\": [ { \"names\": [\"shakespeare\", \"bank\"], \"privileges\": [\"read\"] } ] }' curl -u elastic:changeme -XPOST \"http://localhost:9200/_xpack/security/user/user1\" -H 'Content-Type: application/json' -d' { \"password\" : \"B&J$v,&%2SV*g9Xv\", \"roles\" : [\"kibana_user\", \"shakespeare_bank_read\"], \"full_name\" : \"My Test User 1\" }' (note: the passwords can be randomly generated and u","locales":"","title":"User Impersonation with X-Pack: Integrating Third Party Auth with Kibana"}
{"index":{}}
{"author":"Clinton Gormley","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2017-02-20T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2017-02-20","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. — elastic (@elastic) Query parsers are getting a new option that allows them to produce phrase queries when multi-term synonyms are encountered. For instance say your analysis chain configures and as synonyms and a user searches for , then the produced query will now be instead of . This is interesting as it means that this query would no longer match documents talking about something new that happened at the for instance. Regular expressions are powerful for tokenizing text, by identifying which sequence of characters define a token, or reversing that and identifying those characters that should split tokens. Lucene's PatternTokenizer offers this functionality, using the JDK's builtin regular expressions (java.util.regex.*). Unfortunately, since the JDK uses non-deterministic finite state automata (NFAs), they are vulnerable to nasty adversarial cases that can , and our users have hit this. So for Lucene 6.5.0 we've instead. This means overly complex regular expressions will fail to determinize, and will be detected up front, instead of later on with an unlucky document text, and once the DFA is successfully compiled, tokenization is extremely fast. Unfortunately, since Lucene's DFAs do not support capture groups, we can't yet offer a similar version for PatternCaptureGroupTokenFilter . Fortunately, there is that looks like a relatively straightforward approach to make capture groups work with Lucene: patches welcome! Changes in 5.3: Changes in 5.x: Changes in master: Upcoming changes: Apache Lucene Watch This Space , where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2017-02-20"}
{"index":{}}
{"author":"The Elastic Education Team","category":"News","publish_date":"2017-02-17T00:00:00.000Z","url":"/blog/welcome-to-our-virtual-classroom","seo_title":"","content":" To all of our Elastic Education Alumni around the world, we want to take a moment to say thank you! We appreciate the opportunity to meet you, to teach you, and to listen to your insightful feedback. As Elasticsearch has grown in popularity and use around the globe, we've made a sincere effort to continue to expand our training reach, visit more cities, and improve the quality and breadth of our courses. To date (and to the best of our ability to accurately count and visualize in a Kibana geohash grid), we've had thousands of students complete one of our nearly 1,000 in-person courses delivered around the world: This Kibana-generated word cloud from our survey feedback says it all: The Elastic Education team is excited to announce . We recognize that taking time out of your busy lives to travel to our course locations and spending 8 hours each day can be difficult. Students have been asking us for real-time, instructor-led online training, and we're finally ready to deliver. We've spent the last few months researching methods and technology to bring the highest quality virtual training experience to you with the personal attention you've come to expect from our in-person courses. With our virtual classroom training, you'll have console access to a fully-functional training VM while also being able to see, hear, chat with and talk to your teacher privately and class publicly, live in real time. We're genuinely thrilled and you're going to love this experience! The first delivery will be our course delivered in a , with more courses coming soon. Virtual classroom training will offer our same in-person courses in a manner where the students and instructors can be anywhere. The class is conducted live by the same expert Elastic engineers that deliver our in-person trainings. Students use their computer where only a modern browser is needed to watch slides, audio, and video presented by the instructor! The labs and hands-on parts are even conducted on remote servers, so no special software or requirements are needed on your computer. We are excited to bring share this new delivery format with you: We've already launched two On-Demand trainings you can sign up for today — and . We are already producing and plan to release many more On-Demand courses over the year including courses covering our X-Pack products, Client API Libraries and courses on other topics you've asked for such as Geo, Upgrading and Ingestion. Stay tuned and register at any time. As a special thank you, please use this coupon code 6002-41B7-9CBB for an additional 20% off a public or online course of your choice, and come say hello. ","locales":"","title":"Welcome to our Virtual Classroom"}
{"index":{}}
{"author":"Cameron Price-Austin","category":"User Stories","publish_date":"2017-02-17T00:00:00.000Z","url":"/blog/the-elastic-journey-at-mixmax","seo_title":"","content":" is a platform for all your externally-facing communications. Just like you use Slack to talk within your team, you use Mixmax to talk to people outside of your immediate team, most notably folks in other organizations. One of our most popular features is the Mixmax Live Feed. It shows you how people interact with all the messages you and your team send, providing a chronological overview all email sends, opens, clicks, downloads, meeting confirmations, replies, and more. It’s searchable (using natural language queries across recipients and subject), filterable, live-updating, and provides top-level aggregate statistics. Our customers, such as Looker, Lever and Asana, rely on Mixmax as their primary communications platform, so you can imagine that the live feed crunches a lot of data. At last count, there were over a billion documents in the collections which make up the live feed, and our document storage grows at ~20% month-on-month. In addition, our users expect their feed to load near-instantly, especially as it’s the entry point to our app. Why not Mongo?In May 2016, we began exploring how we’d build this feature. Our primary data store is Mongo, so our initial hope was that we’d be able to construct the feed directly from there. But that approach had a number of flaws: it would have required us to query documents from multiple collections, then interleave, sort and truncate them. That would obviously be inefficient, CPU-intensive and slow. Besides, Mongo couldn’t provide us with the natural language search features we wanted. A couple of us had used Elasticsearch in the past and it seemed like a good fit here. We could dump relevant events into a single index, and it would provide super-fast searching (including the natural language search we wanted), filtering, paging, sorting and aggregation out-of-the-box. Over the course of about two weeks, we built a prototype which satisfied our performance SLAs (server response and page load times), and launched. Schema OptimizationAs we wrote , we began to notice some of our queries performed quite poorly (300-500ms), particularly during periods of peak load. While debugging on a sample cluster locally, we noticed that performance improved dramatically if we stored and queried upon our timestamps in seconds instead of milliseconds (our original choice of milliseconds was a reflection of our predominantly-JS stack). Across a test set of 500 million documents, we saw a 30x improvement. Adrian Grand (Software Engineer at Elastic) helped explain the cause in more detail. The issue is isolated to Elastic versions pre-5.0. In those builds, Elastic indexed] numeric fields using prefix terms. In particular `long` values were indexed using 4 values - one that identified all bits, one that identified the first 48 bits, one that identified the first 32 bits and one that identifies the first 16 bits. These 16 bit precision steps meant there could be up to 2^16=65536 values on the edges. However, if we use seconds, our values were all multiples of 1000. Suddenly, there were only ~66 unique values at most on the edges, which made the queries much faster. We implemented the schema change, which saw those 300-500ms queries drop to ~13ms. As we look to the future and the chance to upgrade to Elasticsearch 5.x, we expect faster queries without this workaround, due to the BKD tree in Elasticsearch. Segment BloatOur Elastic cluster ticked along happily for a few more months, before we began to notice nodes dropping more frequently than they should (once every few days). Usually these were recovered automatically, but in one instance, the replica shards could not be moved back into the primary node and we experienced degraded performance that eventually led to a partial outage. We host our Elastic cluster with Elastic Cloud (who better to provide support, right?). We contacted the team there and after some back-and-forth (which included running a diagnostic that made","locales":"","title":"The Elastic Journey @ Mixmax"}
{"index":{}}
{"author":"CJ Cenizal","category":"Engineering","publish_date":"2017-02-16T00:00:00.000Z","url":"/blog/componentizing-the-kibana-ui-css-that-scales","seo_title":"Componentizing the Kibana UI, part 1: CSS that Scales","content":" Here be CSS at scale Tens of thousands of lines of CSS. Selectors nested eight levels deep. Override upon override upon override until… (gasp)… rears its ugly head! Some may cower and tremble at the very mention of such beasts, but here on the Kibana team, we meet these foes head on, with bellyfuls of coffee and web inspectors at the ready. Am I being too dramatic? Perhaps. But the struggle of writing scalable CSS is . On the team, we’re tackling this problem with a formalized approach towards writing CSS: We build our UI out of components are the building blocks of a user interface. They’re the buttons, form fields, modal windows, and other visual elements which engineers glue together in the process of building out a feature. By writing CSS in the form of components, we make our engineers’ lives easier, because they can reach for existing CSS and markup instead of having to write it themselves. CSS that’s written around components is also easier to maintain, because a component naturally limits the scope of its own styles, preventing any accidental side effects when we make changes to them. Here’s an example of one of our components. We call it a Panel. Components have both a visual representation and a representation in the code, and they’re both identified by the same name. This lets us talk about UI with a common language. Now, whenever a designer, engineer, or product manager refers to a “Panel”, everybody else in the conversation knows exactly what they’re talking about. Next, let’s explore how we keep our CSS , then take a look at the CSS behind our Panel component. Simplicity is the foundation of scalable CSS Simple code has fewer moving parts and clearly expresses intent. The units of logic within it are uncoupled from one another, have explicit and unambiguous interactions, and have well-defined boundaries. Simple CSS possesses these same qualities, but the methods with which we simplify CSS are different than those we would use to simplify code written in imperative languages such as JavaScript or Python. On the Kibana team, we’ve found that the most powerful tool in our simplicity toolbox is the CSS . Here are the general rules on how we use classes to simplify our CSS: Let’s look at how these principles apply to the code for our Panel component. Here’s the markup for creating a Panel: Notice how the UI is clearly identified by the use of class names in the markup. It’s obvious that this is a Panel component containing a header and a body. On the Kibana team, we’ve found that by improving our markup’s readability, we’re also able to build UIs more quickly and reliably. Here’s the CSS that defines the appearance of this markup: Check out how we’ve declared each selector with a single class. This makes it really easy for us to understand how the CSS acts upon the markup, and results in a visual UI component. Lastly, notice that the only selector which has inherited properties is the class. That’s because we know the element to which this selector will be applied will never contain any children beyond the text in the Panel’s title. This is an example of how we sidestep the problems that CSS’s inheritance model can introduce. The BEM naming convention The BEM naming convention has greatly grown in popularity since it was first developed by . Explaining the nuances of BEM is beyond the scope of this blog post, so if you’re interested, I recommend reading more about it in this , or in this . We chose BEM as our CSS naming convention because we needed a way to identify cohesion between the different classes that comprise a component when we read our markup. At the same time, we need a way to differentiate the role each class plays within our markup – some classes play a structural role, some classes modify others, and some are meant to","locales":"de-de,fr-fr,ja-jp,ko-kr,zh-chs","title":"Componentizing the Kibana UI, Part 1: CSS that Scales"}
{"index":{}}
{"author":"Lauren Johnson","category":"News","publish_date":"2017-02-15T00:00:00.000Z","url":"/blog/insiders-guide-to-elasticon-2017","seo_title":"The Elastic{ON} 2017 Attendee Guide","content":" 259,200 seconds. 60,000 breaths. 350,000 heartbeats, give or take a few thousand. Spending three days of your life with us at Elastic{ON} may sound like a small thing, but in the grand scheme of things, it's everything. It's your time. When you honor us by attending , we want to help you make each minute worth your investment. So we surveyed veteran conference attendees and chatted with Elastic employees to surface the best tips, approaches, and practices to help you milk every second. Here's what we learned: 1. Light preparation is helpful.\"It's definitely worth giving the agenda a look in advance to see what sessions might interest you the most.\" –Tim Roes () \"The Elastic roadmap sessions and the sessions with compelling use cases from Elasticsearch practitioners like Thomson Reuters, Yammer, HotelTonight, Eventbrite, Etsy, The New York Times, Adobe, and my own company (Cisco) were worth the time that I spent for the conference.\" –Jaya Gopalakrishnan () \"If you want to get the most out of the conference and haven't worked with the Elastic Stack beforehand, I would recommend visiting the pre-conference training and/or looking up some tutorials online.\"        –Tim Roes 2. Resist the comfort zone.\"Go to a few sessions you don't think you'd like, just to challenge yourself.\" –Mark Walkom () \"If you eventually see that the upcoming talks are not 100% completely in your interest, then take the time to rest in the conference area and, maybe, try out and code something you've learned in another presentation.\" –Thiago Souza () \"Beyond informative sessions, I learned a lot in the hallway track/demo booth. I met people from both ends of the business spectrum doing all sorts of different things with Elasticsearch and the rest of the Elastic Stack.\" –Jaya Gopalakrishnan 3. Own the AMA booth. \"Users walk up with pages and pages of notes. If you've got questions, bring them.\" –Mark Walkom \"We've had people come with one question, watch some talks, come back with a spinoff question, go to some more talks, and by the end of it they came out with something bigger, better, and awesomer than they'd ever expected.\" –Mark Walkom 4. Find your tribe. \"The really tiny breakout sessions can seem a little lonely compared to the huge concert-style performances in the bigger spaces, but that's where you find the five other people who are passionate about the exact same sub-sub-topic as you are.\" –Loren Siebert () \"Women's breakfast /networking event and winning an amazing eco-friendly jacket for my tweets were fantastic!\" –Jaya Gopalakrishnan \"If you've interacted with people on Elastic Discuss, GitHub Issues, or IRC about the Elastic Stack, reach out to them beforehand and set up a time to say hello in person.\" –Loren Siebert \"A total must are the after parties, which are a great opportunity to exchange with other users and get in touch with a lot of people from the Elastic team.\" –Tim Roes 5. Elastic folks want to meet you as much as you want to meet them. \"If you come with a few people in mind that you want to meet, don't be afraid to approach them (or someone else) and ask to speak with that person. Don't be afraid to ask if you can chat to Jordan Sissel or Rashid [Khan]. Everyone that works for Elastic is happy to talk to you.\" –Mark Walkom \"I've learned from my past Elastic{ON} experience that you can also talk with the staff itself. It can be any subject, to discuss an idea or a specific issue. I remember that Shay was very open to speak with me, even though I was not working at Elastic.\" –Thiago Souza 6. Budget stomach space wisely. \"Work your way through the gourmet food trucks, slowly and methodically.\" –Loren Siebert Let me sum up.We're excited about this year's Elastic{ON}, and we'll keep doing our darnedest to ensure that every second you spend at the conference is worth it. If you haven't registered yet, we hope you'll . If you're already booked, w","locales":"","title":"Insider's Guide to Elastic{ON} 2017"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2017-02-14T00:00:00.000Z","url":"/blog/elasticsearch-5-2-1-released","seo_title":"Elasticsearch 5.2.1 released","content":" Today we are pleased to announce the release of , based on . It is already available for deployment on , our Elasticsearch-as-a-service platform. All users should upgrade. Latest stable release in 5.x: Most of the bug fixes in this release are fairly minor, but Lucene 6.4.1 has fixes for two important memory leaks: Please , try it out, and let us know what you think on Twitter () or in our . You can report any problems on the . ","locales":"","title":"Elasticsearch 5.2.1 released"}
{"index":{}}
{"author":"Court Ewing","category":"Releases","publish_date":"2017-02-14T00:00:00.000Z","url":"/blog/kibana-5-2-1-released","seo_title":"","content":" Today we’re releasing Kibana version 5.2.1, which includes multiple important bug fixes, including a security fix for an issue that can crash the Kibana process. In all previous versions of Kibana 5, when configured with SSL, Kibana will fail to release file descriptors on certain requests, which over time will build up until the process crashes. Requests that are canceled before data is sent can also crash the process. We’ve assigned this vulnerability the identifier . Kibana 4 is not affected by this vulnerability. You can get this release at our page. Check out the or the list of bug fixes below. Other bug fixes in 5.2.1 ","locales":"","title":"Kibana 5.2.1 released"}
{"index":{}}
{"author":"Michael McCandless","category":"Engineering","publish_date":"2017-02-14T00:00:00.000Z","url":"/blog/multitoken-synonyms-and-graph-queries-in-elasticsearch","seo_title":"","content":" Long ago I described how which is a good fit for the complex structure of the natural languages we humans speak and write. For example, when you apply the synonym: domain name system → dns while analyzing this text: domain name system is fragile the resulting token graph from Lucene, as encoded using and attributes, looks like this: However, once you add this document to a Lucene index, some of the graph structure is lost because Lucene ignores the , which states where a given token should end, and effectively your token graph to this: Of course, this makes some queries buggy. For example, the phrase query will fail to match this document yet it should. Similarly, the phrase query should not match this document, but will! Worse, if your synonym had inserted more than one token, such as reversing the above collapsing rule into an expanding one: dns → domain name system and then analyzed this text: dns is fragile then even the tokens produced by would already be flattened, before indexing! This has been a longstanding, complex, glaring and embarrassing limitation of synonyms with Lucene, resulting in , long and complex describing hairy, partial workarounds. The Solution Finally, as of , included in , we have a clean solution, as long as you apply synonyms at search-time instead of index-time. The first big change is , replacing the now-deprecated . This new synonym filter produces a correct graph in all cases (except for any exciting bugs!), whether your inserted synonym is a single token or multiple tokens. Along with this we've also added a new , to squash any graph token stream for indexing. If for some reason you really need exactly the same behavior as the old then you should create a followed by a , but just remember that necessarily throws something (the full graph structure) away! The second set of vital improvements is to the query parsers. First, the classic query parser had to , fixed (after years of controversy!) in Lucene 6.2.0. Be sure to call since the whitespace splitting is still enabled by default to preserve backwards compatibility. This change empowers the query-time analyzer to see multiple tokens as a single string instead of seeing each token separately. are also an important precursor. The third query parser fix is to , and create accurate queries as a result, also first added in Lucene 6.4.0. The query parser (specifically the ) now watches the and computes all paths through the graph when any token has a value greater than 1. In Elasticsearch, the , and  queries will all correctly handle a token graph. Multiple Paths These fixes, combined, enable you to apply multi-token synonyms at search-time and achieve accurate results. For example, if the query is: dns is fragile with no quotes, and your expands to also search for , then the query parser will first build up the whole token graph after analyzing your query, and then enumerate the separate paths through the graph: dns is fragile domain name system is fragile and will then separately analyze those two strings to produce sub-queries, taking their union using clauses in a . If instead the original query had double quotes, then this will be the union of two , yielding accurate hits! A Tricky Optimization For Lucene 6.5.0, will analyze the query's token graph for , in order to create a more efficient directly, avoiding possibly dangerous combinatoric explosion of all paths as the graph gets larger. While this is mostly just an optimization, it does raise . For example, if the user did not put quotes around the query, but a multi-token synonym is inserted, should that synonym use a phrase query or just the default query parser operator? Hopefully real-world experiences with graph queries over time will shed some light on these questions. More graph filters Besides synonyms, Lucene has other token filters that should produce graphs! In the next (6.5.0) Lucene release, will be re","locales":"","title":"Multi-Token Synonyms and Graph Queries in Elasticsearch"}
{"index":{}}
{"author":"Suyog Rao","category":"The Logstash Lines","publish_date":"2017-02-13T00:00:00.000Z","url":"/blog/logstash-lines-2017-02-12","seo_title":"","content":" Welcome back to The Logstash Lines! In these weekly posts, we'll share the latest happenings in the world of Logstash and its ecosystem. A reminder to our readers that Persistent Queues are still in beta. Our plan is to GA this feature in 5.4. We've added a few important enhancements in 5.3 to ensure data integrity and improve usability:New Plugin: A simple filter that calculates age by subtracting the event timestamp from the current time. This filter can then be used with the drop filter to drop aged events if it reaches a certain threshold. Thanks to for the contribution. ","locales":"","title":"Logstash Lines: Persistent Queue fixes, a new filter"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2017-02-13T00:00:00.000Z","url":"/blog/brewing-in-beats-heartbeat-sample-dashboards","seo_title":"","content":" Welcome to the weekly ! With this weekly series, we are keeping you up to date with all the changes in Beats, including the latest commits, releases and other learning resources. Heartbeat dashboard for HTTP monitoringWith PR, a new sample dashboard for HTTP monitoring is added to Heartbeat. It contains details like how many hosts are up and down and for each monitoring host, the duration, the time to resolve the DNS name, round trip time for TLS handshake, for HTTP requests, etc. Also, an HTTP duration heat-map is available when using a Kibana version greater than 5.2.0. Filebeat: Apply close_timeout also when output is blockedThe configuration option gives the harvester a predefined lifetime and stops reading a log file after the period has elapsed. Currently does not apply in case the output is blocked. This the behavior of to also close a file handler when the output is blocked. This is solving an important issue that we had in Filebeat where an output blocked for a long time could cause Filebeat to not release its file handlers and in turn block the operating system from freeing up the disk space of deleted files. Other features and fixes:All Beats Filebeat Metricbeat Winlogbeat Dashboards ","locales":"","title":"Brewing in Beats: Heartbeat sample dashboards"}
{"index":{}}
{"author":"Clinton Gormley","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2017-02-13T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2017-02-13","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. — Boaz Leskes (@bleskes) Range and geo-distance queries today always execute using Lucene's BKD trees. This gives best performance when ranges are run on their own but might be slow when the result set of the range needs to be intersected with a selective query since the range query would still need to visit all documents that match the range, and this number might be much higher than the number of matches of other clauses. As of 5.4, if the range query is not the most selective part of the query, it will execute using doc values, which allows for much faster query execution when only a minority of matches need to be verified. This change has the potential of making some queries tens of times faster. The relational structure that is created by fields is totally opaque on the low level: nested documents are just regular documents to Lucene. As a consequence, Elasticsearch automatically applies a filter that excludes nested docs to all queries that are executed. This logic is being improved by applying the filter using a FILTER clause rather than a MUST_NOT clause (since positive clauses are faster as they can skip more efficiently) as well as only adding the filter when needed. For instance if the query is a term query on a field that may only occur in root documents, there is no need to exclude nested documents since they cannot match anyway. Until now, search phase execution has always been hardcoded. You might for instance know about QUERY_THEN_FETCH or DFS_QUERY_THEN_FETCH. Phases are currently being detached in order to make things easier to unit test, but this also means it will be easier to add new phases to the execution of search requests in the future. The progress in graph queries, such as multi-token synonyms, continues! Today, in Lucene 6.4.x and Elasticsearch 5.2.0, when the query parser sees that the search-time analyzer created a token graph for a given query, it enumerates all unique paths through the graph, and then creates a big BooleanQuery with each full path analyzed as a sub-query. But this is dangerous: the number of unique paths can grow exponentially in the number of tokens, and while that is unlikely to happen in actual queries, it is still possible. In Lucene we generally try hard to prevent any \"adversarial\" cases that could lead to denial of service, and so makes graph queries much faster (and safer) by pre-analyzing the graph for its , and then directly creating a BooleanQuery, possibly with nested clauses. Beyond just a fun optimization, the change also alters hit scoring, how settings like minimumShouldMatch interact with synonyms, and what boolean operator is used for the tokens inserted by a synonym when the user's query is not a phrase query. Kibana has to embed a lot of client-side logic in order to select which indices to query using the field-stats API. We are working on moving the burden to Elasticsearch by providing a simpler API that only says which fields may be searched or aggregated, as well as making it cheap to query indices that do not have matches so that Kibana could always send queries to all indices without having to worry about the timestamp range. Changes in 5.3: Changes in 5.4: Changes in 6.0: Coming up: Apache Lucene Watch This Space , where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2017-02-13"}
{"index":{}}
{"author":"Daniel Mitterdorfer","category":"Engineering","publish_date":"2017-02-13T00:00:00.000Z","url":"/blog/we-are-out-of-memory-systemd-process-limits","seo_title":"","content":" It all started on a shiny winter day: While we were analyzing build failures in our Jenkins-based CI farm, this one caught our attention: Our initial investigation revealed that this failure only happened on our build slaves with SuSE enterprise Linux 12 SP2 for every build of Elasticsearch. Other builds on these build slaves, such as our Lucene builds, were unaffected. To reproduce the issue we started a Gradle build right from the command line on the affected machine, and it built just fine. Now the question stands: Why is the JVM unable to create a new OS thread? Do we - for some reason - create an excessive amount of threads during the build? Let’s add a test to our test suite that just spawns threads until the JVM dies: When we ran it via Gradle on the affected system, the test had created roughly 20.000 threads when the JVM finally died. However, the number of threads reported by our “professional” thread monitoring solution ( : ) ) while Jenkins was running never exceeded a few hundred threads. Time to open Pandora’s box and to find out under which conditions the JVM throws this error. The C++ implementation of the native method is the function in . Towards the end of this function, we can see that an when the JVM was unable to create a new native thread. We’ll leave out the details, but a native thread on Linux will be created by . After analysis, we came up with two error conditions: As we were just investigating and needed to reduce the failure paths to check, we started by disabling the thread safety margin check by adding to the JVM options of our build. Needless to say, it was not that easy and disabling the check did not change anything. So we needed to analyze under which conditions can fail. After reading the , we concluded that we must hit a system limit and checked: All in all, these numbers seemed fine. But the build was still failing. Time to go a level deeper. in glibc ultimately uses the system call . Poking through the kernel documentation, we found an interesting feature, namely the , which allows to limit the number of processes in a cgroup. This led us to look at our process hierarchy. PID 932 is the Jenkins slave process: and indeed had a very conservative limit of 512 processes (see ): For testing purposes, we increased the limit to 4096, started the build again, and it finally turned green. Note that to raise the limit persistently, you need to define in the affected service’s configuration file or set for all services in the global systemd config file. ","locales":"","title":"We are out of memory (or: Why systemd process limits ruined my day)"}
{"index":{}}
{"author":"Michelle Carroll","category":"Culture","publish_date":"2017-02-10T00:00:00.000Z","url":"/blog/start-your-programming-journey-with-django-girls-silicon-valley","seo_title":"","content":" We're looking forward to for a lot of reasons. It's the largest gathering of the Elastic Stack community, has a jam-packed , and is a chance to get our distributed engineering team together in one place (and time zone). Bringing everyone together lets us engage in some group volunteer work, and take advantage of the unique skills on the team. One of the organizations we're passionate about supporting is — and we're incredibly excited to bring the just prior to our conference. Django Girls is a worldwide non-profit organization dedicated to bringing underrepresented voices into technology, with a focus on women. It enables folks to organize local workshops, and make learning how to program accessible and fun by building a web application using HTML, CSS, Python and Django. We first got involved with Django Girls via Honza Král, a member of the Elastic consulting team. He's been a frequent coach in past workshops, and made introductions that let us sponsor some of the other workshops around the world — like Django Girls Windhoek and Django Girls Delhi. It was wonderful to be involved and to hear about the successes of the workshops (and the newly-minted programmers). We were even lucky enough to to the organization. Here at Elastic, we're big fans of democratizing technology. Technology is driving and revolutionizing a lot of exciting change — from self-driving cars, to Mars missions, to content distribution, and beyond. Bringing new perspectives to tech makes the industry better, and making the industry more accessible broadens the opportunities that individuals can pursue. We're incredibly excited to bring the ! 🎉 Some Elastic engineers are coming to town a day or two before Elastic{ON}, and using their Sunday, to coach at the workshop on March 5. If you know some folks in the Bay Area who are interested in learning how to program in a free, introductory workshop, please feel free to share this with them and ! Finally: If you're interested in organizing a Django Girls workshop in your town, ! ","locales":"","title":"Start your programming journey with Django Girls Silicon Valley"}
{"index":{}}
{"author":"Thom O'Connor","category":"Engineering","publish_date":"2017-02-09T00:00:00.000Z","url":"/blog/x-pack-security-for-elasticsearch-with-lets-encrypt-certificates","seo_title":"","content":" Security via public key encryption is critical for your data. It seems not a day goes by where we don't hear of yet another hack, and unencrypted network communications allow for data theft to occur with almost trivial effort across untrusted networks. This blog will focus on simplifying in-transit encryption to help protect against this data threat.The Let’s Encrypt™ service is a free, automated, and open non-profit Certificate Authority provided by the Internet Security Research Group™ (\"ISRG\") with the noble mission of encrypting all HTTP transport-level communications with SSL/TLS:Since the public key infrastructure (\"PKI\") is ultimately based on a \"web of trust\", enabling widespread encryption is dependent on a Certificate Authority that can provide this trust at a reasonable cost. The Let's Encrypt certificate authority is the first to do so at no cost, and so is a very economical way to get started with trusted encryption. A tool called \"Certbot\" is distributed to simplify the process:The Certbot functionality is based on a framework called the . To verify the client is authorized for the identified domain, the ACME server will issue a set of challenges based on DNS authorization. In short, if you have control of the DNS records for a domain as well as the ability to bind a webserver process to the desired hostname(s), you should be able to get a certificate via ACME and certbot.While I won't go into great detail on using certbot, the basic steps are very straightforward. As long as you have the ability to start a process to listen on ports 80 or 443 and DNS is correct, the following steps should be sufficient:Please note: in all of the following steps, I'm running the commands as root. In your environment and for security and auditing reasons, you may be using \"sudo\". Pasting \"sudo\" another 50 times below though seemed a little excessive.# wget https://dl.eff.org/certbot-auto # chmod 755 certbot-auto # ./certbot-auto certonlyNote: the Let’s Encrypt CA issues short-lived certificates (90 days). You will need to make sure you renew the certificates every 3 months.Once you have your CA-signed certificates, you'll be ready to setup the Elastic Stack with X-Pack and transport-level encryption. We'll start with a single-node system. For our example, we'll use a server name of \"data.example.com\" - this is obviously not a valid Internet hostname, but you'll have your own. We'll also use a certificate Common Name of \"data.example.com\" in this example. The certbot process will provide you the following files, in the directory location \"/etc/letsencrypt\" by default:/etc/letsencrypt/archive/data.example.com: cert1.pem chain1.pem fullchain1.pem privkey1.pemFor Elasticsearch to access the SSL files, you'll then need to copy them into the Elasticsearch configuration directory path. Since Elasticsearch 2.0, the Java security manager limits the directories from which the Elasticsearch process can read, so it must be located in the Elasticsearch configuration directory:# mkdir /etc/elasticsearch/ssl # cp -pr /etc/letsencrypt/archive/data.example.com /etc/elasticsearch/ssl/ # chmod 750 /etc/elasticsearch/ssl/data.example.com # chmod 640 /etc/elasticsearch/ssl/data.example.com/* # chown -R root:elasticsearch /etc/elasticsearch/ssl/data.example.comFor Kibana, you will also need access to the certificate PEM files. Since many sites will run Kibana on separate nodes from Elasticsearch, and since the group access permissions for Kibana will differ, we'll go ahead and maintain a separate copy of the PEM directory just for Kibana.# mkdir /etc/kibana/ssl # cp -pr /etc/letsencrypt/archive/data.example.com /etc/kibana/ssl/ # chmod 750 /etc/kibana/ssl/data.example.com # chmod 640 /etc/kibana/ssl/data.example.com/* # chown -R root:kibana /etc/kibana/ssl/data.example.comWhile I won't get into great detail here about Elasticsearch installation, I began ","locales":"","title":"X-Pack Security for Elasticsearch with Let's Encrypt™ Certificates"}
{"index":{}}
{"author":"Yannick Welsch","category":"Engineering","publish_date":"2017-02-08T00:00:00.000Z","url":"/blog/tracking-in-sync-shard-copies","seo_title":"Elasticsearch Internals - Tracking in-sync shard copies","content":" Elasticsearch provides failover capabilities by keeping multiple copies of your data in the cluster. In presence of network disruptions or failing nodes, changes might not make it to all the copies. This blog post showcases one of the internal mechanics of Elasticsearch to identify shard copies that are missing changes, providing an in-depth view on how two central components, the consensus module and the data replication layer, integrate to keep your data safe. Data replication in Elasticsearch is based on the primary-backup model. This model assumes a single authoritative copy of the data, called the . All indexing operations first go to the primary, which is then in charge of replicating changes to active backup copies, called shards. Elasticsearch uses replica shards to provide failover capabilities as well as to scale out reads. In cases where the current primary copy becomes either temporarily or permanently unavailable, for example due to a server maintenance window or due to a damaged disk drive, another shard copy is selected as primary. Because the primary is the authoritative copy, it is critical in this model that only shard copies which contain the most recent data are selected as primary. If, for example, an older shard copy was selected as primary after it was on a node that was isolated from the cluster, that old copy would become the definitive copy of the shard, leading to the loss of all changes that were missed by this copy. The following presents how Elasticsearch v5+ keeps track of the shard copies that can safely be selected as primary, also called the shard copies. Safely allocating primary shards Shard allocation is the process of deciding which node should host an active copy of the shard. Allocation decisions are made by the master node and recorded in the , a data structure that also contains other metadata such as index settings and mappings. Allocation decisions contain information about which shards should be allocated to which nodes, and whether they should play the role of primary or replica shard. The master broadcasts cluster state changes to all nodes that are part of the cluster. Having the cluster state available on each node enables smart routing of requests as each node is aware of where primary and replica shards are allocated. Each node inspects the cluster state to determine the shards that it should make available. If the shard is assigned as a primary to a node that already holds a copy of the shard data, the node just needs to load up the local shard copy and make it available for search. If a shard is allocated as replica, the node first copies over missing data from the node that has the primary. When there are not enough replica copies of the shard available in the cluster (as determined by the index setting ), the master can also allocate replica shards to nodes that don’t have any data for this shard, thereby instructing these nodes to create full copies of the primary shard. When a new index is created the master has a lot of flexibility to select nodes in the cluster to allocate the primary shards to, taking and other constraints such as allocation and into account. Allocating existing shard copies as primaries happens on more rare occasions. Examples are full cluster restarts, where none of the shards are initially allocated when the cluster is first formed, or multiple failures over a short time span that cause all active copies to become unavailable. In these situations the master first reaches out to all nodes to find out what on-disk shard copies exist in the cluster. Based on the copies that are found, it decides whether to allocate one of them as primary. In order to guarantee safety, the master has to make sure to only select shard copies as primary that contain the most recent data. To this effect, Elasticsearch uses the concept of , which are unique identifier","locales":"","title":"Elasticsearch Internals - Tracking in-sync shard copies"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2017-02-07T00:00:00.000Z","url":"/blog/brewing-in-beats-support-for-configuration-reloading","seo_title":"","content":" Welcome to the weekly ! With this weekly series, we are keeping you up to date with all the changes in Beats, including the latest commits, releases and other learning resources. New community Beat: Prombeat periodically scrapes time series from a Prometheus server and sends them to Elasticsearch directly or via Logstash. It uses the of Prometheus to access the metrics stored in the Prometheus server. Configuration reloading in libbeatDynamic configuration reloading was first introduced as a per-Beat implementation. Metricbeat is able to dynamically reload its modules, Filebeat is able to reload the prospectors, and Heartbeat is also able to reload the list of monitored targets. With this , the reloading capabilities are abstracted into libbeat to be a reusable component. Load Kibana dashboards with the -setup optionWith this , loads the sample Kibana dashboards, being the rough equivalent of running: . This works for all Beats, not only Filebeat. The script will continue to be shipped in the Beat package in 5.x, to avoid breaking compatibility. Code-wise, the two share most of the code. All flags available for the are available as configuration options. For example, to load the dashboards from a directory you can do: filebeat -e -setup -E \"dashboards.dir=_meta/kibana\" Or to load the snapshot version of the dashboards: filebeat -e -setup -E \"dashboards.snapshot=true\" Unify Beat generatorsGenerating a Beat is similar with generating a Beat based on Metricbeat, so we decided to them to use a single instead of two different scripts. The templates used for generating the Beat are now available under the directory. Other features and fixesMetricbeat Filebeat All Beats ","locales":"","title":"Brewing in Beats: Support for configuration reloading"}
{"index":{}}
{"author":"Kamala Ramasubramanian","category":"Engineering","publish_date":"2017-02-07T00:00:00.000Z","url":"/blog/using-molly-to-model-and-test-data-replication-in-elasticsearch","seo_title":"","content":" This past summer, I collaborated with Elastic to model the data replication protocol of Elasticsearch. Data replication is the logic responsible for indexing documents into all copies of a shard and is an important part of the system. It needs to be both correct and efficient, which are generally opposing requirements. My goal was to determine if there were any scenarios in which Elasticsearch's data replication could be driven to an inconsistent state and to use counterexamples found to refine the next version modeled, thereby gaining confidence as I moved forward. As I made progress, there were some nice insights to be had. Elasticsearch's is a variant of . In the primary-backup model, all client requests are routed to the primary replica which in turn acknowledges the request after it has been replicated on all the backups. The basic write setup involves all writes being re-routed to the primary, followed by a replication step. The primary sends the request out to all replicas, which log the request and then acknowledge the primary. Once the primary has received acknowledgements from all replicas, it acknowledges the sender from which the request was received. The acknowledgements retrace the path of the forwarded request until it reaches the client. In practice, multiple concurrent requests can be in flight. This has implications for operational behavior influencing the results from stale reads, reads after writes, and so forth. Assumptions Indexing in Elasticsearch is highly concurrent, making it interesting from a modeling perspective. Since the Elasticsearch APIs guarantee focus around a single document, we modeled a single document with concurrent accesses, rather than multiple independent documents (as would exist in actuality). For simplicity, we focused on an Elasticsearch cluster with one primary shard and two replicas. We also assume that there exists a master oracle that is fully aware of the state of other processes and is not a typical process in that sense. This master oracle is internally a group of servers, amongst which a consensus protocol is run, but we ignore this for our purposes and treat the externally visible actions of the master oracle as input data. Each message is sent exactly once. Framework used I specified the protocol in the Dedalus language, a relational language which is a superset of Datalog. Protocol state is represented as records in tables, and computation is represented as queries or \"rules\" describing the relationships among records. The rules and facts are used to derive new facts, which can be understood as the flow of data amongst the components. Pre and post conditions are used to specify the global invariants that must hold at the end of a execution. These typically represent some form of correctness and/or durability guarantees. For example: 1. nodeid(\"a\", 1)@1:  2. nodeid(Node, Nodeid)@next :- nodeid(Node, Nodeid):  Statement (1) indicates that an entry (a,1) is created in the table \"nodeid\" for the first time at time 1 and (2) indicates that the entry is maintained in the table at every timestep thereafter. Similarly, pre(X) :- log(Node, X), member(Node, Node):  post(X) :- log(Node, X), notin missing(X):  missing(X) :- log(Node, X), member(Node, Node), member(Node, Other), notin log(Other, X):  The pre statement says If a log exists on a node with a payload X, the post statement asserts that such a log must exist in every other node. The fact that the log must exist on every other node is encoded by the statements: : where Other is a node known to Node: The log is present on the node Other: A utility rule to indicate that the log is present in Node but there exists a node Other which doesn't have it. The implicit assumption here is that every node knows of every other node in the system. This is an example of a consistency invariant. For each given input, the model is executed. An ex","locales":"","title":"Using Molly to Model and Test Data Replication in Elasticsearch"}
{"index":{}}
{"author":"Rashmi Kulkarni","category":"Culture","publish_date":"2017-02-06T00:00:00.000Z","url":"/blog/second-annual-elastic-on-womens-breakfast-now-with-leadership-panel","seo_title":"","content":" At Elastic, we take the philosophy of open source technology beyond just code. In that spirit, last year we hosted our at Elastic{ON}16, which included roundtable discussions with trailblazing women in tech. Our impetus was to connect and inspire women throughout the Elastic ecosystem, and our attendee feedback was incredibly positive. This year, we're excited to build on that tradition of bringing female-identifying members of the Elastic community together by hosting the Elastic{ON} Women's Breakfast event featuring a candid panel discussion with outstanding women in technology as well as our , Co-Founder and CTO, Shay Banon. We hope you'll join us for a half hour of networking time, followed by a moderated panel discussion, and Q&A session. Doors open at 7:45 a.m. on March 8 at Marriott Marquis, 780 Mission Street in San Francisco. Shuttle transportation to Pier 48 will be available when we wrap things up at 9 a.m. Leah Sutton is currently the VP of Global Human Resources at Elastic. Prior to Elastic, she was the Senior Director of HR at Plum Organics where she was responsible for supporting the leadership team through a period of high growth and change post-acquisition by Campbell Soup Company. Shay Banon is the creator of Elasticsearch, the open source, distributed, RESTful search and analytics engine. Before creating Elasticsearch, Banon spent more than a decade building complex distributed systems and open source search solutions. Sophie Chang was the VP of Engineering for Prelert prior Elastic acquisition. In this role, Chang is responsible for managing all aspects of the team’s activities and helping to enhance Elastic’s machine learning-based anomaly detection engine. Shikha Srivastava is a lead Hybrid Cloud architect, where she brings her expertise in architecture, design, and leadership towards collaboratively creating innovative pragmatic solutions that leads to enabling Hybrid solution for the clients. Srivastava enjoys leadership role focussed on making things happen and shipping. Cindy Robbins is executive vice president of Global Employee Success at Salesforce. In this role, Robbins leads the company's efforts to attract, develop and retain the best talent. Robbins has more than 15 years of experience in the field of Human Resources. Maria Zhang leads development efforts at Tinder, building and scaling the app for its global user base, assembling and guiding a world-class team of engineers and challenging everyone to create software instilled with purpose. Prior to joining Tinder, Zhang served as Vice President of Engineering at Yahoo Mobile, garnering the highly coveted Apple Design Award and shipping major product releases. Whether you're looking for insights into the latest industry trends, want to network with other women who are succeeding in technical and leadership roles, or hungry for solutions to the challenges posed by today's ever-evolving business climate, join us for breakfast to learn, grow and most importantly connect. If you're already registered for Elastic{ON}17, watch for an invitation to the event in your email . Feel free to reach out to us at if you don't see one come your way or if you have any questions about the event. To start connecting with our community, join the Women of Elastic Group on. Sponsored by ","locales":"","title":"Second Annual Elastic{ON} Women’s Breakfast: Now with Leadership Panel"}
{"index":{}}
{"author":"Clinton Gormley","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2017-02-06T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2017-02-06","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. — elastic (@elastic) The new solves many of the problems that existed with previous highlighters, including accounting for gaps created by stopword filters. The highlighter can either analyze text directly ( mode), use postings offsets ( mode), or use term vectors ( mode). This should be your new go-to highlighter, although it is still missing a few features supported by other highlighters, such as an upper limit on fragment size, the ability to highlight a single field based on matches in multiple fields (), and the ability to collapse contiguous highlights. The team has been hard at work on the sequence numbers project and the first high level feature has landed last week. Sequence numbers are now used for operation based recovery (as opposed to file based sync). With operation based recovery a primary can bring a replica up to speed by only streaming the operations that happened while the replica was offline. This is a great advantage compared to file base syncing which potentially requires copying gigabytes of data. Operation based recovery is only done if the relevant operations can be found in the primary translog. At the moment the chances of this happening are small and in practice this will only happen when a replica was temporary off line. Future work will make this more likely to a point where op based recovery becomes the standard. Fast infix searching (*abc*) or even just leading wildcard searching (*abc) has come up on Lucene's users list in the past, but has never been implemented, in part because it's seen of an abuse of a search engine: really, you should do a good job tokenizing during indexing up front so that you don't need such costly sub-token operations at search time. But it's also partly because nobody had enough of an itch to actually do the work, that is until now! The initial patch on the issue is too invasive and very heap heavy (using suffix arrays) and isn't using the most efficient (yet, complex) known approach for building suffix arrays, yet the subsequent discussion is a nice demonstration of how healthy open source iterations unfold: one response is to fold the approach into a custom PostingsFormat, while another is to use FSTs to reduce heavy heap usage. It's not clear how the issue will finish but it's possible Lucene will soon offer a better solution for infix searches. Changes in 5.x: Changes in master: Upcoming changes: Apache Lucene Watch This Space , where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2017-02-06"}
{"index":{}}
{"author":"Kalpesh Patel","category":"User Stories","publish_date":"2017-02-02T00:00:00.000Z","url":"/blog/scaling-file-system-search-with-elasticsearch-at-egnyte","seo_title":"","content":" In this article, I want to share the journey of transforming our search functionality from a home grown distributed search solution to Elasticsearch, and how that switch helped to reduce the amount of manpower required to maintain and scale the system that powers search over billions of files and petabytes of content all while saving us money in return. Our Use Case: Searching Files and its Content Stored by Customers powers smart content collaboration and governance in the cloud and on-premise. One of our most popular products is which powers secure file storage and sharing (in both cloud and on-premise environments) for tens of thousands of customers including Nasdaq, Buzzfeed, RedBull, AppDynamics, Yamaha, and more. In each of these cases, the customer stores millions of files and when employees are looking for a specific one or related topic, the search functionality they would use is powered by our Elasticsearch clusters. The initial architecture was home grown and it was an array of 100+ machines, called “indexers”, each hosting metadata translation/full text extraction/indexing/search code and a Lucene index. A group of customers were pinned to an indexer node by the routing engine and system was prone to many issues like: A constant capacity of DevOps was dedicated to baby-sit indexers and move data accordingly. Unsurprisingly, a big percentage of customer complaints would be related to search quality and performance. And we found ourselves saying ‘hey, you can’t expect customers to find a file in a haystack without search.’ The Solution: Separation of Concerns by Migrating to ElasticsearchWe were looking for a solution that had: Solr and Elasticsearch both fit the requirements, I had used Solr for a side project previously. It was a good search solution but making it distributed was configuration heavy and a lot of scripting was needed to make autoscaling work. Conversely, to autoscale in Elasticsearch, you just pick the proper number of primary shards and replicas in your index and add more nodes and the cluster will auto rebalance. We ended up choosing Elasticsearch as its API was much cleaner and intuitive and it was built with a distributed mindset from the outset. We separated the concern of scaling/monitoring/managing the search dataset to Elasticsearch and trimmed down “indexer” code to contain business logic of in-order event processing, metadata translation, content extraction. Plus, since we no longer store the lucene index, we added HA to indexers. We removed huge amount of home grown indexing code and instantly became more scalable. In total, we run close to 80 elasticsearch nodes and 30 indexer nodes and we are easily able to add more nodes on demand. Elasticsearch takes care of auto scaling/rebalancing/replication by adding more nodes/indexes and as a result, we now only have just 1 devops and 1 engineer taking care of entire search stack instead of the constant crew that was needed in the past. This has resulted in significant cost savings. More importantly, though, it has reduced burnout among the team as they can now work on what they love and let Elasticsearch automatedly take care of most of the scaling stuff. Technical Deep DiveAs users uploads files and folders and search them, we use the library to extract full text content out of the files and index that along with file metadata (user, size, filename, folder path, etc.). At the core, there are 2 main models in schema: At a high level, when a file or folder is added/mutated we log an event into event store from Cloud File Server(CFS) and publish a message to RabbitMQ which wakes up indexers to index the events in order into elasticsearch. At 10,000 ft, the flow looks like: Challenges Infrastructure FootprintWe deliberately made the decision to spin up our Elasticsearch clusters in Google Cloud Engine and indexers in our data center index to Google cloud. This allo","locales":"","title":"Scaling File System Search with Elasticsearch @ Egnyte"}
{"index":{}}
{"author":"Steven Schuurman","category":"News","publish_date":"2017-02-01T00:00:00.000Z","url":"/blog/the-next-chapter-shay-banon-to-succeed-steven-schuurman-as-elastic-ceo","seo_title":"","content":" Rarely do you make decisions you know will shape the rest of your life. Even though you fundamentally believe you are making the right choice, there’s still a part of you that's not 100% sure. You’re torn between deep rooted emotions — loyalty, ambition, a strong sense of responsibility — on the one hand, and an equally strong urge to make a change on the other. This is how I felt when I decided to co-found Elastic. It is also how I felt when I told fellow Elastic founder Shay Banon I decided to step down as CEO of Elastic. I knew it was coming, but what I didn’t know I was going to tell him at 07:30am at Schiphol airport:  utterly unprepared. Knowing all too well that the impact of my decision would have an impact of similar magnitude on his life as it would on mine, I shared my decision. The conversation and subsequent series of events that followed I consider exemplary of the deep relationship we both built over the years. Long story short, I barely had to explain why. Shay understood immediately as we have always been 100% transparent with each other – no exceptions. Here is my thinking: in its essence, I feel the time has come for me to hand over the CEO-baton, as Elastic gears up for the next phase in the company’s evolution. I have loved every single element of founding, building, nurturing, shaping and tuning all aspects of the company, but feel my job here as CEO is complete. The foundation is ready. Elastic has the potential to be the world’s next great software company, if it’s lead by a truly inspiring visionary leader, that is not afraid of making grand, bold moves. That person is none other than my entrepreneurial soulmate, Shay. Even though I have carried the CEO title since the inception of the company, he has always been my co-pilot. Wherever we steered our phenomenal rocket ship:  we decided on its course together. Today, I am thrilled to announce that Shay has accepted this new challenge, and will be the CEO of Elastic as of May 1st 2017. I will continue to serve on Elastic’s Board of Directors in a non-executive capacity and watch closely over the company’s success, which I’m sure will continue to blow my mind like it’s done since the beginning. I’d like to express a very deep and profound thank you to my co-founders Uri Boness and Simon Willnauer:  none of what we have built over the past 5 years would have happened without you. An equally special thank you to my insanely talented executive leadership team:  you are brilliant to work with and have made building Elastic easier than a walk in the park on a sunny afternoon. The same holds true for my investors and board members: Peter Fenton from Benchmark Capital, Mike Volpi from Index Ventures, the recently deceased and dearly missed Harry Weller from NEA, Harry’s successor Chetan Puttagunta, and lastly, my SpringSource co-founder Rod Johnson. Your dedication is a wonderful source of inspiration, I will forever carry with me. To all other Elastronauts:  I am proud to have served as your CEO, you know I love you. Speak soon. Steven, since the start of our company, what I loved in our partnership is that the definition of roles between us was very flexible. We did what is right for the company, with no ego involved. This gave me the opportunity to roam the company and help where needed, a role that went beyond a typical CTO role, and beyond being “just” product or technology focused. It’s been a truly amazing 5 years. We are humbled by the adoption of our products — we’re approaching 100 million total downloads — and the growth of our community, which now spans 100 countries with 80,000 members. As it’s been said before, we wouldn’t be here without our entire community, and we’re not going anywhere without your help. I am excited to take the lead and assume the CEO role, navigate our company through its next phase of","locales":"","title":"The Next Chapter: Shay Banon to Succeed Steven Schuurman as Elastic CEO"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2017-01-31T00:00:00.000Z","url":"/blog/elasticsearch-5-2-0-released","seo_title":"Elasticsearch 5.2.0 released","content":" Today we are pleased to announce the release of , the latest stable release, with numeric and date range fields, the cluster-allocation-explain API, keyword normalizers, and partitionable terms aggregations. It is already available for deployment on , our Elasticsearch-as-a-service platform.Latest stable release in 5.x:Full details of the changes in this release are available in the release notes listed above, but there are a few important changes which are worth singling out: ","locales":"","title":"Elasticsearch 5.2.0 released"}
{"index":{}}
{"author":"Kevin Kluge","category":"Releases","publish_date":"2017-01-31T00:00:00.000Z","url":"/blog/kibana-5-2-0-released","seo_title":"","content":" Kibana 5.2.0 has just been released and it has several cool features for you to check out. We've made good progress on visualizations, included a huge community contribution, and brought back Tribe functionality. Return of TribeWe know a lot of our users had used a hack to enable Kibana to query multiple clusters through tribe node. This worked with Kibana 4.x but with Kibana 5.0. And we know that caused some pain and blocked upgrades for people. We've with 5.2.0 and gotten to a much better place for the testing and maintenance of tribe node with Kibana. 5.2.0 introduces the separation of a \"data\" cluster from an \"admin\" cluster. The data cluster is the one you typically think of when you use Kibana, as it's the cluster where the data you see is coming from. And that data cluster can be a tribe node. 5.2.0 allows you to configure Kibana to use a different cluster for its administrative functions. These are things like storing your dashboards and visualizations in the .kibana index. This second cluster is called the \"admin\" cluster, and it cannot refer to a tribe node. By default, the data cluster and the admin cluster are assumed to be the same cluster, and must not be a tribe node. There are still a couple of shortcomings that we are investigating: the Console (formerly known as Sense) does not work with separate data and admin clusters. Similarly, we disable the editing of X-Pack users and roles when tribe is configured to avoid any ambiguity about where changes are being made. We don't yet have an ETA on addressing these things, but they're on our radar. Heatmaps FTWWe have been experimenting with heatmap visualizations for a while, and we now feel confident enough to in the 5.2.0 release! Heatmaps are great when you want to pick out an area of high or low volume in time series data. Heatmaps have been cited as see trends that they might otherwise miss. One common pattern for users will be to create date histograms (with a date histogram aggregation on the X-axis) and then do a terms agg on the y-axis. The buckets in the heatmap are colored based on the metric of your choice. For example, you can choose the count of items in the bucket to influence the color. The heatmap visualization is quite general and you can place any buckets you want on the X and Y axes. The heatmap feature even allows for custom range settings. For example, you could configure that a given heatmap should show red for places where the count exceeds 95, yellow for 75-94, and so on. The start of internationalizationThe initial groundwork for internationalization support in Kibana has landed. Thanks to some Kibana is able to load JSON translation files from plugins, which will in the future be used to translate any and all text labels in Kibana. While this is only the groundwork for the overall internationalization support in Kibana, it is a huge step forward and work on the next phase of development has already begun. We wanted to give a big thanks to the contributors here as it was a considerable amount of work with multiple iterations over the course of many months to get right, and we're thrilled with the results so far. Tile service enhancementsWe introduced the Elastic Tile Service a few months ago. Initially, we offered 10 levels of zoom on the tiles. Now, with 5.2.0, X-Pack Basic users will get up to 12 levels of zoom. Additionally, we are exploring how to increase available tilemap zoom levels to 18 levels of zoom. Starting with the 5.2 release of Kibana, we can make these types of service level adjustments dynamically, without releasing new versions of Kibana, so look for further improvements. Monitoring Elasticsearch in containersRunning Elasticsearch in containers? Want to track CPU usage and throttle time? X-Pack monitoring 5.2.0 adds the ability to monitor the container's utilization metrics that are reported from each of your Elasticsearch instances. If you are running Elasticsearch in a container and would prefer to see these metrics in pla","locales":"","title":"Kibana 5.2.0 released"}
{"index":{}}
{"author":"Monica Sarbu","category":"Releases","publish_date":"2017-01-31T00:00:00.000Z","url":"/blog/beats-5-2-0-released","seo_title":"","content":" Today we are pleased to announce that Beats 5.2.0 was released. This is the latest stable version and it comes with a few goodies like uptime monitoring, network connections tracking, and the Prometheus exporters integration, enough good reasons to upgrade. Latest stable release: Heartbeat, for uptime monitoringHeartbeat (Beta) is the newest addition to the official Elastic Beats. It periodically checks the status of your services to determine whether they are available and measures the round-trip-time. Like all the other Beats, Heartbeat is lightweight and has no dependencies, so you can install it on multiple locations in your infrastructure and monitor reachability in a distributed fashion. The idea for Heartbeat came from the team, who is using it already for monitoring thousands of Elasticsearch and Kibana clusters. A second source of inspiration was one of the first community Beats, , written by Joshua Rich. Heartbeat can also be useful for scenarios other than uptime monitoring, such as security use cases, when you need to verify that no one from the outside can access the services on your private enterprise server. With Heartbeat you can monitor a list of hosts via: Here is an example configuration for using the HTTP monitor to check an Elasticsearch endpoint: # Configure monitors heartbeat.monitors: - type: http # List or urls to query urls: [\"http://localhost:9200\"] # Configure task schedule schedule: '@every 10s' # Total test connection and data exchange timeout #timeout: 16s Heartbeat already supports automatic configuration reloading, so you can dynamically add or remove monitored targets without restarting. By default, it monitors one IP address for a hostname, but it has support for pinging all resolvable IPs for a hostname. This is useful if you are using a DNS-load balancer and want to ping every IP address for the specified hostname. Heartbeat is released as Beta in 5.2.0, and we do not recommend to use it in production during the Beta phase. Track network connections with MetricbeatStarting with the 5.2 release, Metricbeat exports the network connections between your applications on Linux systems, so you can see the traffic exchanged between services. The metricset was added. For each TCP socket, it reports the process that opened the socket, the local and remote IPs involved in the communication, and the direction (incoming, outgoing or listening). It can also perform a reverse lookup on the remote IP. Metricbeat gets the network connections by polling the Linux kernel to get the sockets, so a short polling interval is recommended to catch short lived connections. Because data gets more valuable when you can visualize it, here is a sample of using to visualize the network connections: Collect metrics from Prometheus exportersStarting with the 5.2.0 release, Metricbeat comes with a Prometheus module that collects metrics from the or any application that offers a Prometheus endpoint and indexes them to Elasticsearch. With this module, you can use the Elastic Stack to monitor apps instrumented with the Prometheus libraries, or monitor services for which a Prometheus exporter exists but for which we don’t have a Metricbeat module yet. To fetch metrics periodically from a Prometheus exporter, you just need to configure collector as part of and add the host from where to pull the metrics. The metric will be exported under the field configured in . With this, Metricbeat adds support for dynamic metrics where the metrics and their types are not known in advance. While the Prometheus module can be used to get data from a variety of systems, we recommend using the native Metricbeat modules when they are available, because they structure the data in a way that matches the Elastic stack better. For example to collect metrics from and , you can use the following configuration: - module: prometheus metricsets: [\"collector\"] enabled: true period: 10s hosts: [\"localhost:5555\"] namespace: jmx - module: prometheus metricsets: [","locales":"","title":"Beats 5.2.0 released"}
{"index":{}}
{"author":"Tyler Hannan","category":"Releases","publish_date":"2017-01-31T00:00:00.000Z","url":"/blog/elastic-stack-5-2-0-released","seo_title":"","content":" Say ‘Heya!’ to 5.2.0 It is, again, a substantive release with features and functionality in nearly all areas of the Elastic Stack. And, as per usual, it is available – right now – on . Have questions about this release? We always have an AMA booth at Elastic{ON} and this year will be no different. Talks are being scheduled, parties are being scheduled, the only thing missing is you. Elasticsearch For more detailed information, and many other features, peruse the Elasticsearch . In the 5.0 release, we added some cgroup statistics to the monitoring API allowing you to inspect the container where Elasticsearch is running. In the 5.2.0 release, these have been supplemented with more information and made available in the X-Pack monitoring UI in . If you haven’t yet upgraded to enjoy the features of Elasticsearch 5.x, as a reminder, we’ve also released the , which runs on your existing 2.3 cluster. Use this site plugin to prep for your migration. Kibana ‘Visualize’ all the features, and ‘Discover’ more information in the . When we introduced the Elastic Tile Service, we supported 10 levels of zoom. As of today, X-Pack Basic users can zoom to level 12. More detail is in the post but to get started. Logstash Many highly requested features make an appearance in this release. For more information, grok the . As of 5.2.0, you can also monitor Logstash instances via the X-Pack monitoring app in Kibana. Events Received, Events Emitted, Latency, JVM heap, CPU utilization, and more are available for your to tweak the performance of your ingest pipeline. And, of course, the features are included in the X-Pack basic license for free, so to get started. Beats We don’t ‘let the beat drop’ but we drop the updates in a . ES-Hadoop ES-Hadoop 5.2.0 has also been released today. This version adds support for Spark 2.1. Get It Now! ","locales":"de-de,fr-fr,ja-jp,ko-kr","title":"Elastic Stack 5.2.0 Released"}
{"index":{}}
{"author":"Suyog Rao","category":"Engineering","publish_date":"2017-01-31T00:00:00.000Z","url":"/blog/monitoring-logstash-filters","seo_title":"","content":" At Elastic, we are big fans of providing self-service tools to successfully deploy and maintain our software in production. One of the most popular features of Elasticsearch is the APIs used to retrieve it's operational data. While Logstash was always easy to get started with, it was a bit of a black box when it came to managing it in production. Administrators had to rely on external monitoring systems to make sure Logstash was functioning without any issues. A core theme in 5.0 release was to make it easier to manage Logstash. In this post, I'll show you how to monitor your Logstash filters using the inbuilt APIs and the itself.  What is my ingestion rate? If you have deployed Logstash as part of a high-volume data ingestion pipeline, one of the most important metrics to monitor is “how fast is Logstash ingesting data”. Previously, to answer this question, you had to install the to generate the ingestion stats. Starting , we've made it really easy to monitor Logstash using the Elastic Stack itself in addition to the  which were introduced in Logstash 5.0. The for Elastic Stack bundles an agent that you can install on all your production Logstash nodes. It also ships with charts for the most important metrics information that let you visualize Logstash monitoring data out of the box. The monitoring feature in x-pack is under the which means you get all this awesomeness for free. The APIs mentioned here are all part of the open source Logstash distribution.  Let's get started: cd $LS_HOME bin/logstash-plugin install x-pack This plugin should be installed on Logstash nodes you wish to monitor. The x-pack plugin extends your Logstash to periodically ship all the metrics it collects to your production Elasticsearch cluster. Once installed, you can configure the shipping agent using the file. We strongly recommend that you enable security for your production Elasticsearch clusters. Once you've installed x-pack, you can use the extensive that ships with it. Pick the Logstash you want to get detailed monitoring info: What are my slowest filters? Filters are the workhorses of Logstash, which means they can be the slowest stage of your pipeline, compared to the inputs and outputs stage. The performance of a filter is also dependent on the type of data it processes. For example, the grok filter, a very popular one, uses a regular expression engine to extract fields. And certain types of event data can throw your grok filter into an expensive backtracking loop. Before we attempt to tweak the filters for performance, the first step if to actually figure out which one of them is the slowest: curl localhost:9600/_node/stats/pipeline?pretty ... \"filters\":[ { \"id\":\"2178468f89ab43c6f32e934d3cb587e3c850d93a-4\", \"events\":{ \"duration_in_millis\":24005, \"in\":49482, \"out\":49480 }, \"name\":\"geoip\" }, { \"id\":\"2178468f89ab43c6f32e934d3cb587e3c850d93a-2\", \"events\":{ \"duration_in_millis\":84193, \"in\":49483, \"out\":49482 }, \"matches\":29400, \"failures\":20083, \"patterns_per_field\":{ \"message\":1 }, \"name\":\"grok\" }, { \"id\":\"2178468f89ab43c6f32e934d3cb587e3c850d93a-5\", \"events\":{ \"duration_in_millis\":44353, \"in\":49480, \"out\":49477 }, \"name\":\"useragent\" } ... For all the plugins in our pipeline, this API provides a per-pipeline break down of event stats. One particular field of interest is the field which reports the total amount of time spent in a filter for all the events that passed through it. If you divide by , you should get the average time events have spent in this filter. From the response above, it looks like the grok filter is the slowest, followed by the useragent filter. Zooming in on the grok filters, we can also see that there are quite a bit of grok match failures. It is plausible that the regular expression engine tried parsing the pattern(s), and may have gone into a backtracking loop wasting a lot of CPU cycles, contributing to the overall slowness.   An important tip to our readers — if you see the re","locales":"","title":"Monitoring Logstash Filters: X-ray glasses included"}
{"index":{}}
{"author":"Clinton Gormley","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2017-01-30T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2017-01-30","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. On Immigration and Diversity — Shay Banon (@kimchy) When you want to group result by a particular field it is easy to use the power of a aggregation coupled with a aggregation underneath. This common feature is called field collapsing and we’ve decided to give it a boost! As an aggregation, this feature is widely used but suffers from at least two limitations: it is impossible to page the results (one of the in ES), and the result is an approximation (the top group and the top hits can be inaccurate - a known limitation of the aggregation framework as we trade precision for speed). To solve these two issues we’ve added a . Now it is possible to group results by a particular field and to retrieve the top hits for each group in any search request. Similarly it is possible to page through the results of a field collapsed search request like you would do for any search. This approach can be much faster than the aggregation solution because we apply the collapsing to the top search hits only. It’s less powerful than because the sorting of the group cannot be based on a separate computation for that group, but it’s also more precise. is exploring an alternative analysis architecture to replace Lucene's current analysis API components (Tokenizer, CharFilter, TokenFilter). The new Stage API is simpler to consume, with just reset and next methods, versus 5 methods today. Each analysis stage uses a write-once binding to define attributes, instead of the global AttributeFactory Lucene now uses, giving each stage full control over exactly what attributes the next stage can see. This also fixes the long-standing trap of failing to call clearAttributes in your tokenizer. Graph token filters are much easier to create, since position increment and length are replaced with an explicit to/from arc, and the synonym filter on this branch (finally!) can consume a graph, so you could run WordDelimiterFilter followed by SynonymFilter. Tokens are never removed by stages, but instead marked deleted using a new DeletedAttribute. The changes are being pushed to , but plenty of work remains before this is committable! Elasticsearch Core Apache Lucene Watch This Space , where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2017-01-30"}
{"index":{}}
{"author":"Tyler Smalley","category":"Engineering","publish_date":"2017-01-30T00:00:00.000Z","url":"/blog/kibana-plugin-developers-meet-elasticsearch-clusters","seo_title":"","content":" In Kibana 5.2 we introduced the internal concept of clusters which, as a Kibana contributor or plugin author, will affect how you communicate with Elasticsearch. This change is transparent to the end-user and establishes the foundation for more features to come, beginning with Tribe support. Prior to 5.2 Before 5.2, the Elasticsearch Kibana plugin provided two ways to communicate with your Elasticsearch cluster: you could either use the client directly or use . elasticsearch-js client The elasticsearch-js client provides an interface which is well-documented in the section. It takes care of configuration and creation automatically, so using it is just a matter of calling a method which correlates to the . Let’s see how that looks using as an example: const { client } = server.plugins.elasticsearch:  client.cluster.health().then(response => { console.log(`cluster status is: #{response.status}`):  }) callWithRequest The method ensures that the request is made in the context of the currently authenticated user. It uses the same underlying elasticsearch-js client, but requires a bit more configuration. You'll need to provide the request object, which can be obtained from the route handler, and you'll need to pass the API path as an argument instead of calling it directly. Let’s see how the previous example looks now: const { callWithRequest } = server.plugins.elasticsearch:  callWithRequest(request, 'cluster.health').then(response => { console.log(`cluster status is: #{response.status}`):  }) 5.2 and beyond In 5.2 we now have multiple clusters, and we need to specify one before making requests to Elasticsearch. Currently, there are two primary clusters:  and . The cluster contains the index and anything else used to persist state in Kibana. The cluster contains all of the documents that you have indexed and would like to visualize. To access one of the clusters, the Elasticsearch plugin provides . Let’s see how looks now: const { callWithRequest } = server.plugins.elasticsearch.getCluster('data'):  callWithRequest(request, 'cluster.health').then(response => { console.log(`cluster status is: #{response.status}`):  }) callWithInternalUser In lieu of exposing the client directly, we now offer a method , and other than omitting the request argument it has the same interface as . It’s worth noting that in most situations you will want to use , because it ensures that the user has privileges to perform the action. is more appropriate for startup tasks and migrations, where the credentials specified within the are required. Should you still require the underlying client, you can obtain it by calling on the cluster. Let’s see how this looks: const { callWithInternalUser } = server.plugins.elasticsearch.getCluster('data'):  callWithInternalUser('cluster.health').then(response => { console.log(`cluster status is: #{response.status}`):  }) Writing client-side Javascript? If you're injecting the service into your controllers or services and expect the data cluster, there are no changes necessary. If you're expecting the admin cluster, you'll want to use the  service. What’s Next? If you’re a plugin developer, you will likely need to make some changes for 5.2 compatibility. These changes should be transparent to the user and pave the way for improvements and features to come. If you have questions, reach out to us in or our . If you run into a bug, please file a . ","locales":"","title":"Kibana Plugin Developers meet Elasticsearch Clusters"}
{"index":{}}
{"author":"Shay Banon","category":"Culture","publish_date":"2017-01-29T00:00:00.000Z","url":"/blog/on-immigration-and-diversity","seo_title":"","content":" The context of why I am writing the mail is probably obvious, with the recent acts done around immigration / refugees by the US administration. I have a few things to say about it in the context of our company. The problem we are facing is global, and not tied only to the US. It is also not new. It is important, on many levels, including us being a distributed company and conversing with one another, to internalize it. Nigel Farage and the Brexit in the UK, Geert Wilders in the Netherlands, Marine Le Pen in France, the anti Merkel movements in Germany (a country that has accepted more than a million Syrian refugees!), and many others across the globe are all rising in popularity partially due to the rise of anti immigration movements and the rhetoric that ensues. The fact that this act was done during the international holocaust remembrance day intensifies it. A time when Jews, Muslims and Gypsies were turned down by countries, and so many lives could have been saved. As a Jew, with parents from Morocco and Turkey, who makes sure to trim his beard when traveling abroad, it has worried me for some time, and still worries me today. We fight terrorism to protect our humanity, but if we are losing it along the way, what are we fighting for? There is no reason why we can’t solve this problem without giving up on it. Nations, and yes, companies, are judged on their ability to innovate and solve difficult problems both over a long and short time arcs. Yet, I ask each and every one to make sure not to confuse the extremist and populist with conversations that must be had. When someone says they are for the anti immigration rules, for example, we should not shun them away, but ask why. You will learn that it comes from justified intrinsic fears. A whole segment in our population *feels* and has lost their social mobility, their ability to hope for better life for their next generation, their trust in the education system, and more. It is then that people facing such challenges will resort to quick, populistic answers, life is tough. Terrorism is horrible and scary. Our responsibility is to converse and acknowledge these difficulties, discuss how they can be solved, while maintaining what makes our society great. I wish you the power to converse with each other, oppose the extremist, and the power to distinguish between the two. I want to make it clear, a diverse society is a resilient society. A diverse company is a resilient company. The more diverse we are, let it be ethnicity, religion, gender, sexual orientation, socio-economic background, geographic location, or any other aspect, will make us stronger, more humane, and much much more successful. I am still optimistic. If you take a step back, our world has been going in an upward and to the right trend (good). We have blips along the way, and it is our responsibility to make sure those blips are recovered quickly. It starts with each and every one of you on a personal level. Talk to others and show empathy when needed (almost always), yet tenacity when applicable. Work towards solving the actual difficulties people are facing, and share the load with them to make things better. Humanity, as a whole, is good, we are just doing too good of a job now to make the extremist sound and look bigger than they are, including their hands. What does it mean in the context if our company? A few things: - As far as I know, nobody in our company has been affected by the recent rules. If you think you might have, please mail myself and Leah and we will figure things out. - I realized we never made a statement, to the company, about our stance towards diversity. Here it is: - One of the reasons for the Elastic Cares program is to give our employees the opportunity to invest in such causes, and now you can invest both time and money. - We are working towards completely overhauling our About section on our website with a working group that I lead. We will make sure to represent and talk ab","locales":"","title":"On Immigration and Diversity"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2017-01-30T00:00:00.000Z","url":"/blog/brewing-in-beats-monitor-php-fpm-and-ceph","seo_title":"","content":" Welcome to the weekly ! With this weekly series, we are keeping you up to date with all the changes in Beats, including the latest commits, releases and other learning resources. Monitor PHP-FPMThanks to our colleague , PHP-FPM module is added in Metricbeat to collect server status metrics from . It gives you  like the current number of queued connections that have initiated, but not yet accepted, or the number of active servers that are currently processing a page. Monitor your Ceph storage platformThanks to , Ceph support is added in Metricbeat as a .  is a distributed object, block, and file storage platform. Metricbeat collects periodically metrics from Ceph by submitting HTTP GET requests to . Custom HTTP headers for the Elasticsearch outputWith this , you can configure to add custom HTTP headers in the request to Elasticsearch. An interested use-case might be a custom to Elasticsearch. The HTTP headers should be configured under the Elasticsearch output: output.elasticsearch.headers: X-My-Header: Contents of the header And we can also configure the HTTP headers from the CLI: metricbeat -E \"output.elasticsearch.headers.X-test=Test value\" Other features and fixesMetricbeat Packetbeat Filebeat All Beats Generators ","locales":"","title":"Brewing in Beats: Monitor PHP-FPM and Ceph"}
{"index":{}}
{"author":"Suyog Rao","category":"Releases","publish_date":"2017-01-30T00:00:00.000Z","url":"/blog/logstash-5-2-0-released","seo_title":"","content":" We are pleased to announce the release of Logstash 5.2.0, loaded with new features. If you can't wait to get your hands on it, head straight to our page. You can also view the release notes .The following are the headlining features in this release:Monitoring UISay heya to X-Pack for Logstash. is a single extension that provides security, alerting, monitoring, reporting and graph capabilities across the Elastic Stack. We are thrilled to have Logstash Monitoring as our first X-Pack feature that enables you to easily monitor all your Logstash instances from. These features are included in the , which means you can install it for free.The new monitoring UI complements the rich monitoring APIs that were released in 5.0. In addition to the nice graphs with aggregated information, you now have access to historical data stored in Elasticsearch that allows you to track changes in your production Logstash instances.It's not always that we have screenshots in a Logstash release blog, but hey, this feature is best explained using screenshots!So without further ado:Moar Monitoring APIsThe node stats API has been enhanced with the following new stats:Offline Plugin Management: Take 2There are a number of air-gapped Logstash deployments that need to manage their plugins without access to the internet. The previous was not bullet proof. Although it worked for some plugins, there were user issues reported which suggested that the plugin's dependency chain were not completely downloaded on the staging, online machine. To solve this effectively, we had to completely redesign a much simpler user workflow to introduce a new way of packaging a plugin and its entire set of dependencies.To prepare an offline plugin artifact, you require a staging machine running Logstash that has access to a public or server. bin/logstash-plugin prepare-offline-pack logstash-input-beatsYou can now prepare multiple plugins using wildcards:bin/logstash-plugin prepare-offline-pack logstash-filter-*The output of this command is a file called You then move the compressed bundle to the machine where you want to install the plugins and simply issue an install command like:bin/logstash-plugin install file:///path/to/logstash-offline-plugins-{logstash_version}.zipThat's it! Simple and straightforward.We are super excited for this release of Logstash and look forward to your feedback. You can reach us at our, open issues in your or twitter(). Happy 'stashing! ","locales":"","title":"Logstash 5.2.0 released"}
{"index":{}}
{"author":"Samir Bennacer","category":"Engineering","publish_date":"2017-01-26T00:00:00.000Z","url":"/blog/integrating-elasticsearch-with-arcsight-siem-part-3","seo_title":"Integrating Elasticsearch with ArcSight SIEM - Part 3","content":" In our previous blog in this series (, ) we demonstrated a simple architecture for integrating ArcSight with Elasticsearch. In this blog post,  we will continue with the same theme by showing you how to scale the architecture. If you need to have high indexing throughput and a long retention policy, you can set up a . For the ingestion side, you can use a message queue between ArcSight and Elasticsearch. This provides architectural isolation between the producer (Arcsight Smart Connector) and the consumer, thus allowing Logstash to be scaled independently. To process more data, you simply have to add more Logstash instances for a topic. Kafka also helps to buffer the incoming data in case it exceeds the Elasticsearch cluster’s ability to ingest the data during a peak periods or spikes. Whilst Logstash persistent queues ensure end to end delivery, data loss is still possible due to the loss of a Logstash instance. Through data replication, Kafka protects against this scenario. How to integrate ArcSight with Elasticsearch using Kafka as a messaging queue: The following architecture becomes possible with the ArcSight Smart Connector, with HP recently adding Kafka as a supported destination.  This architecture also requires the user to add a Kafka destination to the ArcSight Smart Connector, with Logstash nodes pulling data from the appropriate Kafka topic before indexing into Elasticsearch. Logstash can read directly from Kafka using the , which integrates natively using the . You can find more details about using Kafka with the Elastic Stack in the blog series “Just Enough Kafka for the Elastic Stack”   and . Setup the Elastic Stack Set up the Elastic Stack and Kafka with DockerIn this example, we use Docker to simplify the installation and configuration of the Elastic and Kafka components. $ docker-compose up. Add Kafka destination to ArcSight Connectors Visualize and explore your data In our next post we will talk about  X-Pack alerting features to detect and alert on more complex patterns within the same dataset. ","locales":"","title":"Integrating the Elastic Stack with ArcSight SIEM - Part 3"}
{"index":{}}
{"author":"Amy White","category":"News","publish_date":"2017-01-24T00:00:00.000Z","url":"/blog/fifteen-new-sessions-added-to-elasticon-17","seo_title":"","content":" As the countdown to Elastic{ON} marches on, continues to come together.This week, we’re happy to announce we’ve added 15 new sessions. While we’re equally excited about all of them, we wanted to highlight a few below.Strengthening Your SIEMWith the release of Logstash 5.1, you can easily connect any device that supports the CEF data format as a codec to the Elastic Stack – which is good news for those of you using a SIEM product. At Elastic{ON}, we’ll have a talk that provides , ArcSight, to the Elastic Stack to add real-time insights and scale to your security efforts.Speaking of Security: Barclays Barclays will share insights into how they approach security at Elastic{ON}. Elena Kvochko, the CIO of the Group Security Function at Barclays, will discuss , which includes utilizing Elasticsearch in key data analytics initiatives to enable cyber security and cyber defense. What’s up, Doc{ker}?If you’re using Docker, with or without the Elastic Stack, we have a session for you at Elastic{ON}. Dimitrios and Toby from the Elastic infrastructure team will cover , whether on a laptop or across a fleet, and Andrew from the Elastic Beats team will show using cgroups and/or the Docker API.Using Rally to Get Your Cluster Size RightRally is our homegrown benchmarking tool for Elasticsearch. At Elastic{ON}, we will show to help with cluster sizing, performance tuning, and capacity planning.How to Use Alerting to Monitor Your ClusterWith the introduction of X-Pack, monitoring and alerting features have extended beyond Elasticsearch to the rest of the Elastic Stack. This talk will go over to get notified on events that can impact overall health and performance of your Elastic Stack deployment.Stay tuned for more agenda updates next week, but for now, make sure you before February 1st to get a $200 discount on your ticket! ","locales":"","title":"15 New Sessions Added to Elastic{ON}<sup>17</sup>"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2017-01-17T00:00:00.000Z","url":"/blog/brewing-in-beats-export-environment-variables-used-to-start-a-process","seo_title":"","content":" Welcome to the weekly ! With this weekly series, we are keeping you up to date with all the changes in Beats, including the latest commits, releases and other learning resources. Export environment variables used to start a process With this , Metricbeat exports the environment variables that were used to start a process. This is a useful feature especially for monitoring Docker containers, where environment variables are often used for configuration. The data is reported as a dictionary under the key. To reduce the number of exported environment variables and to avoid leaking sensitive data, you must define a whitelist in the configuration file under . For example: metricbeat.modules: - module: system metricsets: [process] process.env.whitelist: ['USER', 'PATH'] This feature is available on FreeBSD, Linux and OS X. Add experimental support for plugins with Go 1.8 on Linux Golang 1.8 is expected to be released in February 2017 and it with support for plugins on Linux. We an experiment to add support for plugins in Beats using the newest, not yet released Go version. It creates plugins for: More features and fixes Metricbeat Filebeat Generators All Beats Beats dashboards ","locales":"","title":"Brewing in Beats: Export environment variables used to start a process"}
{"index":{}}
{"author":"Jay Greenberg","category":"Engineering","publish_date":"2017-01-24T00:00:00.000Z","url":"/blog/kibana-and-a-custom-tile-server-for-nhl-data","seo_title":"","content":" The National Hockey League (NHL) provides public access to each game’s play-by-play data, in convenient JSON format (see sample for 1 game), which we can ingest into Elasticsearch with minimal effort.  In this blog post, we will explore approximately 73 thousand plays made (so far) in the 2016-2017 NHL Season. Try Importing the data into Elasticsearch yourself:   As always, we appreciate your feedback, so feel free to ping us on  or .  Or, if you want to kick it up a notch and meet us in person, please come to our big user conference Elastic{ON} in March 2017! ","locales":"","title":"Kibana and a Custom Tile Server for NHL Data"}
{"index":{}}
{"author":"Tanya Bragin","category":"Engineering","publish_date":"2017-01-17T00:00:00.000Z","url":"/blog/mining-earthquake-data-with-kibana-5-and-timelion","seo_title":"Mining Earthquake Data with Kibana 5 and Timelion","content":" We are really excited about all the new improvements in Kibana 5! And to highlight that, we showed a demo at that combined the power of Timelion time series analytics introduced in Kibana 5 with the core features of Kibana you know and love, such as geolocation analytics. Our attendees loved this demo so much that we decided to record it and share it with everyone. Enjoy! What’s new in Kibana 5Before we get to the demo, it might be worthwhile to review what’s new in Kibana 5. The following video is a short walk through highlighting the differences from the previous version. If you are completely new to Kibana, we also have a great . Earthquake data mining demo In this demo, we analyze a that contains data about earthquake magnitude and location for the past several decades. We use Timelion, a new feature of Kibana that introduces an expression language for powerful adhoc analytics of time-series data. For a primer on Timelion, see this . If you’d like to try this yourself, the latest version of Elastic Stack, or take it for a spin in . To help you get started, we have a tutorial on how to and a repo with installation . As always, we appreciate your feedback, so feel free to ping us on , , and . Or, if you want to kick it up a notch and meet us in person, please come to our big user conference in March 2017! ","locales":"","title":"Mining Earthquake Data with Kibana 5 and Timelion"}
{"index":{}}
{"author":"Clinton Gormley","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2017-01-23T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2017-01-23","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. — Nancy Klahn (@MamaKlahn) Elasticsearch Core We have a new aggregation, which allows to analyze co-occurence of filters, e.g. for the following list of terms, tell me how often two of these terms occur together. It has been built in order to improve the graph functionality so that users can better dive into how different nodes from a graph are connected. For instance, put under a date_histogram, it could help analyze how fraudulent bank accounts have exchanged money over time. It is however likely that users will find exciting use-cases for this aggregation outside of the context of graph. When you ask Lucene to filter a query with a numeric range its first step is to build up a bitset marking all documents accepted by your range filter by . This can result in unexpectedly poor performance when the range accepts many documents yet the other parts of the query are restrictive. But , quietly pushed this past week, for the future Lucene 6.5.0 release, Lucene is now smarter: it is able to check up front the expected cost of enumerating all hits for the range versus the expected cost of the other query clauses, and if the range is more costly, it will instead first use the other clauses to enumerate candidate hits and for each hit it will use doc values, instead of dimensional points, to check if it falls within the range filter. For queries that combine restrictive clauses with non-restrictive ranges this can be an enormous speedup. All that is required is you index your range fields using both doc values and points, and then use the IndexOrDocValuesQuery at search time, to express the range. Changes in 5.2: Changes in 5.x: Changes in master: Coming up: Apache Lucene Watch This Space , where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2017-01-23"}
{"index":{}}
{"author":"Rashid Khan","category":"Engineering","publish_date":"2017-01-17T00:00:00.000Z","url":"/blog/guarding-kibana-from-data-hijacking","seo_title":"","content":" As you may have read, open source applications exposed to the internet have recently been targeted for in the same spirit as the that targeted desktop computers years ago.  The recent round of attacks are made all the more insidious by the deleting of the data, leaving data owners with few options. ","locales":"","title":"Guarding Kibana from Data Hijacking"}
{"index":{}}
{"author":"Samir Bennacer","category":"Engineering","publish_date":"2017-01-16T00:00:00.000Z","url":"/blog/hot-warm-architecture-in-elasticsearch-5-x","seo_title":"Elasticsearch Hot Warm Architecture","content":" When using elasticsearch for larger time data analytics use cases, we recommend using time-based indices and a tiered architecture with 3 different types of nodes (Master, Hot-Node and Warm-Node), which we refer to as the \"Hot-Warm\" architecture. Each node has their own characteristics, which are described below. Master nodesWe recommend running 3 dedicated master nodes per cluster to provide the greatest resilience. When employing these, you should also have the discovery.zen.minimum_master_nodes setting at 2, which prevents the possibility of a \"split-brain\" scenario. Utilizing dedicated master nodes, responsible only for handling cluster management and state enhances overall stability. Because they do not contain data nor participate in search and indexing operations they do not experience the same demands on the JVM that may occur during heavy indexing or long, expensive searches. And therefore are not as likely to be affected by long garbage collection pauses. For this reason they can be provisioned with CPU, RAM and Disk configurations much lower than those that would be required for a data node. Hot nodesThis specialized data node performs all indexing within the cluster. They also hold the most recent indices since these generally tend to be queried most frequently. As indexing is a CPU and IO intensive operation, these servers need to be powerful and backed by attached SSD storage. We recommend running a minimum of 3 Hot nodes for high availability. Depending on the amount of recent data you wish to collect and query though, you may well need to increase this number to achieve your performance goals. Warm nodesThis type of data nodes are designed to handle a large amount of read-only indices that are not as likely to be queried frequently. As these indices are read-only, warm nodes tend to utilize large attached disks (usually spinning disks) instead of SSDs. As with hot nodes, we recommend a minimum of 3 Warm nodes for high availability. And as before, with the caveat that larger amounts of data may require additional nodes to meet performance requirements. Also note that CPU and memory configurations will often need to mirror those of your hot nodes. This can only be determined by testing with queries similar to what you would experience in a production situation. The Elasticsearch cluster needs to know which servers contain the hot nodes and which server contain the warm nodes. This can be achieved by assigning arbitrary to each server. For instance, you could tag the node with in elasticsearch.yml, or you could start a node using The nodes on the warm zone are \"tagged\" with in elasticsearch.yml or you could start a node using The box_type attribute is completely arbitrary and you could name it whatever you like. These arbitrary values will be used to tell Elasticsearch where to allocate an index. We can ensure today's index is on the hot nodes that utilize the SSD's by creating it with the following settings: PUT /logs_2016-12-26 { \"settings\": { \"index.routing.allocation.require.box_type\": \"hot\" } } After few days if the index no longer needs to be on the most performant hardware, we can move it to the nodes tagged as warm by updating its index settings: PUT /logs_2016-12-26/_settings { \"settings\": { \"index.routing.allocation.require.box_type\": \"warm\" } } Now how can we achieve that using logstash or beats: If the index template is being managed at the logstash or beats level the index templates should be updated to include allocation filtering. The \"index.routing.allocation.require.box_type\" : \"hot\" setting will cause any new indices to be created on the hot nodes. Example: { \"template\" : \"indexname-*\", \"version\" : 50001, \"settings\" : { \"index.routing.allocation.require.box_type\": \"hot\" ... Another strategy is to add a generic template for any index in the cluster, \"template\": \"*\" that creates new indices in hot nodes. Example: {","locales":"","title":"“Hot-Warm” Architecture in Elasticsearch 5.x"}
{"index":{}}
{"author":"Court Ewing","category":"Releases","publish_date":"2017-01-12T00:00:00.000Z","url":"/blog/kibana-5-1-2-and-4-6-4-released","seo_title":"Kibana 5.1.2 Released","content":" We’re happy to announce the release of Kibana versions 5.1.2 and 4.6.4. Each of these versions includes numerous bug fixes. As usual, you can get the latest Kibana release from our page. Changes in 5.1.2 Changes in 4.6.4 Many of the changes in 4.6.4 were already shipped with 5.0, but we wanted to make them available in 4.6 as well. ","locales":"","title":"Kibana 5.1.2 and 4.6.4 released"}
{"index":{}}
{"author":"Ali Beyad","category":"Engineering","publish_date":"2017-01-25T00:00:00.000Z","url":"/blog/red-elasticsearch-cluster-panic-no-longer","seo_title":"RED Elasticsearch Cluster? Panic no longer","content":" Beep… beep… beep…  It’s PagerDuty telling you that a node has gone down, or a rack has gone down, or your whole cluster has just rebooted.  Either way, your cluster is RED: some shards are not assigned, which means that your data is not fully available. : What do you do?  What do you do?! In earlier versions of Elasticsearch, figuring out why shards are not being allocated required the analytical skills of a bomb defusion expert.  You’d look through the cluster state API, the cat-shards API, the cat-allocation API, the cat-indices API, the indices-recovery API, the indices-shard-stores API… and wonder what it all means.   Now, we have added the : your one stop shop for understanding shard allocation.   The cluster allocation explain API (henceforth referred to as the explain API) was introduced as an experimental API in v5.0 and reworked into its current form in v5.2.  The explain API was designed to answer two fundamental questions: It should be noted that shard allocation problems should be a rare event in the cluster and typically are the result of node and/or cluster configuration problems (e.g. incorrectly setting up ), all nodes holding a valid shard copy having disconnected from the cluster, disk problems, and the like.  But when these problems do occur, a cluster administrator needs the right tools to identify the problem and nurture the cluster back to good health, and that is what the explain API is for. The goal of this blog post is to highlight the power of the explain API by going through some examples of how to use it to diagnose shard allocation problems.   What is shard allocation?Shard allocation is the process of assigning a shard to a node in the cluster.  In order to scale to huge document sets and provide high availability in the face of node failure, Elasticsearch splits an index’s documents into shards, each shard residing on a node in the cluster.  If a primary shard cannot be allocated, the index will be missing data and/or no new documents can be written to the index.  If replica shards cannot be allocated, the cluster is at risk of losing data in the case of permanent failure (e.g. corrupt disks) of the node holding the primary shard.  If shards are allocated but to slower-than-desired nodes, then the shards of high traffic indices will suffer from being located on slower machines and the performance of the cluster will degrade.  Therefore, allocating shards and assigning them to the best node possible is of fundamental importance within Elasticsearch.   The shard allocation process differs for newly created indices and existing indices.  In both cases, Elasticsearch has two main components at work: allocators and deciders.  Allocators try to find the best (defined below) nodes to hold the shard, and deciders make the decision if allocating to a node is allowed. For newly created indices, the allocator looks for the nodes with the least amount of shards on them and returns a list of nodes sorted by shard weight, with those having the least shard weight appearing first.  Thus, the allocator’s goal for a newly created index is to assign the index’s shards to nodes in a manner that would result in the most balanced cluster.  The deciders then take each node in order and decide if the shard is allowed to be allocated to that node.  For example, if filter allocation rules are set up to prevent node <code>A</code> from holding any shards of index <code>idx</code>, then the <code>filter</code> decider will prevent allocation of <code>idx</code>’s shards to node , even though it may be ranked first by the allocator as the best node from a cluster balancing perspective.  Note that the allocators only take into account the number of shards per node, not the size of each shard.  It is the job of one of the deciders to prevent allocation to nodes that exceed a certain disk occupancy thresho","locales":"","title":"RED Elasticsearch Cluster? Panic no longer"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2017-01-23T00:00:00.000Z","url":"/blog/brewing-in-beats-optimize-matching-regular-expressions","seo_title":"","content":" Welcome to the weekly ! With this weekly series, we are keeping you up to date with all the changes in Beats, including the latest commits, releases and other learning resources. Optimize matching regular expressions wasn’t happy with the performance of the regular expressions implementation in the Go standard library so he by factors up to 10x and even 100x. The idea of the optimization is that in Beats we accept regular expressions in places where most people use them for simple substring searches (e.g. exclude_lines, include_lines, or multiline), so we can automatically detect these cases in regexps and switch to faster implementations. See the for benchmark results. Community Beat: Apexbeat extracts configurable contextual data and metrics from Java applications via the and indexes them into Elasticsearch. Community Beat: Connbeat collects TCP connections metadata to index them in Elasticsearch. For each connection, it gives you details about the IP, port of the endpoints involved, together with details about the local process. You can use it to monitor the connections in , and you get extra metadata about your Docker container. The exported data are similar to the ones provided by the metricset in Metricbeat. For now, it works only on Linux systems. Initial setup of Filebeat modules With this , the -setup option is added to the Filebeat command line for loading the Ingest Node pipeline at startup if the Elasticsearch output is configured. In case Elasticsearch is not available, then Filebeat fails to start with an error.  For example, the following command includes the option to initialize the Filebeat Nginx module before running Filebeat: filebeat -e -modules nginx -setup In the near future, we are planning to load also the Kibana dashboards, in addition to the Ingest Node pipeline, when using the flag. Other features and fixes All Beats Filebeat Heartbeat Metricbeat Generators ","locales":"","title":"Brewing in Beats: Optimize matching regular expressions"}
{"index":{}}
{"author":"Suyog Rao","category":"The Logstash Lines","publish_date":"2017-01-23T00:00:00.000Z","url":"/blog/logstash-lines-2017-01-23","seo_title":"","content":" Welcome back to The Logstash Lines! In these weekly posts, we'll share the latest happenings in the world of Logstash and its ecosystem. events.in stats were previously recorded when the events entered the filter stage. This was ok until persistent queues were introduced, which made the existing calculation method inaccurate (due to buffering). modifies the code to calculate \"in\" metrics before inserting into queue. Implementing a process for the queue which helps in recovering data that was written to the data file, but not yet checkpoint'd. Consider a situation when the input has written a bunch of data to the queue, but Logstash crashes before checkpointing. Now, , on startup, we scan if there is any data in the queue but ahead of the checkpoint location and re-process them.Eric Johnson from Google a Logstash input for pulling events from Google service.  ","locales":"","title":"Logstash Lines: Fixes for 5.x version, more metrics APIs"}
{"index":{}}
{"author":"Clinton Gormley","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2017-01-16T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2017-01-16","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. — elastic (@elastic) Elasticsearch Core Multi-word synonyms and synonym graphs There has been much work recently on improving Lucene's handling of , where analysis of text, either from a document during indexing, or a query during searching, produces multiple overlapping paths or interpretations for the tokens. Multi-word synonyms do this and have but thanks to the recent addition of as well as , such analysis chains are finally handled correctly at search time. is also being fixed . These changes have already been exposed in Elasticsearch, and then subsequently in Lucene, thanks to . Graph token streams still present challenges, though, such as the need to use during indexing, but not searching, since a Lucene index cannot represent a graph. There are also a number of token filters that should produce a graph but do not yet, such as and decompounders. Changes in 5.x: Changes in master: Upcoming changes: Apache Lucene Watch This Space , where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2017-01-16"}
{"index":{}}
{"author":"Mike Paquette","category":"Engineering","publish_date":"2017-01-13T00:00:00.000Z","url":"/blog/protecting-against-attacks-that-hold-your-data-for-ransom","seo_title":"Protecting Against Attacks that Hold Your Data for Ransom","content":" Late last week, a was initiated, in which data from thousands of open source databases was copied, deleted and held for ransom.  Although no malware, or “ransomware” was used in these attacks, and they are not related to product vulnerabilities, they nonetheless represent serious security incidents involving a data loss, or even a data breach.  The good news is that data loss from similar attacks is easily preventable with proper configuration. ","locales":"de-de,fr-fr,ja-jp,ko-kr","title":"Protecting Against Attacks that Hold Your Data for Ransom"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2017-01-10T00:00:00.000Z","url":"/blog/brewing-in-beats-dynamic-module-configuration-reloading","seo_title":"","content":" Metricbeat: Dynamic configuration reloading for modules Until now, any change in the modules configuration required a Metricbeat restart. This allows you to define a configuration directory where new files can be added, removed or modified and updates will automatically be processed by Metricbeat. New modules are started and stopped accordingly. This is especially useful in container environments where one Metricbeat instance is used to monitor all services in other containers on the same host, and the set of containers changes dynamically. Metricbeat: Add Kafka consumer groups metricset A new metricset is to the Kafka module that exports the consumer group information, including the offsets. It also exports client to partition assignments (if available) showing the current consumer handling a partition. Like the producer metricset, it queries only the configured broker, and it reports only the groups currently managed by that broker. Filebeat: Generate a new fileset Following the model of the metricset generator in Metricbeat, this adds a generator for Filebeat modules. It creates templates for most of the required files of a fileset. Example run: MODULE=apache FILESET=access make create-fileset Filebeat: Apache2 module Added another sample Filebeat module, this time for the web server, able to read and parse access and error logs. It comes with an example Kibana dashboard. All Beats: Make the decode_json_fields target field configurable The decode_json_fields processor can be used for decoding nested JSON objects from other fields. With this , you can configure the target field, under which to place the parsed JSON. All Beats: Format the exported data By default, the Beats are exporting the data in a JSON format regardless of the output. However, it can be sometimes more convenient to specify a more compact format, so with this opened by you can specify a format string for the console, file, Kafka, or Redis outputs: output.kafka: format: \"%{@timestamp} %{message}\" Metricbeat: Export MySQL query counters Thanks to , Metricbeat now exports for each MySQL query type. The values are available under the mysql.status.command field. All Beats: Generate Makefile documentation The Beats Makefile becomes more and more complex over time, and it becomes necessary to document its targets.  Thanks to , you can run to list the available targets and a short description for each. Metricbeat: Increase the field limit in the mapping Starting with 5.0, Elasticsearch has a default limit of 1000 fields in the mapping. Because it creates mappings for all its modules, Metricbeat already is a bit over the limit. This raises the limit in our template files. In most deployments, these fields will not be used all at the same time, so the effects of increasing the limit shouldn’t be dangerous. ","locales":"","title":"Brewing in Beats: Dynamic module configuration reloading"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2017-01-12T00:00:00.000Z","url":"/blog/elasticsearch-5-1-2-and-2-4-4-released","seo_title":"Elasticsearch 5.1.2 and 2.4.4 released","content":" Today we are pleased to announce the bug fix release of , the latest stable release, and , the latest release in the legacy 2.x series. Both are already available for deployment on , our Elasticsearch-as-a-service platform.All 5.x users are advised to upgrade.Latest stable release in 5.x:Latest stable release in 2.x:Full details of the changes in this release are available in the release notes listed above, but there are a few important changes which are worth singling out: ","locales":"","title":"Elasticsearch 5.1.2 and 2.4.4 released"}
{"index":{}}
{"author":"Clinton Gormley","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2017-01-09T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2017-01-09","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Finished write-up over holidays: Everything we know about for e-commerce sites — Martin Loetzsch (@martin_loetzsch) Elasticsearch Core Apache Lucene Watch This Space , where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2017-01-09"}
{"index":{}}
{"author":"Dale McDiarmid","category":"Engineering","publish_date":"2017-01-09T00:00:00.000Z","url":"/blog/integrating-elasticsearch-with-arcsight-siem-part-2","seo_title":"Integrating Elasticsearch with ArcSight SIEM - Part 2","content":" In the in the “Integrating the Elastic Stack with ArcSight SIEM” blog series, we demonstrated how to send Common Event Format (CEF) data from an ArcSight connector to the Elastic Stack.  We will continue the series with several posts illustrating how to proactively monitor security data in Elasticsearch using X-Pack alerting features.  The X-Pack alerting capability allows users to watch for changes or anomalies in their data and perform necessary actions in response.  For those new to the capability, consider exploring the and .  While we have many of alerting on data in Elasticsearch, we will focus our examples in this blog series specifically on data in the CEF format. Our hope is that providing these examples tailored for the CEF format will be valuable to users looking to complement their existing ArcSight-based security solutions with the Elastic Stack. The alert configurations (“watches”) themselves, however, are applicable to any CEF compliant data with the required fields. The test environment referenced additionally requires no further configuration to integrate with ArcSight - simply forward events to the Logstash TCP port and start enjoying the watches! Throughout this blog series we will focus on rule-based alerting, starting with a relatively simple use case before progressing to more complex pattern detection.  Finally we will explore machine learning-based anomaly detection as we attempt to address the obvious challenges with pure rule-based alerting approaches for more complex pattern detection.  Security analytics watches execute against a constant stream of data in real time. Each watch will execute periodically, accessing the last N seconds and raising an alert if required. Throughout this series we utilise several static historical datasets which cover up to a month period. In order to test, we can’t realistically ingest the data and wait for events to fire over the next month! We have therefore adjusted the watches to execute over the full indexed dataset. Successful Login From an External IP We start with a simple watch to introduce the concepts of using basic alerting in X-Pack to identify specific patterns in your Elasticsearch data. For our first use case, we utilise a common source of event data - authentication logs. The sample dataset consists of inbound SSH logs (in the CEF format) from a single honeypot during the month of November 2016. Below is a sample log line in the CEF format. The highlighted fields will be used in the watch definition.  CEF:0|Unix|Unix|5.0|cowrie.login.success|login attempt [root/!root] succeeded|Unknown|destinationUserName=root externalId=15 startTime=Nov 15 2016 19:22:29 destinationHostName=elastic_honeypot destinationAddress=192.168.20.2 deviceReceiptTime=Nov 15 2016 19:22:29 deviceTimeZone=Z transportProtocol=TCP applicationProtocol=SSHv2 destinationServiceName=sshd devicePayloadId=15 message=login attempt [root/!root] succeeded categoryOutcome=success categoryBehaviour=cowrie.login.success sourceTranslatedAddress=192.168.1.105 sourceAddress=192.168.1.105 deviceDirection=1 cs1=0 cs1Label=isError cs2=SSHService ssh-userauth on HoneyPotSSHTransport,2,192.168.1.105 cs2Label=system cs3=!root cs3Label=password cs4=111f70f0 cs4Label=session<span style=\"font-size: 14px:  white-space: normal: \"> </span> The values of and for our represent a failed and successful login respectively.  The field indicates the target server.  Note that while our example data is from a single server, the provided watch could be applied to a larger infrastructure with potentially hundreds of servers. To determine if the connection originated from an external IP address, we utilise the field . Finally, the indicates the time at which the event occurred. This field will be be parsed by our Logstash config file into the field. To ingest this dataset, we use a simple Lo","locales":"","title":"Integrating Elasticsearch with ArcSight SIEM - Part 2"}
{"index":{}}
{"author":"Nick Knize","category":"Engineering","publish_date":"2017-01-05T00:00:00.000Z","url":"/blog/numeric-and-date-ranges-in-elasticsearch-just-another-brick-in-the-wall","seo_title":"Numeric and Date Ranges...Just Another Brick in the Wall","content":" Imagine being able index your calendar to quickly find all events whose time range conflicts with a proposed event. Or creating a global television guide to find all shows, movies, sports, or broadcasts that are aired during certain time periods. Looking for something even more futuristic? Imagine creating a catalog of in order to more rapidly identify, classify, and diagnose potential malignant activity. From discrete to continuous Until recently, these use cases have been extremely difficult, or next to impossible, to solve at scale using Elasticsearch's discrete and fields. While running queries over discrete points are the most typical and widely used capability, it quickly became clear that having the ability to index and search over continuous ranges would be a tremendously useful feature to support. Thanks to recent advancements in Apache this desire has become a reality in . Elasticsearch welcomes Numeric and Date Range field types We are proud to announce the following new Range field types are included in the Elasticsearch 5.2 release: The mapping definition for these new range data types work the same way as their discrete Numeric and Date counterparts. The following provides an example mapping definition for a document type (called \"conference\") that contains both an integer and date range: PUT events/ { \"mappings\" : { \"conference\" : { \"properties\" : { \"expected_attendees\" : { \"type\" : \"integer_range\" }, \"time\" : { \"type\" : \"date_range\", \"format\" : \"yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\" } } } } } Indexing a document using the above mapping is as simple as defining the range fields the same way you would in a or Range Query. Like Date Range queries, indexing a Date Range field also uses the same Date Math format definition. The following example demonstrates indexing a document with an integer_range and date_range field type: PUT events/conference/1 { \"title\" : \"Pink Floyd / Wizard of Oz Halloween Trip\", \"expected_attendees\" : { \"gte\" : 100000, \"lt\" : 200001 }, \"time\" : { \"gte\" : \"2015-10-31 12:00\", \"lt\" : \"2015-11-01\" } } Querying Numeric and Date Range fields use the same  domain specific language (DSL) definition as before with the addition of a new optional relation parameter. This parameter enables the user to define the type of Range query desired. This is defined as one of: (default), , or and can be combined with other Boolean queries to achieve different desired results (e.g., ). The following example demonstrates a query over the events index: GET events/_search { \"query\" : { \"range\" : { \"time\" : { \"gte\" : \"2015-10-01\", \"lte\" : \"2015-12-31\", \"relation\" : \"within\" } } } } Like discrete numeric and date fields their new range counterparts are restricted to a single dimension only. In the future we plan to lift this restriction and expand them to handle 8 and 4 dimensions, respectively. Lucene implementation On October 18th, we published a blog post describing . This historical account of Lucene’s maturation as a numeric search engine details the technical advancements of numeric indexing and search culminating in the . Instead of continuing to represent numeric data using a structure specifically designed and tuned for text, the Bkd implementation introduced the first flexible tree structure designed specifically for indexing discrete numeric points. These discrete points can now be searched in Elasticsearch using the  DSL definition. Built around the theory and concepts of a (k-d tree), the new codec format also paved the way for providing support for more advanced scientific or data analysis use cases and provide the foundation classes needed to bring the range field feature to Lucene and ultimately to Elasticsearch. Indexing The solution was fairly trivial: for each dimension, represent the dimensional numeric ran","locales":"de-de,fr-fr,ja-jp,ko-kr","title":"Numeric and Date Ranges in Elasticsearch: Just Another Brick in the Wall"}
{"index":{}}
{"author":"Suyog Rao","category":"The Logstash Lines","publish_date":"2017-01-09T00:00:00.000Z","url":"/blog/logstash-lines-2017-01-09","seo_title":"","content":" Welcome back to The Logstash Lines and a happy new year to our readers! In these weekly posts, we'll share the latest happenings in the world of Logstash and its ecosystem.Ability to remove tags field from output post 5.0.x. This was a regression introduced by a previous which went into 5.0.1 and 5.1.0 Offline plugin management has been a pain point for users ever since this feature was introduced. In the existing workflow, the complete set of plugin dependencies weren't downloaded when the offline pack was created on the staging machine. Now we've the workflow and also fixed the bugs. On a staging machine, we now traverse the entire dependency graph to download all the dependencies, package the plugins in an archive which can directly be installed on the LS deployment with no internet access. The original commands have been deprecated in favor of . Once such a pack is created, there is no need to use a special unpack command. Users can simply install plugins using bin/logstash-plugin install file:///<path_to_offline_pack.zip>Most production Logstash configs can been complex with multiple data processing flows created using conditional logic. One experimental feature we're targeting for 5.3.0 is running multiple, separate pipelines in the same LS instance (same JVM). This could simplify LS config and also make it similar to how an ingest node can run multiple ingest pipelines. Stay tuned. ","locales":"","title":"Logstash Lines: Offline plugin management, multiple pipelines."}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2017-01-02T00:00:00.000Z","url":"/blog/brewing-in-beats-track-network-connections","seo_title":"","content":" Happy New Year everyone! 🍾 New community Beat: Cloudflarebeat created to fetch Cloudflare logs via the Enterprise Log Share API. It works by requesting the logs within a time range, ending 30 minutes ago latest, and then saving the gzip content in a local file. The JSON log entries are read from the file, processed and indexed in Elasticsearch. Metricbeat: Track network connections The new reports an event for each TCP socket that it sees on Linux systems. For each TCP socket, it reports the process that opened the socket, the local and remote IPs involved in the communication, and the direction (incoming, outgoing or listening). It can also perform a reverse lookup on the remote IP. It works by polling the Linux kernel to get the sockets, so a short polling interval is recommended if catching short lived connections is required. Once the data is in Elasticsearch, you can visualize the network connections using :Filebeat: Per module Kibana dashboardsWith the addition of Filebeat modules, the Kibana dashboards are between the modules, and they can be gathered by performing the command. As a consequence, the directory is no longer checked in as it can be easily generated. The same approach was applied to . Filebeat: Add integration tests for modulesThe consist of loading the template and the pipeline in Elasticsearch, reading the log files from the module test directory and then check if the expected data is available in Elasticsearch. ","locales":"","title":"Brewing in Beats: Track network connections"}
{"index":{}}
{"author":"Kristina Frost","category":"Culture","publish_date":"2016-12-23T00:00:00.000Z","url":"/blog/elastics-holiday-spirit-the-search-for-sharing","seo_title":"","content":" Last year, about the way Elastic employees from all over the globe came together to rally time and resources to celebrate and prop up people whom they might never have a chance to meet. I wrote about how important giving is, how important caring is, amidst an international climate that can feel threatening and hostile. Then 2016 came. I won’t replay the year’s many lows:  this is not going to be the blog post for folks who believe that the best way to recap this year is to put the dumpster fire .gif on endless repeat. Instead, we’re going to talk about the audacity of continuing to give. Shay Banon, our Co-Founder and CTO, and Leah Sutton, our VP of Human Resources, have recently written at length about Elastic’s commitment to who are using our Stack to . We’re doubling down on . There’s more goodness to come (spoilers!) as we march towards , all of it central to the big aspirations of our corporate heart.  Today, though, we’ve got a whole crew joining me on the virtual stage to talk about the smaller, more personal ways we’re coming together to do good.  John Black, Sr. Director, Global Sales Development: Moustaches and Men’s HealthFor as long as I can remember, my dad has always had a moustache. Except for that one time when I was in 3rd grade, and he decided to shave it off. My sister and I begged him to grow it back because he looked so strange without it.  For the better part of my life, I’ve gone without a moustache. Then in October 2012, I learned about Movember. The was launched in 2003 in Australia. And since then, more than 5 million people – Mo Bros and Mo Sistas, as they’re called – have joined the cause, helping to raise $710 million that have supported 1,200 men’s health projects. These projects are focused on three of the biggest health issues faced by men: prostate cancer, testicular cancer and mental health and suicide prevention. For the past four years, I have been a Mo Bro. Although there are a variety of ways to participate in Movember, I have opted to let the hair on my upper lip grow. And each Movember Eve, I get that mixed feeling of manliness, commitment and discomfort around facing the next 30 days.  I believe in this cause. I believe that men’s health issues are important. And not just because I am a man. But because men, too often, don’t want or know how to talk about their physical and mental health issues. That’s a problem and something that needs to change. Two experiences in my life have made Movember especially personal. First, my grandfather (I called him Papa) died from prostate cancer when he was in his early 70s. This cancer ravaged his healthy body and mind, and we lost him too early. It was frustrating to learn that, had the cancer been detected earlier, he would likely have beaten the disease. And second, I suffered a serious depression when I was 21. I am grateful that I had amazing friends and family who helped me talk about my issues and get me through this difficult time. I’m a son, a brother and a friend. But my roles as husband and father are the biggest catalysts behind my participation in Movember. I want to live a long, healthy life alongside my family. So, I’ll do what I can to boost awareness around men’s health issues. And if growing a Mo will help me raise money to improve, lengthen and save lives, then I’m all in. This Movember, through the generosity of friends, family, and in particular, my Elastic family, I raised $2,236. This money will go toward funding ground-breaking programs across the globe. I am proud and privileged to work at a company whose technology empowers people to make a difference and enact change in their companies, communities and society at large. I love being a part of a company with a heart and soul. We can (and should) do well while doing good. George Young, Area Vice President, Public Sector: Supporting the Youth of NicaraguaMy family has been traveli","locales":"","title":"Elastic’s Holiday Spirit: The Search for Sharing"}
{"index":{}}
{"author":"Zachary Tong","category":"Engineering","publish_date":"2016-12-22T00:00:00.000Z","url":"/blog/a-profile-a-day-keeps-the-doctor-away-the-elasticsearch-search-profiler","seo_title":"","content":" Back in the dark ages of Elasticsearch, understanding why a query was slow required experience, intuition, reading some tea leaves and making sure the moon was precisely aligned with Jupiter (unless it’s January, then Venus was a better option). Of course, the length of a lunar cycle varies from month to month, so we decided a more scientific approach was needed. ​In version 2.2, we added a to Elasticsearch. This API instruments each component of your search query and provides detailed timing information at each node. And in 5.0 we added support for aggregation profiling too, providing detailed timing breakdowns for each bucket and metric in your aggregation tree. No longer do you have to divine knuckle bones and poke around mushy tea leaves:  just send your query to the Profile API and get an accurate readout of which components are being slow. Progress! There is, however, one minor hiccup. The Profile API is . Fill-your-screen-with-megabytes-of-JSON verbose. Users generally expect diagnostics like a profiler to run slower, generate more output and are produce more technical responses than other APIs. So we used that as a license to go whole hog and extract every last bit of information that we could. The Profile API collects timings and method invocation-counts for each component in your query, on each shard in the index, in every index being queried. So if you are profiling a somewhat complex query across several indices – each holding a non-negligible amount of data – the response you get back will be positively huge. Our support team often asks for Profile outputs when customers are experiencing slow queries… and they often forward those outputs to me asking for help. I can assure you, it only takes a few episodes of dealing with a multi-megabyte JSON blob to realize there needs to be a better way to parse the output. The human eye is just not equipped to deal with that many curly braces! Iterating on a useful tool Thus began a project to help visualize and interact with the Profile API’s output. The Search Profiler was originally an in-house tool to help our support engineers. The JSON queries that you construct and send to Elasticsearch are essentially a tree structure. Compound queries (such as Boolean queries) represent tree nodes that hold children queries. These children can themselves be compound queries which hold own children, or they can be “leaf” queries that interact score documents directly (such as a Term query). So the first iteration of the Search Profiler attempted to represent the query – and its associated profiling times – as a tree structure. If you attended you may have noticed this screenshot floating around: This hierarchical layout worked great in small tests. But, as so often in real life, it fell apart when it met real user data. As the query complexity increased, the hierarchical layout became increasingly difficult to fit on the screen, let alone understand in an intuitive manner. Showing many shards in a compact manner became especially troublesome. It is not uncommon to be querying dozens of indices, each with 5+ shards, with a large and complex query. The output from the Search Profiler was almost as verbose as the source JSON. We iterated on the tree structure for a while, adding support for auto-collapsing “uninteresting” branches and dynamic scrolling. But ultimately, the tree structure sacrificed precision detail (which is critically important in a profiling tool!) for a pretty “high-level” view. We also realized that as the query complexity increases, the relative differences between nodes becomes very small (e.g. each node is 0.5% of the total query time). Seeing a hotspot becomes very difficult in a tree view because all nodes assume the same bland, gray color. We decided to move away from the tree design and adopt a more conventional profiling output: hie","locales":"","title":"A Profile a Day Keeps the Doctor Away: The Elasticsearch Search Profiler"}
{"index":{}}
{"author":"Thomas Neirynck","category":"Engineering","publish_date":"2016-12-22T00:00:00.000Z","url":"/blog/tag-that-cloud-a-new-visualization-in-kibana","seo_title":"Tag that Cloud","content":" The 5.1 Elastic Stack arrived last month, and with it, a brand new and svelte visualization for Kibana: Tag Cloud! It is the first new visualization in a long time and we look forward to our users using Tag Cloud to enrich their dashboards, reports and presentations. IntroductionTag or word clouds show a collection of words, terms or small phrases, laid out all adjacent to each other. The size of the tags or words generally corresponds to their importance. In Kibana specifically, importance here means whatever metric you chose to calculate for a term aggregation. Often this would be a count of most prominent terms, but any other metric supported by Elasticsearch can be used to size the tags. You might have seen tag clouds accompany newspaper articles about the occurrence of prominent or unusual words in political speeches, or in online databases for collections of relevant keywords. Or here below, where two clouds provide a quick overview of the most common home worlds and weapons in the Star Wars universe. Creating your own tag cloudThe options in the design panel will be familiar to frequent Kibana users.Users can select the easing function that maps metric value to font size. The default is a linear 1-to-1 mapping. Alternatively, use the log-scaling if you want to downplay differences in metric values. In practice, this one means that the difference in font-size will be smaller for higher values. Finally, use the square root easing function if you want to exaggerate differences in metric value. In practice, that one means that the difference in font-size will be smaller for lower values.For those who like their buffalo wings mildly flavored, the default will generate clouds where all the words are upright. The daring can spice up their clouds with more orientations. Know your audience of course, and choose wisely. In some situations, there will not be sufficient space to position all the words on screen. This occurs when the tags have too large a font, the words are too long, or there are just too many tags to begin with. Kibana will omit tags to ensure there are no overlaps.To avoid these omissions, provision sufficient space for the Tag Cloud to show all the words. Another alternative is to limit the size of your clouds to a lower number of terms, using the size parameter in the data panel.Adding to a dashboardJust like all other Kibana Visualizations, you can add a Tag Cloud to a dashboard. And just like on any other dashboard, you can interact with Tag Cloud to add filters to the filter bar. Pie charts or a simple tables are often used for that same purpose. Now, Tag Cloud serves as a more playful alternative for your end users to drill down in your data. Naturally, Tag Cloud integrates with the rest of the Kibana family. Install X-Pack to include tag clouds in your reports, save and restore them in the Object Manager, and use them in Kibana's Elastic Cloud deployments. Feedback please!With Tag Cloud, after some hiatus, we hitched another wagon to Kibana's Visualization train. But do not presume this is the last one though, or that it will take another year for us to introduce new visualizations. We have big plans for Kibana Visualize and are working on some major new additions and improvements in the coming 5.x releases. So hop aboard, and come experience where this ride will take you! And as always, never hesitate to submit . ","locales":"","title":"Tag that Cloud: A New Visualization in Kibana"}
{"index":{}}
{"author":"Steffen Siering","category":"Engineering","publish_date":"2016-12-21T00:00:00.000Z","url":"/blog/monitoring-kafka-with-elastic-stack-1-filebeat","seo_title":"Monitoring Kafka with Elastic Stack: Filebeat","content":" Kafka clusters provide a number of opportunities for monitoring. At the network level, you can monitor connections between Kafka nodes, Zookeeper, and clients. At the host level, you can monitor Kafka resource usage, such as CPU, memory and disk usage. And Kafka itself provides log files, an API to query offsets, and JMX support to monitor internal process metrics. In this blog post, the first in a series that show you how to use Beats for monitoring a Kafka cluster, we’ll  focus on collecting and parsing Kafka logs by using Filebeat and Elasticsearch Ingest Node. After indexing the Kafka logs into Elasticsearch, we’ll finish this post by building Kibana dashboards for visualizing the data. This blog post is based on The Elastic Stack version 5.1.1.  All configuration files, dashboards and sample log files can be found on . Cluster Setup Our setup contains a Kafka cluster of 3 nodes named kafka0, kafka1, and kafka2. Each node runs Kafka version 0.10.1 and a set of Beats to monitor the node itself. The Beats will send all information collected to Elasticsearch. For visualization we will use Kibana. Meanwhile producers and consumer groups are actively using the Kafka cluster. Collecting Kafka LogsWe want to start by using Filebeat to collect the Kafka log files. Having installed Kafka to , the log files will be written to . Additionally we’re going to collect some GC stats from . With Filebeat just shipping log files as-is, we’ll use to parse the log files before indexing. Prepare ElasticsearchKafka logs by default contain the timestamp when the message was logged, the log level, the Java class responsible for the log message, the message, and an optional stacktrace. This information can be parsed using grok. We will start by configuring the kafka-logs ingest pipeline to parse the log messages, and extract the Java exception class and error messages: $ curl -XPUT 'http://elasticsearch:9200/_ingest/pipeline/kafka-logs' -d@kafka-logs.json {\"acknowledged\" : true} { \"description\": \"Kafka Log Messages\", \"processors\": [ { \"grok\": { \"field\": \"message\", \"trace_match\": true, \"patterns\": [ \"(?m)%{TIMESTAMP_ISO8601:log-timestamp}. %{LOGLEVEL:level} +%{JAVALOGMESSAGE:message} \\\\(%{JAVACLASS:class}\\\\)$[ \\\\n]*(?'trace.full'.*)\" ] } }, { \"grok\": { \"field\": \"message\", \"patterns\": [ \"\\\\[%{DATA:component}][,:. ] +%{JAVALOGMESSAGE:message}\" ], \"on_failure\": [ { \"set\": { \"field\": \"component\", \"value\": \"unknown\" } } ] } }, { \"grok\": { \"field\": \"trace.full\", \"ignore_missing\": true, \"patterns\": [ \"%{JAVACLASS:trace.class}:\\\\s*%{JAVALOGMESSAGE:trace.message}\" ], \"on_failure\": [ { \"remove\": { \"field\": \"trace\" } } ] } }, { \"rename\": { \"field\": \"@timestamp\", \"target_field\": \"beat.read_time\" } }, { \"date\": { \"field\": \"log-timestamp\", \"target_field\": \"@timestamp\", \"formats\": [\"yyyy-MM-dd HH:mm:ss,SSS\"] } }, {\"remove\": {\"field\": \"log-timestamp\" }} ], \"on_failure\" : [{ \"set\" : { \"field\" : \"error.log\", \"value\" : \"{{ _ingest.on_failure_message }}\" } }] } Next we want to set up a second ingest pipeline that extracts garbage collection stats like the amount of memory used before and after garbage collection and the pause times: $ curl -XPUT 'http://elasticsearch:9200/_ingest/pipeline/kafka-gc-logs' -d@kafka-gc-logs.json {\"acknowledged\" : true} The pipeline uses to convert some values and compute delta values on memory usage before and after garbage collection. { \"description\": \"Kafka GC Logs\", \"processors\": [ { \"grok\": { \"field\": \"message\", \"patterns\": [ \"(?m)%{TIMESTAMP_ISO8601:log-timestamp}:.*GC pause.*, %{NUMBER:gc_pause:float}.*Eden: %{MEM:eden.before.used}\\\\(%{MEM:eden.before.total}\\\\)->%{MEM:eden.after.used}\\\\(%{MEM:eden.after.total}\\\\).*Survivors: %{MEM:survivors.before.used}->%{MEM:survivors.after.used}.*Heap: %{MEM:heap.before.used}\\\\(%{MEM:heap.before.total}\\\\)->%{MEM:heap.after.used}\\\\(%{MEM:heap.after.total}\\\\).*Times: user=%{NUMBER:time.user.s","locales":"","title":"Monitoring Kafka with Elastic Stack: Filebeat"}
{"index":{}}
{"author":"Russ Cam","category":"Engineering","publish_date":"2016-12-21T00:00:00.000Z","url":"/blog/solidifying-releases-with-fsharp-make","seo_title":"","content":" It's no secret that we're big fans of and here in the Microsoft team at Elastic, and in general:  It's our Windows build tool of choice for the and , and we whenever we get the chance. For this , we'd like to highlight a few things that we do when we release a new version of the .NET client library, and also introduce a new helper that we hope will benefit other .NET open source projects in documenting breaking changes in new releases. Ultimately, we hope you'll find this post inspiring enough to give FAKE a try and add some polish your own build processes. Current PipelineScripting the buildAs with all dependencies in our .NET projects, the dependencies for our build pipeline are handled with Paket. Taking advantage of groups within the paket.dependencies file, all the dependencies for the build are installed in to a build directory within the packages directory group build source https://www.nuget.org/api/v2 nuget FAKE nuget FSharp.Data nuget GitLink prerelease The  that live under , each with a core function. Each script can easily reference our build dependency assemblies. For example, referencing FAKE #I @\"../../packages/build/FAKE/tools\" #r @\"FakeLib.dll\" open Fake With this referenced, we have the full breadth of the at our disposal. Any build system worth its salt ships with tons of pre-baked solutions to common build problems, so this is not really a unique selling point for using FAKE. What is then? An example of where this type system helps is through . This looks similar tomodule Projects = type DotNetFrameworkIdentifier = { MSBuild: string:  Nuget: string:  DefineConstants: string:  } type DotNetFramework = | Net45 | Net46 | NetStandard1_3 static member All = [Net45:  Net46:  NetStandard1_3] member this.Identifier = match this with | Net45 -> { MSBuild = \"v4.5\":  Nuget = \"net45\":  DefineConstants = \"TRACE: NET45\":  } | Net46 -> { MSBuild = \"v4.6\":  Nuget = \"net46\":  DefineConstants = \"TRACE: NET46\":  } | NetStandard1_3 -> { MSBuild = \"netstandard1.3\":  Nuget = \"netstandard1.3\":  DefineConstants = \"TRACE: DOTNETCORE\":  } With this setup, we can document in code the behavioural constraints for each project, for example, whether it should be built for .NET core, packaged and published to Nuget, and so on. Build all the thingsLong before the had helpers for calling the to build cross platform .NET assemblies, there was the now deprecated DNX/DNU/DNVM set of command line tools to perform this task. With users wishing to use the Elasticsearch .NET clients cross platform, integrating this toolchain into our build pipeline was easy thanks to FAKE. Here's an example of the now defunct tooling we had in place type DotNetRuntime = | Desktop | Core | Both type DnxTooling(exe) = member this.ExecWithTimeout runtime failedF workingDirectory arguments timeout = match (runtime, hasClr, hasCoreClr) with | (Core, _, Some c) -> let proc = c.Process exe execProcessWithTimeout proc arguments timeout | (Desktop, Some d, _) -> let proc = d.Process exe execProcessWithTimeout proc arguments timeout | (Both, Some d, Some c) -> let proc = d.Process exe let result = execProcessWithTimeout proc arguments timeout if result <> 0 then failwith (sprintf \"Failed to run dnx tooling for %s args: %A\" proc arguments) let proc = c.Process exe execProcessWithTimeout proc arguments timeout | _ -> failwith \"Tried to run dnx tooling in unknown state\" |> ignore let Dnu = new DnxTooling(\"dnu.cmd\") let Dnx = new DnxTooling(\"dnx.exe\") , and used this information to determine what could be built. Thankfully, this can now be replaced with DotNetCli.Build (fun p -> { p with Configuration = \"Release\" }) The more we can leverage the tooling, the less we need to maintain ourselves! We currently use both dotnet CLI and MSBuild via FAKE, to compile assemblies targeting .NET 4.5, 4.6 and NetStandard 1.3. Stepping through Code (based on ) allows developers to debug and step through your code when it's hosted on GitHub, without ","locales":"","title":"Solidifying Releases with FAKE (F# MAKE)"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2016-12-19T00:00:00.000Z","url":"/blog/brewing-in-beats-filebeat-modules-for-mysql-and-syslog","seo_title":"","content":" New community Beat: Udplogbeat created that receives the events via local UDP socket (in plain-text or JSON with ability to enforce schemas) to index  them in Elasticsearch. An interesting use case can be that Udplogbeat allows any application to easily log messages locally without writing them to disk. For example it can be used for applications that only support syslog logging. This solution doesn’t guaranty that each log message is sent at least once as in case Udplogbeat is down for any given reason, log messages sent over UDP will not reach Elasticsearch. Filebeat: Mysql and syslog modules (in progress) Recently we started working on a for Filebeat modules. Following fairly closely the model started in Metricbeat, a Filebeat module contains the out of the box configuration needed to read, parse and visualize data from various log files formats. This includes Ingest Node pipelines, Elasticsearch templates, Filebeat prospectors configurations, and Kibana dashboards.  In the past week, we added two more sample modules, for and , to further test the assumptions made by the prototype. Work is currently being done to create automatic integration tests for these modules, then we can replace the prototype with the actual code. Metricbeat: Couchbase support You can now monitor the Couchbase database thanks to the community contribution of that added the to Metricbeat. It collects 3 different metricsets from Couchbase: cluster overview metrics, node metrics and bucket metrics.The Couchbase module is experimental and we are entitled to do any changes, including breaking changes. Metricbeat: Prometheus support Metricbeat is now able to  the Prometheus server itself. In the next steps, we are planning to fetch metrics from the or any application that offers a Prometheus endpoint and index them to Elasticsearch. In this case, you can use Metricbeat and Elasticsearch as an alternative to Prometheus. The Prometheus module is marked as experimental. Metricbeat: Export the number of CPU cores The is now exported as part of the system module in Metricbeat. Thanks for the contribution! Packaging improvements for the community Beats Every community Beat can create deb/rpm/tar.gz for all platforms by simply calling . has made several improvements to the community Beats packaging, allowing them to do their own and . Fix for build files ownership An annoying \"feature\" of our Docker based build system is that the temporary files and directory created by it are shown as owned by root on the host. Cyrille found a workaround that does a . ","locales":"","title":"Brewing in Beats: Filebeat modules for MySQL and Syslog"}
{"index":{}}
{"author":"Dara Gies","category":"Engineering","publish_date":"2016-12-19T00:00:00.000Z","url":"/blog/migrating-to-elastic-cloud","seo_title":"Migrating to Elastic Cloud","content":" Whether you’re trying to reduce operational costs, maintain uptime or generally simplify things, consider , Elastic’s very own hosted Elasticsearch and Kibana offering. Elastic Cloud offers ease-of-use and provides the latest Elasticsearch and Kibana releases, version 5.1 at the time this was written, with the newest features, enhancements and performance improvements. Also included are X-Pack features, such as security, monitoring, alerting & notification, Graph, and reporting. Elastic Cloud is backed by Elastic’s excellent support team and by the engineers who develop Elasticsearch and Kibana. Interested in trying out Elastic Cloud? Let us show you how you can migrate your data to our hosted offering, regardless of whether you currently self-host Elasticsearch on your own metal,  run Elasticsearch in some other cloud offering, such as EC2, or use one of the many Elasticsearch hosting services that exist, such as the Amazon Elasticsearch Service. Migration Approaches If you have existing Elasticsearch indexes and are considering migrating to Elastic Cloud, there are a few approaches to consider. Regardless of the approach, keep in mind that you first have to configure your  Elastic Cloud target cluster. Creating a new cluster is very easy, but cluster settings like scripts and index settings like custom analyzers and mappings aren’t automatically copied over and must be configured prior to migrating index data. Feed From The SourceIf you have a copy of the original data some place other than your Elasticsearch cluster, then it might be simplest to load from there. Create an Elastic Cloud cluster, configure it, and feed away. Ingesting into an Elastic Cloud cluster is no different than feeding a cluster running on your own servers. You can use , , the or any other means at your disposal. If the original source isn’t available, is cumbersome to access, has changed, or is no longer up to date, then two other migration approaches are available: reindexing from remote or snapshotting and restoring. Reindex From A Remote ClusterElasticsearch 5.x provides the ability to index from a remote cluster with the . With reindexing, it’s possible to spin up a cluster in Elastic Cloud and index data from an existing remote cluster.  First create the destination index on the cluster in Elastic Cloud and then use reindex with the `remote` option to pull data from your old cluster and index it into the current cluster, taking advantage of the latest , and Elasticsearch 5.1 improvements. Reindexing effectively rebuilds the index from scratch so it is more expensive to run than _snapshot, which simply copies complete indexes. The tradeoff is that restored indexes can't take advantage of those nice data structures because they are not rebuilt. Restore Snapshots From S3It’s also possible to . With this approach, snapshot to an S3 bucket from an existing Elasticsearch cluster, spin up an Elasticsearch cluster in Elastic Cloud, and then restore the snapshot from S3 into Elastic Cloud. Bear in mind that you’re not limited to restoring snapshots from S3 buckets. Snapshots that exist on any addressable external storage can be restored into Elastic Cloud. How to Reindex from Remote Reindex from Elasticsearch on EC2 into Elastic Cloud Log into and create a new cluster, minimally the same size as your Elasticsearch on EC2 cluster or larger if you anticipate growth. Be sure to enable Kibana so you have access to the console. From your Elastic Cloud cluster, you can issue the reindex command to index data stored remotely in Elasticsearch on EC2. The reindex command requires access to port 9200 or 9243, so you should associated with the EC2 Elasticsearch cluster and add an inbound custom TCP entry for port 9200 or 9243, allowing access to the Elastic Cloud clusters. Issue the reindex command: POST reindex { \"source\": { \"remote\": { \"host\": \"http://[ec2 public hostname]:9200\" }, \"index\": \"bank\", \"query\": { \"m","locales":"","title":"Migrating to Elastic Cloud"}
{"index":{}}
{"author":"Samir Bennacer","category":"Engineering","publish_date":"2016-12-15T00:00:00.000Z","url":"/blog/integrating-elasticsearch-with-arcsight-siem-part-1","seo_title":"Integrating the Elastic Stack with ArcSight SIEM - Part 1","content":" In this blog series we will provide an overview of how to extend and complement the capabilities of your existing SIEM to create an effective security analytics solution for your organization. For the purposes of example, we will demonstrate the use of an X-Pack enabled Elastic Stack with one of the SIEM solutions...ArcSight. The following demonstrates an example of Elasticsearch with the ArcSight SIEM. The existing ArcSight Smart Connector can be used to send data to Elasticsearch, with multiple possible approaches to configuration. The simplest solution is to add a CEF syslog destination to the ArcSight Smart Connector allowing it to send data to the Logstash. In Logstash we will use  Logstash ArcSight module to setup this integration. The module includes a Logstash configuration for ingesting and enriching CEF-formatted data from ArcSight Smart Connectors, while bundling a set of Kibana dashboards to view events from common sources. Setup the Elastic Stack Setup the Elastic Stack with Docker For a quick setup you can download an example definition to help you to install all the elastic stack with x-plugin ( step 1 to 5 ), then issue: $ docker-compose up But First, ensure that: Configure ArcSight Smart Connectors Explore your data with Kibana ","locales":"","title":"Integrating the Elastic Stack with ArcSight SIEM - Part 1"}
{"index":{}}
{"author":"Luca Cavanna","category":"Engineering","publish_date":"2016-12-14T00:00:00.000Z","url":"/blog/state-of-the-official-elasticsearch-java-clients","seo_title":"Elasticsearch Java Clients","content":" Java programmers have two choices when communicating with Elasticsearch: they can use either the REST API over HTTP, or the internal Java API used by Elasticsearch itself for node-to-node communication. So, what's the difference between these two APIs?  When a user sends a REST request to an Elasticsearch node, the coordinating node parses the JSON body and transforms it into its corresponding Java object.  From then on, the request is sent to other nodes in the cluster in a binary format -- the Java API -- using the transport networking layer.  A Java user uses the Transport Client to build these Java objects directly in their application, then makes requests using the same binary format passed across the transport layer, skipping the need for the parsing step needed by REST. What are the problems with this approach?This solution is quite powerful, and didn’t require us to write specific Java client code for Elasticsearch as the Java API was already used and maintained internally. The Java API is also theoretically more performant than REST, as it skips the parsing step and allows clients to use the binary protocol. that the performance of the HTTP client is close enough to that of the Transport client that the difference can be pretty much disregarded. Over time, we came to realize that the Java API has some downsides: Backwards compatibilityWe are very careful with backwards compatibility on the REST layer where breaking changes are made only in major releases. On the other hand, we make breaking changes to Elasticsearch’s internal classes all the time, which is necessary in order to move the project forward. Those changes result in changes to the binary format, which we compensate for by having version-specific serialization code.  It is this compatibility layer that allows you to do a rolling upgrade of an Elasticsearch cluster. That said, running a mixed version cluster is something we only recommend during the upgrade process, not as the status quo. Having older versions of nodes or clients in the cluster limits the support of newer features, as the older client simply doesn't know how to write or read requests in the newer binary format.   When you upgrade your cluster, you should upgrade all nodes and clients to have the same version. The requirement to upgrade all Java clients makes upgrades harder, because it affects all the applications that communicate with Elasticsearch. And all of those changes we made to the internal Java API?  Your application has to be adapted to cope with them.  Fortunately, Java has a compiler which complains when an interface has changed, so updating your Java app shouldn't be too complicated.  That said it can still be a pain in the neck to do for every upgrade. The REST interface is much more stable and can be upgraded out of step with the Elasticsearch cluster. JVM versionWe also recommend that the client and the server are on the same Java version. This used to be a strict requirement before Elasticsearch 2.0, when we used Java serialization for exceptions.  These days, having exactly the same Java version probably isn't as important as it used to be, but given how low level the binary format of the Java API is, it is advisable to use the same JVM version on all nodes and clients. The REST client can use the same version of the JVM that is used by your application. DependenciesThe Java API is not published as a separate artifact, which means that, your project has to depend on the whole Elasticsearch, including all the server code that is not really needed on the client side. This means you have some strange dependencies like lucene and log4j2, which you may not need in your application, and can actually end up conflicting with libraries that you have in your classpath. This problem has been reduced with the recent removal of the Guava dependency, but it still exists. The low-level REST client","locales":"","title":"State of the official Elasticsearch Java clients"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2016-12-13T00:00:00.000Z","url":"/blog/elasticsearch-2-4-3-released","seo_title":"Elasticsearch 2.4.3 released","content":" Today we are pleased to announce the bug fix release of , based on . This is the latest release in the legacy 2.x series.Latest stable release in 2.x:Users should upgrade if they are affected by any of the issues listed in the release notes above. The more important changes are:Please , try it out, and let us know what you think on Twitter () or in our . You can report any problems on the . ","locales":"","title":"Elasticsearch 2.4.3 released"}
{"index":{}}
{"author":"Clinton Gormley","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2016-12-12T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2016-12-12","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. End-to-end Recommender System with and — Spark Tech Center (@apachespark_tc) Elasticsearch Core Apache Lucene Watch This Space , where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2016-12-12"}
{"index":{}}
{"author":"Tanya Bragin","category":"Engineering","publish_date":"2016-12-13T00:00:00.000Z","url":"/blog/using-painless-kibana-scripted-fields","seo_title":"","content":" Kibana provides powerful ways to search and visualize data stored in Elasticsearch. For the purpose of visualizations, Kibana looks for fields defined in Elasticsearch mappings and presents them as options to the user building a chart. But what happens if you forget to define an important value as a separate field in your schema? Or what if you want to combine two fields and treat them as one? This is where Kibana scripted fields come into play. Scripted fields have actually been around since the early days of Kibana 4. At the time they were introduced, the only way to define them relied on , a scripting language in Elasticsearch which deals exclusively with numeric values. As a result, the power of scripted fields was limited to a subset of use cases. In 5.0, Elasticsearch introduced , a safe and powerful scripting language that allows operating on a variety of data types, and as a result, scripted fields in Kibana 5.0 are that much more powerful. In the rest of this blog, we'll walk you through how to create scripted fields for common use cases. We'll do so by relying on a dataset from  and use an instance of Elasticsearch and Kibana running in , which you can spin up for free. The following video walks you through how to spin up a personal Elasticsearch and Kibana instance in Elastic Cloud and load a sample dataset into it.  How scripted fields workElasticsearch allows you to specify scripted fields on every request. Kibana improves on this by allowing you to define a scripted field once in the Management section, so it can be used in multiple places in the UI going forward. Note that while Kibana stores scripted fields alongside its other configuration in the index, this configuration is Kibana-specific, and Kibana scripted fields are not exposed to API users of Elasticsearch. When you go to define a scripted field in Kibana, you'll be given a choice of scripting language, allowing you to pick from all the languages installed on the Elasticsearch nodes that have dynamic scripting enabled. By default that is \"expression\" and \"painless\" in 5.0 and just \"expression\" in 2.x. You can install other scripting languages and enable dynamic scripting for them, but it is not recommended because they cannot be sufficiently  and have been deprecated. Scripted fields operate on one Elasticsearch document at a time, but can reference multiple fields in that document. As a result, it is appropriate to use scripted fields to combine or transform fields within a single document, but not perform calculations based on on multiple documents (e.g. time-series math). Both Painless and Lucene expressions operate on fields stored in . So for string data, you will need to have the string to be stored in data type . Scripted fields based on Painless also cannot operate directly on . Once scripted fields are defined in \"Management\", user can interact with them the same way as with other fields in the rest of Kibana. Scripted fields automatically show up in the Discover field list and are available in Visualize for the purposes of creating visualizations. Kibana simply passes scripted field definitions to Elasticsearch at query time for evaluation. The resulting dataset is combined with other results coming back from Elasticsearch and presented to the user in a table or a chart. There are a couple of known limitations when working with scripted fields at the time of writing this blog. You can apply most Elasticsearch aggregations available in Kibana visual builder to scripted fields, with the most notable exception of the . You can also filter on scripted fields via the filter bar in Discover, Visualize, and Dashboard, although you have to take care to write proper scripts that return well-defined values, as we show below. It is also important to refer to the \"Best Practices\" section below to ensure you do not destabilize your environment, when using scripted fields. The followi","locales":"de-de,fr-fr,ja-jp,ko-kr,zh-chs","title":"Using Painless in Kibana scripted fields"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2016-12-12T00:00:00.000Z","url":"/blog/brewing-in-beats-new-community-beats-for-mongodb-and-amazon","seo_title":"","content":" Last week we released Beats 5.1.1 with support for Kafka and Docker monitoring in Metricbeat. For more details about the new features added, please read the. New community Beat: Mongobeat Mongobeat, created by ,  discovers instances in a Mongo cluster and collects information from the commands db.stats() and db.serverStatus(). It provides than the mongodb module in Metricbeat. New community Beat: Amazonbeat created to read data about the Amazon products and index them to Elasticsearch. The Amazon prices can vary every day, if not every hour. So, an interesting use case would be to follow the price variation for a specific Amazon product, and maybe set an alert with Watcher when the price goes below a certain threshold. Filebeat modules prototype We have started working on the Filebeat modules, and we have the first . Following fairly closely the model started in Metricbeat, a Filebeat module contains Ingest Node pipelines, Elasticsearch templates, Filebeat prospectors configurations, and Kibana dashboards that together are able to read, parse, and visualize data from various log files. There’s already a fully working such module for Nginx (loadable via a prototype python script for now), and we plan to add many more such modules, for things like Mysql, Syslog, Mongodb, Kubernetes, and everything else under the sun. Modules are in a very early stage, but you can follow the progress on this . Metricbeat: Fetch docker stats in parallel Fetching Docker container stats can be very slow with lots of containers, taking up to 2 seconds. A is open on the Docker side to add support for bulk api, that would increase the performance and reduce the number of opened TPC connections. Until the feature is available in Docker, an workaround was introduced in the Docker module of Metricbeat to create the requests for getting the container stats in parallel. The main downside is that it opens a lots of TCP connections. Filebeat: Add enabled configuration option for prospectors To enable/disable a prospector in Filebeat, you had to uncomment/comment the entire configuration for the prospector. To make it easier, the was introduced in the prospector configuration. By default, the enabled configuration option is set to true. Winlogbeat: XML unmarshall optimization In the current implementation the data is read into a encoded as UTF16, converted into in order to use and then converted to a string. The XML was unmarshalled from the string. The XML unmarshal process was in Winlogbeat by converting the data directly to UFT8 and then saving the result into a reusable . The XML is then unmarshalled directly from the data in buffer. New flags to import_dashboards The import_dashboards script now accepts to connect to Elasticsearch. You can pass a certificate authority in the argument, a client certificate under argument and a certificate key in a argument. The client certificate together with the key must be in a PEM format. In addition, you can by passing the -insecure option. By default, it’s false. ","locales":"","title":"Brewing in Beats: New community Beats for MongoDB and Amazon"}
{"index":{}}
{"author":"Greg Marzouka","category":"Engineering","publish_date":"2016-12-13T00:00:00.000Z","url":"/blog/nest-5-0-released","seo_title":"","content":" Today, we are happy to announce the release of NEST and Elasticsearch.Net 5.0. If you recall, our 2.0 release was full of breaking changes, not only due to the breaking changes in Elasticsearch 2.0, but also because the 2.0 client was a near complete rewrite. , we realize how painful it made upgrading from 1.x to 2.x for our users. This 5.0 release reaps all the benefits of the refactoring that was done in 2.0, introducing minimal breaking changes in comparison. The sole focus of NEST 5.0 was adding support for Elasticsearch 5.0 while minimizing breaking changes as much as possible. That being said, it is still a major version bump, so some breaking changes are to be expected! See the for a complete summary. Elasticsearch 5.0 Features, of which are in the 5.0 release of NEST and Elasticsearch.NET. We'll recap some of the more notable features here. Ingest NodeOne of the biggest features in Elasticsearch 5.0 is . As the documentation states, ingest node allows you to pre-process documents directly in Elasticsearch before indexing takes place. There are a number of and associated with an ingest pipeline, all of which are now supported in NEST 5.0. For more information, check out our more detailed blog post which showcases how to use ingest node with NEST. X-PackNEST also fully supports all of the APIs, including , ,  and . Previously, Watcher (now called Alerting) 1.x APIs were supported in NEST 1.x as a separate NuGet package. This felt like a good idea at first as it allowed us to expose the necessary bits for third-party packages (official or community) to easily extend the client. However, as it transpired, this quickly became too unwieldy to support as the client and Watcher packages were on different release cycles. By moving it into the main package, we can ensure that it's always up to date and compatible with the latest version of NEST. Client FeaturesIn addition to the new Elasticsearch 5.0 APIs, NEST 5.0 also introduces a few new abstractions and client-specific features. .NET Core supportWhen we released NEST 2.x, we supported cross platform .NET with a dotnet 5.1 release of the client, built using DNX toolchain. Now that DNX has been superseded by the  toolchain, we continue to support cross platform .NET in the form of a netstandard 1.3 versions of the 5.x and 2.x client, making NEST usable across Windows, OSX, Linux and any other platform that . Say Heya to the all new propertyThe fluent and attribute-based mapping APIs in NEST are powerful, and does a very good job at inferring property types. There was a slight nuance however when mapping scalar properties (i.e. numeric and date types) in that it was up to the caller to supply the actual numeric type (, , etc...) of the property being mapped. Without this, NEST would default to the Elasticsearch defaults ( in 2.x and in 5.x). For instance, in order to map in the following POCO as an integer in Elasticsearch, the following was required: public class MyClass { public int MyInteger { get:  set:  } } client.Map<MyClass>(m => m .Properties(ps => ps .Number(n => n .Name(p => p.MyInteger) .Type(NumberType.Integer) .Coerce() .IgnoreMalformed(false) ) ) ):  With the new mapping methods, the same thing can be achieved in a less verbose and much smarter fashion: client.Map<MyClass>(m => m .Properties(ps => ps .Scalar(p => p.MyInteger, n => n .Coerce() .IgnoreMalformed(false) ) ) ):  which will automatically infer the correct numeric type based on the CLR type. Idiomatic async supportThe client now follows best practices in regards to asynchronous code execution. In previous versions of NEST, a could only be supplied using . We've made this much easier in 5.0, and it is now possible to provide an optional directly to any of the async endpoint methods. We also ensure we call on all async/await methods, since the callback context is not needed, improving performance slightly. New Connection Settings on a per-request basis is very handy for debugging as it preserves the or","locales":"","title":"NEST 5.0 released"}
{"index":{}}
{"author":"Shay Banon","category":"Culture","publish_date":"2016-12-12T00:00:00.000Z","url":"/blog/reflections-from-the-land-down-under","seo_title":"","content":" More than three years ago in Australia, a then-employee of Campaign Monitor named Mark Walkom took a huge leap with our software and founded the . A year later, Mark became our very first employee and support engineer in Australia, and today he continues to be the fearless leader of that community, which now has more than a thousand Elastic users. Needless to say, I’ve been wanting to visit Australia, see Mark, and meet his tribe of Aussie Elastic users for some time.  After sorting through visa issues for a year, I finally secured entry into the land down under and made the 22-hour flight from Holland to New Holland. I visited users in Sydney and Canberra. We held the Elastic{ON} Tour event at the Sydney Doltone House, and more than 200 attendees came to hear talks from , , and . In addition to meeting lots of customers, partners, and users, I was also able to do a few town hall sessions and talk to the . As expected, there were lots of questions about our latest , but to my surprise, there was also much interest on topics such as our , our open source business model, and how we’re building a distributed company across more than 35 countries around the world. Visiting the birthplace of our Australian community and talking with so many users about the technical and cultural challenges that they’re solving was the highlight of my trip. Other highlights? Summer, beaches, and the warm December weather. I highly recommend the Spit Bridge to Manly Walk, which I ran. Most surprising parts? The dialect and the deadly animals! I’d like to thank our wonderful Elastic community for their warm welcome and continued support in Australia and New Zealand. I hope to be back as soon as the universe allows. 'Til next time. Cheers  ","locales":"","title":"Reflections from the Land Down Under"}
{"index":{}}
{"author":"Nicolás Bevacqua","category":"Engineering","publish_date":"2016-12-12T00:00:00.000Z","url":"/blog/color-coded-visualizations-react","seo_title":"","content":" For the past few days I’ve been working on a data visualization that displays Elasticsearch node distribution across different cloud instances and zones. This article shows off the visualization, the color generation script, an interesting performance issue, and the solution I arrived at. User interfaces for cloud services are typically boring, long lists of nodes, clusters, regions, proxies, users, snapshots, logs, and lists. Identifying patterns in the data is mostly a responsibility of the UI implementor, and not so much the consumer. When such an identifiable pattern emerges, implementors should come up with ways to break away from these long tabular data lists and display data in more valuable ways. Tabular data is by its very nature repetitive and boring. There is nothing wrong with displaying boring data, but visualizations can certainly help identify outliers when we have long data lists. The following screenshot shows a small fraction of a list of allocators, along with their health and available capacity. An allocator is a container that may host several Elasticsearch or Kibana Docker instances, and perhaps some available capacity. These allocators are sorted by available capacity, which becomes immediately obvious when we use a visualization to display how capacity is distributed in each allocator. The visualizations are relative to each other in width, so it’s easier to understand the relationship between different allocators in terms of total capacity. Note that this is just the end-result of what we arrived at after around two weeks of on-and-off iteration on the design and UX of the visualization. Let’s go over that iteration process. Design Iteration As usual, the design process started with a simple PR made that displayed bars which depicted how full an allocator was relative to its total capacity. Then proposed using a color palette to illustrate breaks between different nodes. That ended up in a redesign implemented, where he also made the full visualization width relative to the largest allocator’s total capacity. This redesign also get a notion of how many nodes an allocator has, and roughly how big they are. ’s PR got merged shortly after that, but my own ideas as well. I removed the border used to display how much total capacity an allocator had, instead choosing to rely on a striped repeating linear gradient to represent available capacity. I also added tooltips that mentioned how large the nodes were, and made each slice in the visualization a link to that node’s view. The links turned out to be pretty useful, as they avoid going through an allocator’s page when we want to visit a particular node – especially when they’re large and thus easy to identify. Then came in with one of my favorite suggestions: using color shades per node size variation. This change made colors more meaningful communicators of how the data is shaped. Before, each node took a color from a repeating list with a few colors. After, as seen below, each node with same capacity is colored in different shades of one color, making colors more meaningful. We experimented with color shading. At first we used the original colors Kim had used. Then I came up with a brighter color scheme, but that was too bright. Then we used some of the brand’s colors, and lastly I improved the shade algorithm quite a bit. The design process is still ongoing – as you know, web applications are never really “done”! We’re experimenting with a small line at the bottom that describes whether the node we’re illustrating is a Kibana instance or an Elasticsearch instance. This piece of work also normalizes colors so that all nodes of the same capacity use the same color – across all visualizations. Colors changed a bit, , because bright contiguous shades of red, yellow, and green could be mistaken for cluster state. We’ve also added a legend so that humans know at a glance what each col","locales":"","title":"Thousands of Color-coded Visualizations in React"}
{"index":{}}
{"author":"Tyler Hannan","category":"Releases","publish_date":"2016-12-08T00:00:00.000Z","url":"/blog/elastic-stack-5-1-1-released","seo_title":"","content":" Our unified build and release process, which began in 5.0, continues. Let’s begin by addressing any confusion that may arise. It is our first minor release since the heady days of 5.0.0 and it is version 5.1.1. Yup, you read that right. Version 5.1.0 doesn’t exist because, for a short period of time, the Elastic Yum and Apt repositories included unreleased binaries labeled 5.1.0. To avoid any confusion, and upgrade issues for the people that have installed these without realizing, we have decided to skip the 5.1.0 version and release 5.1.1 instead. It is, again, a substantive release with features and functionality in nearly all areas of the Elastic Stack. And, as per usual, it is available – right now – on . Elasticsearch For more detailed information, and many other features, peruse the Elasticsearch . If you haven’t yet upgraded to enjoy the features of Elasticsearch 5.x. As a reminder, we’ve also released the , which runs on your existing 2.3 cluster. Use this site plugin to prep for your migration. Kibana ‘Visualize’ all the features, and ‘Discover’ more information in the . Logstash Many highly requested features make an appearance in this release. For more information, grok the . Beats We may collect data from the edge, but all the updates are in a single Beats . X-Pack X-Pack gains a new feature in the Search Profiler. This is discussed, in some detail, in the post. At its simplest, the profiler UI is an easy way to visualize where time is being spent during search requests. Your slowlog will be a mystery no longer! Even better, this is available for free as part of X-Pack with a Basic license. Continuing in the same vein of updates to Basic license features, we have updated the Monitoring feature of X-Pack to include an advanced view. Even more charts to help you monitor the health of your cluster. With more charts comes more questions, so we also added tooltips to the Monitoring charts. now to get started. ES-Hadoop ES-Hadoop 5.1.1 has also been released today. This version sees a set of much needed fixes to the handling of raw JSON data in Pig, Hive, and Cascading. Get It Now! ","locales":"","title":"Elastic Stack 5.1.1 Released"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2016-12-08T00:00:00.000Z","url":"/blog/elasticsearch-5-1-1-released","seo_title":"Elasticsearch 5.1.1 released","content":" Today we are pleased to announce the release of , based on . This is the latest stable release and is already available for deployment on , our Elasticsearch-as-a-service platform.Latest stable release:Elasticsearch 5.1.0 doesn’t exist because, for a short period of time, the Elastic Yum and Apt repositories included unreleased binaries labeled 5.1.0. To avoid confusion and upgrade issues for the people that have installed these without realizing, we decided to skip the 5.1.0 version and release 5.1.1 instead.Full details of the changes in this release are available in the release notes listed above, but there are a few important changes which are worth mentioning. ","locales":"","title":"Elasticsearch 5.1.1 released"}
{"index":{}}
{"author":"Kevin Kluge","category":"Releases","publish_date":"2016-12-08T00:00:00.000Z","url":"/blog/kibana-5-1-1-released","seo_title":"Kibana 5.1.1 released","content":" We’re excited to announce the immediate release of Kibana 5.1.1! What happened to 5.1.0, you say? The long answer is complicated and there’s really no one at fault, but for the sake of this blog post let’s just say that it’s Simon’s fault. The short answer is that we accidentally published incorrect 5.1.0 builds a couple weeks ago for about five seconds, so let’s just move on without throwing blame around. See our for more details about why this is Simon’s fault. Onward! Despite the minor release designation there are some major features here. Tag that cloud The terms agg and significant terms agg are useful for seeking out interesting parts of a large body of text data or getting statistics on tags or labels on other documents. 5.1.1 adds the visualization to provide a graphical view of this data, with each word’s size representing its importance. There’s a lot you can tweak with tag cloud, including word orientation and all the font properties of the word. Return of the charts For Marvel 2.0 we simplified the UI considerably by removing many of the charts that were referenced less often. And of course people, a lot of people, missed them! For 5.1, we have added an Advanced View to X-Pack’s monitoring feature. The index and node views both gained several charts to expose things like segment counts and detailed memory usage. Monitoring tooltips in 5.1.1 continue to help you understand the data you’re seeing. To help with usability, we moved the legend from being inside the chart to outside, so you can focus on the data with less clutter. Your profile never looked better Elasticsearch 2.2 added the , which allows you to get detailed information about the parts of your queries that take time. You can then refine or re-write the slower parts to improve. The iteration has been done with the REST API previously. Now, for 5.1.1, the X-Pack basic (free) install includes a UI for the profiler. You can find it in the Dev Tools area of Kibana after installing X-Pack. The profiler UI gives you a tree structure of the query with timing associated with each branch, which allows you to dig into the most time consuming parts. It can also help you quickly iterate, easily enabling the viewing of new (and hopefully faster!) queries. It’s available in our Basic Edition, which is free for production or dev use with registration. Fastbana Speaking of performance, Kibana 5.1.1 is fast. How fast? Really efficient! Err… I mean really fast! But also more efficient, too. In this release, we made four key changes to Kibana specifically focussed on performance. First, we across the entire project. For any given UI component, the change will go unnoticed. But the cumulative effect across the entire UI is a more responsive Kibana. Next, we took a swing at global warming by caused by the progress indicator. The net effect here was almost a 50% improvement on overall CPU usage while the progress indicator was present. That’s right, that animated pink line at the top of the screen had that much of an impact. Browsers, eh? Then we turned our attention to two long standing performance problems in Discover. Up until 5.1.1, Discover could grind to a halt when you were searching against indexes that have a large number of fields. The primary culprit was a formatting mechanism we added all the way back in 2014. This was purely an aesthetic utility that doesn’t even have a pronounced effect any longer. So we , and now Discover doesn’t fall over when your indexes have tons of fields. The last performance-related change we made is a feature that requires your intervention, but it allows Discover to query indexes that have very large fields. Sort of the opposite problem as the previous paragraph, Discover would fail miserably when it encountered indexes that had even a single _source field with a huge amount of data in it. This could be anything from a large deeply nested JSON object or even binary data, neither of whic","locales":"","title":"Kibana 5.1.1 released"}
{"index":{}}
{"author":"Monica Sarbu","category":"Releases","publish_date":"2016-12-08T00:00:00.000Z","url":"/blog/beats-5-1-1-released","seo_title":"","content":" Today we are pleased to announce that Beats 5.1.1 was released. This is the latest stable version and it comes with a few goodies like support for monitoring Kafka and Docker containers with Metricbeat, and other good reasons to upgrade. Latest stable release: Docker module in MetricbeatStarting with Beats 5.0.0, you are able to collect periodically metrics about your containers by reading cgroup data directly from /proc. To read the cgroup data, you need to mount the proc filesystem and the cgroup filesystem from the host inside of the container. The benefit of collecting container metrics directly from cgroup is that it works for all types of container technologies you are using, not only for Docker containers, as Metricbeat reads directly from the cgroup pseudo filesystem provided by the Linux kernel. However, this approach alone is not able to get the container metadata like the Docker container name and labels. More details about the cgroup data you can find in the . Another way to collect the information about your Docker containers is by interrogating periodically the Docker API. You can get similar details about your containers as by reading cgroup data, and includes: With Beats 5.1.1, we are releasing a new Docker module in Metricbeat, that collects periodically information about your Docker containers by querying the Docker API. The Docker module is based on the code of a community Beat, , created by Ingensi and kindly migrated to a module in Metricbeat by . The advantage of querying Docker API over reading the cgroup data is that it’s easier to set-up as typically the only required configuration is the address of the Docker server. For now, the Docker module is marked as experimental, meaning that we reserve the right to do any breaking changes. Kafka module in Metricbeat Beats 5.1.1 comes with support for monitoring the Kafka nodes thanks to a new Metricbeat module. The Kafka module is inspired from the community Beat, , created by from Elastic. The Kafka module connects to the local Kafka node and reads periodically details about the partitions, like topic names, available event offsets, and replication status.The Kafka module is experimental, and we're already looking into adding more Kafka metricsets to it.New processor: add cloud metadataYou are now able to enrich each event generated by Beats with instance metadata from the machine’s hosting provider by configuring the processor. When the Beat starts, it automatically detects the hosting provider and caches the instance metadata. There are three cloud providers supported at the moment: If the processor is configured, the event is enhanced with the `meta` field. Here is an example: { \"meta\": { \"cloud\": { \"availability_zone\": \"us-east-1c\", \"instance_id\": \"i-4e123456\", \"machine_type\": \"t2.medium\", \"provider\": \"ec2\", \"region\": \"us-east-1\" } } } New processor: decode JSON from arbitrary fieldsFilebeat can decode JSON encoded logs already since version 5.0, but it didn’t have so far a good way of handling more complex JSON-in-JSON situations. Such events can happen, for example, if one is using the Docker JSON logging driver (which wrapps every line in a JSON object) and the application also outputs in JSON. Thanks to a community contribution by , libbeat got a new processor, called , that can decode JSON from an arbitrary field. This processor is useful in Filebeat, but can be used also, for example, in the Packetbeat configuration to decode the body of HTTP requests. While at first sight the JSON processor is more general than the native JSON decoding we have in Filebeat, and it could replace it entirely, it’s important to note that the native JSON decoding is applied before the multiline processing. This means that if a multiline event is scattered across multiple JSON objects, Filebeat can first decode the JSON objects and then stick together the related lines. FeedbackIf you want to make use of the new features added in Beats 5.1.1, please , install it, an","locales":"","title":"Beats 5.1.1 released"}
{"index":{}}
{"author":"Suyog Rao","category":"Releases","publish_date":"2016-12-08T00:00:00.000Z","url":"/blog/logstash-5-1-1-released","seo_title":"","content":" A note about Logstash 5.1.0 Version 5.1.0 doesn’t exist because, for a short period of time, the Elastic Yum and Apt repositories included unreleased binaries labeled 5.1.0. To avoid confusion and upgrade issues for the people that have installed these without realizing, we decided to skip the 5.1.0 version and release 5.1.1 instead. Logstash is a critical component in many data ingestion architecture, shipping millions of events to Elasticsearch and other outputs. One of the highly anticipated features for Logstash has been improvements in data resiliency. Today, we are glad to announce our first iteration of persistent queues, which aims at improving data resiliency. By default, Logstash uses in-memory queuing between the pipeline stages (input → filter, and filter → output) to transfer events. The size of the queue is fixed and not configurable. If Logstash terminates abruptly, either as the result of a software failure or the user forcing an unsafe shutdown, all in-flight events are lost. To prevent loss in these scenarios, you can configure Logstash to use persistent queues. With persistent queues enabled, Logstash persists events before processing them. The queue size is variable with configurable limits, which means that you can buffer events in Logstash instead of at the source or edge node which will also help manage situations that can result in backpressure at the source. With such features, for a simple, single instance deployment that requires message buffering, you can now use the inbuilt persistent queue instead of deploying and managing a message queue, such as Redis, RabbitMQ, or Apache Kafka. If your production deployment already uses a message queue like Kafka, you can continue to use it, and we'll continue to integrate seamlessly with those. Keep in mind that the persistent queue is not replicated and works only with a single Logstash instance, which means you cannot share the queue across multiple instances. For use cases that require such distributed handling, our recommendation is to continue to use your favorite message queue product. To enable this feature in Logstash, set queue.type to be persisted in your logstash.yml. For more configuration options and detailed description, check our for this feature. We have plenty of planned enhancements in the pipeline for this feature and would love your feedback and improvements as we iterate in the upcoming releases. Ever wish to find out what is taking a long time in your Logstash Pipeline? Continuing on our 5.x theme of making Logstash easier to operate, we are introducing a new slowlog feature. In 5.0, we introduced thes, that helps peek into any costly operations at the thread level, but there was no easy way to see what actions are taking long and what events triggered this slow-ness. In 5.1, filters can be configured to log event data and related context when it exceeds a specified execution time. These slowlogs will be collected in a separate file called We've had many reports in the past that the Date Filter when using multiple patterns. This filter allows you to a pattern, and if it fails, step through other patterns until one of them is successful. This filter has been enhanced to handle failing patterns efficiently. While re-working the implementation to solve this problem, we were able to deliver a general performance boost for all date processing configurations. The verdict? for all cases, and in some pattern sequences, a scorching 16x increase in throughput. In some Logstash deployments, the administrators have no control over the type and size of events that are processed by Logstash. By default, Logstash is started with a small heapsize for the JVM and does not need a big memory footprint. When a large event (in bytes) flows through Logstash, it can consume the entire heap allocated eventually crashing Logstash. Truncate is a new filter that allows you to truncate fields longer than a given byte-length. This plugin now supports 0.10.1.0 re","locales":"","title":"Logstash 5.1.1 released"}
{"index":{}}
{"author":"Amy White","category":"News","publish_date":"2016-12-06T00:00:00.000Z","url":"/blog/first-wave-of-elasticon-17-sessions-revealed","seo_title":"","content":" , our annual user conference, is one of the highlights for us each year. The conference is mainly about bringing a community of developers together who are utilizing the Elastic Stack to achieve various goals . . . it’s about all of us being on this journey separately, yet together, with the Elastic Stack being what unites us. This is why we view the sessions at Elastic{ON} as a way to share stories. Our engineers get to share everything they’ve been working on for the past year, as well as where they’re thinking of going in the coming year. They want to hear your reactions and thoughts to their plans. They want your ideas on how we should be shaping the direction we’re going. Our users and customers get to share the journey they’ve each been on while using Elasticsearch, Logstash, Beats, and Kibana to do things we never would have dreamed of. We get ideas and inspiration when hearing these stories as to how we should continue to develop our Stack, so we can best help them accomplish their goals.  Because when it comes down to it, it's really about creating something wonderful – together. Today, we’re pleased to unveil . We expect to finalize the agenda with 30+ more sessions in January, but there is so much goodness in there now that we wanted to highlight just a few talks we’re particularly jazzed about:  Machine Learning Comes to the Elastic Stack, we started down the path to more tightly integrate their unsupervised machine learning engine into the Elastic Stack. At Elastic{ON}, you can about how it works, what problems it can solve for your business, and how it can proactively detect and alert you to anomalies into the behavior and performance of your business and systems. Afterwards, you can find them at the Ask Me Anything booth – which is open all day, every day of the conference – to pick their brains about what you heard. Walgreens’ Journey to Creating an End-to-End Search PlatformWalgreens is one of the the largest drugstore chains in the United States, interacting with 8 million customers each day. Somewhere along the way, their Endeca-powered search platform couldn’t keep up. Sound familiar? We thought it might, which is why we invited Syed Ali, a Senior Technical Architect at Walgreens, to , which led them to also set up a log management and analysis cluster, as well as utilize Kibana to view technical and business metrics. You’ll hear about things like capacity planning, cluster setup and architecture, data ingestion using Logstash and a bulk API framework, relevancy tuning, performance optimization, query analysis using Packetbeat, how they handle monitoring and operations, and, of course, challenges and resolutions. The Queries Behind Uber Marketplace’s Core Data SystemI think we’ve all wondered how Uber utilizes data to manage dynamic pricing and supply/demand/trips, which is why we’re stoked to have Jae and Isaac . They plan to map out their data flow – from Kafka and Samza to Spark and ES-Hadoop, give some insights into how they handle their surge multiplier calculation, as well as share some general tips and tricks they’ve learned along the way – such as how they’re solving many data modeling challenges with Elasticsearch partial document update with dynamic scripting and lightweight transaction concepts.  Walmart Will Share How Bananas Impact Fraud DetectionOK, maybe not exactly. But thanks to the Elastic Stack, they know that they sell 156 of them a second (banana bread, anyone?). In this talk, you’ll learn how used to detect fraud and gain insights into customer purchasing patterns, as well as track store performance metrics. Is Time-Series Your Jam? Learn How Workday Built Its Metrics PipelineWith the release of Elasticsearch 5.0, we deepened our support for numbers to better help with the time-series use cases we’re starting to see all over our user base. Workday, which provides enterprise cloud applic","locales":"","title":"First Wave of Elastic{ON}<sup>17</sup> Sessions Revealed"}
{"index":{}}
{"author":"Tudor Golubenco","category":"Brewing in Beats","publish_date":"2016-12-05T00:00:00.000Z","url":"/blog/brewing-in-beats-metricbeat-kafka-module-improvements","seo_title":"","content":" Welcome to Brewing in Beats! With this series, we're keeping you up to date with all that's new in , from the details of work in progress pull requests to releases and learning resources. Pass arrays or dicts via -E and env variablesIt is now possible (again, since this was also possible in versions < 5.0, but we had a regression) to specify an . This is particularly useful when specifying multiple output hosts. For example, this works as expected now: if is an environment variable in the form . This feature didn't make the cut for the 5.1 release, but will be available in a next minor release.Metricbeat Kafka module improvementsPreparing for the first release of the Kafka module, we’ve got , including TLS support and SASL authentication (available in Kafka 0.10). Winlogbeat fix for “invalid bounds” error messageWhen reading a batch of large event log records the Windows function EvtNext returns errno 1734 (\"The array bounds are invalid.\"). This seems to be a bug in Windows because there is no documentation about this behavior. The handles the error by resetting the event log subscription handle (so events are not lost) and then retries the EvtNext call with maxHandles/2. Winlogbeat benchmarks and performance improvementsAndrew is working on of Winlogbeat and fixing some performance issues, like the . Automatic tests with ES/LS 2.xWe realized we don’t have automatic integration tests with any of the 2x versions of Elasticsearch and Logstash, but list them on our support matrix, so we’re . Filebeat: restate publish_async as experimentalThe feature allows Filebeat to continue reading batches of log lines while waiting for confirmation that the current batch was processed by the next stage. This option can improve throughput but also cause larger memory and CPU consumption. We’re now in the docs and via a warning that this feature is considered experimental at the moment. ","locales":"","title":"Brewing in Beats: Metricbeat Kafka module improvements"}
{"index":{}}
{"author":"Tudor Golubenco","category":"Brewing in Beats","publish_date":"2016-12-05T00:00:00.000Z","url":"/blog/brewing-in-beats-processor-for-decoding-json","seo_title":"","content":" Welcome to Brewing in Beats! With this series, we're keeping you up to date with all that's new in , from the details of work in progress pull requests to releases and learning resources.New community Beat: RedditbeatRedditbeat, by , indexes into Elasticsearch new Reddit submissions of one or multiple sub-reddits. Because why not. Libbeat processor for decoding JSONFilebeat can decode JSON since version 5.0, but it didn’t have so far a good way of handling JSON-in-JSON situations. This can happen, for example, if one is using the Docker JSON logging driver and the application also outputs in JSON. Thanks to a community contribution by , libbeat got a new processor that can . This is useful for Filebeat, but can be used also, for example, by Packetbeat to decode the body of HTTP requests. Metricbeat: raw MySQL fieldsThe general approach we take with Metricbeat is to carefully curate the available metrics and only export the most actionable ones, which we usually put in the sample dashboards. We also rename these metrics to follow a common naming scheme and make sure to select the correct Elasticsearch type and Kibana format for each of them. While this approach gives a good out of the box experience that people usually like, it has the disadvantage that if someone needs a more obscure metric, it’s often not readily available. To compensate, we’re experimenting with adding the option to automatically capture all metrics with their original names (or changed just enough to be usable in Elasticsearch/Kibana) under a `raw` sub-document. The first module to . Metricbeat: Kafka module mergedThe is now merged and will be available in the 5.1 release as experimental. Metricbeat: MongoDB module improvementsThe MongoDB module was to extract the key metrics from the relatively new wiredTiger storage engine and to work well with MongoDB 3.4. We now also have a sample dashboard for MongoDB. Metricbeat: HAProxy module improvementsThe fields created by the HAProxy module were to match our naming conventions and got better documented. Metricbeat: Docker module improvementsThe same reviewing work was done for the from the Docker module. Also, the Docker labels are now exported as This makes them easier to use in Kibana. Additionally, the from the container metricset. Special thanks to for early testing the Docker module and suggesting improvements on it. Metricbeat: parse and sanitize the connection URLMetricbeat now offers a safer and standardized method for modules to (the contents of the `hosts` field) so that we have a good way of extracting the sensitive information from them. Filebeat: faster shutdown when dealing with a large number of filesFilebeat is now able to when receiving the shutdown signal. This makes the shutdown more responsive when the scan has to deal with lots of files. Metricbeat: fix service timeout on startup@maddin2016 fixed a bug where the Metricbeat startup was taking too long and could cause a service timeout when being started under windows. ","locales":"","title":"Brewing in Beats: Processor for decoding JSON"}
{"index":{}}
{"author":"Doug Nelson","category":"User Stories","publish_date":"2016-12-02T00:00:00.000Z","url":"/blog/the-trix-to-creating-a-total-ly-new-customer-experience-elastic-general-mills","seo_title":"","content":" Elastic{ON} Tour: Chicago was the first Elastic event that I had attended and I was extremely impressed with the quality of the event. From the ability to have one-on-one time with the Elastic engineers to the professional and prepared Elastic event staff to the venue:  everything was really well done. Even more, it was great to learn about all of the up and coming (now ) features of the Elastic Stack v5.0 and I found the talk about and were particularly interesting and timely as we look to build out additions to our search platform.At General Mills, Elasticsearch has proven to be a good choice as the engine powering our Index Content Delivery system for our core sites of BettyCrocker.com, Pillsbury.com, Tablespoon.com and Quericavida.com. It was great being able to tell the story of how we migrated from Endeca to Elasticsearch and highlight how our new Elastic-based core engine performs reliably, quickly, and most importantly in a way that provides us the framework to constantly improve our consumer experience.For me, speaking at Elastic{ON} Tour: Chicago represented a great way to contribute back to the amazing Elastic community. Reflecting back, speaking was a fun experience, but the best part was afterward at the Ask Me Anything booth when I was approached by folks from various companies who were at the same crossroads I traveled a year ago and were looking to learn from our experiences. It was great to be able to give truly meaningful answers to their questions. ","locales":"","title":"The Trix to Creating a {Total}ly New Customer Experience: Elastic @ General Mills"}
{"index":{}}
{"author":"Uri Cohen","category":"Releases","publish_date":"2016-12-01T00:00:00.000Z","url":"/blog/introducing-elastic-cloud-enterprise-public-alpha","seo_title":"Introducing Elastic Cloud Enterprise - Public Alpha","content":" Sometimes, to understand the future it is important to reflect on the past. That notion holds true as we introduce Elastic Cloud Enterprise. A few years ago, we knew the demand for a rock-solid hosted Elasticsearch offering would grow. We explored the market and determined the had the best technology and approach available. Today, they are the core team building out Elastic Cloud, the same product all grown up. Fast-forward 20 months, we have since invested heavily in providing the best Elasticsearch and Kibana service running on AWS, and soon, other public cloud providers. New releases of the Elastic Stack are available in Elastic Cloud on the same day, upgrades are made as simple as possible, (like security, alerting, and monitoring) are readily available, and support comes from those responsible for developing the codebase.As Elastic Cloud adoption increased, many of our users and customers expressed the desire to deploy and manage clusters in their own environments with the same ease that we do in Elastic Cloud. That day has officially arrived. Say ”Heya” to the public alpha of Elastic Cloud Enterprise. Whether you are deploying and managing one cluster -- or thousands -- Elastic Cloud Enterprise is our Elastic Cloud product installed on the infrastructure (metal or hosting provider) of your choice. What is it, really?Put simply, it is our Elastic Cloud product, hosted and managed by you. Multiple clusters of Elasticsearch, Kibana, and all the features of X-Pack are orchestrated from a single console. If you need to manage multiple Elastic deployments across teams or geographies, you can leverage Elastic Cloud Enterprise to centralize cluster management for: Centralizing the management of clusters with Elastic Cloud Enterprise enforces uniform versioning, data governance, backup, and user policies. Increased hardware utilization through better management can also reduce total cost. It is worth in greater detail as some of the things the team have built are not only useful, but also quite inventive. Who is it for?Recently, we have had the opportunity to run a series of events in cities throughout the world. We always have engineers, support, and others manning an Ask Me Anything (AMA) station throughout the day. This year’s event was no different. In fact, some of the conversations confirmed our vision for this product. We heard users talk about large deployments and the challenges that go along with operational lifecycle management. Others are fielding high demand for the Elastic STack across their organization and provisioning clusters for various teams and divisions to successfully (and securely) perform monitoring or logging. Understanding, “who it is for” requires reflection on the benefits that Elastic Cloud Enterprise brings. Easy operational lifecycle management (from provisioning, cluster sizing, upgrades, backups, etc) and tenant isolation is particular valuable for those who manage the Elastic Stack. Optimized hardware utilization is particularly beneficial for the IT Admin (if a unique role). As the ‘user’ of a cluster you get access that is easier, faster to provision (from request to endpoint accessible), and more reliable. x-as-a-ServiceThe question often manifests itself like: ‘I want to offer Logging as a Service to my organization” or “What are the design patterns to offer logging internally?” It is tempting to approach this by deploying hundreds of nodes in a single cluster and attempting to segregate data. Perhaps, the appropriate design pattern is quite different. What if your logging, or search, as a service strategy was to spin up hundreds of small nodes (managed by an orchestration framework) and provide each team in your organization with an endpoint and simple instructions?Elastic Cloud Enterprise enables this in a simple, replicable way. ‘Center of Excellence’In additio","locales":"","title":"Introducing Elastic Cloud Enterprise - Public Alpha"}
{"index":{}}
{"author":"Zeynep Pehlivan","category":"User Stories","publish_date":"2016-11-30T00:00:00.000Z","url":"/blog/elasticsearch-powering-twitter-archives-at-institut-national-de-l-audiovisuel","seo_title":"Elasticsearch powering Twitter Archives at Institut National de l’Audiovisuel","content":" Since its creation in 1974, (Ina) is, by law, in charge of collecting, preserving and making available French audiovisual collections. In 1992 due to its existing obligations, Ina was designated by law as being responsible for the Legal Deposit of radio and television.  In 2006, the Legal Deposit’s scope was extended to cover French public web content splitting responsibility between the French national library (BnF) and Ina. Since then, DLWeb team at Ina is in charge of archiving all the broadcast-related French websites e.g. radio and TV channels websites, blogs, etc. and also providing interfaces to help users to better visualize and analyze web archives. As part of these efforts, the team has also been collecting and archiving tweets since 2014 by using Twitter APIs to follow 12,000 users and 600 hashtags.  To index the web page archives, representing , the DLWeb team migrated its search engine to Elasticsearch. In this project, Elasticsearch showed great power and flexibility for search so we decided to also use it to index the Twitter archives, which represent , with an average of . Our Twitter index is based on our principal Elasticsearch deployment with 32 shards spread over 4 servers - 32 nodes. Each server has 4 SSD of 1TB and 96 Gb of RAM, and after doing several tests:  we have found a performance sweet spot with 2 elasticsearch nodes by SSD. So we finally decided to use 8 elasticsearch nodes with 6Gb of heap size by server to optimize IO and CPU operations. We will be moving to 12 servers - 96 nodes soon.  Providing an efficient search interface combined with multiple filters (e.g. dates, hashtags, mentions, etc.) is very important while working with archived data. Our search interface combines search queries and filters of Elasticsearch as shown below. Mining Twitter data requires not only full text search but also data aggregation at different levels. To satisfy diverse research needs, we need to offer generic solutions. With Elasticsearch we can search and filter data as well as perform analytics at the same time in a single request. Kibana helps us to build our dashboards and decide which kind of aggregations we can provide to our users. By using aggregations and sub-aggregations we are able to provide diverse analytics (e.g. timelines, top hashtags, top distributions, etc.) for a given query with a high performance as shown below. Legal Deposit web archives of Ina may only be accessed for study and research purposes at .  DLWeb team is involved in several national and international research projects based on Twitter data such as ASAP: and REAT: where the interface described above is used by several researchers. ","locales":"fr-fr","title":"Elasticsearch powering Twitter Archives at Institut National de l’Audiovisuel"}
{"index":{}}
{"author":"Leah Sutton","category":"Culture","publish_date":"2016-11-29T00:00:00.000Z","url":"/blog/elastic-heart-the-center-of-the-elastic-cause-awards","seo_title":"","content":" \"The good we secure for ourselves is precarious and uncertain until it is secured for all of us and incorporated into our common life.\" — Jane Addams When you are new to a company (I started last January as VP of Human Resources), there is a heritage and context to the culture that takes a while to learn. Much of it is subtle, nuanced and intangible because culture, in many ways, is felt rather than described. Elastic is also the most distributed and global company I’ve ever been a part of — 400+ people in 34 countries and counting. Being new to Elastic and to open source software, one aspect of our culture that was particularly notable to me is the special relationship we have with our user community and our customers. They truly are an extension of our wonderful, global family. As soon as you ask someone about the customers or users they work with, a light comes on. Faces become animated, eyes sparkle, voices rise in excitement (and yes, maybe there is an occasional eye roll, we’re human after all). Passion for our community shines through and one begins to understand that the work the Elastic community does with our products, and support in all forms of that work, is a driving force at Elastic. Whether it’s our developers, support engineers, marketing folks or sales teams, if we touch a customer or user on a daily basis we want them to succeed to achieve the best work of their lives using the Elastic Stack. When we recruit and hire, that passion for our product and community is a quality we seek and recognize the instant we see it. There is a deep respect for the breadth of problems that our customers and users solve with our products but there is a special joy and humility felt by all of us when we learn about how the Elastic Stack is being used to solve problems beyond the merits of solving a particular use case or building a commercial application. These use cases are about building something, doing something, fundamentally good in the world, such as Giant Oak using the Elastic Stack to . We feel how much they inspire us and each other and we celebrate their power to change the world for the better using software. Our software. This is why we are deeply excited to launch the first-ever — a celebration of three Elastic user-community projects that use the Elastic Stack to advance the human condition, improve the global environment or help a particular population in need. We want these stories to be shared, celebrated and known by our entire user community:  hopefully inspiring more game changing ideas. This year’s award winners will be recognized at . Our goal is to give the awards annually and we would love to see the program and the projects it sparks grow over time. If you are an Elastic user and believe your work is fulfilling a good cause, I encourage you to . We want to tell your story and celebrate the difference you are making in the world. ","locales":"","title":"Elastic Heart: The Center of the Elastic Cause Awards"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2016-11-29T00:00:00.000Z","url":"/blog/elasticsearch-5-0-2-released","seo_title":"Elasticsearch 5.0.2 released","content":" Today we are pleased to announce the bug fix release of , based on . This is the latest stable release and is already available for deployment on , our Elasticsearch-as-a-service platform. All users are advised to upgrade.Latest stable release:Full details of the changes in this release are available in the release notes listed above, but there are a few important changes which are worth singling out: ","locales":"","title":"Elasticsearch 5.0.2 released"}
{"index":{}}
{"author":"Kevin Kluge","category":"Releases","publish_date":"2016-11-29T00:00:00.000Z","url":"/blog/kibana-5-0-2-released","seo_title":"Kibana 5.0.2 Released","content":" Ta-da! And then Kibana 5.0.2 appeared from a cloud of bits. We’re happy to announce this latest release. It has a security fix and a handful of useful bug fixes as well. Advanced Settings and Short URLs now run as the current user With X-Pack installed, operations in the Advanced Settings panel of the Management tab and operations from the short URL service were performed as the “Kibana Server” user regardless of the user that is currently authenticated. As a result, a user that was defined as read-only could make changes to the global settings of Kibana. This does not allow access to protected data, but it could allow a rogue user to change Kibana configuration to alter Kibana’s appearance or Kibana’s default index. 5.0.2 ensures these operations are run as the currently authenticated user. This is described as ESA-2016-10 on our . Bug fixes in 5.0.2 There are several good bug fixes in 5.0.2, including: The have a few more details on the fixes. Of course, our page has been updated with 5.0.2 as well. Depending on your current version, you can find the correct in our documentation. ","locales":"","title":"Kibana 5.0.2 released"}
{"index":{}}
{"author":"Dimitrios Liappis","category":"Engineering","publish_date":"2016-11-24T00:00:00.000Z","url":"/blog/is-your-elasticsearch-trimmed","seo_title":"Is your Elasticsearch \"Trimmed\"?","content":" Is your Elasticsearch “Trimmed”? Here at Elastic we regularly benchmark the performance of Elasticsearch. The results are . Looking at the results, we have observed a recurring pattern of performance degradation: In the picture above we can see peak performance on 2016/09/26 gradually dropping until it recovers again on 2016/10/03, followed by another steady performance drop recovering on 2016/10/10. Have you spotted the pattern yet? Dismissing the possible influence of periodic astronomical effects, we started looking at the scheduled weekly jobs on the operating system instead, for an explanation. Our benchmarks run on bare metal servers, use two SSD drives in software RAID-0 configuration and have Ubuntu 16.04 installed. Following is the content of on a Ubuntu Server 16.04 LTS based VM. Notice the job. What on earth is TRIM? Quoting the : SSD drives are composed of groups of Flash (NAND) memory cells. Three major differences from traditional magnetic disks can affect the performance of SSD drives: These limitations have forced manufacturers to implement where blocks containing pages with data and pages marked for deletion are emptied copying pages with data to free blocks. Therefore SSD drives, internally, create more IO than explicitly generated by the OS for write operations:  this phenomenon is called . When the OS gets a request to delete a file, the filesystem only updates metadata and doesn’t erase the disk addresses holding the actual data. This worsens write amplification on SSD drives during GC runs. To address this problem, OSes can issue the TRIM command to make the SSD drive aware of erased data:  this brings two advantages: What does it mean practically? It means that your SSD drives will operate faster if the filesystem passes information to them about files that have been deleted. This what the crontab process does on Ubuntu. Does it really make such a big difference? You bet! After moving the cronjob (after 2016/10/17) to a daily schedule we can see the earlier graph stabilizing: Picture inserted here The detrimental effect of no TRIM can be seen running ten consecutive runs of a different (PMC) benchmark below. The filesystem got TRIMmed only once, right before starting the benchmarks: Picture inserted here Caveats On CentOS-7 the and associated systemd timer is disabled by default, so you will need to enable the service and probably override the default timer schedule as well. If you are using LVM and frequently perform resize operations you will need to set to 1 in . The Linux encryption layer, if used, also requires special handling. Instructions for configuring TRIM under / can be found at the ArchLinux . The TRIM issue affects Cloud instances as well, if they provide direct access to SSD disks. This is the case with Instance Store-Backed Linux AMIs on . Conclusion Elasticsearch is an IO-intensive application. If you are running it on SSD storage you should check if TRIM is enabled and, depending on your load, adjust the frequency of the TRIM job. You will be surprised with the performance gains! ","locales":"de-de,fr-fr,ja-jp,ko-kr","title":"Is your Elasticsearch \"Trimmed\"?"}
{"index":{}}
{"author":"Clinton Gormley","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2016-11-28T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene---2016-11-28","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. How fstrim impacts Elasticsearch peformance — elastic (@elastic) Elasticsearch Core Changes in 2.x: Changes in 5.0: Changes in 5.x: Changes in master: Ongoing changes: Apache Lucene Watch This Space , where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2016-11-28"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2016-11-22T00:00:00.000Z","url":"/blog/elasticsearch-2-4-2-and-1-7-6-released","seo_title":"Elasticsearch 2.4.2 and 1.7.6 released","content":" Today we are pleased to announce bug fix releases of and . This is the final release in the 1.x series. These releases are already available for deployment on , our Elasticsearch-as-a-service platform.Elasticsearch 2.4.2:Elasticsearch 1.7.6:Full details of the changes in this release are available in the release notes listed above, but there are a few important changes in Elasticsearch 2.4.2 which are worth mentioning: ","locales":"","title":"Elasticsearch 2.4.2 and 1.7.6 released"}
{"index":{}}
{"author":"Michael McCandless","category":"Engineering","publish_date":"2016-11-22T00:00:00.000Z","url":"/blog/sparse-versus-dense-document-values-with-apache-lucene","seo_title":"","content":" Recently we've made some big changes to Apache Lucene around how doc values are indexed and searched, including , based on the . These changes fix doc values so you only pay for what you use, just like all other parts of the Lucene index. These changes will be in Lucene's next major release (7.0) and will likely not be back-ported to any 6.x release, so it will be some time until Elasticsearch exposes this. What are doc values? Doc values are Lucene's column-stride field value storage, letting you store numerics (single- or multi-valued), sorted keywords (single or multi-valued) and binary data blobs per document. These values are quite fast to access at search time, since they are stored column-stride such that only the value for that one field needs to be decoded per hit. This is in contrast to Lucene's stored document fields, which store all field values for one document together in a row-stride fashion, and are therefore relatively slow to access. Doc values can be used to hold scoring signals, e.g. using to combine multiple signals into a score, or for sorting, grouping, aggregations, , etc. Indeed Lucene's default scoring signal, , a single per text field/document holding index-time scoring signals (the field's length and boost, quantized) used to implement our , is also just a doc values field. Plumbing first The changes began with , which was \"simply\" a low-level raw plumbing change switching out how doc values are accessed at search time from the previous random-access API to an iterator API instead. You can see it as annotation in the . Because an iterator API is a much more restrictive access pattern than an arbitrary random access API, this change gives codecs more freedom to use aggressive compression and other optimizations, like our postings implementations do. That change was already massive enough that we decided to break out all such codec improvements to future issues. Since this was a plumbing change only, we (see annotation ) at first as the existing Lucene codec had to use temporary silly wrapper classes to translate its random-access API into an iterator API. Sparse cases, where not all documents have a value for each doc values field, should especially benefit from this change. In the past, Lucene's non-sparse encoding of such fields has been particularly unnerving, especially when you call and see the size of your index suddenly grow 10-fold! But even dense cases, where most documents have a value for the field, should see performance gains as well, since the more restrictive API gives codecs more compression freedom. Codec Improvements Fortunately, after the initial plumbing change, worked hard to improve our default codec to take advantage of the more restrictive iterator APIs: These changes brought back much of our search performance on the dense use cases tested by Lucene's existing nightly benchmarks, and in some cases . With all these improvements, and I'm sure many more to come, we finally get Lucene's doc values to a point where only pay for what you use, just like the rest of Lucene's index parts. Sparse benchmarks Along with these Lucene improvements we've added a to track our progress and any . As usual, the sources to run this benchmark , and pull requests are welcome! The benchmarks index a 20 M document subset from the . Each document represents a single (anonymous!) taxi ride in New York City, either via a green or yellow cab. Green taxi rides are about 11.5% and yellow taxi rides are around 88.5%, making a good test for mostly sparse and mostly dense fields, vs. 100% dense fields. We index the same set of documents in three different ways: We also run a few basic searches over those three indices, including on cab color, both and , a which matches all documents, and a . Back testing, where we checkout the master sources at a specific point in time in the past and then run the benchmark, was particularly tricky si","locales":"","title":"Sparse versus dense document values with Apache Lucene"}
{"index":{}}
{"author":"Alexander Reelsen","category":"Engineering","publish_date":"2016-11-21T00:00:00.000Z","url":"/blog/writing-your-own-ingest-processor-for-elasticsearch","seo_title":"","content":" With Elasticsearch 5.0, a new feature called the ingest node was introduced. To quote the You can use ingest node to pre-process documents before the actual indexing takes place. This pre-processing happens by an ingest node that intercepts bulk and index requests, applies the transformations, and then passes the documents back to the index or bulk APIs. So, by you can configure the way a document should be transformed before it is being indexed. There are a fair share of with Elasticsearch, but it is also very easy to roll your own. This blog post will show how to write an ingest processor that extracts URLs from a field and stores them in an array. This array could be used to pre-fetch this data, spider it, or to simply display the URLs connected with a text field in your application. We will also show how easy it is to test your plugin, including a real integration test, when using Gradle. Creating a plugin skeletonIn order to get started, you could simply check out the Elasticsearch source code and copy-paste another ingest processor plugin over. For example, the , or plugin) are existing ingest plugins that can be found in Elasticsearch. However, this means you would need to remove a lot of code and only then start coding. The alternative for this would be to use a tool called cookiecutter. Cookiecutter is a command line tool to create projects from templates. I created such a template to make it easier for you to write your own ingest processor. So, let’s get started and create a plugin from a cookiecutter template. If you have not installed , run pip install cookiecutter Now we can start creating our plugin skeleton by running cookiecutter gh:spinscale/cookiecutter-elasticsearch-ingest-processor You will be asked four questions. The name of the plugin, a package directory (don’t change it), a description, your name (needed to fill into the license) and the Elasticsearch version to run against. If you pick as the name of your plugin, all the naming conventions in this document will apply to your local setup as well. If you check out the newly created directory, you will find a fully fledged plugin, which allows you to even run a few tests out of the box. You can run and see the tests pass. Now our next step should be to add dependencies and write a test. Note: You are required to run to do plugin development. If you are running on mac OS, you can ensure this by running . So, let’s add the following dependency to dependencies { compile 'org.nibor.autolink:autolink:0.6.0' ... } Autolink is a small helper library to easily extract URLs from text. If you use IntelliJ, you might want to run now and then import the project into IntelliJ. Tests firstBefore writing any code, we should not only make up our mind, what the processor is supposed to do, but also write a test reflecting this behaviour and then work on the implementation. To do this, we open up and write a useful test, where an input field contains several URLs and we want an output field to be a list of those URLs. public void testThatProcessorWorks() throws Exception { Map<String, Object> document = new HashMap<>():  document.put(\"source_field\", \"this is a test field pointing to http://elastic.co and http://example.org/foo\"):  IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document):  UrlExtractProcessor processor = new UrlExtractProcessor(randomAsciiOfLength(10), \"source_field\", \"target_field\"):  processor.execute(ingestDocument):  Map<String, Object> data = ingestDocument.getSourceAndMetadata():  assertThat(data, hasKey(\"target_field\")):  assertThat(data.get(\"target_field\"), is(instanceOf(List.class))):  @SuppressWarnings(\"unchecked\") List<String> urls = (List<String>) data.get(\"target_field\"):  assertThat(urls, containsInAnyOrder(\"http://elastic.co\", \"http://example.org/foo\")):  } You can now run to see the test fail or just start in your IDE of choice (mino","locales":"","title":"Writing Your Own Ingest Processor for Elasticsearch"}
{"index":{}}
{"author":"Clinton Gormley","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2016-11-21T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2016-11-21","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Every shard deserves a home via — Daniel Berman (@proudboffin) Elasticsearch Core Changes in 5.0: Apache Lucene Watch This Space , where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2016-11-21"}
{"index":{}}
{"author":"Guy Boertje","category":"Engineering","publish_date":"2016-11-18T00:00:00.000Z","url":"/blog/logstash-dude-wheres-my-chainsaw-i-need-to-dissect-my-logs","seo_title":"Introducing Logstash Dissect","content":" Most Logstash users that want to extract structured fields from an unstructured event data use the popular . This filter uses regular expressions under the hood and is versatile to many forms of text. Over time, users tend to use Grok for all of their structured extraction needs. This, in many cases, is analogous to using a sledgehammer to crack a nut. Regular expressions are sledgehammers. They are extremely powerful, but they can be unforgiving -- a poorly built regex can slow down the Logstash pipeline when run on certain lines of text. The pipeline is designed to recover, but the overall performance becomes unpredictable leading to unhappy users. My colleague João Duarte, while the performance characteristics of the Grok Filter noticed that match failures incur increasing performance penalties. Most of these issues boil down to problems, which essentially means that the regex engine descends into excessive backtracking, leading to a slower Logstash pipeline. Another colleague, , has recently re-implemented the Beats Input to handle input data asynchronously that led to an impressive throughput increase. In Logstash processing, the slowest component slows down the entire pipeline, and typically filters are the heavy lifters. I thought it wouldn’t really help to have a couple of speedier inputs with a slow Grok filter for field extraction, and hence was born. Design Essentially, field extraction in Dissect is a kind of 'split' operation and normally a split is done with one delimiter that is expected to be repeated in the source text. In contrast to Grok, which asks you to specify the regex patterns for fields of interest, Dissect asks you to specify the patterns of delimiters between the fields of interest. Dissect queues up the delimiters and searches the source text once from left to right, finding each successive delimiter in the queue. Different search algorithms are used for 1, 2 and 3+ byte delimiters. Dissect is written in Java with a very thin JRuby extension wrapper to expose the Java internals to Logstash's JRuby runtime environment. It also directly uses the rewritten Java Event code, released in Logstash 5.0. This is why Dissect is only compatible with Logstash 5.X. Dissect uses the bytes behind the JRuby UTF-8 Ruby String directly without the expensive conversion to a Java String beforehand. The delimiter searching will only remember where in the source bytes each delimiter was found. The Event field and value setting uses the found indices and lengths only once to create the strings to set into the Event. The format of the patterns of fields and delimiters is similar to Grok: In the above example, the yellow highlight shows the delimiters and blue highlight the field names. I quickly realised that various field syntaxes were needed to differentiate fields of extraction. There are four at the time of writing: Normal, Skip, Append and Indirect. Normal field type This type simply adds the found value to the event using the specified text as the name of the field, e.g. . Skip field type The pattern format requires you to specify each delimiter by putting them between and meaning there will be times when you need to specify fields but you don't need the values in the event - it should be ignored or skipped. To do this you can have an empty field e.g. or a named skipped field which is prefixed with a question mark \"?\", e.g. . You might choose to name your skip fields to make the final dissect pattern more readable. Append field type For the same reason - needing to specify all delimiters, you might need to rebuild a split value. Taking the syslog date as an example, if the source text looks like this - you will need to specify fields for the month, day and time. Maybe you want fields these as separate in your events but more likely you will want to rebuild these into one value. This is what an Append field doe","locales":"","title":"Logstash Dude, where's my chainsaw? I need to dissect my logs"}
{"index":{}}
{"author":"Mark Drago","category":"User Stories","publish_date":"2016-11-18T00:00:00.000Z","url":"/blog/powering-a-bi-application-with-elasticsearch-at-yodle","seo_title":"","content":" I’ve attended two at this point, both in NY in 2015 and 2016.  In 2015, my team and I were about to start work on a large project that had Elasticsearch at its core, and in 2016 I had the opportunity to give a talk at the conference about that very project.I really enjoy the Elastic{ON}Tour conferences as I’m able to hear about what’s coming next from the team at Elastic, learn how other attendees have leveraged , and have conversations that spark ideas for what we could be building next. For example, I had the opportunity to speak with members of the team about ways we may be able to leverage Prelert to augment our existing alerting infrastructure or perhaps even use it in future versions of our own product.The folks at gave a great talk about how they leveraged Elasticsearch to detect unwanted behavior in an enterprise.  I enjoyed this talk as the history of their architecture in some ways mirrored the path we took when building out our own product.  We both started with a Hadoop based solution and eventually made our way over to using Elasticsearch.Another great talk was given by Elastic about the 5.0 release and their product roadmap.  It’s great to hear about all the new features that we’ll be able to incorporate into our product going forward.  Given that our use-case is heavily numeric we’re looking forward to .  Additionally, while we’ve experimented with groovy scripts in the past, enabling the painless scripting language by default will provide a new option for us when deciding how to structure our documents and queries.When Elastic approached me about speaking at Elastic{ON} Tour I jumped at the chance.  It was a great opportunity to discuss our use case of Elastic and tell the story of .  Centermark is a distributed marketing automation platform that Yodle provides to franchise and multi-location businesses.  It facilitates the management of the marketing efforts for the hundreds of geographically distributed businesses in a franchise network providing them with actionable insights about the marketing and operations of their franchise.In the talk I walk through the evolution of the Centermark product as well as the history of its technical architecture that led us to Elasticsearch, particularly appreciating the speed with which it tears through the data. It’s that speed that allows us to deliver business insights to our customers. We’ve been working on Centermark for a while, and as tends to be the case, we have looked back on some of our architectural decisions more fondly than others. In particular, back in 2013 we made a decision to use Hadoop to pre-aggregate metrics which, while helping us achieve our goals around pageload speed, made it difficult to provide our customers with the flexibility that they needed. Publicly walking through the shortcomings of the early versions of our product was a bit exposing, but also offered a sense of atonement. Ultimately I believe this is what makes our journey interesting as it’s an honest story about how we found the right architecture for our product using the Elastic Stack, but not on the first try.For more information about Yodle or the Centermark product, check out these links: , , , . ","locales":"","title":"Powering a BI application with Elasticsearch at Yodle"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2016-11-15T00:00:00.000Z","url":"/blog/elasticsearch-5-0-1-released","seo_title":"Elasticsearch 5.0.1 released","content":" Today we are pleased to announce the bug fix release of based on . This is the latest stable release and is already available for deployment on , our Elasticsearch-as-a-Service platform. This release contains some important bug fixes and all users are advised to upgrade, especially X-Pack Security users.Latest stable release:Full details of the changes in this release are available in the release notes listed above, but there are a number of important bug fixes which are worth mentioning: ","locales":"","title":"Elasticsearch 5.0.1 released"}
{"index":{}}
{"author":"Nathan Zamecnik","category":"News","publish_date":"2016-11-15T00:00:00.000Z","url":"/blog/level-up-your-logging","seo_title":"","content":" How can we share Elastic Stack knowledge with more students around the world, while continuing to deliver a world-class level of training? Since we believe strongly that hands-on experience is the cornerstone to learning, how do we make sure that hands-on labs are a key component of our training? How do we deliver online training in a format that is personal and relevant? Can we maintain the high level of quality our students expect and demand? These were the questions we faced as we looked to roll out our first Elastic Stack on-demand training course. And we believe we've succeeded with our first online course -, based on Elastic Stack version 5.0. This half-day, introductory course covers the entire Elastic Stack in professional quality audio and video - Elasticsearch, Kibana, Beats, Logstash, and X-Pack, and builds in about 3-4 hours of hands-on exercises that allows our community to learn the essentials of logging and then go deeper with advanced exercises. As our first foray into this new medium, we researched and discovered a style that fits our Elastic culture. Using whiteboard-inspired learning, we can explain concepts that go beyond traditional slides, providing first the conceptual overview, then diagramming the concepts visually, and then drilling deeper into the implementation and finally cementing the understanding with labs. The training continues to have the in-person, one-to-one feel that we deliver in our classroom courses, but allows us to reach a broader global community of students with high-value curriculum at a reasonable cost. Finally, with a shorter course, we are able to focus intensely on one of a huge spectrum of Elasticsearch use cases. First up - logging! This course has over 60 unique modules split into 6 sections and includes numerous challenging quizzes and hands-on labs which will leave you with a fully functional Elastic Stack solution. Learn how to collect system metric data using Topbeat and how to extract and ship log data using Filebeat to Logstash for transformation and loading into Elasticsearch. You will then learn how to visualize and explore this data using Elasticsearch aggregations and searches in Kibana quickly and correctly. Take a look at our course trailer to get a glimpse. Is it time to level-up your logging? We hope you enjoy this course and welcome your feedback. ","locales":"","title":"Level-Up Your Logging!"}
{"index":{}}
{"author":"Michael McCandless","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2016-11-14T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2016-11-14","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. All you need to know about 5.0 - Part 1 - Search — Itamar Syn-Hershko (@synhershko) Elasticsearch Core Apache Lucene Watch This Space , where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2016-11-14"}
{"index":{}}
{"author":"Suyog Rao","category":"Releases","publish_date":"2016-11-15T00:00:00.000Z","url":"/blog/logstash-5-0-1-and-2-4-1-released","seo_title":"","content":" Today we are announcing 5.0.1 and 2.4.1 releases -- bug fix updates for 5.0.0 and 2.4.0 respectively. You can it here or read the for more details. For 2.4.1, you can find the release notes . Bug FixesBelow we list some important bug fixes in 5.0.1: ","locales":"","title":"Logstash 5.0.1 and 2.4.1 released"}
{"index":{}}
{"author":"Court Ewing","category":"Releases","publish_date":"2016-11-15T00:00:00.000Z","url":"/blog/kibana-5-0-1-and-4-6-3-released","seo_title":"Kibana 5.0.1 released","content":" Today we’re releasing Kibana version 5.0.1 with a security fix as well as a couple other important bug fixes. We’re also release Kibana version 4.6.3 with the same security fix. Kibana 5.0.1 and 4.6.3 fix an open redirect vulnerability in the short URL feature that would allow an attacker to create a redirect from the Kibana domain to a different website. We’ve assigned this vulnerability the identifier . Thank you to the GE Digital Security Team for finding and reporting the issue. Check out the and head on over to the page to get the latest version. As always, review the documentation as well. Kibana 5.0.1 bug fixes: ","locales":"","title":"Kibana 5.0.1 and 4.6.3 released"}
{"index":{}}
{"author":"Tudor Golubenco","category":"Brewing in Beats","publish_date":"2016-11-14T00:00:00.000Z","url":"/blog/brewing-in-beats-kafka-module-in-metricbeat","seo_title":"","content":" Welcome to Brewing in Beats! With this series, we're keeping you up to date with all that's new in , from the details of work in progress pull requests to releases and learning resources. New community Beat: Consulbeat This new collects service health metrics from and pushes them to the Elastic stack. Metricbeat Kafka module (in progress) Not merged yet, but Nicolas has made good progress on adding support for monitoring Kafka with Metricbeat. The work is partially inspired by , so hat tip to Dale. Fix: Remove duplicate fields from the index patterns We had an issue where several fields, including @timestamp, were . This happened because we build up the Elasticsearch mapping template and the Kibana index pattern from several fragments, so some duplicates sneaked up on us. We’ve to automatically check if the same field is accidentally defined in multiple places, so this shouldn’t happen again. Filebeat: tail_files is applied only once at start A problem with using the tail_files option was that people used it to skip old files but, because it was applied on any new file, it could easily drop the first few lines from each file. We’ve now to only apply the tail_files option at the Filebeat start. Remove intermediary files during generation We’ve got a lot of auto-generated files in the repository, to the point that it is hard for occasional contributors, and sometimes even ourselves, to figure out where things need to be changed. So we’re looking to reduce the number of auto-generated files that we commit in git, and this takes a first step and removes the intermediary fields.yml files used for generating the templates, docs, and index patterns. Packetbeat code cleanup Steffen continued his work on applying coding standards to the older parts of our code base, with a massive against Packetbeat, mostly renaming things and reducing the exported fields. Godocs for go-ucfg is our fairly advanced (we worry about it becoming self aware soon) library for handling configuration files. For the moment it just got . ","locales":"","title":"Brewing in Beats: Kafka module in Metricbeat"}
{"index":{}}
{"author":"Russ Cam","category":"Engineering","publish_date":"2016-11-17T00:00:00.000Z","url":"/blog/the-future-of-attachments-for-elasticsearch-and-dotnet","seo_title":"","content":" For a long time, Elasticsearch has supported the indexing of attachments through the mapper-attachments plugin. Installing this plugin has provided the capability to index Word documents, PDFs as well as many other text-based document attachments, extracting the content from each file including metadata such as content type, author and keywords and making it searchable in Elasticsearch. The Not Too Distant PastWorking with the has historically been a slightly awkward affair with . From NEST 2.3.3 onwards, we've introduced an type to make working with attachments a much smoother experience. In this post, we'll walk through some typical use cases in working with the plugin and the type with Elasticsearch 5.0, and provide an introduction to the , one of the processors available in the suite of processors for the . Since the mapper-attachments plugin is deprecated in 5.0 and will be removed in 6.0, the ingest-attachment processor plugin is the recommended way to index attachments in Elasticsearch 5.0. InstallationTo get started with indexing attachments, the first step is to install the mapper-attachments plugin. For the purposes of this post, I'm going to use Elasticsearch 5.0. As with all plugins in Elasticsearch, installation is handled by calling the script within the Elasticsearch directory elasticsearch-plugin.bat install mapper-attachments After successfully installing the plugin, it will be available to use when the node is started, or shutdown and restarted. If you're using the plugin with a version of Elasticsearch prior to 2.2, a specific version of the mapper-attachments plugin will be needed:  consult the to understand which version needs to be installed for your environment. Document Definition and MappingOnce the plugin is installed and our node is running, we're all ready to index our first attachment. To keep things simple, we'll use a simple Word document saved in format whose content contains the following Our document type looks like the following public class Document { public int Id { get:  set:  } public string Path { get:  set:  } public Attachment Attachment { get:  set:  } } It contains an id to uniquely identify the document, a path specifying where the original file is located on a file share and finally, the attachment that will be indexed. Now that we have a POCO type definition for the document, let's create an index and a mapping for it. For working with Elasticsearch 5.0 from .NET, we can use the 5.x release candidate of NEST var documentsIndex = \"documents\":  var connectionSettings = new ConnectionSettings() .InferMappingFor<Document>(m => m .IndexName(documentsIndex) ):  var client = new ElasticClient(connectionSettings):  var indexResponse = client.CreateIndex(documentsIndex, c => c .Settings(s => s .Analysis(a => a .Analyzers(ad => ad .Custom(\"windows_path_hierarchy_analyzer\", ca => ca .Tokenizer(\"windows_path_hierarchy_tokenizer\") ) ) .Tokenizers(t => t .PathHierarchy(\"windows_path_hierarchy_tokenizer\", ph => ph .Delimiter('\\\\') ) ) ) ) .Mappings(m => m .Map<Document>(mp => mp .AutoMap() .AllField(all => all .Enabled(false) ) .Properties(ps => ps .Text(s => s .Name(n => n.Path) .Analyzer(\"windows_path_hierarchy_analyzer\") ) .Attachment(a => a .Name(n => n.Attachment) .NameField(nf => nf .Name(n => n.Attachment.Name) .Store() ) .FileField(ff => ff .Name(n => n.Attachment.Content) .Store() ) .ContentTypeField(ct => ct .Name(n => n.Attachment.ContentType) .Store() ) .ContentLengthField(clf => clf .Name(n => n.Attachment.ContentLength) .Store() ) .DateField(df => df .Name(n => n.Attachment.Date) .Store() ) .AuthorField(af => af .Name(n => n.Attachment.Author) .Store() ) .TitleField(tf => tf .Name(n => n.Attachment.Title) .Store() ) .KeywordsField(kf => kf .Name(n => n.Attachment.Keywords) .Store() ) ) ) ) ) ):  The connection settings use a neat feature of the NEST client that allows a POCO type to be associated with a particular ind","locales":"","title":"The Future of Attachments for Elasticsearch and .NET"}
{"index":{}}
{"author":"Christoph Wurm","category":"Engineering","publish_date":"2016-11-15T00:00:00.000Z","url":"/blog/new-way-to-ingest-part-2","seo_title":"A New Way To Ingest - Part 2","content":" This is the second part of a two-part series about ingest nodes, a new feature in . ","locales":"","title":"A New Way To Ingest - Part 2"}
{"index":{}}
{"author":"Mike Paquette","category":"News","publish_date":"2016-11-14T00:00:00.000Z","url":"/blog/prelert-selected-for-2016-sinet-cybersecurity-innovators-award","seo_title":"","content":" On September 15th, the same day that we , we were notified by that Prelert had been named one of the 2016 SINET 16 Innovators!The SINET team was operating in ‘real-time!’ In their press release, they referred to Prelert as “Elastic/Prelert,” even though our announcement occurred only a few hours earlier than theirs!Since more and more users are deploying the Elastic Stack for security-related use cases, we were very excited that Prelert, based on its machine learning security analytics integration with the Elastic Stack, was selected as one of only 16 technology companies to receive this award. SINET programs are supported by the US Department of Homeland Security, Science and Technology Directorate.  Each year, SINET evaluates the technologies and products of hundreds of emerging Cybersecurity companies from all over the world, and selects the 16 most innovative and compelling companies. These 16 companies, known as the , are invited to present their products and solutions on stage in at the annual SINET Showcase in Washington, DC. Last week, Elastic’s Dave Erickson, Director of Solution Architecture, accepted the award and presented a demo of Elastic and Prelert’s capabilities to the showcase attendees. As Shay Banon and Prelert’s founder, Steve Dodson, , our teams are hard at work creating a more integrated product, in which Prelert becomes a feature of the Elastic Stack. Think Prelert node(s). Until then, we have put a Beta label on the existing Prelert integrations with Elastic.  We look forward to sharing more on our progress soon! Oh, and if you want to see Prelert in action and hear more about our developments, come join us at one of our stops in EMEA or in Asia Pacific. Or join us at our annual user conference, in San Francisco, March 2017. ","locales":"","title":"Prelert Selected for 2016 SINET Cybersecurity Innovators Award"}
{"index":{}}
{"author":"Shay Banon","category":"News","publish_date":"2016-11-14T00:00:00.000Z","url":"/blog/elastic-cause-awards-cause-doing-good-matters","seo_title":"","content":" Technology itself is not an end goal, it’s the means to the end of solving a problem, meeting requirements, eliminating challenges, building something new, innovating or changing how we experience what’s possible. Software enables us to progress toward a future that’s even better and I care very deeply about that power.As Elastic has grown and you - the extraordinary community has grown rapidly with us - we are continually excited, inspired, motivated and thrilled by how you are using the Elastic Stack to help improve the world. We’ve seen Elasticsearch used to track malaria vaccinations in South Africa. At Elastic{ON}16 we heard a talk by about their work using Elasticsearch to fight human trafficking. We’ve read research by a group of computer engineers in India using Elasticsearch to build a natural disaster prediction system. We were so moved, proud and humbled by these stories that we would love to learn about more impassioned work we know you are doing. Even further, we consider it our responsibility to share it back with you, to let everyone know the reach and impact this very special Elastic community possesses.So this year, we are launching the first-ever to celebrate three Elastic user-community projects that are using the Elastic Stack to advance the greater good. Whether your work is focused on improving the human condition, working for a better global environment or helping a particular population through technology, we would like to hear how the Elastic Stack plays an important role.The Cause Awards will be awarded annually, beginning with . Each of the selected project teams will receive complimentary conference tickets and hotel stay for up to two project members and will be featured in various special events at the conference.We’re excited to hear about your cause and how you’re using the Elastic Stack to positively and passionately impact the world. The deadline is December 1, . ","locales":"","title":"Elastic Cause Awards - 'Cause doing good matters"}
{"index":{}}
{"author":"Joshua Backing","category":"Engineering","publish_date":"2016-11-11T00:00:00.000Z","url":"/blog/every-shard-deserves-a-home","seo_title":"Every Shard deserves a home.","content":" Here are some great slides from our course that help explain the concept. We'd recommend that you take the full course to understand this even better, but I'll provide an overview from our training here:Shard allocation is the process of allocating shards to nodes. This can happen during initial recovery, replica allocation, rebalancing, or when nodes are added or removed. Most of the time, you don't need to think about it, this work is done by Elasticsearch in the background. If you've ever found yourself curious about these particulars, this blog will explore shard allocation in several different scenarios. This is our 4 node cluster, it's what we'll use for the examples within our content: We'll cover several different scenarios. Scenario 1. Index creation For index c, we're creating one primary shard, and one replica. The master needs to create index c, and assign 2 shards for c0, a primary and a replica. The way that the cluster will balance the cluster is by: There are certain restrictions on this process, and those restrictions are imposed by allocation deciders. They evaluate every decision the cluster tries to make and make a yes/no decision. They run on the master. You can think of it like the master proposing changes, and the deciders tell the master if there are any blocks that prevent the master's proposition. The cleanest example of this is that you cannot put a primary and a replica on the same node. There are other examples as well: Shard initialization. Once marked initializing, the node assigned will detect it was assigned the new shard. An empty lucene index will be created and once this is done the master will be alerted the shard is ready, master marks the shard as started, and sends out another modified cluster state. Once the node assigned the primary gets the updated cluster state it'll mark the shard as started. Because it's a primary, we'll now be allowed to index: All of this communication, as you've seen, is done via modified cluster state. Once this cycle is complete, the master will perform a re-route and reevaluate shard assignment, potentially making decisions about what was throttled in the previous iteration. When replicas are assigned, it's important to understand we're going to copy any missing data from the primary back to the replica. After this, the master will again mark the replica as started and broadcast a new cluster state. Scenario 2. Time to move a shard. From time to time your cluster may need to move already existing shards within the cluster. This can happen for a number of reasons: It's possible for us make a segment without fsyncing, allowing the filesystem to keep in memory or pending flushes to disk. This is done for performance reasons, and because of this the transaction log needs to be cleaned. This is done with an elasticsearch flush. On issuing elasticsearch flush, lucene commit, which does 2 things: This allows us to flush the transaction log and guarantees we have all the data. For relocation, if we capture a given set of segments and keep them around we have a point in time consistent and immutable snapshot of our data. In our example, when the cluster wants to move a0 from node4 to node5, first the master does it by marking shard a0 as relocating from node4 to node5. Node5 interprets this as an initializing shard copy on itself. An important thing to note around this behavior is that while rebalancing is happening it may appear a replica is being moved from node4 to node5, when relocating data it will always be copied from the primary shard(node1). In the example below, we have an empty node5 and a node with the primary, node1. Keep in mind the two data storage mechanisms we mentioned previously, the transaction log and the lucene segments. We're illustrating Node5 responding to a request from the master to initialize a shard after reading an updated cluster state. Node5 goes to node1 and ask to start a recovery process. No","locales":"","title":"Every shard deserves a home"}
{"index":{}}
{"author":"Adam Reeve","category":"User Stories","publish_date":"2016-11-11T00:00:00.000Z","url":"/blog/detecting-insider-threats-with-elastic-redowl-analytics-an-elastic-on-tour-reflection","seo_title":"","content":" Elastic is a fundamental part of our platform at RedOwl, so when they brought the to New York we were excited to attend and share with attendees how we make use of the Elastic Stack to help our clients better understand their Insider Risk threats.For me personally it was a great opportunity to learn about the progress towards GA for and understand the thinking behind the Elastic Stack and reorganization. Given our background in analytics, the introduction to was very interesting and started a number of conversations internally about how we might be able to leverage this new functionality in the future. During the breaks we took the opportunity to meet with some of the Elastic engineers on site, as well as spend some time at the Ask Me Anything booth chatting with other attendees about our platform. In particular, it was great to discuss more of the details of Prelert with Gaurav Gupta, VP of Product at Elastic, as we're very intrigued by the approach it takes of putting analytics functionality into a new node type within the Elastic cluster. We also networked with a number of attendees on topics ranging from how the Elastic ecosystem allows our solution to scale within extremely large enterprises to the applicability of our behavioral analysis to the performance of athletes and sports teams! When Elastic asked us to present at this year's event, we were very eager to take part. We're huge supporters of their products, and being able to help others see the possibilities beyond simple search was an exciting opportunity. The whole process was very straightforward – from slide templates through to presenting on the day, and we were happy to see both a full house and some great questions!For our presentation we wanted to cover a few bases. First, I spoke about the problem space that RedOwl operates within, and how our application's ability to combine intelligence from multiple streams of unstructured data sets it apart both in functionality and in implementation challenges. I also explained how Elasticsearch fits into our overall stack alongside Apache Nifi, RabbitMQ and other supporting technologies. In the second part, Russell Snyder, Principal Engineer at RedOwl, gave a brief history of how and why we moved from Hadoop to Elasticsearch, and explained some of the complex queries and aggregations we use to provide insight to our users. I hope you find it interesting! ","locales":"","title":"Detecting Insider Threats with Elastic @ RedOwl"}
{"index":{}}
{"author":"Steve Dodson","category":"Engineering","publish_date":"2016-11-10T00:00:00.000Z","url":"/blog/ai-dreams-ml-promises-ba-how-elastic-and-prelert-fit-together","seo_title":"","content":" At Salesforce’s recent , artificial intelligence (AI) dominated as a theme. Perhaps the most prominent announcement was (\"everyone’s data scientist\"), which signaled the company’s entry into the AI space and promises to make the lives of their users easier. It’s an exciting move and a great demonstration of “AI for everyone”.Investment and in AI have also increased significantly recently, and even AI veterans such as are describing AI as “.”This is all great validation for the direction Elastic is headed with Prelert, , who build machine learning technologies on top of the Elastic Stack. However, there is a lot of noise and terminology in the space, and I’d like to describe how Prelert fits in and hint at where the product will go with Elastic.TerminologyThe market is full of terms such as Artificial Intelligence, Machine Intelligence, Machine Learning, Advanced Machine Learning, Deep Machine Learning, Unsupervised Machine Learning, Supervised Machine Learning, Semi-Supervised Machine Learning, Reinforcement Learning, Deep Learning, Parametric Machine Learning, Non-Parametric Machine Learning, Behavioral Analytics, User Behavioral Analytics, User Entity Behavioral Analytics, Predictive Analytics, Prescriptive analytics, Advanced Analytics, Statistics, Cognitive Computing, Smart Machines and so on. These terms are used technically and in product marketing material, and it can be difficult to understand what exactly these terms mean, and what a product actually does. For example, after the recent Elastic{ON} Tour event in NYC, I was asked the following questions:To cut through this noise, I’ll try to explain what Prelert (currently) does and what terms we use to describe it.What Prelert (currently) does and how we describe itTechnically, Prelert automatically learns a predictive model for the distribution of at given time, based on the historical values we have seen to date. This predictive model can be used to compute the probability of current behaviour given historical behaviour. If feature values are unpredictable (low probability) we classify them as anomalous. For example, given a time series data stream (e.g. transactions per minute), Prelert will automatically learn the typical behaviour of the time series as data is being streamed into the Prelert engine. As Prelert sees more data the models become more accurate. Prelert also ages out old models as typically systems change over time.This diagram shows how Prelert automatically learns the behaviour of a time series as the data is streamed in (left to right). The diagram also shows Prelert models becoming more accurate over time - Prelert automatically learns the periodicity in the data and then as more data is seen the variance in the models decreases, and the models fit the raw data more accurately.This technology allows users to answer questions such as “Are there unusual log messages in my log file? Has the performance of my instance changed? Is this user behaving unusually?”, which are difficult to answer using simple keyword searches, simple aggregations or visualisation.Behavioral AnalyticsBehavioral Analytics is less of a technical term, but is rather a way the market describes types of analysis performed by this technology. Originally coined to describe analysis of consumer behaviors when interacting with web sites, this term now refers to the analysis of various types of entities, including users, systems, applications, and other IT and business-related entities.Prelert is used for these use cases and so at a high-level we use the term ‘behavioral analytics’ to describe Prelert’s technology.Artificial Intelligence? Machine Learning?Firstly, we don’t really define a hard line between Artificial Intelligence (AI) and Machine Learning (ML). If intelligence is the ability to acquire and apply knowl","locales":"","title":"Artificial Intelligence Dreams, Machine Learning Promises, Behavioral Analytics? - How Elastic and Prelert Fit Together"}
{"index":{}}
{"author":"Rashid Khan","category":"Engineering","publish_date":"2016-11-09T00:00:00.000Z","url":"/blog/timeseries-if-then-else-with-timelion","seo_title":"","content":" Oh Timelion conditionals, we hardly know thee. Time to fix that. Introduced quietly into Timelion some time ago, the function is a powerhouse of, uh, power. Also, the title of this article is a total lie, I have more than one way I can call . The other is . It works exactly the same as , but is much shorter, so from here on out we’re using . allows us to change points based on a point-wise comparison. It supports less than (), less than or equal (), equal (), greater than () and greater than or equal () and compares points with either a number, or the same position in another series. For the purpose of this article I’m going to assume you’ve already used Timelion at some point. If you haven’t, there’s a handy-dandy tutorial built into the application itself. Click the friendly lion icon in Kibana 5 and he’ll help you learn Timelion’s simple syntax for timeseries taming. Alrighty, let’s get down to business. Easy does it Let’s start simple. While has four arguments, we are starting with these three for now In the following screenshots the green line will always be our original, and we’ll use other colors for our modified series. Let’s set every point less than 500 to 0 Well that was simple. How about we set everything over, or equal to 500 to 1000, effectively making the series binary with 1000 representing points greater than or equal to 500, and 0 being points under 500. We could string together two calls like this But wait, we can make that shorter using the fourth argument to . That fourth argument is , and it sets the point to a value if the condition match. So try this instead Cool! Those are the basics, but can munge more than static numbers. Read on brave soul. Dynamic? Dynamite! So far we’ve compared points to a static number. What if we wanted to compare our series to another series. For example, draw a dot at 0 when the series exceeds its 10-point moving average, otherwise show nothing. Below you will see that we’re passing as the parameter. This will perform a point-wise comparison between the moving average and our original series. The other part of our premise, showing nothing, can be accomplished by passing to . Yeah, that’s ok, but we can do better by also passing a series to . We can pass back the original series and draw some bars pointing out the spots where the series exceeds the moving average by using instead of Altogether now Now we take our new found knowledge and use it to get fancy, like drawing a chart that shades the area under the line green where we exceed the average, and red where we don’t Nice. Very nice. Still wondering what all this Timelion business is? Read the tutorial and still want more? and I’ll walk you through the nitty gritty. Plus you can watch me fumble with a microphone while trying to type with the other hand. ","locales":"","title":"I have but one .condition(). Timeseries if-then-else with Timelion"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2016-11-01T00:00:00.000Z","url":"/blog/brewing-in-beats-improve-metricbeat-on-windows","seo_title":"","content":" Welcome to Brewing in Beats! With this series, we're keeping you up to date with all that's new in , from the details of work in progress pull requests to releases and learning resources. Metricbeat: Improve Windows support Andrew replaced the usage of in the Gosigar library, so the system module of Metricbeat . Eliminating C is great news as it makes the code safer, easier to maintain, and easier to cross-compile. With this change, got fixed or added in the process: Metricbeat: Fixes and improvements in the Docker module Thanks to a community contribution by and issue was fixed where CPU values were reported as negative. The unit test coverage was also improved. Filebeat: Fix for state loading Just in time for 5.0, we had a few fixes in the Filebeat code that load and interpret the registry file on startup: Golint improvements This addresses a few coding style issues in the Beats project that were detected with golint. ","locales":"","title":"Brewing in Beats: Improve Metricbeat on Windows"}
{"index":{}}
{"author":"Shay Banon","category":"Releases","publish_date":"2016-10-26T00:00:00.000Z","url":"/blog/elastic-stack-5-0-0-released","seo_title":"","content":" In February of 2016, following Elastic{ON} 16, I wrote a post titled . Today, after almost a year of substantial effort, including 5 Alphas, 1 Beta, and 1 Release Candidate we are pleased to announce the GA release of the Elastic Stack. And, importantly, it is available – today – on . If you want hosted Elasticsearch and Kibana there is no other place to start with the most recent code. We are committed to making Elastic Cloud the best place to run hosted Elasticsearch. In fact, we even made the Release Candidate available on cloud for testing purposes. Our team is celebrating today. I hope you join us. The GA release is available today. Join the Elastic Team for a live virtual event on November 3 to learn more about the release and ask the creators questions (AMA style). Before exploring the release in detail, I want to take the opportunity to reflect on what has brought us to this point. Our CommunityDuring the recent Elastic{ON} Tour, I have begun each session discussing a brief history of the last several years. This session culminates in the announcement that we have reached a combined 75 Million downloads. When I first began the project, I hoped for widespread adoption. But the passion and fervor of our community continues to delight and amaze me. Pioneer ProgramWith that in mind, I want to share the results of the . The program began with a simple premise. Your usage of the Elastic Stack is of the utmost importance in informing our development as well as ensuring we release the highest quality product available. I am pleased to say that the community has filed 146 issues since the first Alpha release in April. Our community is one of our most valued assets at Elastic. In fact, one of the most discussed changes in this release was the name “Elastic Stack”. The Elastic Stack But Elastic Stack is more than just a name. When we began this release cycle we committed to developing, building, testing, and releasing the entirety of the Stack together. This is important, internally, to ensure compatibility. And, for you, it helps speed deployment, decrease version confusion, and make it easier for developers to add capabilities across the entirety of the Elastic Stack. A Feature TourWhen I began this post, I intended to provide an overview of key features in each product. But, it was hard to know where to begin and where to stop. Each of our team and tech leads have created a post that discusses the features specific to their product. And there is no one better suited to tell the story than them. I am, particularly, excited about a few items but rather than enumerate in detail, I will provide a brief overview and encourage you to read the detail posts for each product. This is but a sample, I’ve left out BKD trees, and , the immense effort put into , the eye-meltingly beautiful redesign of Kibana (we never knew how much we hated borders until we removed them), Kafka output in Beats, and so much more. This is a massive release. Reading the individual posts is a must to begin to understand the scope of improvement. X-PackAt Elastic we loved extensions. So much so that we built them and gave them interesting names. Shield, Marvel, and Watcher all described individual closed source features that didn’t take away for open source capability but were additive for our customers. Unfortunately, as the range of these features grew to include Graph and Reporting, the install process became difficult and, at times, quite confusing. Say Heya to X-Pack! One pack that adds security, alerting, monitoring & management, reporting, and graph capabilities to the Elastic Stack. Our engineering process for 5.0 wasn’t limited to the Elastic Stack, but we’ve also extended X-Pack by adding: X-Pack is available to trial and has both commercial and free (Basic) license options. We are particularly excited to make some X-Pack features available for free and details are available on our page. In ClosingI am i","locales":"de-de,fr-fr,ja-jp,ko-kr","title":"Elastic Stack 5.0.0 Released"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2016-11-06T00:00:00.000Z","url":"/blog/brewing-in-beats-new-sample-dashboards-for-docker-and-redis","seo_title":"","content":" Welcome to Brewing in Beats! With this series, we're keeping you up to date with all that's new in , from the details of work in progress pull requests to releases and learning resources. New sample Kibana dashboardsWe added a sample Kibana dashboard for in Packetbeat, and also sample Kibana dashboards for the and modules in Metricbeat. Please find below a screenshot of the Docker module dashboard. Available on HomebrewMetricbeat is part of Homebrew , and we are working to make Filebeat and Packetbeat part of Homebrew as well. Metricbeat: Hide username/password If the username and the password were part of the DSN in the hosts configuration option of MongoDB module, they were exported as part of field. To fix the , we  added two separate configuration options for username and password to the module in Metricbeat. A similar change was done for the module in Metricbeat. Define exported fields per moduleWe define for each Beat the type and the format of each field that is exported under . This information is used to not only generate the Elasticsearch index template, but also to generate the index pattern, that lets Kibana know how to format a certain field. With the growing number of supported protocols in Packetbeat, it grows also the number of fields that Packetbeat exports, and also the length of the file. To make it easier for the user to understand and add new fields into , we decided to under , and use to group all the from all the modules into a single per Beat. Metricbeat has a similar approach, and splits the fields per module under . In addition, we also changed that the , to be defined under . Libbeat: Fix console output on WindowsEnabling console output was on Windows systems, due to a check that stdout is actually available. We the issue by disabling the check on Windows systems. Filebeat: State handling change for ignore_older If a file is falling under during startup, . With the previous logic the whole file was sent in case a line was added and it was inconsistent with files which were harvested previously. ","locales":"","title":"Brewing in Beats: New sample dashboards for Docker and Redis"}
{"index":{}}
{"author":"Tal Levy","category":"Engineering","publish_date":"2016-11-02T00:00:00.000Z","url":"/blog/ingesting-and-exploring-scientific-papers-using-elastic-cloud","seo_title":"","content":" With Elasticsearch 5.0 released, let's explore some of its newest features! One of those features is the brand spanking new . I won’t dive deep into the Ingest Node internals here, but If you’d like a refresher, check out our introducing it. One specific ingest processor that we will explore is the . We will explore how to use it to index a dataset of PDFs for analysis. Previously, you would have used the to index PDFs into Elasticsearch 2.x. Since this is a type of pre-processing step, it should not have been done in Elasticsearch itself. Now that we have a way to manage pre-processing more effectively, this plugin has been converted into an Ingest Node processor. The Ingest Attachment processor makes it simple to index common document formats (such as PPT, XLS, PDF) into Elasticsearch using the text extraction library . More specifically, we will walk through an example of using the new Ingest Node on Elastic Cloud to explore the text of thousands of academic documents hosted on the repository for scientific papers Elastic Cloud With Ingest AttachmentFirst, we need to set up a new Elasticsearch cluster with Ingest Node (and the attachment plugin) on Elastic Cloud. Here is how we set up a new 5.0 cluster with the relevant ingest plugins: We have selected the Ingest AttachmentProcessor Plugin to be installed into our cluster. Now that we have a cluster set up, let’s collect the data and index! Ingest PipelineTo prepare Elasticsearch for indexing, we will define an ingest pipeline that will process a base64 encoded field called , and then remove the original field. the attachment processor will introduce a new set of fields in the document under the field. Such fields as , , , and (the processed text). PUT _ingest/pipeline/arxiv-pdf { \"description\": \"parse arxiv pdfs and index into ES\", \"processors\" : [ { \"attachment\" : { \"field\": \"pdf\" } }, { \"remove\" : { \"field\": \"pdf\" } }, ] } Data Collection and IndexingBefore we start indexing, let me introduce the actual set of documents we will be analyzing. hosts all of their papers on Amazon’s . We follow this to download a subset of the papers. We will be using the to index these documents into Elasticsearch. Here is a snippet of the indexing code: for ok, result in streaming_bulk( client, documents(), index=\"arxiv\", doc_type=\"arxiv\", chunk_size=4, params={\"pipeline\": \"arxiv-pdf\"} ): action, result = result.popitem() if not ok: print(\"failed to index document\") else: print(\"Success!\") In the above example code, we use the helper method of the Elasticsearch client to index our documents. is another helper function to read our files from the directory and convert them into Elasticsearch documents. The important parameter to highlight here is the . This parameter tells Elasticsearch to use our pipeline when indexing our document so that it can be pre-processed with the attachment processor. That’s it! Now we can start exploring the textual content of these papers. Related Terms in KibanaNow we can begin exploring the relations between terms in these papers. Using the in Kibana, we can begin seeing relationships between keywords in the text that are not necessarily popular, but are interesting or more relevant to our search queries. Here are just a few searches that show words that may be related. This type of analysis is useful to determine potential for improving term relevancy. Conclusion5.0 is out and the new Ingest Node is just one of the many new and exciting features. You can experience all the new features on now! ","locales":"","title":"Ingesting and Exploring Scientific Papers using Elastic Cloud"}
{"index":{}}
{"author":"Court Ewing","category":"Releases","publish_date":"2016-10-26T00:00:00.000Z","url":"/blog/kibana-5-0-0-released","seo_title":"Kibana 5.0.0 released","content":" It is with a profound sense of pride that we announce the official release of Kibana 5.0.0. In the lead up to this moment, we’ve compared Kibana to sliced bread, repurposed 14th-century poetry, and have even drawn parallels between our pre-release process and the First World War. Was this all over the top? Definitely. Was some of it in poor taste? Probably. But it was done with love, and we’re positively thrilled to finally deliver it to you all today. Kibana 5.0.0 requires Elasticsearch 5.0.0, and you can try it out on today. Head on over to the to check out the release, read up on the , or feast your eyes on some highlights: A brand new design A great visualization tool deserves a great design, and now Kibana’s design is finally up to snuff. The color scheme in general was refreshed from the ground up, and we sought to take advantage of the maximum possible screen real estate without sacrificing the accessibility of the UI. Oh, and we got rid of those stupid borders. You know the ones we’re talking about. Seriously, who puts borders on widgets in a dashboard? Not us… anymore. Time series data, meet Timelion After months being relegated to {re}search status, Timelion is now a part of Kibana core. For those unfamiliar, Timelion is a visualization tool with a query DSL that lets you ask interesting questions over time: These are the types of questions that Timelion was made for. And did I mention that the charts themselves are beautiful? Console, the best way to build custom Elasticsearch queries Sense is now Console, and it now ships with Kibana core. Console is like cURL if cURL came with out of the box request/response formatting, autocompleted Elasticsearch API syntax, and remembered your previous requests. In other words, Console is nothing like cURL, and that was a terrible analogy. Console uses the same configuration details as Kibana, so make your free-form requests to Elasticsearch without worrying about custom headers or the like. Painless scripted fields You can now choose the language of your scripted fields. In addition to the existing Lucene expression support, you can choose any scripting language that is configured in your Elasticsearch cluster. This means you can even use the brand new Painless scripting language that ships with Elasticsearch 5.0. Painless works a lot like Groovy, but we’ve put extra care into making it more secure. X-Pack Want out of the box monitoring for your Elasticsearch and Kibana nodes? How about first-class authentication and security controls or the ability to create PDF reports of your Kibana visualizations? You can try all of these things in Kibana 5.0 with a single CLI command. Check out the dedicated for more details. Upgrade from Kibana 4 Assuming you’re not relying on deprecated Elasticsearch functionality, your searches, visualizations, and dashboards from Kibana 4.6 should continue to work in Kibana 5.0. Just and go. Stuck on Kibana 4.1? No problem! We have a for you as well. Previous posts If you’re so inclined, peruse the blog posts for the various pre-releases to check out even more of the features in Kibana 5.0.0: Thanks for all of the help! We didn’t make Kibana 5.0 happen all on our own. Our endless thanks to all of those that tried out the pre-releases and submitted bug reports, pull requests, and excellent feedback to help make this the best Kibana release to date. Now what are you waiting for? Head to the and start using Kibana 5.0.0 today! ","locales":"de-de,fr-fr,ja-jp,ko-kr","title":"Kibana 5.0.0 released"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2016-10-26T00:00:00.000Z","url":"/blog/elasticsearch-5-0-0-released","seo_title":"Elasticsearch 5.0.0 GA released","content":" With 2,674 pull requests by 310 committers added since the release of Elasticsearch 2.0.0, we are proud to announce the release of , based on . This is the latest stable release and is already available for deployment on , our Elasticsearch-as-a-service platform. Go and download it today! You know you want to. Elasticsearch 5.0.0 has something for everyone. This is the fastest, safest, most resilient, easiest to use version of Elasticsearch ever, and it comes with a boatload of enhancements and new features. ","locales":"de-de,fr-fr,ja-jp,ko-kr","title":"Elasticsearch 5.0.0 released"}
{"index":{}}
{"author":"Tudor Golubenco","category":"Releases","publish_date":"2016-10-26T00:00:00.000Z","url":"/blog/beats-5-0-0-released","seo_title":"","content":" It’s finally here! After about 11 months of work, five Alpha versions, one Beta, and one RC, we have a new major version of your favorite data shippers. And it’s not only Beats that are releasing today. In fact, starting with this release, all the projects from the Elastic stack and with the same version numbers down to the patch level. That’s why we’ve got the nice jump from 1.3.1 to 5.0.0 in version numbers for Beats. If you’ve been following the alphas and betas, you know already what’s in, so we won’t stay in your way if you’re just looking for the link. If not, then this blog post highlights the biggest changes coming with 5.0. If you are upgrading from 1.x, please read the docs and the guide. Metricbeat - You know, for metricsMetricbeat replaces Topbeat as the primary tool for collecting metrics in the Elastic stack. Like Topbeat, Metricbeat collects “top” like statistics about host and per process resources (CPU, memory, disk, network). Unlike Topbeat, Metricbeat also collects metrics from systems such as Apache, HAProxy, MongoDB, MySQL, Nginx, PostgreSQL, Redis, or Zookeeper, with more to come in the near future. The system module in Metricbeat offers all the functionality of Topbeat, and it also extends this functionality with new features like per container metrics, disk IO metrics, and network IO metrics. The configurations of the two Beats are not compatible, but from one to another is fairly simple. And the good news doesn’t stop here: Metricbeat takes advantage of the new number types and storage implementation of Elasticsearch 5.0, which make numbers . Filebeat - under the hood improvementsWe’ve refactored the way Filebeat stores the state of of harvested files (which files are opened, inode numbers for each, how far we’ve read in each one, etc.). We’ve also cleaned up the settings around when the files should be closed, ignored or forgotten from the state. We expect these changes to improve the robustness of Filebeat against corner cases (for example inode reuse or very quick file rotation), so we recommend that you upgrade. In addition, Filebeat can now natively decode JSON objects from log lines. This is useful for , where the logging library writes the metadata directly formatted as JSON. Packetbeat FlowsPacketbeat now reports statistics like packet count and byte count about IP and TCP flows, regardless of the upper layer protocols. This opens Packetbeat to a new set of use cases, giving insights into how the traffic is flowing through the network and offers a limited level of visibility into encrypted traffic. In addition, Packetbeat got support for a few more application protocols, including AMQP, Cassandra, EDNS, DNSSEC, and NFS. Filter with the power of processorsBeats “processors” are simple rules that manipulate the events just before they are shipped from the Beat. While similar in nature with the processors or Logstash filters, don’t expect the same power. The Beats processors are... well, lightweight, and focus on filtering the data to avoid sending too much of it over the network when it will be discarded anyway by Logstash. That said, processors give you great flexibility in choosing the data that should be shipped. Here is a quick example: processors: - drop_event: when: equals: http.code: 200 - drop_fields: fields: [“http.phrase”] Kafka OutputWe listened to your feedback and we’ve added native Kafka output support in Beats, at the same time greatly improving the existing Redis output. This means that if you are passing all messages through a Kafka queue anyway, you no longer need an extra Logstash instance to convert between Beats and Kafka. The Beats Kafka output is compatible with Kafka versions 0.8, 0.9, and 0.10 and supports encryption, authentication, as well as dynamic topic and partition selection. Configuration flexibilityBeats 5.0 uses the same YAML-based configuration format, but improved with many small features that make t","locales":"de-de,fr-fr,ja-jp,ko-kr","title":"Beats 5.0.0 Released"}
{"index":{}}
{"author":"James Baiera","category":"Releases","publish_date":"2016-10-26T00:00:00.000Z","url":"/blog/es-hadoop-5-0-0-released","seo_title":"Elasticsearch for Apache Hadoop 5.0.0","content":" Elasticsearch for Apache Hadoop, affectionately known as ES-Hadoop, enables Hadoop users and data-hungry businesses to enhance their work-flows with a full-blown search and analytics engine, in real-time. And now, the moment you’ve been waiting for. Drumroll please. Developers and Data Scientists, I am pleased to present to you ! After several early access releases, a boatload of feedback posts, and much abundant waiting, it’s finally arrived! The Elastic Stack has made it to 5.0, and crossing the finish line along with it is ES-Hadoop! This release contains a substantial number of stability improvements, bug fixes, and shiny new features that we hope all of you will enjoy. And so, without further ado… What’s New in ES-Hadoop 5.0? Out with the Old … In with the new! Sometimes you need to step backward to move forward. We’ve bumped up the versions for a handful of integrations. In doing so, we’ve removed support for some older versions. If you are using the older versions, it would be best to update them before moving to ES-Hadoop 5.0 for maximum compatibility. Hello Hive 1.0, Goodbye Hive 0.13 and 0.14 Hive 1.0 has been released for quite a while and the majority of distributions have already moved to it. As such, support for Hive 0.13 and Hive 0.14 (two releases that were plagued by serious issues) has now been dropped, cleaning up the code base. Hello Storm 1.x, Goodbye Storm 0.9 Storm support has been upgraded to 1.0.x. As this version is not backwards compatible with Storm 0.9.x, support for these versions had to be dropped. Hello Spark 2.0, Goodbye Spark 1.0-1.2 Our support for Spark has been updated with the recent release of Spark 2.0. This version of Spark is not backwards compatible with any previous Spark versions. We have decided to keep support for Spark 1.3-1.6 as . SparkSQL was originally released in Spark 1.0-1.2 as an alpha component. Since then SparkSQL has become stable in Spark 1.3, but the API has significantly changed. Supporting three very different versions of Spark is a bit much. Because of this, support for Spark 1.0-1.2 has been removed. HDFS Repository The HDFS Repository has experienced a substantial upgrade and is now part of Elasticsearch proper. Because of this upgrade, we have removed it from the ES-Hadoop project. Note that the HDFS plugin in Elasticsearch 5.0 is not just conveniently packaged but also better integrated. Among these improvements is no longer needing to disable the JVM - an option that isn’t even available anymore. (Hadoop/Spark) + Slice API = More Parallel A substantial change has been added to support the use of Elasticsearch’s new Scroll Slicing functionality. Now you can state the maximum number of documents you wish to see per input task and the framework will attempt to subdivide input splits to increase your computing parallelism. Isn’t sharing beautiful? Ingest Node We heard about this cool new feature called the that was available in the alpha releases and coming out in Elasticsearch v5.0. We thought “Oh man, we ingest stuff, this node ingests stuff. We need to schedule a brunch with it immediately to trade gossip.” With the release of ES-Hadoop 5.0 you can now specify an ingest pipeline to send your data to, as well as target only ingest nodes to cut down on unnecessary traffic. We’re still waiting to hear back from you about brunch, Ingest Node. Call us! Native Support for Spark Streaming Spark is pretty fast, but sometimes you need your data even faster. We loved hearing that some of you were using ES-Hadoop with Spark Streaming, but we also felt the same heartache about the limitations that you were running into. We decided to do something about it. ES-Hadoop now natively supports consuming s from Spark Streaming! We’ve included some fixes for the most commonly reported Spark Streaming issue of running out of connection resources during small processing windows. M","locales":"de-de,fr-fr,ja-jp,ko-kr","title":"Elasticsearch for Apache Hadoop 5.0.0"}
{"index":{}}
{"author":"Suyog Rao","category":"Releases","publish_date":"2016-10-26T00:00:00.000Z","url":"/blog/logstash-5-0-0-released","seo_title":"Logstash 5.0.0 Released","content":" Logstash 5.0.0 has landed. 5 alphas, 1 beta and an RC later, we are super excited to announce the general availability of the biggest release of Logstash yet. And it’s not just Logstash, the entire Elastic Stack is having a release party and they’re all available on  for you to try immediately. If you can't wait to get your hands on it, head straight to our page. But we do recommend reading through the important of breaking changes. If you are looking for a complete set of release notes, you can find it. Breaking ChangesWe recommend users to read before upgrading from older versions. HighlightsAn important theme for Logstash in this release was to enable self-service troubleshooting. Improving performance is an aspect we try to focus on every release, and in 5.0.0, we've made great strides across the board. Below you will find the headlining features for this release: Logstash is no longer a black boxA new monitoring feature provides runtime visibility into the Logstash pipeline and its plugins. This component collects various kinds of operational metrics while Logstash processes your data, and all of this information can be queried using simple APIs. Most of the APIs were modeled after Elasticsearch and we've tried to be consisted in regards to the end-points and even the JSON responses. The monitoring APIs can be grouped broadly into these areas: Please see the for details. Continuing on our theme of providing more visibility into the inner workings of Logstash, we've enhanced Logstash's internal logging framework. First off, we've moved to Log4j2, the popular Java logging library. For users, this means you can now get logs granularly down to a single plugin level. To increase granularity and log levels, one could modify the log4j2.properties file and restart Logstash, but that is both tedious and leads to unnecessary downtime. Instead, we've exposed APIs that can be used to dynamically update logging levels. These settings get effective immediately and do not need a restart. Another benefit of moving to Log4j2 — file rotation policies can be configured directly using the properties file. Performance: Java EventIn this release, we've completely rewritten a critical part of Logstash's pipeline infrastructure in Java. Data flow in Logstash is encapsulated using an internal Event object that is passed to plugins, used for conditionals, field reference lookups and in the future, will be persisted on disk. So what does this mean for users? In our, we've seen consistent throughput increases across multiple configurations. In some cases, we observed up to 75% increase in events processed through Logstash. If you are a plugin developer or maintain a custom plugin, we encourage you to read to adapt your code to the new plugin API. Ease of use Users can now set Logstash's options in a settings file, logstash.yml, instead of using command line arguments. For example, you can specify pipeline settings, the location of configuration files, logging options all in one file. Logstash release packages (Debian, RPM) has been given an overhaul in this release. Previously, Logstash used /opt/logstash directory to install the binaries, whereas Elasticsearch used /usr/share, and /var. To make user experience consistent across our products — a theme you'll hear a lot in our 5.0.0 releases — we've to reflect Elasticsearch RPM and DEB layouts. Additionally, we added systemd and upstart support to run Logstash as a on *nix platforms. Enhanced integrations and new plugins This plugin works out of the box with Elasticsearch 5.0.0. It has also been made threadsafe to take better advantage of the recent changes in the pipeline architecture. Other enhancements include a new connection pool to efficiently reuse connections to Elasticsearch, exponential backoff for connection retries, and better handling for sniffing. Logstash 5.0 is compatible with Elasticsearch 5.x, 2.x, and even 1.x. Apache Kafka had","locales":"de-de,fr-fr,ja-jp,ko-kr","title":"Logstash 5.0.0 Released"}
{"index":{}}
{"author":"Steve Kearns","category":"Releases","publish_date":"2016-10-26T00:00:00.000Z","url":"/blog/x-pack-5-0-0-released","seo_title":"X-Pack 5.0.0 Released","content":" Alongside the 5.0 release of the Elastic Stack, we are excited to introduce the world to X-Pack — a single extension that provides security, alerting, monitoring, reporting and graph capabilities across the Elastic Stack. X-Pack is the evolution of - and replacement for - the standalone plugins Shield, Watcher, Marvel, Reporting, and Graph. While we loved these individual plugins, we were running out of clever names and were beginning to struggle with an increasingly complex, 15+ step installation and configuration process. As the name literally says, X-Pack is a “pack,” and it is the first of its kind. A pack is a simple, but important concept - it is a single zip that contains extensions for one or more products in the Elastic Stack. And thanks to our aligned version numbers and release train, it’s now easy to build and test extensions that bring UI components to Kibana, new APIs to Elasticsearch, and so much more. We hope you go forth and build interesting packs of your own, but before you do, there’s a lot more to know about X-Pack! Installation & Configuration We spent a lot of time thinking about ways we could make it easier to install and configure. The install process is now just two commands, and you are ready to get started with the full range of X-Pack functionality. As part of the installation process, we automatically create two native users - , an admin account, and which is a service account used by the Kibana backend. These users are created with a default password of , which the Kibana backend will use by default. This means that there is absolutely no configuration necessary when you’re just getting started. Of course, before you go into production, you will need to change the default passwords, and configure SSL, but even that is now easier and more consistent across the stack. Security Creating and managing security in the Elastic Stack just got a whole lot easier. X-Pack builds on the capabilities introduced in Shield, which include authentication, role-based access control, encrypted communication, audit logging, and login and session support for Kibana. Newly added with X-Pack 5.0 is a management UI in Kibana for creating and managing both users and roles: Alerting X-Pack alerting features build on the capabilities of Watcher, as a highly-available alerting engine that runs inside Elasticsearch and is configured via APIs. Creating an alert involves specifying 4 simple parameters - a schedule, query, condition, and one or more notification actions, such as email, Slack, HipChat, PagerDuty, or a webhook. X-Pack 5.0 adds a number of new options, including the ability to specify a condition per-action, which makes it easier to send different types of notifications at different thresholds. For example, if application response times exceed SLAs for 1 minute, use a webhook to create a ticket for the ops team to look into tomorrow. If the response times exceed SLAs for 30 minutes, it’s time to page someone. Fun fact - the API still uses the term Watcher, out of respect for the many use-cases beyond alerting that it enables. Monitoring X-Pack has a goal of providing monitoring capabilities for the entire Elastic Stack. With Marvel, we introduced the most effective monitoring tool for Elasticsearch, and X-Pack 5.0 expands this to include monitoring for Kibana: Reporting X-Pack makes it easy to create and share PDFs of Kibana visualizations and dashboards. Combine reporting with alerting capabilities to send periodic screenshots of dashboards to users that don’t have direct access to Kibana, or attach supporting information to the notification emails triggered by X-Pack alerts. Graph X-Pack provides a new way to explore your data with the graph API and UI. Rather than summarizing, slicing, and dicing the properties of your documents, Graph lets you ask questions in terms of the entities (the machines, services, people, bands, etc) and how they are related to ","locales":"de-de,fr-fr,ja-jp,ko-kr","title":"X-Pack 5.0.0 Released"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2016-10-25T00:00:00.000Z","url":"/blog/brewing-in-beats-getting-ready-for-5-0-0-ga","seo_title":"","content":" Welcome to Brewing in Beats! With this series, we're keeping you up to date with all that's new in , from the details of work in progress pull requests to releases and learning resources. Libbeat: Fix issues when using Kafka as output The community found the that the Kafka output raises a protocol error when Kafka 0.10 is configured. An of the Sarama library to master solved the issue. In addition, another appeared as the Kafka output connection was closed by broker on SASL/PLAIN. The includes a fix for it, and exports a series of Sarama Kafka metrics that can be used for debugging. The fix is backported to the 5.0 branch, and it will be part of the 5.0 release. Metricbeat: Count each unique mountpoint once in fsstats The system.fsstat metricset contains filesystem metrics aggregated from all mounted filesystems. It includes information like the number of mounted filesystems, total number of the files, total size, total used size etc. There was an reported by the community that the number of mounted filesystems is too high on CentOS, compared with what `df -h` command displays. The problem appeared when two filesystems have the same mounted point, as we were counting them twice. The changes from the count mounted filesystems by the number of unique mounted points. Filebeat: Ignore duplicate state entries Fixed an issue in the 5.0 branch where upgrading the registry file from 1.3 could result in and multiple harvesters started on the same file. Packetbeat: Get the GeoIP information of your clients Starting with 5.0.0-alpha3, the support in Packetbeat for GeoIP is deprecated, and it will be removed in 6.0. Instead, you can get the GeoIP information of your client's’ IP by using Ingest Node or Logstash. More precisely, by defining a GeoIP processor in the Ingest GeoIP plugin or by defining a filter in Logstash. For a smooth integration with Ingest Node, Packetbeat adds the of type geo_point in the mapping template, and then Ingest Node completes it with the GeoIP information, so you can easily visualize it in Kibana without any extra configuration needed. The change will be available in the 5.1 release. For more details about it, please check our . Hearbeat: Fix panic when trying to start the scheduler The fixes the Heartbeat when starting the scheduler. Dashboards: Fix Topbeat 1.x dashboards A time frame is associated with the Topbeat dashboard, that was causing Topbeat dashboard 1.3 to fail to load. The pull request removes the time frame associated with the Topbeat dashboard. ","locales":"","title":"Brewing in Beats: Getting ready for 5.0.0-GA"}
{"index":{}}
{"author":"Court Ewing","category":"Releases","publish_date":"2016-10-24T00:00:00.000Z","url":"/blog/kibana-4-6-2","seo_title":"Kibana 4.6.2 Released","content":" Today, we’re releasing Kibana version 4.6.2 to address a high severity cross-site scripting (XSS) vulnerability. The vulnerability, , allows an attacker to execute arbitrary JavaScript in other users’ browsers. Any users of Kibana versions 4.3-4.6 should upgrade to version 4.6.2 as soon as possible. Version 4.1.11 is not affected. This version is available immediately on our page as well as through apt and yum. ","locales":"","title":"Kibana 4.6.2 released"}
{"index":{}}
{"author":"Tudor Golubenco","category":"Brewing in Beats","publish_date":"2016-10-20T00:00:00.000Z","url":"/blog/brewing-in-beats-heartbeat-uptime-monitoring","seo_title":"","content":" Heartbeat 💓 Heartbeat is the newest Beat to . Heartbeat does active uptime monitoring via ICMP, TCP, and HTTP and stores the availability status and the round trip times in Elasticsearch. It already supports things like SSL, authentication, or proxies. Heartbeat is also able to reload the list of monitored targets and to resolve all the IP addresses behind a DNS name and test them all. This is useful, for example, to check the uptime of all the hosts behind an ELB. Heartbeat development is still ongoing. We plan to release a first officially supported versions in the next few months. Libbeat: Add EC2, GCE, or DigitalOcean metadata The new Beats processor does a one time query of the respective cloud APIs and adds a meta object in all events created by the Beat, making it easy to filter in Kibana by things like the region or the machine type. See the for example objects from each of the supported cloud platforms. Beats-tester: test default configuration Now that we can easily overwrite config settings with -E, we can , and only overwrite what we need to be different in tests. This has the advantage that we test the default configuration files in realistic conditions. Fix requested privileges on Windows Fixed an issue where the Metricbeat system module was . We expect this to fix where Metricbeat was not able to read process information. Fix “actual free” memory on Windows In the Metricbeat system module for Windows, to refer available physical memory, not available virtual memory. Fix for the Kafka output Fixes an issue where the Kafka output would of messages if one of them was too large to be accepted by Kafka. Fix for Filebeat Fixed an issue where if If and were used together, states from the registry. ","locales":"","title":"Brewing in Beats: Heartbeat, for uptime monitoring"}
{"index":{}}
{"author":"Suyog Rao","category":"The Logstash Lines","publish_date":"2016-10-11T00:00:00.000Z","url":"/blog/logstash-lines-2016-10-11","seo_title":"","content":" Welcome back to The Logstash Lines! In these weekly posts, we'll share the latest happenings in the world of Logstash and its ecosystem.Last week has been a slow week with people recovering from our engineering all-hands in Prague, but this update spans the last 3 weeks:We've to merge the persistent queue feature branch to master and 5.x. This is still not feature complete, but it's pretty close. Also, long-living feature branches are no good.. The goal of this iteration is to:All existing unit tests are passing, and we've added file based integration tests last week.Open items:We now have an end-end integration test framework in core which uses a LS binary to test against real services like Kafka, ES and Filebeat. Tests are run locally and do not need a VM. It is super easy to configure — uses travis style files to setup services, and tests are written using RSpec. We already have a few integration tests built using this framework on travis, and they will soon be added to logstash-ci Jenkins platform. With this change, we have integration tests running in every plugin repo in core to validate code changes. ","locales":"","title":"Logstash Lines: Getting ready for 5.0.0-rc1"}
{"index":{}}
{"author":"Tudor Golubenco","category":"Releases","publish_date":"2016-10-10T00:00:00.000Z","url":"/blog/beats-5-0-0-rc1-released","seo_title":"","content":" Today we’re publishing the first public release candidate for the 5.0 version of the stack. Let your excitement grow, the 5.0 day is approaching! Here is what changed in RC1: Make future upgrades easierIn order to simplify adding new fields between minor versions, we have done two changes: With these changes, Elasticsearch will be able to guess the types of future fields, even if the updated mapping is not loaded yet, so the upgrade procedure will be simpler. Other fixes ","locales":"","title":"Beats 5.0.0-rc1 released"}
{"index":{}}
{"author":"Michael McCandless","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2016-10-17T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2016-10-17","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Upgrade to 5.0.0-rc1 paying back. A small benchmark in my project shows consistent reduction in response time by 25%.— DoHyung Kim (@_dohyung_) Elasticsearch Core 2.4: 5.0: 5.x: master: ongoing: Apache Lucene Watch This Space , where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2016-10-17"}
{"index":{}}
{"author":"Antonio Bonuccelli","category":"Culture","publish_date":"2016-10-12T00:00:00.000Z","url":"/blog/if-you-love-somebody-set-them-free","seo_title":"If you love somebody, set them free","content":" “If you love somebody, set them free”. This is what it feels like working at Elastic, to be loved, to be free.At previous work tenures I have always somewhat suffered the requirement of having to be in the office to do my work and always dreamed I could just work anywhere I wanted. Once you prove yourself to your manager, team and company, trust is established, everyone is on the same page on what are the objectives to achieve and expectations are set, there shouldn’t be a substantial reason for being denied to do so. End of the day it’s “quid pro quo” and if everyone is happy, people can only give their best to their work peers, to their managers, to their company  and last but not least to their customers. These were the days before joining Elastic, right now I think of these as a previous life.Elastic Support is a family: this means we share joys and pains, trust is established and each of us will step in and help out when any one is in need, whatever the reason. Since we’re all distributed, I honestly believe it couldn’t be otherwise, the toy would just break apart. No one is expected to be baby-sat, micro-managed, or be told how to achieve objectives (common sense applies):  we are all experts, each in his or her own domain of knowledge.As we are pretty much all distributed across the globe, great tools like help us being more connected and bring us closer to each other. This does make a huge difference over using just cold plain-text chat or emails: we are human beings and we have a need for being social. As any other office based worker, we do have coffee together and talk about whatever over work breaks. Once we also had a little “hat party” :-)Of course, every now and then we feel the need to gather and discuss in person all the things that can help us be a better team, provide better support to our customers, review and innovate our practices and remind ourselves to keep having fun.This is more or less what our support gathering within the broader engineering all hands looked like about a year ago in Barcelona:Since then our team has almost doubled in size, this picture below was taken a couple of weeks ago in our latest in Prague: Looking back at when I’ve joined our in September 2014 our team has grown fast, from 6 engineers to 34 (+566%) and this is a tremendous testament to the growth of our user base and to the success of our products, of which we’re all incredibly proud.As the Elastic Stack scales smoothly the need to handle your data, from collection to consumption, so do we with our culture and values as the company grows.We all know “all work and no play makes Tony™ a dull boy,” so it is imperative to keep having fun and we’re certainly enabled to do so, with the right tools : -) Given all the above, I have had the chance to work very independently and explore planet earth, like I could never do before: so on my first year anniversary at Elastic I did ask my manager “Can I go and work from Asia for a few months?” The answer was a big capital letters “YES!” After seeing his dancefloor performance at the last partyI am not that much surprised he allowed me to do so, but taking incredible things for granted is the last of the mistakes one wants to make.So I did have the chance to achieve one of my dreams, visit the other side of the world, spend a few months discovering the unknown, see friends there that I hadn’t seen in ages, explore different cultures and all the amazing things that travel can bring to our life, making it richer.Asia always fascinated me for the very different cultures you can find there, also I have several all-time friends who live there, so this was a great chance to go and see them.And I did fabulous things like:meeting my colleagues for dinner in wonderful Tokyoexploring stunning floating markets and temples in Bangkoksnake tasting in massive Shanghai making monkeys jealous by drin","locales":"","title":"If you love somebody, set them free"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2016-10-10T00:00:00.000Z","url":"/blog/elasticsearch-5-0-0-rc1-released","seo_title":"Elasticsearch 5.0.0-rc1 released","content":" Today we are on the edge of our seats with excitement, because we’re announcing the release of based on . This means that the GA release is VERY VERY CLOSE! This is the seventh in a series of pre-5.0.0 releases designed to let you test out your application with the features and changes coming in 5.0.0, and to give us feedback about any problems that you encounter.Open a bug report today and become an . This is an RC release and is intended for .This release contains a number of minor enhancements and bug fixes that have been added since 5.0.0-alpha5 (all of which you can read about in the release notes linked above), and one big change: strict URL query-string parameter parsing. ","locales":"","title":"Elasticsearch 5.0.0-rc1 released"}
{"index":{}}
{"author":"Michael McCandless","category":"Engineering","publish_date":"2016-10-18T00:00:00.000Z","url":"/blog/apache-lucene-numeric-filters","seo_title":"The Evolution of Numeric Range Filters in Apache Lucene","content":" The project, which Elasticsearch builds on, as a pure text search engine, indexing tokens (words) from a document to build an on-disk inverted index so you could later quickly search for documents containing a specific token. This hard problem was already challenging enough! The project became very successful with time, and naturally users wanted to index numbers too, to apply numeric range filters to their textual searches, such as \"find all digital cameras that cost less than $150\". Everything is textThe problem was, to Lucene, everything had to be a simple text token, so the obvious way to work with numbers was to index each number as its own text token and then use the already existing , accepting all tokens in a single alphabetic range, to filter on the numeric range. The immediate challenge with this approach is that Lucene sorts all tokens alphabetically (in Unicode code point order), which means simple numbers in decimal form won't be in the right order. Imagine indexing these numbers, sorted as Lucene does in its index: Now if you want to find all numbers in the range of 17-23, inclusive, will incorrectly include the number , shocking your users! Fortunately the fix, way back when, was simple: just left-zero-pad your numbers to the maximum length number, e.g.: Those leading s do not change the numeric value, but they do cause Lucene to sort in the correct order so that matches only the numbers in the requested numeric range. See how now sorts (correctly) before . It turns out this was a viable approach, used by Lucene users for years! However, it was a bit wasteful, with all these extra s in the index (although they do compress well!). It was also inflexible: what if you suddenly needed to index a 10 digit number but you only left-zero-padded to 9 digits for all of your already indexed numbers? You had no choice but to fully re-index. Worst of all, this approach was slow and index-bloating if you have many unique numbers (high cardinality) across your documents: would need to visit every single unique number, then iterate the document(s) containing that number. This could be exceptionally slow in extreme cases. Yet, despite all these performance flaws, it was functionally correct, and it was all we had many years ago: beggars can't be choosers. Enter Uwe SchindlerLucene's numeric range filtering saw massive improvements in Lucene's 2.9 release. Around seven and a half years ago, suddenly appeared and offered a better solution, called , that he had implemented for his own usage of Lucene. It is wonderful when a determined user develops something impressive for their own application (\"necessity is the mother of invention\") and then also works hard to contribute it back for the benefit of all users. Uwe persisted and polished and after many iterations added far better numeric support to Lucene. Such is the way of healthy open-source projects! , after half a year it since it was such an important feature and offered sizable performance gains. In graduating, it also exposed a very consumable API: you simply add , etc., at indexing time, and filter using , etc. The ease-of-use of this API was a big contributor to its fast adoption, also an important open-source lesson. Surprisingly, this approach also indexed numbers as if they were textual tokens (the inverted index), but it did so at multiple precision levels for each indexed number, so that entire ranges of numbers were indexed as a single token. This made the index larger, because a single numeric field is indexed into multiple binary encoded tokens. But at search time it meant there were often far fewer terms to visit for a given range, making queries faster, because the requested query range could be recursively divided into a union of already indexed ranges (tries). Indices got larger and queries got faster. The tokens were also ascii only, never able to take advant","locales":"","title":"The Evolution of Numeric Range Filters in Apache Lucene"}
{"index":{}}
{"author":"Shay Banon","category":"News","publish_date":"2016-10-17T00:00:00.000Z","url":"/blog/elasticon-17-a-user-conference-for-the-whole-elastic-stack","seo_title":"Elastic{ON}17: Announcing our 3rd Annual Conference, March 2017 in San Francisco Elasticsearch, Kibana, Logstash, Beats -- a user conference for the whole Elastic Stack","content":" Elasticsearch, Kibana, Logstash, Beats -- a user conference for the whole Elastic Stack Let me start with a big thank to our users who continue to inspire us to build new products and continue improving on all of our products.It’s because of you that we make every effort possible to give you access to our engineers, product managers, and solutions architects wherever in the world you are through IRC, forums, chat, email, and face-to-face at meetups, conferences, and events.Fast forward to March 7-9 2017 in San Francisco: A week full of product roadmaps, deep dives, use cases, networking and unlimited one-on-one time at Ask Me Anything.During this week, we’ll be hosting our our , and San Francisco’s Pier 48 will transform into the single, largest global gathering of developers who use the , X-Pack and the . Whether you’re a user of Elasticsearch, Kibana, Logstash, Beats, or use our X-Pack features either on-premise or in the cloud, this year’s Elastic{ON} will bring together 2,500 developers, architects, devops, IT, and security folks to share ideas and best practices on how to solve use cases from search, logging, analytics, and more.While it’s really awesome that I get the opportunity to meet many of you, what I love most about Elastic{ON} is hearing about the wonderful new relationships that are created. Last year, met his Sydney-based support engineer for the first time  after hundreds of tickets and conversations about how to scale their infrastructure. As we send our entire engineering team to Elastic{ON}, attending Elastic{ON} is the only event where any attendee can meet Clint Gormley then  Simon Willnauer minutes later, Elasticsearch team and teach leads:  spend time with the creators of Kibana (Rashid), Logstash (Jordan), and Beats (Monica and Tudor):  or meet at our Ask Me Anything booth, Nick Knize, our geo expert from Texas, Mark Harwood, Graph creator from the UK, or Britta Weber, our Elasticsearch scoring expert from Berlin, to name a few.Like with everything we do, we’ve listened to your feedback from the past two Elastic{ON}s and are excited to introduce some cool new things this year: I’m excited to invite you to and the rest of the Elasticians in San Francisco this March and experience the biggest gathering of Elasticsearch users ever.  <p>If you have a great story to tell, please <a href=\"https://www.elastic.co/elasticon/speak\">submit a talk</a> ASAP. Deadline is October 24th. &nbsp:  </p> Hope to see everyone in March:  of last year. ","locales":"","title":"Elastic{ON}<sup>17</sup>: Announcing our 3rd Annual Conference"}
{"index":{}}
{"author":"Tyler Hannan","category":"Releases","publish_date":"2016-10-10T00:00:00.000Z","url":"/blog/elastic-stack-release-5-0-0-rc1","seo_title":"","content":" We’ve reached a milestone in the 5.0.0 release.  Today, we are pleased to announce that the first 5.0 release candidate is available for download. At Elastic, a release candidate indicates that the products should look, and feel, very similar to the final GA release. We do use this period to squash any final bugs that may arise. In addition, this is the first time we're releasing an early access version in Elastic Cloud, which makes it even easier to test such releases now. To start with, we're rolling it out to Ireland and Virginia AWS regions. This is the first step in making all early access releases, including betas, alphas and even nightly builds, available on cloud so that our users can easily test them out and provide feedback on the latest and greatest Before you get too excited, keep in mind that this is still a release candidate, so don’t put it into production. If you open a bug report, today, you too can become an . And now, without further ado, some highlights from rc1. ElasticsearchFor more detailed information, and many other features, peruse the Elasticsearch . From 2.4.x to 5.0. The magical journey. The undiscovered country. The next frontier. Other coined phrases. As a reminder, we’ve also released the , which runs on your existing 2.4 cluster.  Use this site plugin to prep for your migration. KibanaThe theme here is squashing bugs. Shimmy your way over to the Kibana to see everything that’s changed. In the meantime, some highlights: Logstash Thanks to all your feedback on our prereleases! We’ve squashed a few bugs and improved log messages when Logstash is started as a *nix service. Details in the . BeatsLightweight data shippers. Not so lightweight changes. Explore them all in the . Get it Now!Happy testing. Your feedback is instrumental in making 5.0 successful. And, don't forget that X-Pack is here and continually being updated with each pre-release. ","locales":"ja-jp","title":"Elastic Stack Release - 5.0.0-rc1"}
{"index":{}}
{"author":"Court Ewing","category":"Releases","publish_date":"2016-10-10T00:00:00.000Z","url":"/blog/kibana-5-0-0-rc1","seo_title":"Kibana 5.0.0-rc1 released","content":" There’s all sorts of hip shaking and loose footing going on right now as we publish the first release candidate of Kibana 5.0 - we just can’t contain our excitement. The 5.0 GA is fast approaching, and with this release of 5.0.0-rc1, we’re on track to deliver something big. Something truly unique. Something you never saw coming… unless you download this release candidate and test it out. Then you would have seen what’s coming and could ignore what we just said. In fact, if you could go ahead and do that, that’d be great. In all seriousness (as opposed to less-than-all seriousness), we need your help! This release candidate has all of the features that will be shipping with 5.0, and we want to find every last remaining bug in the build so 5.0 GA can be the most stable Kibana yet. and test it out today. If you find any bugs, please report them on , and we’ll send you some swag as part of our Elastic Pioneer program. : This is pre-release software that will only work with . Please test it, but do not use it in production. In addition to the changes listed below, check out the blog posts for our previous 5.0 pre-releases to get a handle on everything that’s changed since 4.6: CSV upload has been removed We were really excited to ship the CSV upload tool in 5.0.0-alpha4 because it was the first time Kibana could be used to ingest data into Elasticsearch. Unfortunately, after the feature was tested more broadly both internally and externally, it became clear that it was not of the quality that users require, so we’ve pulled it from the 5.0 release. We share your frustration on this one, but a feature that only works properly for a very specific use case and essentially falls apart for everyone else isn’t good enough to ship with Kibana. Performance improvement on visualization rendering Kibana charts are tricky beasts. Visualizations are not simply rendered and forgotten. Kibana must re-render them whenever users resize their browser, toggle the legend, etc. This works great once the entire page is loaded, but it adds a lot of complexity while visualizations are being loaded for the first time. In the midst of that complexity, Kibana was doing most of the work necessary to render each chart twice. This sort of rendering behavior is by no means a requirement, and it negatively impacted the rendering performance of visualizations. We’ve changed that behavior by more carefully controlling the order in which the various components of a visualization get rendered, and the result is faster initial loads for charts in Kibana. Other changes What’s next? Hopefully an awesome GA release! But if we find enough bugs in rc1, we’ll take a crack at another release candidate. Time will tell, so stay tuned. In the meantime, the 5.0.0-rc1 release and help us find some bugs. If you find any, please report them on , and feel free to ask any questions or provide any feedback on our , , or . ","locales":"","title":"Kibana 5.0.0-rc1 released"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2016-10-09T00:00:00.000Z","url":"/blog/brewing-in-beats-new-community-beat-for-network-devices","seo_title":"","content":" New community beat: Openconfigbeat from Arista Networks created to get statistics from network devices over the OpenConfig interface. is a collaborative effort by network operators to develop a common interface for managing network devices. Libbeat: Make future upgrades easierIn order to make it easier to upgrade between minor versions when new fields of type float are exported, we make sure that float fields are always sent as even if the current value happens to be an integer. For example 5 will be sent as 5.00000, so Elasticsearch defines it as a float in the absence of a mapping. You should set all strings to be by default, so if new string fields are added, you can build aggregations on them even if no template is defined yet. Libbeat: Export beat version in each eventEach Beat adds its version in each event under the field, so you can identify the events generated by a certain release version. Even if it adds an overhead to the exported traffic, this information is valuable during upgrades. Winlogbeat: Limit the number of windows event logsAdd a to allow users to control the number of event log records that are read from the Windows API in a single batch. By default is set to 100, and it’s available only on Windows Vista or newer versions. Metricbeat: Report system-wide information from Docker moduleEnhance the Docker module in Metricbeat by exporting the info metricset to include like the number of total containers or the number of running/stopped/paused containers. Libbeat: Fix drop_fields processor when the first field is unknownFix the processor as it was not dropping any field when the first field from the list was unknown and had a nested format like . Metricbeat: Remove dots in field names from the Redis moduleSome dots found their way in the fields of our Metricbeat Redis module, and to maintain compatibility with the 2.X Elasticsearch versions. Libbeat: Deprecate port setting for Redis and Logstash outputA list of Redis/Logstash servers can be configured under the hosts settings, and you can specify a port for each server. If no port is configured, you could configure a default under the port option. We this in 5.0 in order to avoid the confusion of having two ways of configuring the port. ","locales":"","title":"Brewing in Beats: New community Beat for network devices"}
{"index":{}}
{"author":"Jason Tedor","category":"Engineering","publish_date":"2016-10-05T00:00:00.000Z","url":"/blog/bootstrap_checks_annoying_instead_of_devastating","seo_title":"Bootstrap checks: Annoying you now instead of devastating you later!","content":" Elasticsearch 5.x has ten new which run when Elasticsearch starts up to check for configuration problems that might cause exciting failures after a node sees serious use. If any of these check fail, the node will abort during startup if it is bound to a non-local IP address. We earnestly hope that these checks are just a minor annoyance and that they prevent major heartache.Serial KillerConcrete example time! We recently added a that fails if we detect that the JVM that Elasticsearch is running in is using the \"serial\" collector. The serial collector is designed for single-threaded applications or applications running with extremely small heaps. Elasticsearch is very much a multi-threaded application and will not fare well with an extremely small heap so the collector isn’t a good fit for it. This collector can cause nodes to time out of the cluster because this collector stops the world while it is running and those pauses can be quite substantial for large heaps because the collection process is single-threaded. If these pauses run longer than ninety seconds the master node might time out its periodic pings to the node and remove it from the cluster. When the node finishes its long GC cycle it will rejoin the cluster, causing the node to \"bounce\" in and out of the cluster.We hope the saves someone a lot of trouble down the line. In fact we’re fairly sure it will because we’ve seen the serial collector in action. One thing buying a subscription from Elastic entitles you to is support and \"one of my nodes is periodically leaving the cluster and coming back\" is a thing that comes up from time to time. Sometimes we track the failures back to the serial garbage collector. Users won’t see those failures in production on 5.x because Elasticsearch will refuse to start with the serial garbage collector at all. Since the startup scripts explicitly set up a different collector we expect this check to trigger very rarely but given how insidious the failure mode is we thought it was worth checking.The File Limits are Too Damn LowAnother issue that we see very frequently is \"too many open files\". This happens when a user (often unknowingly) has their file descriptor limits set far too conservatively, like 4096. Elasticsearch needs lots of file descriptors to open many indices composed of many segments (and, on Unix-based operating systems, things like network sockets are file descriptors too). If this limit is set too low and Elasticsearch bumps up against the limit, things like shard allocation can fail and disaster can ensue.So, we added the . It’s the original bootstrap check, inspired by opened on our , and on our . It simply checks at startup the current file descriptor limit and screams loudly if it’s below 65536. It can take a few minutes to track down exactly the right incantation to increase the limit on your operating system and this is super annoying but it sure beats losing data in production because you never knew the limit should be higher.These Warnings go to ElevenA core value of the Elasticsearch team is \"it should be easy to start exploring Elasticsearch\". In the past few years this with values like \"Elasticsearch should be secure\" and \"Elasticsearch should be stable\" but it is still important. In that vein, we still want Elasticsearch to start for folks that just download a distribution and start it without configuring anything. Every failing bootstrap check will simply print a .[2016-09-23T12:19:25,588][WARN ][o.e.b.BootstrapCheck ] [DOteo05] max file descriptors [4096] for elasticsearch process likely too low, increase to at least [65536]and Elasticsearch will start as though nothing was wrong. We hope these warnings makes a first time user think \"Elasticsearch cares enough about stability to warn me when something is poorly configured.\"The trouble is that people ign","locales":"","title":"Bootstrap checks: Annoying you now instead of devastating you later!"}
{"index":{}}
{"author":"Pius Fung","category":"Engineering","publish_date":"2016-10-03T00:00:00.000Z","url":"/blog/you_get_a_report_you_get_a_report","seo_title":"You get a report! You get a report!","content":" Everybody gets a report! ","locales":"","title":"You get a report! You get a report!"}
{"index":{}}
{"author":"Adrien Grand","category":"Engineering","publish_date":"2016-09-29T00:00:00.000Z","url":"/blog/elasticsearch-as-a-column-store","seo_title":"Elasticsearch as a column store","content":" If you have no idea what questions you will want to ask your data when you start ingesting it, columnar storage is probably a good option for you: it helps in two areas that are often close to the heart of users who deal with large amounts of data: You might have noticed the quotes around in the above paragraph. The reason is that however fast a linear scan is, it is still a linear scan and performs in linear time with the amount of queried data. Elasticsearch takes a different approach that consists in indexing all fields by default and only exposing queries that can leverage these indices so that identifying the matching documents does not need to visit all the data. Well, almost only, there is query that might run a linear scan in order to identify matching documents, the . The purpose of this query is to not require you to reindex for once-in-a-while questions that you had not thought you would need to ask your data when you indexed it, such as finding all documents whose value for field X is less than the value of field Y. But other than this one, all queries make use of some form of index in order to quickly identify matching documents. While Elasticsearch does not use a column-oriented view of the data for searching, it still needs one for workloads that work best with columnar data such as sorting and aggregations. In the next sections, we will do a quick survey of the history of columnar data in Lucene and Elasticsearch. First, there was fielddataLucene was originally designed as a search library, allowing users to get the most relevant documents for a particular query. But soon users wanted to do more: they wanted to be able to sort by arbitrary fields, aggregate information about all matching documents, etc. To cover these needs, Lucene added a feature called , which would \"uninvert\" the inverted index in order to build a column-oriented view of the data in memory. The initial release of Elasticsearch back in 2010 used the same mechanism with what it called , which was similar to , but with more flexible caching management. Then doc valuesThe growing use of was embarassing: it required an amount of memory that was linear with the amount of indexed data, and made reopening the index slower since entries had to be reloaded on all new segments, some of which being potentially very large due to merging. So on the one hand you had an efficient inverted index structure that allowed to find matching documents, but most collectors then had to rely on this inefficient memory-intensive data-structure in order to compute interesting things about the data. This is what lead Lucene to introduce in Lucene 4.0, which was released in the end of 2012. Just like , doc values provide a column-oriented view of the data, except that they are computed at index time and stored in the index. Their memory footprint is very low and getting doc values ready to use on a new segment is a matter of opening a file. The fact that doc values are computed at index time also gives more opportunities for compression. The longer it takes to uninvert fielddata, the longer users have to wait before changes to the index becomes visible, which is undesirable. On the other hand doc values are either computed asynchronously at merge or on small datasets at flush time, so it is fine to spend more time doing interesting compression. Here is a non exhaustive list of some compressions techniques that doc values use that fielddata doesn't: Elasticsearch integrationDoc values became available in Elasticsearch in version (February 2014) as an opt-in. However at that time, the performance did not quite match that of fielddata yet: Elasticsearch was hiding doc values behind its existing fielddata API, introducing overhead, and it took Lucene some time before introducing a and a that helped performance significantly. Both these concerns got fixed in , which was the first release to have matching performance of field","locales":"","title":"Elasticsearch as a column store"}
{"index":{}}
{"author":"Joe Fleming","category":"Releases","publish_date":"2016-09-28T00:00:00.000Z","url":"/blog/reporting-2.4.2-released","seo_title":"Reporting 2.4.2 Released","content":" Today we are releasing Reporting version 2.4.2, a patch release that brings one important change to the interface. From the beginning, users with access to Reporting have always been able to download and view reports that were created by any other user, but we didn’t make this clear in the way we presented completed reports in the interface. To avoid confusion about permissions and access, we now show and make available all reports from all users of Reporting. Changelog: ","locales":"","title":"Reporting 2.4.2 Released"}
{"index":{}}
{"author":"Christoph Wurm","category":"Engineering","publish_date":"2016-09-27T00:00:00.000Z","url":"/blog/new-way-to-ingest-part-1","seo_title":"A New Way To Ingest - Part 1","content":" With , it is time we took a closer look at how to use one of the new features, Ingest Nodes. ","locales":"","title":"A New Way To Ingest - Part 1"}
{"index":{}}
{"author":"Dimitrios Liappis","category":"Engineering","publish_date":"2016-09-23T00:00:00.000Z","url":"/blog/releasing-beta-version-of-elastic-docker-images","seo_title":"Introducing beta releases of Elasticsearch and Kibana Docker images","content":" Introducing Docker images for Elasticsearch and Kibana Beta releases of the Elasticsearch and Kibana Docker images are here! They track the latest versions of Elasticsearch and Kibana 5.0 and are pre-installed with our awesome ! The images are hosted on Elastic’s own Docker Registry. Instructions can be found on the and GitHub pages, but let’s see how easy it is to launch an Elasticsearch + Kibana stack with them. First, ensure that: Download an example definition to help us bring up Elasticsearch and Kibana, then issue: : the above command assumes that you don’t have Elasticsearch and Kibana listening on the . Feel free to adjust the ports in the . You will see logs from Kibana and Elasticsearch racing on your screen. To access Kibana, visit and you will be greeted by the Kibana 5.0 login page! Since is pre-installed, you can use the default credentials, (login: , password: ) but you are to change those via the Management menu in Kibana. Now let’s add some data. Using curl we will create an index called containing data for two users: Back to the Kibana UI, let’s configure an index pattern for the data. Enter for the index name and untick , as shown below: Click “create” and now you can view the inserted data in the Kibana Discover page: Terminating your containers is as simple as :  this will not destroy the Elasticsearch data volume so if you again your data will still be present. To terminate the containers the data volume use instead. The images are in beta, and we . However there are a number of best practices that we have compiled . We welcome issues and PRs. Stay tuned for the full-release version! Example docker-compose.yml: ","locales":"","title":"Introducing beta releases of Elasticsearch and Kibana Docker images!"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2016-09-22T00:00:00.000Z","url":"/blog/elasticsearch-5-0-0-beta1-released","seo_title":"Elasticsearch 5.0.0-beta1 released","content":" Today we are excited to announce the release of based on . This is the sixth in a series of pre-5.0.0 releases designed to let you test out your application with the features and changes coming in 5.0.0, and to give us feedback about any problems that you encounter.Open a bug report today and become an . This is a beta release and is intended for . Indices created in this version .Over 300 enhancements and bug fixes have been added since 5.0.0-alpha5 (all of which you can read about in the release notes linked above), but there are three changes in this release that deserve special mention below: huge improvements to indexing performance, switching fields to Lucene’s LatLonPoint, and making Painless the new default scripting language. ","locales":"","title":"Elasticsearch 5.0.0-beta1 released"}
{"index":{}}
{"author":"Marcelo Rodriguez","category":"Engineering","publish_date":"2016-10-04T00:00:00.000Z","url":"/blog/anatomy-of-a-watch","seo_title":"Anatomy of a Watch","content":" Elasticsearch has many use cases and often those use cases involve knowing when an event that is streaming into the system meets certain conditions and having additional data from the event at our disposal. For these scenarios, we developed Watcher. This component is very powerful and flexible alerting tool, however, because it is so configurable sometimes reviewing the documentation can be daunting and seem very complex. In this article, I’ll present the main concepts and describe the basics of Watcher to create alerts from a simple configuration for testing and a more real world implementation. By the end of these steps, you will not only have installed and configured Watcher with active alerts but you will understand the fundamentals necessary to tailor Watcher to your environment. For this demonstration, the following software versions were used: The installation instructions below assumes that your Elasticsearch nodes have access to the Internet. If they do not, you can manually download the .zip, tar.gz or .deb packages from the Elastic site. Navigate to your Elasticsearch installation directory and run the plugin tool bin/plugin install license Install the Watcher plugin: bin/plugin install watcher Repeat this procedure on every node in the cluster. For this article, We’ll be using a generic gmail account to send the alerts. Watcher is capable of sending the alerts to different targets such as Slack, HipChat, and PagerDuty. If you are interested in sending the alerts to different targets, you can find information in the Administering Watcher section of the documentation. To use a gmail account as in the examples to follow, you must disable the two factor authentication to allow Watcher to send alerts from the account.  To configure the account for Watcher, add the following in elasticsearch.yml substituting your account credentials. The settings are indent sensitive, use spaces not tabs: #------WATCHER CONFIG------ watcher.actions.email.service.account: work: profile: standard email_defaults: from: my.account@gmail.com smtp: auth: true starttls.enable: true host: smtp.gmail.com port: 587 user: my.account@gmail.com password: mypassword If you plan to use scripting in the watch, you’ll also need to enable scripting by adding the following to the file: #------SCRIPTING CONFIG ----- script.engine.groovy.inline.elasticsearch-watcher_watch: on You may already have enabled scripting for other uses with the following, if you have, then the above setting is not needed. #------SCRIPTING CONFIG ----- script.inline: on script.indexed: on A watch is basically an alert configuration. It is where we define the schedule, input, conditions, and actions that control what Watcher will do. The documentation examples may appear complex, however, they can be broken down into sections that ask just four basic questions from Watcher: We’ll go through how to answer these questions for Watcher and wrap up the configuration at the end so we can submit it to Elasticsearch. This configuration is answered by using the following syntax. In the example below, we want Watcher to check to every 5 minutes. This is done with the parameter by specifying a with an . \"trigger\" : { \"schedule\" : { \"interval\" : \"5m\" } } *There are several ways that you can specify the triggers such as by specifying certain times, days, etc. and also by specifying an interval as we have done in this example. This question is answered by using the parameter. There are several types of inputs we can use but for our purposes, we will use two types, the input, which we will use for basic testing, and the input to get data from an index in Elasticsearch. Since the inputs are loaded into the watch context, you can use the loaded information later as conditions, actions or transforms. In this example, we’re using the input which is just basically an inline document. We’ll use this to test our Watcher configuration and demonstrate how to pull a value, “r","locales":"","title":"Anatomy of a Watch"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2016-09-28T00:00:00.000Z","url":"/blog/elasticsearch-2-4-1-released","seo_title":"Elasticsearch 2.4.0 released","content":" Today we are pleased to announce the bugfix release of based on . This is the latest stable release and is already available for deployment on , our Elasticsearch-as-a-service platform. All users are advised to upgrade, especially Windows users and X-Pack users.Latest stable release:Full details of the changes in this release are available in the release notes listed above, but this release contains four important bug fixes which are worth mentioning: ","locales":"","title":"Elasticsearch 2.4.1 released"}
{"index":{}}
{"author":"João Duarte","category":"Engineering","publish_date":"2016-09-28T00:00:00.000Z","url":"/blog/do-you-grok-grok","seo_title":"Do you grok Grok?","content":" One the most common tasks when parsing log data is to decompose raw lines of text into a set of structured fields which other tools can manipulate. If you’re using the Elastic Stack, you can leverage Elasticsearch’s aggregations and Kibana’s visualizations to answer both business and operational questions from the information extracted in the logs, like ip addresses, timestamps, and domain specific data. For Logstash, this deconstruction job is carried by , a filter plugin that helps you describe the structure of your log formats. There are which abstract concepts such as , and . In order to match a line with the format: with the grok library, it’s only necessary to compose a handful of patterns to come up with: Which will create the structure: Easy right? Yes! Great! Are we done here? No! Because.. “I’m using grok and it’s super slow!!” That is a very common remark! Performance is a topic that is often brought up from the community as, often enough, users or customers will create a grok expression that will greatly reduce the number of events per second being processed by the logstash pipeline. As mentioned before, grok patterns are regular expressions, and therefore this plugin’s performance is severely impacted by the behaviour of the regular expression engine. In the following chapters, we’ll provide some guidelines on do’s and don’ts when creating grok expressions to match your log lines. Measure, measure, measure In order to validate decisions and experiments during grok expression design, we need a way to quickly measure performance between two or more expressions. For this, I created a small jruby script that uses the logstash-filter-grok plugin directly, bypassing the logstash pipeline. You can fetch . We’ll be using it to collect performance numbers to validate (or destroy!) our assumptions. Beware of the performance impact when grok fails to match Although it is very important to know how fast your grok pattern matches a log entry, it is also essential to understand what happens when it doesn’t. Successful matches can perform very differently than unsuccessful ones. When grok fails to match an event, it will add a tag to the event. By default, this tag is . Logstash allows you then to route those events somewhere where they can be counted and reviewed. For example, you can write all the failed matches to a file: If you find that there are multiple pattern match failures, you can benchmark those lines and find out their impact on the pipeline throughput. We’ll now use a grok expression that is meant to parse apache log lines and study its behaviour. First, we start with an example log entry: And use the following grok pattern to match it: Now, we’ll compare the matching speed of a successful match against three other log entries which don’t conform to the format, either at the start, the middle, or at the end of the line: These log lines were benchmarked using the script described at the start, and the result is presented below: We can see that, for this grok expression, depending on the location of the mismatch, . This helps explain user reports on grok maximizing CPU usage when lines don’t match, like . What can we do about it? Fail Faster, Set Anchors So now that we understand that match failures are dangerous to your pipeline’s performance, we need to fix them. In regular expression design, the best thing you can do to aid the regex engine is to reduce the amount of guessing it needs to do. This is why greedy patterns are generally avoided, but we’ll come back to that in a bit, as there’s a much simpler change that alters how your patterns are matched. Let’s come back to our lovely apache log line… …which is parsed by the grok pattern below: There’s a performance problem hiding in plain sight which exists due the natural expectations from the user of the grok plugin: the assumption that the grok expression we wrote will only match our log line from start","locales":"","title":"Do you grok Grok?"}
{"index":{}}
{"author":"Tyler Hannan","category":"Releases","publish_date":"2016-09-22T00:00:00.000Z","url":"/blog/elastic-stack-release-5-0-0-beta1","seo_title":"","content":" We’ve reached a milestone in the 5.0.0 release. Today, we are pleased to announce that the first public beta is available for download. At Elastic, a beta indicates that the products have reached feature freeze and the final effort is in squashing any unexpected bugs or regressions. It should look, and feel, very similar to the final GA release. Of course, it is software, so we reserve the right to change our mind and implement/change things that make the world better for you. Before you get too excited, keep in mind that this is still a beta, so don’t put it into production. If you open a bug report, today, you too can become an . And now, without further ado, some highlights from beta1. ElasticsearchFor more detailed information, and many other features, peruse the Elasticsearch . From 2.3.x to 5.0. The magical journey. The undiscovered country. The next frontier. Other coined phrases. As a reminder, we’ve also released the , which runs on your existing 2.3 cluster.  Use this site plugin to prep for your migration. KibanaTo quote the , “Kibana is where the party is”. Logstash Did we say we <3 logs? The Elastic Stack is great for log management and analysis, and thousands of users use Logstash and the rest of the stack to crunch application logs of all kinds. Logstash as a software is no exception — we emit internal logs which can be used by operators to troubleshoot issues. In this release, we've significantly improved the debugging capability of Logstash by revamping its internal logging framework. Oh, and new, shiny APIs. Check out the deets in this . BeatsWe collect data from the edge. We document all the updates are in a single Beats . ES-HadoopES-Hadoop v 5.0.0-beta1 has also been released today. Peruse all the information in the . Get it Now!Happy testing. Your feedback is instrumental in making 5.0 successful. And, don't forget that X-Pack is here and continually being updated with each pre-release. ","locales":"","title":"Elastic Stack Release - 5.0.0-beta1"}
{"index":{}}
{"author":"James Baiera","category":"Releases","publish_date":"2016-09-22T00:00:00.000Z","url":"/blog/es-hadoop-5-0-0-beta1","seo_title":"","content":" ​I am excited to announce the release of Elasticsearch for Apache Hadoop (aka ES-Hadoop) built against Elasticsearch 5.0.0-beta1. : This is an beta release and is intended for purposes only. Beta releases are normally more stable than Alpha releases, but crazy things can still happen when running this code. Indices created with this version . For the sake of your own sanity, we do not advise using this version in production. Think of the hamsters. What’s new? Spark 1.3-1.6 Streaming Support Spark is pretty fast, but sometimes you need your data even faster. We loved hearing that some of you were using ES-Hadoop with Spark Streaming, but we also felt the same heartache about the limitations that you were running into. We decided to do something about it. ES-Hadoop now natively supports consuming DStreams from Spark Streaming 1.3-1.6! We’ve included some fixes for the most commonly reported Spark Streaming issue of running out of connection resources during small processing windows. May your TIMED_WAIT’s be few, and your Spark Streaming Jobs live long and prosper. Ingest Node We heard about this cool new feature called the that was available in the alpha releases and coming out in Elasticsearch v5.0.0. We thought “Oh man, we ingest stuff, this node ingests stuff. We need to schedule a brunch with it immediately to trade gossip.” Starting in ES-Hadoop 5.0.0-beta1 you can now specify an ingest pipeline to send your data to, as well as target only ingest nodes to cut down on unnecessary traffic. We’re still waiting to hear back from you about brunch, Ingest Node. Call us! Fast Acting Bug Repellant Computers are hard. We thank our lucky stars every day that our friends in the community are so helpful when it comes to reporting issues. When you open up your copy of ES-Hadoop, you’ll find a fresh batch of bug fixes already applied. These bugs range from issues with overwriting data with SparkSQL, memory leaks in the network code, false warnings about version compatibility, and a bunch more. You can check out all those fixes (and their gritty details) . Cheers to the bug hunters! Feedback Anticipation is one of those cool things in life, but we understand that sometimes waiting is a real bummer. That’s one of the reasons why we work so hard to make these early access releases for you all. It’s like getting to open your birthday presents a few weeks in advance! You’re happy, we’re happy, and if anything ends up being broken, we have some time to get it fixed before the big special day. So, please, try this at home! You can ES-Hadoop 5.0.0-beta1, try it out, find out how it breaks, and let us know what you did on , , or in the . We are forever indebted to our early adopters, so much so that we created the ! Now that’s the sound of sweet, sweet gratitude! ","locales":"","title":"Elasticsearch for Apache Hadoop 5.0.0-beta1"}
{"index":{}}
{"author":"Court Ewing","category":"Releases","publish_date":"2016-09-22T00:00:00.000Z","url":"/blog/kibana-5-0-0-beta1","seo_title":"Kibana 5.0.0-beta1 released","content":" We are pleased to announce the immediate availability of Kibana 5.0.0-beta1. I’m not ashamed to admit it - the Kibana team really knows how to beta. Those Elasticsearch nerds might have a monopoly on consistency, and those squares working on Beats might get their stuff installed on thousands of servers before you’ve finished reading this post, but Kibana is where the party is. We’re shipping so many features in this release, it’s taken me three drafts of this blog post to try to whittle it down to highlights. And that doesn’t even include the dozens of bug fixes that are baked right in. . You won’t be disappointed. Hopefully we’re preaching to the choir at this point, but become an by opening a valid bug report and we’ll send you free stuff. : This is beta software that will only work with . Please test it, but do not use it in production. Upgrading 5.0.0-beta1 to any other version is not supported. Time series data with Timelion Timelion is a plugin no more. It’s now a fully supported, first-class citizen of Kibana core. Analyze your time series data with style, or at least with a custom DSL that let’s you do some really cool stuff. And don’t forget to embed those bad boys in your dashboards. Set scripting language for scripted fields You can now select which Elasticsearch scripting language you’d like to use in scripted fields in Kibana. It’s so easy, it’s painless. Sorry… that pun was bad. But seriously, you can and should use the new . Set position of visualization legends Because you deserve better than being hamstrung by right-aligned legends. Release the creative beast within and make your dashboards stand out with the new and improved, slightly-more-options-than-before legend positioning feature. Share exactly what you’re looking at, or share a link directly to the saved object We’ve revamped the Sharing UI so you can easily select between a URL to what you’re seeing on the screen right now versus a link to the underlying saved object that will remain up to date over time. No more sending people links to stale data, unless that’s the kind of thing you’re in to. X-Pack Reporting You’ve seen it for Kibana 4.6, and now you can get it in X-Pack 5.0. Go ahead and generate PDF reports of your visualizations and dashboards to your heart’s content. Saved workspaces in X-Pack Graph Building awesome graphs takes time, effort, and sometimes a little bit of luck. Don’t let that all go to waste - save your workspace and come back to it later. Stop URL length errors once and for all This one sits squarely in the gray area between feature and bug fix, but the underlying issue is pretty gnarly, so we might as well shout to the heavens about it. If you have non-trivial dashboards on Internet Explorer, then you probably have hit the URL length limit and have experienced the bottomless sadness that is Kibana falling apart before your eyes. Fix this once and for all by enabling the setting in Advanced Settings under Management. Note that when this setting is enabled, you must use the sharing UI when you want to link someone to something you’re looking at rather than grabbing the URL directly from your browser. It’s a trade off we want to get rid of, but for now we wanted to get this fix out to anyone that’s being plagued by URL length limits in their browser. The highlights of the non-highlights Like I said before, there’s just too darn much to highlight effectively in a single blog post. Here are some of the other features that don’t get the full section treatment: All the bugs We fixed dozens of bugs in 5.0.0-beta1. You read that right… dozens with a “d”. Though I guess that makes sense since you can’t spell “dozens” any other way. Wrap up Download the beta1 release and help us track down any last bugs. Issues can be filed on , feedback is appreciated on our , and feel free to reach out to us on or as well. ","locales":"","title":"Kibana 5.0.0-beta1 released"}
{"index":{}}
{"author":"Monica Sarbu","category":"Releases","publish_date":"2016-09-22T00:00:00.000Z","url":"/blog/beats-5-0-0-beta1-released","seo_title":"","content":" Beats 5.0.0-beta1 releasedWe are totally stoked to announce the 5.0.0-beta1 release, another major milestone on our road to Beats 5.0. IMPORTANT: This is a beta release and it is intended for testing purposes only. More modules in Metricbeat is a new addition to the Beats lineup for the 5.0 release. It’s a lightweight shipper that periodically interrogates external services, and sends their metrics to Elasticsearch. Metricbeat is replacing Topbeat in the 5.0 release, and it incorporates all the metrics provided by Topbeat plus many more.  In this release, PostgreSQL and HAProxy are added to the list of supported . The module exports statistics about each PostgreSQL database, the background writer process activity, and each PostgreSQL process. Thanks to a community contribution by, Metricbeat is able to collect statistics from , an open source TCP/HTTP load balancer. To enable gathering the statistics from HAProxy, you must enable the stats socket via TCP. For more details please check the . Get visibility into your containersMetricbeat is able to collect control group metrics from the Linux kernel. Control groups, which are commonly referred to as cgroups, is a mechanism for allocating resources - such as cpu time, memory, or block I/O time - to a process or set of processes. Cgroup metrics are especially useful for collecting detailed metrics from processes running inside containers because each container is assigned to a cgroup. The benefit of collecting container metrics directly from cgroups is that it works with any container tool (e.g. Docker, rkt, runC, LXC, systemd). Metricbeat reads directly from the cgroup pseudo filesystem provided by the Linux kernel, so it has no dependency on the APIs provided by the container tools. For more details about how to enable cgroups functionality in Metricbeat, please check the . Please note that cgroups is experimental in 5.0.0-beta1, and it’s disabled by default. Easily import Kibana dashboardsStarting with the 5.0.0-beta1 release, each Beat package comes with an app, called import_dashboards, which replaces the bash/powershell scripts to import the Kibana dashboards. The reason behind this change is to make it easier for us to maintain a single Golang application instead of two scripts, and for the user to easily import any Kibana dashboards by running a single command. To keep the Beat package light, starting with the 5.0.0-beta1 release, we are releasing the sample Kibana dashboards for all the Elastic Beats in a separate common package, instead of including the dashboards in each Beat package. The import_dashboards app accepts a URL to any local or online zip archive that contains the Kibana dashboards. If no URL is provided, by default the app imports the Elastic Beat dashboards of the same version as the Beat package. ./scripts/import_dashboards Share your own Beats dashboardsYou can now make use of any Kibana dashboards created by the community, by passing the URL to a zip archive that contains the dashboards, visualizations, and searches. ./scripts/import_dashboards -url https://github.com/monicasarbu/metricbeat-dashboards/archive/1.1.zip Do you have some awesome Kibana dashboards to share with the community? , upload the archive to your GitHub repository, and share it with the community by creating a topic in the . Improvements to the Kafka outputIn 5.0.0-beta1 the Kafka output received many more enhancements since first introducing Kafka support for 5.0.0-alpha releases. The Kafka Go client library by Shopify has been upgraded to version 1.10.0, adding support for Kafka 0.10. In addition to 0.10 support,  SASL/plain authentication support has been added, so you can configure the username and password used to connect to Kafka.  The beta1 release enables users to configure the topic selection and event partitioning strategy. You can choose between `random` event distribution, `round robin` event distribution, or `hash`-ing, where a hash-value is computed eith","locales":"","title":"Beats 5.0.0-beta1 released"}
{"index":{}}
{"author":"Tudor Golubenco","category":"Brewing in Beats","publish_date":"2016-09-21T00:00:00.000Z","url":"/blog/brewing-in-beats-dashboards-containerized-processes","seo_title":"","content":" Welcome to Brewing in Beats! With this series, we're keeping you up to date with all that's new in , from the details of work in progress pull requests to releases and learning resources. Filebeat: -once flag to exit after finishing current work When the flag is used, Filebeat starts all configured harvesters and prospectors and runs each prospector until the harvesters are closed. It is recommended to use the flag in combination with so harvester directly close when the end of the file is reached. This could be used for measuring the Filebeat performance or for loading a fixed set of files. Kibana dashboards for containerized processes Since recently, Metricbeat can report cgroup data for any running process (see this for details), and now we’ve also added for this data. Added /net/ and /ns/ support to procfs library Andrew to the procfs Golang library, which does proc file system parsing and which we plan to use in Metricbeat. Updated to Go 1.7.1 Our compiler images used in packaging, docker files used in tests, and our CI tools configurations were all (from 1.7). Filebeat: fix concurrent harvesters In case newly started harvesters did not persist their first state before the next scan started, it could have happened that multiple harvesters were started for the same file. This could have been caused by a large number of files or the output blocking. There are more details in the . Release 1.3.1 We’ve released the , which includes a  in Filebeat, discovered and fixed by , a contributor. ","locales":"","title":"Brewing in Beats: Dashboards for containerized processes"}
{"index":{}}
{"author":"Jack Conradson","category":"Engineering","publish_date":"2016-09-20T00:00:00.000Z","url":"/blog/painless-a-new-scripting-language","seo_title":"","content":" A little over a year ago, I received a phone call one evening about an opportunity to come build an embedded programming language into Elasticsearch.  Initially, I thought the idea was a crazy one.  Why build a brand new language when there are already so many to choose from?  But, as it turns out, it is very difficult to properly secure a language that can be executed remotely without that feature being thought of during the initial design phase.   So, with that in mind, I’m pleased to introduce Painless, a new scripting language in Elasticsearch 5.0, designed from the ground up to be both secure and performant. Painless is a dynamic scripting language with syntax similar to Groovy.  Painless scripts are composed of optionally some number of static methods and a required single main section of code.  Some highlights of Painless include the following: Painless can be used anywhere in Elasticsearch where scripts can normally be run by specifying the 'lang' parameter as 'painless'.  Painless will also become the default language in Elasticsearch 5.0.  . On a final note, I would like to thank both Robert Muir and Uwe Schindler for their significant contributions related the dynamic side of Painless, and also to Nik Everett for his significant contribution of regular expressions.  Uwe even had several methods related to array length added directly to Java to eek out the best dynamic performance possible. ","locales":"","title":"Painless: A New Scripting Language"}
{"index":{}}
{"author":"Suyog Rao","category":"Releases","publish_date":"2016-09-22T00:00:00.000Z","url":"/blog/logstash-5-0-0-beta1-released","seo_title":"","content":" We are pleased to announce that Logstash 5.0.0-beta1 has been released today. You can review the changes or jump directly to.Note: This is a beta release. Please do not use this in your production environment.Logstash's LogsDid we say we <3 logs? The Elastic Stack is great for log management and analysis, and thousands of users use Logstash and the rest of the stack to crunch application logs of all kinds.Logstash as a software is no exception — we emit internal logs that can be used by operators to troubleshoot issues. In this release, we've significantly improved the debugging capability of Logstash using its own logs. We've migrated to use the, and for our users, this means tons of new functionality:PUT /_node/logging { \"logger.logstash.outputs.elasticsearch\" : \"DEBUG\" }Monitoring EnhancementsWe've received good feedback about the new from our pre-releases. In this release, we've incorporated most of the feedback we received:Breaking Change: New Elasticsearch TemplateThe index template for 5.0 has been changed to reflect Elasticsearch's. Most importantly, the subfield for string multi-fields has changed from .raw to .keyword to match ES's default behavior. The impact of this change to various user groups are detailed below:FeedbackAs we approach the 5.0 release, we welcome and appreciate all your feedback. You can open issues on our, or start a conversation on our. Also, we would love for you to be a part of our! ","locales":"","title":"Logstash 5.0.0-beta1 released"}
{"index":{}}
{"author":"Tyler Hannan","category":"News","publish_date":"2016-09-21T00:00:00.000Z","url":"/blog/50-percent-more-disk-same-price-elastic-cloud-hosted-elasticsearch-gets-even-better","seo_title":"","content":" In our last post, titled , we shared that we have been able to extend a 15% discount. Basically… Phenomenal, cosmic cloudy powers.Itty, bitty annual price. In that post, we mentioned a few infrastructure updates that we were rolling out. They have now been deployed to the entire Elastic Cloud fleet and it is important to understand the impact. 50% More Storage, Same Price. The Elastic Cloud service operates on a fixed ratio of Memory to Storage. When we first launched, it was fixed at 1:8. We subsequently upgraded to 1:16 and, today, only a few months later, we are pleased to announce that we have increased the Memory to Storage ratio to 1:24. You now get 24GB of storage for every 1GB of RAM. Most importantly, we’ve done this without increasing the price. The flexibility of managing the fleet of deployed cloud clusters means we are able to monitor usage, cost, sizing and settle on a ratio that provides the highest value and widest applicability for all use-cases. Were you concerned, in the past, that your log data was too voluminous for using Elastic Cloud? Now you have 50% more storage. New Region But wait, there’s more…As discussed in the last post, our approach to adding regions to Elastic Cloud is fairly straightforward. Your feedback guides our roadmap. US West (Oregon) was a frequently requested region. For some very compelling reasons: A ratio of 1:24 and the decreased cost of a new region means with Elastic Cloud is cheaper than it has ever been. ","locales":"","title":"50% More Storage, Same Price. Elastic Cloud (Hosted Elasticsearch) Gets Even Better"}
{"index":{}}
{"author":"Michael McCandless","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2016-09-19T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2016-09-19","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Building an AR navigation system for visually impaired users with ElasticSearch — Erik Schlegel (@erikschlegel1) Elasticsearch Core Apache Lucene Watch This Space, where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2016-09-19"}
{"index":{}}
{"author":"Colin Goodheart-Smithe","category":"Engineering","publish_date":"2016-09-19T00:00:00.000Z","url":"/blog/instant-aggregations-rewriting-queries-for-fun-and-profit","seo_title":"Instant Aggregations: Rewriting Queries for fun and profit","content":" In 1.4.0 Elasticsearch gained a shard level ‘Request Cache’ which caches the result of the query phase on each shard keyed on the search request itself. Until 5.0 this feature was disabled by default since it wasn’t as useful as it could be for most of the computationally heavy use cases. The cache was intended to make searches faster, especially for aggregations. One problem was that ordering in JSON is not deterministic so although two requests may be logically the same, when rendered to JSON strings they may not be equal. The other main problem is that these computationally heavy use cases tend to use time windows relative to the current time so subsequent requests will tend to have slightly different time ranges. Enabling it would likely be a waste memory for most of the users  since they would rarely ever get a cache hit. So why would we add such a feature? Here at Elastics, we try to follow the rule of “progress over perfection”. Even if we can’t utilize it’s full potential we get a step closer to our goals. Now after 2 years of engineering effort and representation, we can finally take full advantage of it. From 5.0 the request cache will be enabled by default for all requests with `size:0`. The request cache is most useful for analytics use cases and generally analytics use cases use search requests with `size: 0`. So how did the changes made since 1.4.0 enable us to better use this feature? The search request cache gives massive improvements in search performance when running the same search multiple times on a static index. However, search requests are rarely exactly the same, especially in the time series use case. Lets look at some of the characteristics of typical search for time-series use cases: The above are common characteristics of search requests for a lot of time series use cases including common patterns of usage for Kibana. A lot of Kibana dashboards contain quite a few visualisations a lot of which are complex and can take a number of seconds for the dashboard request to be executed by Elasticsearch. Because search requests are rarely the same, the search request cache can’t be used effectively to improve search performance for these use cases. Or can it? How can we make the search request cache work when the time range is always moving? Let’s solve this by considering an example. Imagine we have the details for every residential house sale in the UK since 1995 (luckily this information is conveniently ). We can index this into 21 yearly indices (e.g. house-prices-1995, house-prices-1996, …, house-prices-2016). For the purposes of this explanation let’s make each index only have 1 shard (though the same idea can easily be extended to multiple shard indices without modification) Now we can run queries against the indices by using requests like this: ``` GET house-prices-*/_search { \"query\": { \"range\": { \"date\": { \"gte\": \"2013-09-01\", \"lt\": \"2016-03-01\" } } } } ``` The figure below shows three of these such queries overlaid on the yearly indices. These queries are typical for dashboard style use cases in that they differ only by small amounts (imagine a refreshing dashboard) A couple of things stand out from this diagram: So the only indices which affect the matching documents between the three queries are house-prices-2013 and house-prices-2016. If we could rewrite the queries on each shard based on the range of values present in that index we could make the queries able to actually utilize the request cache. Below is the diagram with the same three queries rewritten to make them more cachable in the search request cache: You can see that in the diagram above, the first query is rewritten and run as a `match_none` query on the `house-prices-2012` indexes shard and a `match_all` query on the `house-prices-2014` and `house-prices-2015` indices ","locales":"","title":"Instant Aggregations: Rewriting Queries for Fun and Profit"}
{"index":{}}
{"author":"Shay Banon","category":"News","publish_date":"2016-09-15T00:00:00.000Z","url":"/blog/welcome-prelert-to-the-elastic-team","seo_title":"Welcome Prelert to the Elastic Team","content":" I am happy to announce that Prelert and Elastic are joining forces. Ever since we started Elastic, our goal has been to allow users to easily find relevant data or insights within large amounts of data. Search is a wonderful way to do it, and the ability to slice, dice, and aggregate the data in an unconstrained way allowed users to feel they are in control of the data, compared to the other way around. But we can take it a step forward, and with Prelert, we just did. Prelert has developed an unsupervised machine learning engine that can plow through large amounts of data and automatically find those insights our users today have been proactively finding using search. We view the Prelert technology as a generic engine that can apply to many different use cases, which maps very nicely with what we are trying to do with the Elastic Stack. It has been proven to be extremely successful within specific use cases. Finding anomalies within transactions / operational metrics, detecting uncharacteristic user behavior, finding a population of attacking IP addresses, and much more. Let me stop here and let Steve Dodson, Prelert CTO and Founder, share his thoughts: At Prelert we work on developing machine learning technologies that allow users to understand the behavior of their data. Joining Elastic gives us a fantastic opportunity to bring this technology to a large range of users and data, and will allow us to execute on the vision we set out for Prelert over 7 years ago. The journey to this point initially started when I was offered the opportunity to help diagnose an application issue on a large trading platform in an investment bank. Sporadically, traders were becoming disconnected from the trading platform and the goal was to proactively preempt this issue and diagnose the root cause. The problem was that the data was overwhelming, with 100s of log files from 100s of servers, and 10,000s of performance metrics from the applications and systems. Simply searching through the logs or metrics didn’t yield sufficient insight into the problem, and it started to become clear that hidden in this data was the behavior of the system, and if we could model the normal behavior of the system we could isolate the unusual activity. Based on this premise we built out a prototype that statistically modelled the behaviour of the system via the logs and metrics allowing the customer to identify, diagnose and resolve the issue. (First Prelert UI showing anomalies and root cause!) The great thing was that at the same time we were developing this technology, users with large IT systems were moving from legacy monitoring solutions, to solutions that could collect and centralize storage of all their logs, performance metrics and data. Elasticsearch is a great example of this, and showed how this new generation of search technologies could be used by customers to give significant insight into their data. Also, the questions users were asking from this data were becoming broader, and instead of relying on silo’ed tools, users were using these platforms to answer IT Operations, Security, Business and other questions. Layering Prelert on top of these technologies was the natural next step, as users were asking questions that required statistical and machine learning analysis of the data. Another really exciting discovery was that customers then started using Prelert to answer questions such as “which bus routes are congested?”, “is the temperature across my environment normal?”, “has there been a change in the number of retail transactions?” along with traditional IT Operations questions. This showed that our technology was not only robust to diverse use cases, but had the opportunity to be broadly applicable across diverse time series data. (Current Prelert UI in Kibana) However, a challenge we kept running into was that customers wanted our analytics more and more tightly integrated in","locales":"","title":"Welcome Prelert to the Elastic Team"}
{"index":{}}
{"author":"Luca Cavanna","category":"Engineering","publish_date":"2016-09-14T00:00:00.000Z","url":"/blog/the-great-query-refactoring-thou-shalt-only-parse-once","seo_title":"The Great Query Refactoring: Thou shalt only parse once","content":" . When writing software, adding cool new features is of course always great fun. But sometimes it’s also important to work on internal changes in the code base that enable those shiny new additions in the future. For example, there were for Elasticsearch floating around that were essentially blocked by the lack of having a good intermediate representation for search requests arriving through the REST layer, which prevented early query optimizations and delayed parsing to the shard level. For the upcoming release of Elasticsearch 5.0 we embarked on a large refactoring to change the way search requests work internally. In this blog post we want to highlight some of the changes and challenges that came along with the refactoring of queries and the search request, how it helped us improve our testing and how it enables great new features like \"Instant Aggregations\", that will be highlighted in a following blog post.How search requests were sent across the cluster prior to the RefactoringWhen you send a search request to a node in the cluster, the node receiving the request coordinates the search request from then on. The coordinating node identifies what shards the search request needs to be executed on, and forwards it to the nodes that hold those shards via the internal node-to-node . Each shard returns a set of documents, whose ids will be sent back to the coordinating node that is responsible for reducing the matching documents obtained to the top matching ones that need to be returned to the client. Once those documents are identified, they are fetched as part of the fetch phase and finally returned.Before Elasticsearch 5.0, each node received the original search request, parsed the query and used other information available from the shard (like mappings) to actually create the Lucene query that was then executed as part of the query phase. The body of the search request is not parsed on the coordinating node but rather serialized via the transport layer untouched, as an opaque byte array which holds nothing more than its json representation. This was historically done to avoid having to write serialization code for every single query and every single section of a search request, as elasticsearch uses its own binary serialization protocol. We eventually came to the conclusion that this was the wrong trade-off. So what's the problem with this?While simply forwarding incoming requests on the coordinating node as described above is simple and fast (at least on the coordinating node), there are some drawbacks: All these drawbacks arise because before 5.0 there is no intermediate representation of a query within elasticsearch, only the raw JSON request and the final Lucene query. The latter can only be created on the shards, because Lucene queries are not serializable and often depend on context information only available on the shard. By the way, this is actually true not only for the \"query\" part of the search request, but for many other things like aggregations, suggestions etc.. that can be part of a search request. So, wouldn't it be nice to be able to parse queries to an intermediate representation once and early? That way we would be able to eventually rewrite them, and optimize their execution, for example by shortcutting them to queries that are less expensive.What we didFor the queries we achieved this by splitting the query parsing into two phases: As a result of the split, the code is better organized as parsing is decoupled from lucene query generation. Having a serializable intermediate format for queries meant that we had to write serialization code for all of the queries and search sections supported in elasticsearch. Also, every query implements `equals()` and `hashCode()`, so that they can be compared with each other for easier caching and to aid their testing.Once we moved parsing to the ","locales":"","title":"The Great Query Refactoring: Thou shalt only parse once"}
{"index":{}}
{"author":"Tudor Golubenco","category":"Brewing in Beats","publish_date":"2016-09-14T00:00:00.000Z","url":"/blog/brewing-in-beats-haproxy-module-metricbeat","seo_title":"","content":" Welcome to Brewing in Beats! With this series, we're keeping you up to date with all that's new in , from the details of work in progress pull requests to releases and learning resources. Metricbeat: new HAProxy module Thanks to a community contribution by , Metricbeat got an 🎉 Filebeat: The return of the Symlinks Filebeat used to somehow unintentionally follow symlinks when opening files. So we deprecated them in 1.3 and were planning to remove the support for symlinks in 5.0. However, we’ve got some pretty strong pushback from Kubernetes users (K8s uses symlinks to), so we’ve listened and . However, the symlink support is off by default and marked as experimental at this point. Filebeat: close_removed and clean_removed are now on by default This will keep the registry file from growing to quickly. For background: in 1.x the registry file was a map using the filename as a key. In 5.x the registry file is a list. A side effect of this change is that the registry file grows even if the same file names are reused (think of the usual syslog file names). By making , we’re making sure the registry file stays small. Metricbeat: Improvements to the MySQL module A community contribution by adds several to the MySQL module in Metricbeat. Store the version in the Elasticsearch templates We now of the Elasticsearch templates we provide. This can help with troubleshooting and perhaps we could use it in the future to provide automatic mappings upgrades. import_dashboards: automatically select the right Beat Now that the import_dashboard scripts were rewritten in Go and download the beats-dashboards package when executed, we wanted to make sure that it does the “right thing” when called without any arguments. So we now the at build time to have a different `-beat` argument depending on the package. Packetbeat: Improve handling of HTTP messages larger than 10MB Old implementation dropped messages larger than 10MB (hard coded), to protect against memory DoS. However, in some cases the transaction was still recorded, but the parsing of the headers was incomplete. makes the support for large messages explicit, by adding a mode to the parser that \"sees\" the segments without storing them. ","locales":"","title":"Brewing in Beats: HAProxy module in Metricbeat"}
{"index":{}}
{"author":"Andrew Kroh","category":"Engineering","publish_date":"2016-09-14T00:00:00.000Z","url":"/blog/monitoring_container_resource_usage_with_metricbeat","seo_title":"","content":" - Monitoring Container Resource Usage with Metricbeat === is a new addition to the Beats lineup for the 5.0 release. It is a lightweight shipper for host and service metrics. Metricbeat is replacing Topbeat in the 5.0 release, and it incorporates all of the metrics provided by Topbeat plus many more. One of the capabilities of Metricbeat is the ability to collect control group metrics from the Linux kernel. Control groups, which are more commonly referred to as cgroups, are for allocating resources (i.e. cpu, memory) to a process or set of processes and metering resource usage. This feature is brand new and will be released in Metricbeat 5.0.0-beta1. The feature itself is still evolving and for that reason it is marked as experimental. As we receive feedback there may be enhancements to the metrics it reports. How do I enable cgroup metrics in Metricbeat? To enable the cgroup metrics you must add as part of the system module definition within your Metricbeat configuration file. metricbeat.modules: - module: system metricsets: [process] cgroups: true If you are planning to deploy Metricbeat in a container there are some additional configuration items to be aware of. The full details are in the Metricbeat documentation, see the section titled . But in short, you need to mount both the host machine’s proc filesystem and the cgroup filesystem inside of the container. Here’s an example using Docker (you will need to provide your own container image). sudo docker run \\ --volume=/proc:/hostfs/proc:ro \\ --volume=/sys/fs/cgroup:/hostfs/sys/fs/cgroup:ro \\ my/metricbeat:latest -system.hostfs=/hostfs How are cgroups related to container monitoring? Each container is assigned to a cgroup which allows for limiting and metering resource usage of the process running in the container. We can use this fact to collect detailed metrics from processes running inside containers. The benefit of collecting container metrics directly from cgroups is that it works with any container tool (e.g. Docker, rkt, runC, LXC, systemd). Metricbeat reads directly from the cgroup pseudo filesystem provided by the Linux kernel, so it has no dependency on the APIs provided by container tools (which are subject to change between releases). As with any design decision, there is a tradeoff for not using the container tool APIs. Metadata that is only known to the container tool (e.g. names and labels) cannot be read by Metricbeat. We will be addressing this limitation in the future. What kind of metrics can cgroups provide? Control groups provide a wealth of information about resource usage. Metricbeat reports both the limits assigned to the cgroup (if any) and the statistics captured by the cgroup. You can view a here. These are the areas that Metricbeat focuses on: For a detailed look at the metrics reported by Metricbeat, have a look at the . Metricbeat sends all of this data as part of the system process metricset. This means that Metricbeat is providing a process centric view versus a container centric view. Metricbeat examines each process and includes the cgroup metrics if the process is a member of a (non-root) cgroup. If you are running one process per container then a process centric view of the data is equivalent to a container centric view. But if you run multiple processes per container then there will be some duplication of the cgroup metrics among the processes running in the same container. Generally the container ID is used as the name of the cgroup. Metricbeat stores the cgroup name in the field. This value can be used to associate the data to a specific container. If you have any Metricbeat related questions or feature requests, please connect with the Beats engineering team on . ","locales":"","title":"Monitoring Container Resource Usage with Metricbeat"}
{"index":{}}
{"author":"Simon Willnauer","category":"Engineering","publish_date":"2016-09-13T00:00:00.000Z","url":"/blog/the-tale-of-caching-and-why-it-matters","seo_title":"The tale of caching and why it matters…","content":" It’s early 2013 and an unusually sunny day in Amsterdam and a group of people are meeting around table soccer and ping pong tables for what we call a company all-hands. Just recently Rashid Khan, one of the big characters behind Kibana, joined Elastic and we are still just a handful of engineers. I’m hacking around trying to get checksums to work for recovery, listening to a conversation between Shay and Rashid. It’s Kibana’s initial dashboard slowness that causes this intense conversation. Even though Kibana fires up almost identical searches each time you open the home page, elasticsearch has to recompute everything from scratch. Someone might ask, no caching eh? True! A closer look under the hood shows that searches are almost the same, but are subject to this annoying property of time: it never stands still. If you have used Kibana yourself you might have realized that a default filter is always based on the current time (NOW) going backwards for a defined time range. In other words you never fire the same query more than once a millisecond. Then the discussion got serious: Rashid and Shay started talking about caching and adding REST level primitives to control the cache key. Time to stop working on checksums:  I gotta get involved! If you try to solve one of the hardest problems in computer science and the discussion is heading towards allowing the user to control it, you are either a really brave engineer or all other options would require you to be a hell of a brave engineer! The discussion continued for a while and ideas basically went through the roof.  You might have experienced this in your day to day job before. Luckily, we had so many other problems to solve at that time that we just dropped the ball on it for a while. Fast forward: it’s October 1st and my calendar says “The Dudes are in Berlin” meaning that Shay and a bunch of other team leads were coming into town for some planning sessions. That’s usually an intensive time in a distributed company like Elastic since we don’t meet in person more than twice per year. After 3 days of discussions, brainstorming and arguing Shay and I went out for Schnitzel to this awesome Austrian place near my house. Honestly, neither Shay nor I were really up for any more discussions but suddenly the caching thing came up again. I don’t blame anybody:  I’m not sure who opened that particular can of worms. Anyway, this time we came up with a plan! Admittedly, not low hanging fruit, but something that could actually work well, is fully transparent, easy to test and can be disabled if it’s not working. You noticed that escape hatch, did you? Caching is hard but let me explain what we had in mind. Bear with me, I’m going to take a big swing: Elasticsearch is based on Apache Lucene™ which works based on point-in-time view of an index. Such a snapshot is basically a set of write once index segments, each holding a subset of the indexed documents. The software construct we use to represent such a snapshot is what we call a “top-level” indexreader. We know that, unless the top-level reader changes, queries are idempotent or in other words, cacheable. In Elasticsearch there is exactly one Lucene index per shard, so we can simplify things to use one top-level reader per shard. Now, if we can identify the outer bounds of an index for any date field we could also make much better decisions if for instance all or even no documents at all would match a certain filter and therefore could rewrite the query to  or match-no-docs respectively. If we could manage to do that then we could put queries that appear to be un-cachable into the we added basically just before that Schnitzel brainstorming session. The request cache utilizes Lucene’s top-level reader as well as the search request’s binary representation (the plain unmodified JSON, YAML or CBOR bytes) as a combined cache ke","locales":"","title":"The tale of caching and why it matters…"}
{"index":{}}
{"author":"Tyler Hannan","category":"News","publish_date":"2016-09-12T00:00:00.000Z","url":"/blog/announcing-annual-discounts-for-elastic-cloud-standard","seo_title":"","content":" The current forecast?Cloudy with a 15% annual discount. If you haven’t perused our , you may be unaware that we sell hosted Elasticsearch in 3 ways. These subscription levels are Standard, Gold, and Platinum. They are distinguished by features and different SLAs for support. For the purpose of this post, I want to focus exclusively on Cloud Standard, our most affordable feature-packed offering. Starting Simple, Starting Small, Full AccessAs the creators of Elasticsearch, our support team is backed by the engineers who wrote the code. And, for critical use cases, this is an incredibly important requirement. However, we know that the business of hosted products differs slightly. The ability to start small and expand on demand is an assumption for any “hosted” or “SaaS” or “other cloudy term” offering. With that in mind, we believe Cloud Standard is a fantastic starting point for those looking for a hosted Elasticsearch solution.Does your cluster require less than 64 GB of RAM in total? All Platinum X-Pack Products & Features (including Field & Document-level security and Graph) are available for no additional cost. And, starting last week, annual subscriptions of Cloud Standard enjoy a 15% discount on the total cost. The math is simple. Design the cluster you desire in the , then apply the discount and calculate the annualized cost. You can get the details, and sign the contract, by . But why a discount for annual customers? Put simply, we believe that Elastic Cloud is the best hosted Elasticsearch service around. And, for those of you who desire an ongoing relationship, we want to recognize your commitment with a discount. Don’t dismay. The hourly pricing isn’t going away. But it doesn’t stop there. Infrastructure Matters. Here Are Some Updates to Ours. Is there a manager or business user in your life who just wants to see a report -- either periodically or at a scheduled interval -- in their email? Reporting matters, so much so that we wrote an entire about it. What we didn’t share in that post is that the reporting release required rethinking Kibana instance sizing (Yeah, Elastic Cloud comes with Kibana) and ensuring that the asynchronous report process would not negatively impact the experience of usage of Elasticsearch and visualization and discovery in Kibana. Report away in Cloud. You want ‘em?  You got ‘em. Our approach to expanding available regions has been simple. Let you, the user, tell us what is the priority. There has been enough feedback on a specific region, that we will be adding the ability to launch Elastic Cloud clusters in soon. Getting the latest release, on the day of release, simply isn’t enough. We’re starting to expose milestone releases on Cloud as well, so you can kick the tires of the cutting edge. This requires a substantial amount of tooling and work with our build process and will be first realized with the release of 5.0 RC1 for public consumption on Elastic Cloud. (Still not familiar with 5.0 or the ? You should be.)  The goal is to be  able to expose nightlies. (Do you test nightlies or use updated, non-release software in your environment? Soon you can in Cloud.) Seeing is Believing. Coding is Understanding.We have long said that the best way to understand the Elastic Stack is to use it. The same is true for Elastic Cloud.  It is simple, really simple.It is powerful, absurdly powerful.It is -- now -- cheaper, 15% cheaper. ","locales":"","title":"Announcing Annual Discounts for Elastic Cloud Standard"}
{"index":{}}
{"author":"Suyog Rao","category":"The Logstash Lines","publish_date":"2016-09-12T00:00:00.000Z","url":"/blog/logstash-lines-2016-09-12","seo_title":"","content":" Welcome back to The Logstash Lines! In these weekly posts, we'll share the latest happenings in the world of Logstash and its ecosystem. PUT _node/settings {\"logger.logstash.output.elasticsearch\" : \"error\"} GET _node/logging?pretty { .... \"loggers\" : { \"logstash.registry\" : \"WARN\", \"logstash.instrument.periodicpoller.os\" : \"WARN\", \"logstash.instrument.collector\" : \"WARN\", \"logstash.runner\" : \"WARN\", \"logstash.inputs.stdin\" : \"WARN\", \"logstash.outputs.stdout\" : \"WARN\" ... } } This project is nicely. Last week we got all the core unit tests to pass with the in-memory queue infrastructure. Integration tests are yet to be added which will use the real file-backed queue. Plan is to merge this feature branch to master in 2 weeks, following a code review.PH is working on adding tons of improvements to this output to make it easier to maintain/test. We're also adding popular such as — using event based data to determine the target S3 bucket/location. Like Also, updating to AWS ruby client v2 should fix many bugs and knock off some enhancements like multi-threaded uploader. Suffice to say, this plugin needed some love. ","locales":"","title":"Logstash Lines: Monitoring API, Logging Enhancements, S3 Output changes"}
{"index":{}}
{"author":"Court Ewing","category":"Releases","publish_date":"2016-09-06T00:00:00.000Z","url":"/blog/kibana-4-6-1","seo_title":"Kibana 4.6.1 and Reporting 2.4.1 released","content":" Today we’re releasing Kibana version 4.6.1, which includes a fix for a regression that we introduced in last week’s release, and Reporting version 2.4.1, which includes a high severity security fix. We recommend that users upgrade as soon as possible. Users of Elastic Cloud will get these updates automatically. Upgrading To upgrade Kibana, follow the instructions in the . If you had previously installed Kibana 4.6.0 with apt or yum, you should be able to upgrade Kibana through your package manager instead. To upgrade Reporting, uninstall the current version and reinstall version 2.4.1: The Changes The that was fixed in 4.6.1 would cause a fatal error whenever an aggregation would order by Term. Reporting 2.4.1 includes a fix for a CSRF vulnerability () that could allow an attacker to generate superfluous reports whenever an authenticated Kibana user navigates to a specially-crafted page. Conclusion You can grab Kibana from the page. If you have any questions, please don’t hesitate to reach out to us on our , , or . ","locales":"","title":"Kibana 4.6.1 and Reporting 2.4.1 released"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2016-08-31T00:00:00.000Z","url":"/blog/elasticsearch-2-4-0-released","seo_title":"Elasticsearch 2.4.0 released","content":" Today we are pleased to announce the release of based on . This is the latest stable release and is already available for deployment on , our Elasticsearch-as-a-service platform. Latest stable release: Full details of the changes in this release are available in the release notes listed above, but this release contains two important changes which are worth mentioning: ","locales":"","title":"Elasticsearch 2.4.0 released"}
{"index":{}}
{"author":"Erwann Cloarec","category":"User Stories","publish_date":"2016-09-07T00:00:00.000Z","url":"/blog/dockbeat-a-new-addition-to-the-beats-community","seo_title":"Dockerbeat: A new addition to the Beats Community","content":" : The previously published article has now been modified to reflect the name change from DockerBeat to Dockbeat.Dockbeat is an open source project created by , based on the Beats platform. Ingensi is a business unit of located in France. Our main goal is to create big data solutions offering powerful decision-making tools to unveil the value in the customer data. For that, we employ our technological expertise in Hadoop, Elasticsearch, and Docker. At Ingensi, we use the Elastic Stack as the basis of our log management solution. We also use Elasticsearch as a search engine in our application as we need fast responding searches. Docker Big Data platform: Ingensi meets DockerFor resource optimization reasons, when we receive a cluster deployment request from our customers, we deploy it in a shared big data infrastructure based on . In 2015, we had our first shared big data infrastructure deployed and we naturally used the Elastic Stack to monitor the entire platform: hosts, network, users, etc. However, our processing workflow did not allow us to monitor Docker containers. That’s when comes into play! The Beats platform allows you to build lightweight data shippers for different types of data that you can search and analyze in Elasticsearch, and visualize in Kibana. From there, we had the idea to create our own beat to collect Docker statistics. Hello, Dockbeat!Since the release of Docker 1.5., a new functionality was added to Docker's API: Docker stats. This endpoint returns a live stream of a running container's resource usage statistics, such as CPU, memory, network, disk IO, etc. Therefore, we've decided to collect all these metrics in order to capitalize on the use of our existing Docker infrastructure. Based on the Beats platform, we started to develop our own Beat: Dockbeat. How does Dockbeat work?Dockbeat is used for Docker monitoring. It is responsible for collecting the containers' statistics and send them to Logstash or directly to Elasticsearch. As it is very lightweight, Dockbeat’s impct on the server is very low. The collected data can then be visualized in a Kibana dashboard. Today, we are able to view in real time all the containers running on our servers, their consumption, and thus, quickly identify any anomalies. This new beat exports five types of documents corresponding to the different metric sets collected : Great! How can I use Dockbeat?Simple: Dockbeat is hosted on . You first need to clone and build the project (see ) or simply  you need. In a basic Docker installation, Dockbeat should work out of the box with its default configuration. If you have your own settings, check the configuration file dockbeat.yml. In this file you can configure:  There is no specific configuration to be edited, unless you need to enable the TLS docker daemon. Finally, you can launch your beat by running the following command : ./dockbeat -c etc/dockbeat.yml Dockbeat todayDockbeat grew up and became operational. Today, we are happy to announce the It's an open source project and we love to receive contributions from the community. You can contribute to Dockbeat in many ways:  So, feel free to contact us for further information or contributions, we will be happy to hear from you. The Team is a Big Data Engineer , Docker passionate, Elastic fan and Hadoop professional, Dockbeat co-creator. is a Software Engineer . Interested in Big Data and OpenSource technologies, Docker & Elastic enthusiast, Dockbeat co-creator. is an Engineering Student in final year. Curious and ambitious, she loves to discover new intelligent technologies. Elastic fan. She's the blog post author and a Dockbeat contributor. ","locales":"fr-fr","title":"Dockbeat: A new addition to the Beats Community"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2016-09-05T00:00:00.000Z","url":"/blog/brewing-in-beats-new-community-beat-for-rets","seo_title":"","content":" New community Beat: Retsbeat We’re the Packetbeat functionality that maps IP address to host names using Elasticsearch or Redis as a backend. While this can be useful functionality, the current implementation is limited (only works with the Elasticsearch and Redis outputs) and it's done in the wrong place (outputs). We do hope to have a better implementation before or soon after removing the current one. ","locales":"","title":"Brewing in Beats: New community Beat for RETS"}
{"index":{}}
{"author":"Suyog Rao","category":"Releases","publish_date":"2016-08-31T00:00:00.000Z","url":"/blog/logstash-2-4-0-released","seo_title":"","content":" We are pleased to announce that Logstash 2.4 has been released. Please see the for a detailed change report and you can binaries on our products page. New Plugin API Compatibility While 2.4 does not have many headlining features, it's an important release that brings in key internal API changes. These changes make 2.4 compatible with plugins developed for. In other words, Logstash 2.4 will be able to install most plugins that use the 5.0 API. As maintainers, this API backport eases efforts which mean we don't have to maintain 2 separate code branches for each plugin. We have a of plugins. The Logstash 2.4 release enables new plugin features that are not available in the remaining 2.x series. We've recently updated a bunch of plugins to add new functionality. A popular enhancement request has been to bring in new features in Kafka's 0.9.x and 0.10.x releases like . In 2.4, we don’t package Kafka input/output plugin versions that support newer Kafka versions (see the ). This is because most config options for these plugins have been changed which makes it not compatible, and we don't intend to break configurations in point releases. If you still want to get all the goodness of the latest Kafka versions in 2.4, fear not. You can simply install the plugin on top of 2.4, like so: bin/logstash-plugin install —version 5.0.4 logstash-input-kafka This is just an example. Like Kafka, there are many plugins which have been updated. They'll be packaged in our upcoming 5.0 release, but if you can't wait, you can always install them on top of 2.4. Please review the changelog for individual plugins before upgrading to them. has been reimplemented using Netty, an asynchronous IO framework for Java. This rewrite for performance brings it in line with Logstash Forwarder + LS combination. In some test cases, it is faster. Go on, migrate all your LSF instances to the Filebeat goodness. I've heard Filebeat 5.0 is packed with features, so you don't want to miss out on that. As part of the Beats refactor we now only supports private keys in the format, you can use to convert them, newer version of OpenSSL will already create the key in the right format. Oh and a note for our subscription customers — LSF will reach EOL on Nov 7, 2016.  Back in the day, Logstash used to emit its logs in JSON format. Because of endless encoding issues and crashes in the logger, we had to roll it back. We've worked through most of these issues, so we're bringing structured logging back. You can tell LS to switch to JSON logging by using the CLI flag. Be aware that logging in JSON is verbose and may fill up disk even faster than before! Kibana. Beats. And now Logstash has it too! We're talking about the plugin generator tool that makes it easier to bootstrap new plugins for Logstash. Previously we've recommended developers to clone/fork the, but now you can simply do: bin/logstash-plugin generate --type input --name xkcd --path ~/ws/elastic/plugins This subcommand bootstraps a new plugin logstash-input-xkcd with the right directory structure and all the required files (templates) for you to start developing this plugin right away. So, go on, create that input to stream those fine comic strips to Kibana! We are super excited for this release of Logstash and look forward to your feedback. You can reach us at our, open issues in your or twitter(). Happy 'stashing! ","locales":"","title":"Logstash 2.4.0 released"}
{"index":{}}
{"author":"Joe Fleming","category":"Releases","publish_date":"2016-08-31T00:00:00.000Z","url":"/blog/reporting-2-4-0-released","seo_title":"Reporting 2.4.0 Released","content":" Reporting is a new product in our commercial lineup that allows you to easily generate PDFs of your Kibana searches, visualizations, and dashboards. It’s great for getting a snapshot of your data and sharing it with anyone. If you attended or watched recordings from Elastic{ON} 2016, or if you’ve been following our latest alpha releases, you know that we’re shipping Reporting in 5.0. But we just couldn’t wait to get it in your hands, so we decided to make it available even sooner, as a plugin for Kibana 4.6. Reporting has been a long time coming, and we’re excited to make it available today, ahead of schedule. Using Reporting The Reporting plugin adds a new interface right inside Kibana that allows you to create a report based on what you have open. You can also use it with Watcher to trigger reports in response to an event, or simply have reports emailed on a set schedule - it’s up to you. In fact, reports can be automated using any tool that can make an HTTP request. Reporting also includes a page under in Kibana that allows you to download (or re-download) generated reports. Scaling Reporting Reporting works in the background, in an asynchronous manner, using Elasticsearch as the source of truth. Kibana instances pointing at the same Elasticsearch cluster all work on the jobs together, allowing you to distribute your reporting jobs across multiple machines. If reporting jobs aren’t being worked through fast enough, spin up another Kibana instance and double your reporting power. You can even start up extra instances based on demand, and shut them back down when you don’t need them. One Last Thing The is a great place to go to begin using this feature and sharing reports. We’re also excited to announce that Reporting is immediately available to all users of . All it takes is an upgrade to 2.4.0 and you’re ready to generate reports. Want to learn more? to see the new feature in action. ","locales":"","title":"Reporting 2.4.0 released"}
{"index":{}}
{"author":"Lauren Johnson","category":"User Stories","publish_date":"2016-09-08T00:00:00.000Z","url":"/blog/elasticsearch-support-an-investment-that-keeps-paying-off-at-symantec","seo_title":"Symantec's Elasticsearch Support Story","content":" Even with 18 years of search experience and a PhD in computer science, this senior Symantec engineer finds value in Elastic’s dedicated support and training. ***** ***** After we decided to migrate to Elasticsearch, I needed to learn more about it because I hadn't used it before. I could learn a lot of stuff on my own, but I signed up for a training session and gained more knowledge quickly.   Then we signed up for a . I started with some small tickets and then they got more interesting as time went on. I knew Greg only through the support ticket system at first, and we had a good rapport there. Then we met at Elastic{ON}, which helped further develop our relationship. The initial questions and tickets were pretty much about standing up clusters and all the little gotchas that you encounter when you're trying to get everything up, running, and tuned to the point where you’re getting acceptable performance. You’re not getting crashes or the standard newbie kinds of things that happen when you're picking your way through the landscape for the first time. It’s a teaching process, so as Geena and Symantec became more comfortable with things in the early tickets, they could move on to more connected, complex scenarios, and now they’re using the product in new and interesting ways where we can help. And because we have engineers who are extremely deep and know the internals of the products, support saves Symantec time and effort. You don't go down a path where you say, “This looks fine now,” but in another two or three years, you think, “Oh, wow, I wish I hadn’t done that.” We help you find those things way in advance. You can’t underestimate the value of something like that. As Greg said, it’s an educational process. I started at Symantec two years ago, and at this point I’m considered the Elasticsearch expert in this 10,000-employee company. I did three , I went to , and I filed a lot of tickets. That really got me here — plus the , but the documentation only tells you what you can do. When you’re trying to figure out what you want to do for your use case, it’s helpful to have Elastic support. I've used Lucene directly for years, so I don't file tickets about data modeling or analyzers, but about the distributed system. For example, we had a system where we were doing event logging from security events, so all the machines out there from our customers were sending us events like “Antivirus definitions were updated” or “There was a failed phishing attack.” We have a daily index that we can search and aggregate on that data. We noticed that some of our queries that searched more than 14 days worth of events were really slow. Not only were they slow, but we were getting search rejections because the queue was filling up. So I put in a ticket for that. I asked, “Why is the queue filling up? It’s only 14 days.” Greg writes back and explains that it depends on how many shards you search, so if you have five shards in 14 days, that multiplies out to 70 shards. And if you search 90 days of shards, it’s pretty much hopeless. What happens is that when you do a search, it creates an item on the thread queue for each shard that you search. Then it really adds up. Coming to understand this was really important for getting our application to work with many users searching at the same time. Geena and her team are fairly adept, so they don’t necessarily need a lot of guidance on the things that are mapped out explicitly. What they generally come to me for are explanations of how things work. Like, how does the number of shards that you have impact your actual search, especially when you have this many indexes? Or, when you get a rejection, what’s the process going on behind the scenes in the actual execution queues? How are those things queued up, and why do you ","locales":"","title":"Elastic Support: An Investment That Keeps Paying Off at Symantec"}
{"index":{}}
{"author":"Adrien Grand","category":"Engineering","publish_date":"2016-09-07T00:00:00.000Z","url":"/blog/strings-are-dead-long-live-strings","seo_title":"Elasticsearch replaces string type with two new types text and keyword.","content":" Text vs. keywordWith the release of Elasticsearch 5.0 coming closer, it is time to introduce one of the release highlights of this upcoming release: the removal of the type. The background for this change is that we think the type is confusing: Elasticsearch has two very different ways to search strings. You can either search whole values, that we often refer to as keyword search, or individual tokens, that we usually refer to as full-text search. If you are familiar with Elasticsearch, you know the former strings should be mapped as a string while the latter should be mapped as an string. But the fact that the same field type is used for these two very different use-cases is causing problems since some options only make sense for one of the use case. For instance, makes little sense for a string and it is not obvious whether applies to the whole value or to individual tokens in the case of an string (in case you wonder: it does apply to the value, limits on individual tokens can be applied with the token filter). To avoid these issues, the field has split into two new types: , which should be used for full-text search, and , which should be used for keyword search. New defaultsAt the same time we did this split, we decided to change the default dynamic mappings for string fields. When getting started with Elasticsearch, a common frustration is that you have to reindex in order to be able to aggregate on whole field values. For instance imagine you are indexing documents with a field. Aggregating on this field would give different counts for and instead of having a single count for which is usually the expected behaviour. Unfortunately, fixing this problem requires to reindex the field in order for the index to have the correct structure to answer this question. To make things better, Elasticsearch decided to borrow an idea that initially stemmed from Logstash: strings will now be mapped both as and by default. For instance, if you index the following simple document: { \"foo\": \"bar\" } Then the following dynamic mappings will be created: { \"foo\": { \"type\" \"text\", \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } } } As a consequence, it will both be possible to perform full-text search on foo, and keyword search and aggregations using the field. Disabling this feature is easy: all you need to do is to either map string fields explicitly or to use a dynamic template that matches all string fields. For instance the below dynamic template can be used to restore the same dynamic mappings that were used in Elasticsearch 2.x: { \"match_mapping_type\": \"string\", \"mapping\": { \"type\": \"text\" } } How to migrateIn most cases, the migration should be pretty straightforward. Fields that used to be mapped as an string { \"foo\": { \"type\" \"string\", \"index\": \"analyzed\" } } Now need to be mapped as a field: { \"foo\": { \"type\" \"text\", \"index\": true } } And fields that used to be mapped as a string { \"foo\": { \"type\" \"string\", \"index\": \"not_analyzed\" } } Now need to be mapped as a field: { \"foo\": { \"type\" \"keyword\", \"index\": true } } As you can see, now that fields have split into and , we do not need to have 3 states for the property (, and ), which only existed because of string fields. We can use a simple boolean in order to tell Elasticsearch whether searching the field should be possible. Backward compatibilityBecause major upgrades usually have their own challenges, we did our best not require you to upgrade all mappings at the same time as you upgrade your cluster to Elasticsearch 5.0. First, the field will keep working on existing 2.x indices. And when it comes to new indices, Elasticsearch has some logic that will make it automatically convert string mappings to an equivalent or mapping. This is especially useful if you have index templates that add mappings with string fields: these templates will keep working with Elasticsea","locales":"","title":"Strings are dead, long live strings!"}
{"index":{}}
{"author":"Tyler Hannan","category":"Releases","publish_date":"2016-08-31T00:00:00.000Z","url":"/blog/release-bonanza-elasticsearch-graph-shield-watcher-marvel-reporting-logstash-2-4-beats-1-3-and-kibana-4-6-are-now-available","seo_title":"","content":" The returns! The train rolls on! Other release related sayings! ","locales":"ja-jp","title":"Release Bonanza! Elasticsearch, Graph, Shield, Watcher, Marvel, Reporting, Logstash 2.4, Beats 1.3, and Kibana 4.6 are Now Available!"}
{"index":{}}
{"author":"Clinton Gormley","category":"Engineering","publish_date":"2016-08-30T00:00:00.000Z","url":"/blog/elasticsearch-the-server","seo_title":"Elasticsearch, the server","content":" Back in early 2010, when the first version of Elasticsearch was released, the future (we were reliably informed) was orange. Or was it black? Or green? Nobody really knew. Nobody knew how the NoSQL space would envolve, nor how Elasticsearch would end up being used. Back then, we tried to have a horse in every race. You could talk to Elasticsearch with JSON, YAML, SMILE, CBOR, Thrift, and Memcached protocol. You could specify settings using JSON, YAML, Java properties, and environment variables. We logged to Log4J and SLF4J. There were three ways to configure mappings. And templates. And analyzers. Elasticsearch could run standalone or embedded. Your HTTP web servers could join the Elasticsearch cluster as real nodes. Plugins could override pretty much any part of Elasticsearch, replacing integral parts of the core. One of the things that made Elasticsearch so popular was its flexibility and its familiarity. It was easy to get started, and easy to integrate Elasticsearch into your application, no matter how unusual the design. Six years later, Elasticsearch has evolved into a powerful search and analytics engine. It has been downloaded by millions of people and is a core technology depended upon by hundreds of thousands of users and companies. Flexibility and leniency are no longer as important as: Flexibility comes at a price: complexity. With so many alternatives it is impossible to test them all and to be sure that they actually work. Complexity interferes with the goals listed above. We can’t have it all, so we have to narrow our focus to be able to deliver a solution that can be relied on. ","locales":"ja-jp","title":"Elasticsearch, the server"}
{"index":{}}
{"author":"Tudor Golubenco","category":"Brewing in Beats","publish_date":"2016-08-29T00:00:00.000Z","url":"/blog/brewing-in-beats-configurable-kafka-partitioning-strategies","seo_title":"","content":" Welcome to Brewing in Beats! With this series, we're keeping you up to date with all that's new in , from the details of work in progress pull requests to releases and learning resources. Kafka output: configurable partitioning strategies The Kafka output in libbeat now several strategies for selecting the Kafka partition: hash, round robin, random. The hash partitioner works on a key configured via the string formatter (i.e. can be a field or a combination of fields). Long running Filebeat tests To detect potential issues with long running filebeat instances, a was created. Metricbeat is used to monitor Filebeat and report the open file handlers from the operating system to Elasticsearch. Log total internal metrics values on shutdown The Beats now on shutdown at INFO level. This completes the of reducing the log verbosity on INFO/WARN while still providing the needed information for troubleshooting. Docs on Metricbeat container support Besides improving the documentation on cgroups, includes a on how to run Metricbeat in a container to monitor the other containers and the services they run. Packetbeat: IPv6 support for matching processes When running on the same host as the monitored application, Packetbeat can correlate the network traffic with the process that created the traffic based on data in the proc file system. This , coming from a community contribution by , makes this work also when IPv6 addresses are involved. Packetbeat: display IP addresses when listing the devices Via another , this time by , the flag from Packetbeat now also lists the IP addresses. This makes it easier to figure out which device is which on Windows. Sample dashboards import/export improvements We’ve got for the scripts that we use to save, package, and load the sample dashboards. Fix panic on empty host array was affecting the libbeat Logstash output, in the master version only. ","locales":"","title":"Brewing in Beats: Configurable Kafka partitioning strategies"}
{"index":{}}
{"author":"Aaron Mildenstein","category":"Releases","publish_date":"2016-09-07T00:00:00.000Z","url":"/blog/new-features-in-curator-4-1","seo_title":"New features in Curator 4.1","content":" I am very pleased to announce the release of !  There are some new features I’m excited to tell you about. ","locales":"","title":"New features in Curator 4.1"}
{"index":{}}
{"author":"Matt Bargar","category":"Releases","publish_date":"2016-08-31T00:00:00.000Z","url":"/blog/kibana-4-6-0-released","seo_title":"Kibana 4.6.0 released","content":" The newest version of Kibana is here and it’s got something for everyone. Linux packages have seen major improvements and Kibana 4.6.0 adds support for the much anticipated Reporting plugin. As usual, this release of Kibana supports the latest and greatest version of Elasticsearch (2.4.0) allowing you to upgrade your Elastic Stack with ease. Read on for more details about what’s new or dive right in by grabbing the now. ","locales":"","title":"Kibana 4.6.0 released"}
{"index":{}}
{"author":"Michael McCandless","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2016-08-29T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2016-08-29","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top News Had an awesome time presenting zero-downtime re-indexing of - slide deck: — Mahdi Ben Hamida (@mahdouch) Elasticsearch Core Changes in 2.x: Changes in master: Ongoing changes: Apache Lucene Watch This Space , where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2016-08-29"}
{"index":{}}
{"author":"Toby McLaughlin","category":"Engineering","publish_date":"2016-08-26T00:00:00.000Z","url":"/blog/serverless-elasticsearch-curator-on-aws-lambda","seo_title":"Serverless Elasticsearch Curator on AWS Lambda","content":" In this post, we demonstrate how Elastic's Infrastructure team runs  as a serverless application on . We share the rationale, the tools, and of course, the code. Servers, Services, and ServerlessAs the team responsible for managing Elastic's internal systems, we want to build great systems without increasing the burden of system management. If we deploy a server instance to manage our existing infrastructure, we are creating overhead, which ultimately makes us less effective. To keep our overhead low and our effectiveness high, we often eschew  for  and more recently,  computing. Elastic Cloud We use Elasticsearch to store and analyse all sorts of things. We keep system metrics from , logging from Puppet, even a running history of our GitHub issues to track our performance as a service team.We use  for our Elasticsearch clusters. The Cloud service gives us Elasticsearch, without expanding our server footprint, and thus our management overhead. Serverless We also run a selection of tasks as \"serverless\" processes on Lambda. This article focuses on our serverless approach to running Curator. With all the time-series and log data we collect, we certainly have a need for Curator, but it would be a shame to run servers just to host it. LambkinGetting a function into Lambda in a repeatable, automated way is a reasonably complex task. We use our open-source tool to reduce that complexity. Lambkin creates skeleton functions on demand and helps us publish, run, and schedule Lambda functions. The ProcedureHere, we will step through a process for setting up a serverless Elasticsearch Curator system identical to the one we use internally. The example is implemented in Python, the language used for both Curator and Lambkin. To follow along, you'll need a Python environment with the \" command available. Deep knowledge of Python is not required, however. The solution provided will run as-is, and is configurable for your environment by editing a simple YAML file. Let's go:Install Lambkin and virtualenvsudo pip install lambkin virtualenv Configure AWS credentials and default regionIf you don't already have your AWS account configured, a simple way to do it is:sudo pip install awscli aws configureMore detail is available in the .Create a new skeleton Lambda function lambkin create serverless-curator cd serverless-curator In the new directory, you'll find some skeleton files. These files make up a valid, ready-to-run Lambda function in Python, and a context for managing any dependencies it might have. Create the Python functionThe primary file is . It contains the body of our Python function. Feel free to examine it if you're interested. The function doesn't do much yet, but it's already possible to publish and run it on Lambda:lambkin publish --description='Just a test.' lambkin run You should then see some output from Lambda, ending with a JSON object returned by the sample function. Like this: {\"from\": \"Python\", \"hello\": \"World\"} Replace the skeleton function It's time to replace the example function with something more useful. Full source code for a working Curator function is provided in . The function makes use of a YAML configuration file where you can declare index patterns across multiple Elasticsearch clusters. Be sure to create the file . A  is also provided in the Gist. Install Python requirementsOur new function requires some library packages from the , so we need to ensure they will be available in Lambda. Edit the  file, changing its contents to: certifi==2016.8.8 elasticsearch-curator==4.0.6 PyYAML==3.11 Then install the packages: lambkin build The function's requirements are now installed. Lambkin uses  to ensure that each function gets its own isolated dependencies. Publish and try the new versionWe'll also update the description, and set a long timeout in case we will be processing a lot of indices: lambkin publish","locales":"","title":"Serverless Elasticsearch Curator on AWS Lambda"}
{"index":{}}
{"author":"Issac Kelly","category":"User Stories","publish_date":"2016-08-25T00:00:00.000Z","url":"/blog/arduino-based-home-weather-station-on-the-elastic-stack","seo_title":"An Arduino-Based Home Weather Station Built with Arduino, Elasticsearch, and Kibana","content":" I’m far from a meteorologist. I’m a hacker with a garage/office that I spend way too much time in. I have a bias toward things that feel like data. A friend told me that I was maybe being a bit of a garage troll:  I am tucked away from the sun and warmth. I decided that I needed to figure out if she was right. This was a perfect opportunity to dive into the Internet of Playful Things: Arduino for my weather measurements, Elasticsearch for storage, and Kibana for viewing and analysis. With off-the-shelf parts, open source libraries and a Saturday afternoon available, I got to work. I choose the ESP8266 more and more often lately. It’s a microcontroller with WiFi capabilities that can be used by itself or with most other platforms. Folks have built compatibility layers for Node.js, Python, and Arduino with it. It has become a very popular device because of its price and capabilities. For only a couple bucks, you can add WiFi to any hobby hardware project. For about sixteen dollars, you can get a battery-powered Arduino and node/lua-compatible development board. For this project I chose a board from . Elasticsearch and Kibana provide a very tidy combination for storing and visualizing the sensor data coming out of my hardware. Prebuilt modules from Adafruit make creating your own wireless weather station simple. Things you’ll need Electronics Tools Nice to have Computer For Elasticsearch and Kibana, you can setup an instance on or you’ll need to share a network with your devices and use a local instance of both. Hardware Build I recommend using a breadboard for most prototypes and temporary projects. The first step is to solder the header pins onto the board. Adafruit ships most of its modules without the pins soldered on. If you haven’t soldered before, there are lots of really good guides to getting started. SparkFun has . If you’re getting really into it you should dig into the . A note on soldering and tools: Most guides on how to solder are written by people who do it a lot. Like most other hobbies and professions, using good tools will make the job easier. It’s also expensive. All you really need to get started is an iron, solder, and something to cut and strip wires with. Get better tools as you need them. My , yours might too. You might also have a which may be able to assist with tools, equipment, and techniques. My first toolset, which got me through about 10 years of experimenting. This sensor board uses SPI to communicate with the microcontroller. The ESP8266 has SPI support, so we hook up Data (SDA) and Clock (SCL) lines to the SDA and SCL lines of each board for communication. We also hook up the 3v and Ground (GND) lines to power sensor board. That’s it! 4 wires is all we need. Arduino Software If you haven’t already, download and install the Arduino IDE and ESP8266 Board Package. . You’ll also need to install the libraries below. if you need help installing Arduino Libraries. Having WiFi makes it very easy to publish directly to Elasticsearch via the HTTP interface. The only real challenge was getting the Arduino code to generate an Elasticsearch compatible timestamp. I think that we’ve got a clever and straight-forward solution, by combining the arduino `mils()` function with an NTP generated unix timestamp. Change the SSID and password in the sketch below and you should be ready to start. Configuring Elasticsearch Before we start sending data we’re going to prime Elasticsearch to index the documents we’re sending. Make sure to check the Elasticsearch URL (my `response = requests.put` line) to match your Elasticsearch endpoint. I’m using Python and the requests library, but you can use whatever tool you’re familiar with. import requests import json data = { \"mappings\": { \"reading\": { \"properties\": { \"temperature\": {\"type\": \"float\"}","locales":"","title":"An Arduino-Based Home Weather Station on the Elastic Stack"}
{"index":{}}
{"author":"Jaleh Dastmalchi-Round","category":"Engineering","publish_date":"2016-08-24T00:00:00.000Z","url":"/blog/monitoring-the-search-queries","seo_title":"Monitoring the Search Queries","content":" Ever wonder how your users are using your Elasticsearch cluster? Have you felt the need to investigate the queries sent to the Elasticsearch cluster by your users? Using Packetbeat you can keep an eye on what comes and goes and avoid those nasty surprises your users may throw at your cluster. You can use Elastic plugins to monitor your own Elastic Stack, with the addition of the road to great monitoring started and is continuing to improve in the future generation as part of the bundle. Long term, this will best provide insight into cluster performance, even behind SSL. Elastic recommends that you make use of these components first and foremost, and that you do have a dedicated Elasticsearch cluster running on at least one node, for this purpose. This monitoring cluster is a great place to also store additional query detail:  so if you don't already have a monitoring cluster this gives you another great reason to set it up as it is imperative that you send the query data to a separate cluster. Please note this blog focuses on monitoring search traffic over HTTP only:  current versions of Packetbeat do not support inspecting encrypted payloads What’s a monitoring cluster?A monitoring cluster is a cluster dedicated for storing and analyzing the monitoring data from your production Elasticsearch cluster. Keeping your monitoring data on a separate cluster is highly recommended:  if things do go wrong in production, you want insight to this data and you want them somewhere you can access them (outside the “fire zone”). This separation becomes essential if you are planning to monitor search queries via Packetbeat. If you have Marvel but this is you wish to set up a monitoring cluster this is a good starting point. Make sure to give your monitoring cluster a face by installing a dedicated Kibana instance for monitoring. Picture itGetting ready for the data You’ll need this for to filter the traffic to specific portions of the traffic you are interested (i.e., only search queries). I created config file called sniff_search.conf with below content:  it includes extracting query_body and the index that has been searched into their own fields. You can go as crazy as you wish here with extracting bits that are useful to you. input { beats { port> 5044 } } filter { if \"search\" in [request]{ grok { match => { \"request\" => \".*\\n\\{(?<query_body>.*)\"} } grok { match => { \"path\" => \"\\/(?<index>.*)\\/_search\"} } if [index] { } else { mutate { add_field => { \"index\" => \"All\" } } } mutate { update => { \"query_body\" => \"{%{query_body}\" } } } } output { if \"search\" in [request] and \"ignore_unmapped\" not in [query_body]{ elasticsearch { hosts => \"10.255.4.165:9200\" } } } On Linux: ./bin/logstash -f sniff_search.conf Start sniffing # Select the network interfaces to sniff the data. You can use the \"any\" # keyword to sniff on all connected interfaces. interfaces: device: any http: # Configure the ports where to listen for HTTP traffic. You can disable # the HTTP protocol by commenting out the list of ports. ports: [9200] send_request: true include_body_for: [\"application/json\", \"x-www-form-urlencoded\"] #elasticsearch: # Array of hosts to connect to. # Scheme and port can be left out and will be set to the default (http and 9200) # In case you specify and additional path, the scheme is required: http://localhost:9200/path # IPv6 addresses should always be defined as: https://[2001:db8::1]:9200 #hosts: [\"Localhost:9200\"] ### Logstash as output logstash: # The Logstash hosts hosts: [\"10.255.4.166:5044\"] On Linux: sudo ./packetbeat -e -c packetbeat.yml -d \"publish\" This is an example of what a document will look like: { \"bytes_in\" => 537, \"client_ip\" => \"10.255.5.101\", \"client_port\" => 52213, \"client_proc\" => \"\", \"client_server\" => \"\", \"ip\" => \"10.255.4.167\", \"port\" => 9200, \"path\" => \"/logstash-*/_search\", \"beat\" => { \"hostname\" => \"ip-10-255-4-167.eu-west-1.compute.internal\", \"name\" => \"ip-10-","locales":"","title":"Monitoring the Search Queries"}
{"index":{}}
{"author":"Daniel Mitterdorfer","category":"Engineering","publish_date":"2016-08-23T00:00:00.000Z","url":"/blog/benchmarking-rest-client-transport-client","seo_title":"Benchmarking REST client and transport client","content":" With the release of Elasticsearch 5.0, we will add a . It offers a lot of advantages compared to the transport client, especially looser coupling of your application to Elasticsearch: You just need the Elasticsearch Java REST client JAR and its dependencies on your application’s classpath which is much more lightweight. Also the REST API is much more stable than the transport client interface which needs to match exactly with your Elasticsearch version. At Elastic we care a lot about performance and we also want to ensure that the new Java REST client is fast enough. So we compared the performance of the transport client against the Java REST client. All benchmarks use only a single client thread because we are mainly interested in what a single client can achieve. The main purpose of a multi-threaded benchmark would be to demonstrate the scalability (or lack thereof due to contention effects) and might be another interesting area we can look at. For this benchmark we chose two typical operations: bulk indexing and search. As we want to benchmark the client, not the server, we use a “noop” Elasticsearch plugin that we have implemented specifically for benchmarking. It does nothing except accepting requests and sending corresponding responses. By using this plugin, we ensure that Elasticsearch does the minimal work that is needed to serve a request and put as much pressure as possible on the client. We look at two key performance characteristics of both client implementations: We also measure latency at defined throughput levels. This means that we are not hitting Elasticsearch as hard as we can but the benchmark driver attempts to reach a specific throughput, called “target throughput”. The reason is that we want to measure whether and how latency changes under varying load. In addition to target throughput, we also look at the actually achieved throughput. We have published all in the Elasticsearch repository so you can try the benchmark by yourself. For the bulk index benchmark you also need the from . Benchmark Setup Client: Elasticsearch (server): Both machines are connected via a direct 1GBit Ethernet connection. Bulk Index benchmark Command line parameters: where is either or and varies as stated above. Search benchmark Command line parameters: where is either or and vary as stated above. Results Bulk Indexing Below we can see the achieved throughput for both client implementations with the “geonames” data set in documents per seconds: The HTTP client has between 4% and 7% smaller bulk indexing throughput than the transport client. Remember that these are lab conditions: We do not process requests in Elasticsearch to stress the clients as much as possible. To get a more realistic picture, we also did a test with complete request processing in Elasticsearch and there the achieved throughput was nearly identical: This shows that a lot of factors influence performance. So, as always with performance topics, it is best to measure yourself. Create a test environment, take a set of representative data and benchmark the two client implementations against each other to get a feeling for the performance characteristics in your case. Search What would a search engine be good for if you couldn’t search? So we have also analyzed the performance of search requests in this benchmark. To explain the results, we need to take a short detour and talk a little bit about what operating a search engine has in common with operating checkouts in a supermarket. Typically, a search engine should be able to provide search results as quickly as possible. For its operation, it is important to run the search engine at a sustainable throughput rate but not at peak load. To understand the reason for that, consider a checkout in a supermarket: When there are not much customers in the supermarket, you just need one cash register. The cashier ca","locales":"","title":"Benchmarking REST client and transport client"}
{"index":{}}
{"author":"Michael McCandless","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2016-08-22T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2016-08-22","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top News “Less Code, More Nodes, More Features“ Application Scaling with Elasticsearch @ StockTwits | Elastic - — Kraut Klíck (@QIMP3G) Elasticsearch Core Changes in 2.x: Changes in master: Ongoing: Apache Lucene Watch This Space , where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2016-08-22"}
{"index":{}}
{"author":"Michael McCandless","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2016-08-16T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2016-08-16","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top News Powering Transactions Search with Elastic – Learnings from the Field — PayPal Developer (@paypaldev) Elasticsearch Core Changes in 2.x: Changes in master: Ongoing: Apache Lucene Watch This Space , where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2016-08-16"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2016-08-22T00:00:00.000Z","url":"/blog/brewing-beats-new-beats-dashboards-management","seo_title":"","content":" New community beat: Springbeat In addition to , Valentin Crettaz created to collect different data from Spring Boot applications running with the actuator module. For now, it supports only health and metrics data. Please give it a try and let us know what do you think. Packetbeat: Add Cassandra support With this , Medcl adds support for Cassandra in Packetbeat, so you can now monitor the network traffic exchanged between your applications and Cassandra, and visualize it with Kibana. The PR comes with a sample Kibana dashboard for Cassandra traffic that you can use as a starting point for your dashboards. Easily import/export the Beats dashboards We were providing two scripts for importing the Kibana dashboards for a single Beat in Elasticsearch, one in bash and one in powershell in order to have support for Unix and Windows systems. Maintaining two scripts became complex with the time, and we decided to the script in Golang that can be compiled for all Go supported platforms.  The script is called , and it will be part of the Beat package. With this change, we are trying to make it easier for the community to share their own custom Kibana dashboards. If you created some awesome Kibana dashboards, just create a zip archive that has a certain , and share it with us. Starting with the next release, we will provide the sample Kibana dashboards for all the Elastic Beats in a separate common package, instead of including the dashboards in each Beat package. To import the Kibana dashboards and the index pattern for a single Beat, together with the dependencies, visualizations, searches, you just need to run: ./scripts/import_dashboards -beat Metricbeat This will download the right version of the Metricbeat dashboards, and import them for you to your local Elasticsearch node. You can also specify a different Elasticsearch URL in `-es URL`, or an username/password (-user USER -pass PASSWORD) to connect to Elasticsearch. By default, this uses the default index pattern,  in this case, but you can specify a different index pattern in the . Please check the for more details. You can also use the script to import any custom Kibana dashboards of an Elastic Beat or a community Beat: ./scripts/import_dashboards -url https://github.com/elastic/my-dashboards/archive/v5.0.0.zip Where the represents the zip archive with the Kibana dashboards of a Beat or multiple Beats. Metricbeat: Monitor Filebeat with Metricbeat Add  in Metricbeat to monitor Filebeat. It exports statistics like the number of renamed, open or truncated files per harvester or per prospector. Completed Postgresql module in Metricbeat Added metricsets for per-database statistics and for the background writer, finalizing the new in Metricbeat. Switch to Go 1.7 We have the Golang version to 1.7, so all the Beats version 5.0 and 1.3 will run with Go 1.7. This is needed for supporting OS X Sierra and should also bring some performance improvements. Upgrading required us to update also the OS X cross compiler. Packaging Makefile refactoring This moves part of the Makefile from the packer to the main Beats Makefile. The result is that the community Beats no longer have to provide a custom Makefile for packaging, simplifying the Beat generator and making it easier for the community authors to package their Beats. Vendor libbeat in community Beats The Beat generator now of the generated Beat. This encourages the practice of dependency vendoring (which we do for the official Beats) and makes it easier to cross-compile and package the Beats. Metricbeat: Introduce experimental flag for cgroups Mark experimental the cgroups support in the system module of Metricbeat. In this regard a new configuration option was added to enable/disable gathering data from cgroups for each process. By default it’s set to false to not send any cgroups information. # EXPERIMENTAL: cgroups can be enabled for the process metricset. #cgroups: false Document","locales":"","title":"Brewing in Beat: New Beats dashboards management"}
{"index":{}}
{"author":"Aaron Mildenstein","category":"Engineering","publish_date":"2016-08-16T00:00:00.000Z","url":"/blog/just_enough_redis_for_logstash","seo_title":"","content":" Message queues are used in Logstash deployments to a surge in events which may lead to slowdown in Elasticsearch or other downstream components. Redis is one of the technologies in use as a message queue. Due to its speed, ease of use, and low resource requirements it is also one of the most popular ways as well.That is probably why it has been around for a very, very long time—since Logstash version 1.0.4, released more than five years ago!With all of the recent changes to improve the Logstash pipeline—as well as improvements to the Redis plugin itself—you may not be getting all the performance the Logstash Redis input plugin is capable of delivering.Logstash attempts to detect the number of cores in your system and set the number of pipeline workers to match. But pipeline workers only matter for filter and output plugins.  It has no effect on input plugins. If you haven’t set `threads` in the Redis input plugin, you are only ingesting a fraction of what you could.The test setup included the most recent stable version of Logstash (2.3.4) with Redis 3.0.7 running on the same machine, which is a 2015 MacBook Pro, for reference. The Logstash configuration used isinput { redis { host => \"127.0.0.1\" data_type => \"list\" key => \"redis_test\" # batch_count => 1 # threads => 1 } } output { stdout { codec => dots } }Run this with This test setup is only designed to test for maximum throughput. Actual performance numbers will vary widely depending on what other plugins are used.By iterating through a few options for only the  setting, and using all other settings at their defaults, we see results like these:Let’s run the same tests again, but with , and add  to our Logstash command line to take full advantage of that increase:As you can see, increasing the default  (which is 125), to 250 results in a decrease in performance.It’s been common in the past for users to use a larger  when working with Redis.  This is no longer best practices, and will actually reduce performance.  This is primarily due to changes in the Logstash pipeline architecture. Even an increase of  to 250 results in a drop of performance from 89.5K/sec down to 72.5K/sec when . The sweet spot for  is right around 125, which is the default  (there’s a reason for that!).Diving deeperRecent changes to the Redis input include turning on batch mode by default and using a to execute the batch retrieval commands on the Redis server. Lua scripts also act as a transaction so we can take a batch and shorten the queue in one step. This forces other Redis connections to wait. Note, your Logstash Redis output plugin in the upstream Logstash config is one such connection.Also, you should be aware that the manner in which events are added to Redis by the upstream Logstash can affect the performance of the downstream Logstash instance. For example, while preparing the results of the tests, we found that querying the list size with  in too often would affect the measurement. Try to understand these factors in your upstream Logstash instance:You might want to look at using bigger sizes e.g. 500 on the upstream Redis output - this will buffer the events before calling Redis and doing this may give the downstream Redis input threads time to jump in to pull from Redis. However, if the event generation rate is high then this may not be as helpful - 50K/sec in batches of 500 means a batch buffer fills in 10 milliseconds.We have seen that the performance of the Lua script suffers proportionately with larger batch sizes. This means that larger batch sizes on the Redis input will affect the performance of both of the Logstash instances on either side of Redis.Generally, the goal of tuning the Redis input plugin config and the pipeline batch size and worker threads is to make sure that the input is able to keep pace with the filter/output stage. You should know what the average throughput of your filter/output stage is with t","locales":"de-de,fr-fr,ja-jp,ko-kr","title":"Just Enough Redis for Logstash"}
{"index":{}}
{"author":"Greg Nieman","category":"User Stories","publish_date":"2016-08-16T00:00:00.000Z","url":"/blog/behind-the-elastic-stack-working-with-usaa","seo_title":"Behind the Elastic Stack: Working with USAA","content":" One of the upsides of working in a support capacity for a new and growing software stack is the opportunity to watch a customer installation go from the “green field” stage to become an integral part of their organization.   At Elastic, each client is assigned a set of dedicated support engineers to assure the continuity of experience that comes from day-to-day familiarity with their installation and use case. USAA’s Cyber Threat Operations Center was one of my first accounts when I joined Elastic, and I was privileged to watch that transition their first year of deployment. “As they become more comfortable with the products and start attempting harder, bigger, more ambitious things, we’re there to advise them along the way.”At the beginning, there were a variety of “How do I do this?” and “Why isn’t this working?” types of questions that are common as an organization goes through the first stages of learning and deployment. As they learned how to address common initial production issues that are part and parcel of large deployments and how to design proactively to address them long term, these gradually segued into more complex and difficult items – topics such as “Where did my heap go all of a sudden?”, starting a discussion on how quickly can consume available memory, and how to use in your mappings to alleviate it. And now, a little over a year later they’ve managed to make Elasticsearch an integral part of their threat management strategy – analyzing 2-4 billion security and server events per day originating from over two dozen feeds. They’ve also become evangelists, . It’s a tangible implementation of how someone can utilize the entire Elastic Stack to add a significant business value. “You want someone who is actively participatory in their solution, because not only does the problem get fixed, but they gain valuable skills and expertise.”Neelsen Cyrus, the primary architect visionary for this system, is my primary point contact. He and all the other USAA personnel have always been curious, flexible, and engaged during all our interactions, and helping them achieve – and possibly even surpass – their initial goals has been more than gratifying. Meeting Neelsen at was one of the high points of the conference for me, and we look to continue building on an already significant achievement in the coming year (I also found out he is our customer with the most rocking tattoos . . . although we’re still waiting for the Elastic cluster logo to show up at some point!). We took a few minutes to share what it’s been like working together so far in the below video – I hope you enjoy it. ","locales":"","title":"Behind the Elastic Stack: Working with USAA"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2016-08-14T00:00:00.000Z","url":"/blog/brewing-in-beats-export-cgroup-metrics-linux","seo_title":"","content":" Metricbeat Reports cgroup Metrics on Linux, which is short for control group, is a mechanism for allocating resources - such as cpu time, memory, or block I/O time - to a group of processes. Cgroup metrics are especially useful for collecting metrics from containerized processes because normally each container is assigned to its own cgroup. This allows Metricbeat to collect detailed cpu, memory, and disk metrics from processes and even attribute those processes to a specific container ID. When processes are assigned to a specific cgroup, Metricbeat will report the stats and configured limits of the cgroup. The is reported in the system process metricset. Metricbeat is capable of collecting data from the cpu, , , and subsystems. Here’s the . Metricbeat: New Postgresql moduleWe’ve merged the first version of the Metricbeat , for now having only a basic Metricset with data about each PostgreSQL process. this week. Metricbeat: Add file descriptor usage the number of file descriptors for each process is now exported in Metricbeat under . This information is available on Linux and FreeBSD. Packetbeat: Refactor HTTP exported fieldsPreviously Content-Type and Content-Length were exported only for the HTTP response. The exported fields were called  and . With this Content-Type and Content-Length are also exported for the HTTP request. To make it easier to understand, the request and the response details are grouped under and , so the following breaking changes are made: Packetbeat: Export http bodyWith this , the body of the HTTP request and HTTP response are exported. You can configure what type of HTTP attachments to export by configuring the include_body_for option. For example to include the json attachments of the HTTP transactions, you need to configure the following: include_body_for: application/json and the json attachment for the HTTP request is exported in and the attachment for the response is exported in . Filebeat: Avoid exporting fields as pointersThe event exported by the Beats shouldn’t contain fields of type pointers, only basic types. This fixes the type of the message field exported by Filebeat, and exports it as string instead of pointer to string. Packetbeat: Fix mappings for Packetbeat flowsFix mappings of the source statistics and destination statistics for Packetbeat flows as they were marked as not_analyzed strings instead of longs. Metricbeat: Fix action from MetricSet filtersThe drop_event filter was causing the MetricSet data to be nil, but the event was still being sent. This causes the event to actually be dropped. Libbeat: Accept array of strings in processor’s condition the contains condition used in processors to accept an array of strings, so you check if an exported field contains a certain string. processors: - drop_event: when: contains: tags: \"service-1\" Docs: Restructure the FAQ pageIn the current version, the questions available under the FAQ page of each Beat were splitted one per page, which made it a bit difficult to search for your problem, especially with the growing number of the questions available for a single Beat. This organize the FAQ in a single page, where you can use the browser shortcuts to easily search for keywords. is what the FAQ page for Packetbeat looks like. Metricbeat: Change type to dateThe start time of the process was exported as string (eg. “12:03”), which made it difficult to filter by time. The type of the is changed to date in this . Metricbeat: Replace nanos with nsThe fields in nanoseconds that are exported by Metricbeat under nanos, are replaced with ns. This way becomes . This breaks the compatibility with Metricbeat 5.0.0-alpha4. Libbeat: Fix Elasticsearch error parsingFix regression in the Elasticsearch bulk-request error parsing. This resolves when ingest node processors do not accept the document provided. Libbeat: Update kafka clientThis updates the kafka client library adding k","locales":"","title":"Brewing in Beats: Export cgroup metrics on Linux"}
{"index":{}}
{"author":"Adrien Grand","category":"Engineering","publish_date":"2016-08-11T00:00:00.000Z","url":"/blog/indexing-ipv6-addresses-in-elasticsearch","seo_title":"Support for IPv6 in Elasticsearch","content":" Starting with Elasticsearch 5.0, the field will support indexing IPv6 addresses. Why only now?The ability to index IPv6 addresses has been . The reason why we have been pushing back so far is that we had no way to index IPv6 addresses efficiently. Our two options were basically to either index them as sortable strings, or to index them with multiple levels of precision like we did for 32-bits and 64-bits numerics. The issue with indexing as a sortable string is that you get terrible range performance: this typically requires visiting every single matching value, so querying a large IPv6 subnet would have been very slow. Then it would be tempting to index IPv6 addresses with multiple levels of precision, but this raises another issue: the more levels of precision, the fewer terms you need to visit at index time and the faster the range queries. However, these additional terms also have a cost in terms of index size and indexing speed. With only 32 or 64 bits of data, we managed to find trade-offs that provided good search performance with a reasonable indexing slow down and increase of the index size. But the trade-off is more complicated with 128 bits of data as you either get terrible indexing performance or terrible search performance. What changed?Lucene 6 introduced a new index structure called . In particular, up to 128 bits by configuring a number of dimensions equal to 1. Conceptually, it is not that different from how Lucene indexed numerics with multiple level of precision, in the sense that it is putting together data that are likely to match the same ranges. Except that points compute these ranges dynamically based on the data that is being indexed, rather than obeying to a static scheme. This is a huge difference since it means we do not index ranges that will not be useful at search time, which is exactly what was adding bloat to the index and making indexing slow in previous versions. How does it work?IPv6 addresses will be supported on all indexes that are created after the upgrade to 5.x, there will be no way to add IPv6 addresses to indexes that were created on Elasticsearch 2.x without reindexing. Internally, all IP addresses are now represented as a 128-bits IPv6 address. If you index an IPv4 address, it will be automatically translated to an at index time, and then converted back to an IPv4 address when returning sort values or aggregations. For instance, IPv4 address would internally be indexed as . You might be worried that indexes will be larger in the case that you only need to index IPv4 addresses since they only need 32 bits of data while they are indexed as IPv6 addresses that need 128 bits of data. However IPv4-mapped IPv6 addresses all start with the same 12 bytes, which is something that makes compression easy. And since the new data-structure that we use for indexing numerics is more space-efficient that the one we were using previously, you could actually expect disk usage reduction. ","locales":"","title":"Indexing IPv6 addresses in Elasticsearch"}
{"index":{}}
{"author":"Eric Alford","category":"User Stories","publish_date":"2016-08-10T00:00:00.000Z","url":"/blog/application-scaling-with-elasticsearch-at-stocktwits","seo_title":"Application Scaling with Elasticsearch @ StockTwits","content":" In this article, I want to share with you how at StockTwits we overhauled our message sharing system that took us from frequent downtime and general slowness to lightning fast requests and very happy users – all while allowing us to continue to scale in the future as traffic increases.Our Use Case: Mo Cashtags, Mo Problems is the largest social network dedicated to the finance community, with 1.5 million monthly active visitors. One of our primary features is the ability to view chronological streams of message posts based on specific filters. More specifically, we need the ability to query our collection of posts and get only posts that have a “cashtag” mentioned in them ($AAPL, $GOOG, etc). We also need the ability to query for posts from users that a specific user is following. This needs to be done quickly and at scale, regardless of how much our traffic grows. The initial architecture for this use case involved heavy database queries on MySQL and a lot of complex caching with Redis. Imagine you are a user and every stream you want to look at is an individual set in Redis. If you have 100,000 followers, that means every time you post a message, that message would have to get inserted in 100,000 individual sets in Redis so your followers can see your posts. This worked in the beginning, but was no longer scalable as our user base continued to grow. As we added more streams, the complexity snowballed: more code, more maintenance, more points of failure. We were recently forced to rethink how our entire system was architected and realized that it is a perfect use case to implement Elasticsearch. The Solution: Two Indexes, One QueryImagine we have 2 indexes. One called “messages,” which has a body and user_id field on it, and another called “friendships,” which has a following field that is an array of user id’s that a particular user is following. Message: body: The text of the message user_id: id of the user who shared the message Friendships: following: Array of user ids of which the user is following Since each document in Elasticsearch has an value, we can assign each message document the id value we have in our MySQL database for easier reference in the future. Each friendship document is better suited by using the user_id value from our database as the value for the document. This is pretty straight forward, but how do you query these indexes to get a stream of messages that a single user is following? Let’s say is logged in. His user_id is 123 and he is requesting his stream of messages from the users he follows. All we have to do is query the messages index with a get request to the url http://yourhost:9200/messages/message/_search with the JSON query attached: { \"query\": { \"bool\": { \"filter\": [{ \"terms\": { \"user_id\": { \"index\": \"friendships\", \"type\": \"friendship\", \"path\": \"following\", \"id\": 123 }, } }] } }, \"size\": 15, \"sort\": [{ \"id\": { \"order\": \"desc\" }}] } Let’s take a deeper look at how this query works. We are asking our messages index to give us a filtered set of messages in which the user_id field must be equal to any ids that user 123 is following, similar to an IN query in SQL. It knows the set of following user ids for user 123 because we have specifically told it to route through the friendships index and use the following field for the set of following user ids. The size field is used to limit the result set Elasticsearch gives back and the sort field says we want to sort in descending order. This is pretty straightforward when looking at it from the outside but there is some Elasticsearch is doing under the hood of this. Before Elasticsearch 2.0, it was up to the developer to provide caching logic for these queries. For example, if we wanted to cache the following ids that this query uses, we used to have to provide a field with the name of the key and expire that cache man","locales":"","title":"Application Scaling with Elasticsearch @ StockTwits"}
{"index":{}}
{"author":"Tudor Golubenco","category":"Releases","publish_date":"2016-08-09T00:00:00.000Z","url":"/blog/beats-5-0-0-alpha5-released","seo_title":"","content":" We’re happy to announce the fifth alpha release of the Beats 5.0 series. As we slowly approach the GA release, we're polishing the Beats with lots of small improvements and fixes.  You can find all the details in the release notes below, or read the rest of the blog post for the highlights. IMPORTANT: This is an alpha release and is intended for testing purposes only. Please do not deploy in production. Yada yada. Automatically load the right Elasticsearch template Elasticsearch 5.0 comes with several , but it is still able to use the old 2.x templates for backwards compatibility. This is good news because it means that you can use Beats 1.x with either Elasticsearch 2.x or 5.x. However, because we want the Beats 5.x to use the new mapping features, but also be able to work with Elasticsearch 2.x, we added a feature that can query the Elasticsearch version and automatically load the best template for you. This way, you have complete freedom in planning your rolling upgrades. More filtering flexibility Beats 5.0.0-alpha5 introduces simple processors (previously called \"generic filters\") that allow you to flexibly choose which events or fields to drop based on simple conditions. Starting with alpha5, these conditions can also be combined with logical operators (AND/OR/NOT) so they don’t have to be so simple anymore. You can then express conditions like “drop all logs about 200 and 404 responses, except if the response time is larger than 100 milliseconds”. Filebeat registry file cleanup + more options Filebeat stores its state about the files it reads in a registry file on disk. This way, it can avoid shipping the same log lines after a restart. With the 5.0.0-alpha5 release, we introduce new configuration options that allow you to configure when to remove entries from this registry file. This means you can make sure that the registry file doesn’t grow forever. On the same note, other Filebeat configuration settings were renamed for consistency and we now have a short explaining what each Filebeat component does. Override settings from the CLI via the -E flag Do you like how Elasticsearch allows you to set any configuration setting from the command line by using the flag (yes, it used to be , but was renamed in 5.0 to avoid confusion with the JVM flags)? Well, the same is now possible for all Beats. For example, you can quickly enable the console output by adding . More configuration flexibility On the same theme with the flag, you can now specify multiple configuration files by repeating the flag. Settings in subsequent config files override those that precede them. You can use this, for example, for setting defaults in a base configuration file, and overwrite settings via local configs. We have also standardized on using as a way to disable most things in the configuration file. For example, if you want to disable a Packetbeat protocol without commenting out 10 lines of config, add . Want to disable an output? The same does the trick. New defaults for the logging verbosity We did some rethinking of the Beats logging strategy. Part of this, we’ve switched the default log level from ERROR to INFO and reduced the verbosity of most warnings and info messages. To compensate, we added a set of internal metrics which also get logged to the INFO level every 30 seconds. This means you get good visibility into what is happening inside the Beats without getting huge files during periods of high traffic. Become a Pioneer A big Thank You to everyone who has tried the previous alpha releases and or . We’d like to also remind you that if you post a valid, non-duplicate bug report during the alpha/beta period against any of the Elastic stack projects, you are entitled to a . ","locales":"","title":"Beats 5.0.0-alpha5 released"}
{"index":{}}
{"author":"Tudor Golubenco","category":"Brewing in Beats","publish_date":"2016-08-09T00:00:00.000Z","url":"/blog/brewing-in-beats-configurable-index-patterns","seo_title":"","content":" Welcome to Brewing in Beats! With this series, we're keeping you up to date with all that's new in , from the details of work in progress pull requests to releases and learning resources. New community beat: Burrowbeat is a new Beat that can be used to monitor consumer lags in Kafka queues. It is based on the project. Configurable index patternsThe Beats now offer . They used to enforce daily indices of the form , which is still the default. But it’s now possible to do weekly or hourly indices using a syntax similar to Logstash, or something like to work with the new from Elasticsearch, or even to automatically split the data into separate indices based on any field from the events. Use scaled floats for percentagesAfter adding support for half floats a few weeks back, Adrien and added scaled floats to Elasticsearch 🎉. Scaled floats are stored as longs behind the scenes, which makes them benefit of the compression scheme used for integers in Lucene. This makes them a great fit for the way we store percentages in Beats: a number between 0 and 1 that gets formatted as a percentage by Kibana. This switches our percentages to use scaled_float. We had quite a few of those, so we can expect a significant improvement in the storage footprint. Automatically generate Kibana index patternsThe sample Beats dashboards contain the Kibana index patterns, which allow us to define custom formatting for some of our fields (think of percentages). Providing the index pattern also saves the user a step while getting started. We used to create these index patterns by exporting them from Kibana, in a mostly manual process. To improve on this, Monica created a script to from our files, which are the primary source data for everything that the Beats export. Lookup functionalityThe allows attaching arbitrary metadata to events by calling external scripts. The scripts are called with parameters that can be taken from the original event, and the results cached for that parameters. This can be used, for example, to attach extra metadata to every file read by Filebeat by calling the Kubernetes APIs. The same could be used to add custom metadata to every process monitored by Metricbeat. Since the results are cached, the external script is called only once per file/process. into an experimental branch called , which we plan to merge into master after we add several security checks around calling external scripts. Filebeat: shorter close_inactive defaultThe option sets after what time interval Filebeat closes the open files that don’t receive updates. The old default used to be 1h, which could mean that we keep a lot of open files in the case of quickly rotating files. This PR changes the default to 5m. If a file receives updates after it was closed, it is picked up again by Filebeat, so the lower default doesn’t mean any risk of data loss. ","locales":"","title":"Brewing in Beats: Configurable index patterns"}
{"index":{}}
{"author":"Shay Banon","category":"Releases","publish_date":"2016-08-09T00:00:00.000Z","url":"/blog/elastic-stack-release-5-0-0-alpha-5","seo_title":"Elastic Stack Release - 5.0.0-alpha5","content":" So many alphas. So many new things we want the community to explore, utilize, abuse, and...hopefully...find compelling. The release train continues. (*choo choo*) It’s here!  Say “Heya” to alpha 5. Or, say “” if you would prefer... Before you get too excited, keep in mind that this is still an alpha, so don’t put it into production. And, since it is an alpha, it is not available on . But, because Elastic Cloud is the official hosted Elasticsearch and Kibana offering on AWS, you'll be able to deploy releases on the day of 5.0 GA. (We wouldn't want you to wait for all this goodness!)  If you open a bug report, today, you too can become an . And now, without further ado, some highlights from alpha 5. ElasticsearchFor more detailed information, and many other features, peruse the Elasticsearch . From 2.3.x to 5.0. The magical journey. The undiscovered country. The next frontier. Other coined phrases. As a reminder, we’ve also released the , which runs on your existing 2.3 cluster.  Use this site plugin to prep for your migration. KibanaWe’re heads down and laying the groundwork for beta1 throughout Kibana, but we couldn’t help but squeeze a few key updates into this release. Logstash “Go faster”, you said. “Ok!”, we replied. Speed and a few more additions feature highly in this release. BeatsWe may collect data from the edge, but all the updates are in a single Beats . ES-HadoopES-Hadoop v 5.0.0-alpha5 has also been released today. Peruse all the information in the . Get it Now!Happy testing. Your feedback is instrumental in making 5.0 successful. And, don't forget that X-Pack is here and continually being udpated with each Alpha release. ","locales":"ja-jp","title":"Elastic Stack Release - 5.0.0-alpha5"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2016-08-08T00:00:00.000Z","url":"/blog/elasticsearch-5-0-0-alpha5-released","seo_title":"Elasticsearch 5.0.0-alpha5 released","content":" Today we are excited to announce the release of based on . This is the fifth in a series of pre-5.0.0 releases designed to let you test out your application with the features and changes coming in 5.0.0, and to give us feedback about any problems that you encounter. Open a bug report today and become an . This is an alpha release and is intended for . Indices created in this version . Development of Elasticsearch 5.0.0 is nearing completion. This release contains more bug fixes and enhancements than it does new features (all of which you can read about in the release notes linked above), but there are a few gems worth mentioning. Also take a look at the release announcements for , , , and to read about features like: Known networking bug in 5.0.0-alpha5We have discovered is a major bug in the new Netty4 implementation in this release which affects any REST requests greater than 1024 bytes in size, and which will generate an exception similar to the following:This is due to incorrect handling of the HTTP header, and it can be worked around in one of three ways: ","locales":"","title":"Elasticsearch 5.0.0-alpha5 released"}
{"index":{}}
{"author":"James Baiera","category":"Releases","publish_date":"2016-08-09T00:00:00.000Z","url":"/blog/es-hadoop-5-0-0-alpha5","seo_title":"","content":" ​I am excited to announce the release of Elasticsearch for Apache Hadoop (aka ES-Hadoop) built against Elasticsearch 5.0.0-alpha5. : This is an alpha release and is intended for purposes only. Crazy things might happen when running this code and indices created with this version . For the sake of your own sanity, we do not advise using this version in production. What’s new? Spark 2.0 !!! It’s here! It’s here! It’s finally here! It’s exciting enough to warrant three exclamation points in the header! This version has added preliminary support for Spark 2.0! Give it a try and let us know what needs improving! Every sentence in this section has an exclamation point at the end of it! Hurray! (Hadoop/Spark) + Slice API = More Parallel A substantial change has been added to support the use of Elasticsearch’s new Scroll Slicing functionality. Now you can state the maximum number of documents you wish to see per input task and the framework will attempt to sub-divide input splits to increase your computing parallelism. Isn’t sharing beautiful? Squashing Bugs Have sub-fields in your mapping named “properties”? Fixed. Don’t like DataFrames saving null values? Fixed. Tired of not seeing why your bulk indexing requests don’t report why they failed? Double fixed. Take a look at in this release! Feedback Now you might be wondering, “Why would I want to try an Alpha Release? Aren’t these things normally riddled with bugs?” Well, yeah, sometimes. We’re tracking a few things that we already know we’ve broken (like ), but we’re only human. Thats why we need the help from all of you awesome early adopters! So, please, try this at home! You can ES-Hadoop 5.0.0-alpha5, try it out, find out how it breaks, and let us know what you did on , , or in the . A crisp high five is waiting for all who participate! Not a huge fan of high fives? There’s always the instead! ","locales":"","title":"Elasticsearch for Apache Hadoop 5.0.0-alpha5"}
{"index":{}}
{"author":"Suyog Rao","category":"The Logstash Lines","publish_date":"2016-08-08T00:00:00.000Z","url":"/blog/logstash-lines-2016-08-08","seo_title":"","content":" Welcome back to The Logstash Lines! In these weekly posts, we'll share the latest happenings in the world of Logstash and its ecosystem.LS has used a home-grown library called Cabin to do all its structured logging. As we write more Java components in core and in plugins, we've had to our logging framework across the board to Log4j. Also, log4j brings in nice features like log rotation, per-component based logging... With this feature, we will also be able to control setting log levels dynamically via an API. Tal recently demoed a of this, and we're putting finishing touches.Logstash's plugin documentation gets generated from the asciidoc embedded in the code itself. Recently we've been running into many issues with plugin generation tool, which was written a while ago. PH has been to make it more robust while adding much-needed enhancements like versioned docs, generating directly from Github repo, easy html preview, dealing with dependencies etc. ","locales":"","title":"Logstash Lines: Grok timeout option, SSL for TCP output and more"}
{"index":{}}
{"author":"Chris Earle","category":"Engineering","publish_date":"2016-08-08T00:00:00.000Z","url":"/blog/elasticsearch-verifying-data-integrity-with-external-data-stores","seo_title":"","content":" Elasticsearch is sometimes used alongside other databases. In those scenarios, it's often hard to implement solutions surrounding due to the lack of transaction support across all systems in play. Depending on your use case, it may or may not be necessary to verify the data exists in both data stores, where one serves as the so-called \"source of truth\" for the other. As Support Engineers at Elastic, we frequently see requests that ask things like the best way to structure your data for verification as well as simply how to do verification efficiently. This blog post will walk through a few examples that we have seen and helped to create, ranging from simple to advanced, that verify Elasticsearch contains the necessary data from a database like PostgreSQL. Modeling Data for VerificationThe way that your data is stored, both in and out of Elasticsearch, can make a big difference in the difficulty of verification. The first thing that you need to decide is how much needs to be verified: That decision has ramifications on the amount of effort required to perform verification. Evolution of Verifying ExistenceFortunately this is not a deep, philosophical question. Instead, it's simply the question: \"do all of the documents exist in Elasticsearch?\" Elasticsearch offers a lot of ways to verify existence, as long as you fully understand what is happening. Remember that , but it can get documents directly in real time. This means that, right after indexing a document, it may not be visible to searches, but it will be available to any direct get request. One By OneFor small scale deployments, the simplest approach is to perform requests against individual documents, then confirm that the HTTP response code is not 404 (page — or document in this case — not found). HEAD /my_index/my_type/my_id1 An example response would be just headers for success: HTTP/1.1 200 OK Content-Type: text/plain:  charset=UTF-8 Content-Length: 0 And for non-existence, it looks practically the same, minus the response code: HTTP/1.1 404 Not Found Content-Type: text/plain:  charset=UTF-8 Content-Length: 0 This requires that you perform requests against an index expected to have documents. As you might expect, that does not scale very well. Batch ProcessingThe next thing that you might choose to do is to perform these in batch requests using Multi GET API: . GET /my_index/my_type/_mget { \"ids\": [ \"my_id1\", \"my_id2\" ] } Another equivalent approach to this is to simply search for the IDs and return the expected number. GET /my_index/_refresh GET /my_index/my_type/_search { \"size\": 2, \"query\": { \"ids\": { \"values\": [ \"my_id1\", \"my_id2\" ] } } } First, the endpoint is invoked to ensure that everything indexed is searchable. This removes the \"near real time search\" aspect of searching. Under normal searching conditions, you should not be doing that, but it makes total sense here! Presumably your verification processing happens all at once, so calling at the start of a job is sufficient for the rest of it as long as you're not checking for new data that comes in after the process starts. That offers a superior request handling, but it does not avoid the background work, which remains unchanged, and it means that each document is going to be returned as well, which means added overhead. Search, Then BatchFrom batch processing, people generally branch off to try to beat the problem by subdividing it. First, they will often perform a search to determine what is missing and only then begin digging for the missing document: GET /my_index/_refresh GET /my_index/my_type/_search { \"size\": 0, \"query\": { \"ids\": { \"values\": [ \"my_id1\", \"my_id2\" ] } } } If you check the response's value, then it should match the number that you expect. In this simple example, it should be . If it's not 2, then you need to switch gears and either: There Must Be A Better WayThe long-winded or t","locales":"de-de,fr-fr,ja-jp,ko-kr","title":"Elasticsearch: Verifying Data Integrity with External Data Stores"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2016-08-03T00:00:00.000Z","url":"/blog/elasticsearch-2-3-5-released","seo_title":"Elasticsearch 2.3.5 released","content":" Today we are pleased to announce the release of based on . This release contains only bug fixes for Marvel, Watcher, and Shield — there are no changes to core Elasticsearch. It is already available for deployment on , our Elasticsearch-as-a-service platform.Latest stable release:Full details of the changes in this release are available in the release notes listed above, but the most important change is that Shield again works correctly with Tribe Nodes. ","locales":"","title":"Elasticsearch 2.3.5 released"}
{"index":{}}
{"author":"Nicolás Bevacqua","category":"Engineering","publish_date":"2016-08-03T00:00:00.000Z","url":"/blog/art-of-pull-request","seo_title":"","content":" The Art of a Pull Request ","locales":"","title":"The Art of a Pull Request"}
{"index":{}}
{"author":"Suyog Rao","category":"The Logstash Lines","publish_date":"2016-08-02T00:00:00.000Z","url":"/blog/logstash-lines-2016-08-02","seo_title":"","content":" Welcome back to The Logstash Lines! In these weekly posts, we'll share the latest happenings in the world of Logstash and its ecosystem. In preparation for persistence feature landing in 5.x, we added to the 5.0 pipeline. We now have a separate read (consumer) client and write (producer) client that deals with queue operations. We also encapsulated all the event batch management to a separate class. A couple of us were involved in a user's throughput issue with Redis input for LS 2.3.4. User was migrating from 1.4.x and had experienced a slowdown. We'll convert all the good info in this issue to a blog. We are still input's performance. The current rewrite in Java has given us a 50% increase in throughput performance, but our preliminary tests had shown ~100% increase! Anyhow, the good news is that the current beats input + filebeat combo is much faster than the LSF + lumberjack input combo, which was our original goal for this project. We are shipping this change in 5.0.0-alpha5 and will backport it to 2.4. ","locales":"","title":"Logstash Lines: Beats Input Performance and other fixes"}
{"index":{}}
{"author":"Michael McCandless","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2016-08-01T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2016-08-01","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top News — Christoph Wurm (@ChristophWurm) Elasticsearch Core Changes in 2.x: Changes in master: Ongoing changes: Apache Lucene Watch This Space , where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2016-08-01"}
{"index":{}}
{"author":"Adrien Grand","category":"Engineering","publish_date":"2016-08-04T00:00:00.000Z","url":"/blog/searching-numb3rs-in-5.0","seo_title":"","content":" Lucene 6.0 introduced new exciting feature called . While the name of the feature puts a lot of emphasis on the fact that it supports multiple dimensions, this feature behaves more than decently in the case of a single dimension and actually better than the that was used in previous versions of Lucene. This caused us to refactor number-based fields (, , , , , , and ) to use this new data-structure for indexing numbers. How does it work? The current data-structure that underlies dimensional points is called a , which in the case of a single dimension is a simple binary search tree that stores blocks of values on the leaves rather than individual values. Lucene currently defaults to having between 512 and 1024 values on the leaves. This is quite similar to the b-trees that are used in most regular databases in order to index data. In the case of Lucene, the fact that files are write-once makes the writing easy since we can build a perfectly balanced tree and then not worry about rebalancing since the tree will never be modified. Merging is easy too since you can get a sorted iterator over the values that are stored in a segment, then merge these iterators into a sorted iterator on the fly and build the tree of the merged segment from this sorted iterator. Comparison with the old number implementation While Mike already reported that than the old implementation, things got even better with recent optimizations in Lucene 6.1 and the upcoming 6.2: So I re-ran the same benchmark as in the to get a more up-to-date comparison in the case of a single dimension. google.charts.load('current', { packages: ['corechart', 'bar'] }):  google.charts.setOnLoadCallback(draw1DQueryTime):  function draw1DQueryTime() { var data = google.visualization.arrayToDataTable([ ['Approach', 'Query time (msec)', { role: 'style' }], ['Points', 25.0, '#e62739'], ['Numeric Field', 39.3, '#9068be'], ]):  var options = { title: 'Query time (msec)', chartArea: { width: '50%' }, hAxis: { minValue: 0 }, vAxis: { }, legend: { position: \"none\" }, }:  var chart = new google.visualization.BarChart(document.getElementById('query_time_1d_div')):  chart.draw(data, options):  } google.charts.setOnLoadCallback(draw1DIndexTime):  function draw1DIndexTime() { var data = google.visualization.arrayToDataTable([ ['Approach', 'Index time (sec)', { role: 'style' }], ['Points', 62.3, '#e62739'], ['Numeric Field', 217, '#9068be'], ]):  var options = { title: 'Index time (msec)', chartArea: { width: '50%' }, hAxis: { title: 'Index time (msec)', minValue: 0 }, vAxis: { }, legend: { position: \"none\" }, }:  var chart = new google.visualization.BarChart(document.getElementById('index_time_1d_div')):  chart.draw(data, options):  } google.charts.setOnLoadCallback(draw1DIndexSize):  function draw1DIndexSize() { var data = google.visualization.arrayToDataTable([ ['Approach', 'Index size (MB)', { role: 'style' }], ['Points', 251, '#e62739'], ['Numeric Field', 744, '#9068be'], ]):  var options = { title: 'Index size (MB)', chartArea: { width: '50%' }, hAxis: { title: 'Index size (MB)', minValue: 0 }, vAxis: { }, legend: { position: \"none\" }, }:  var chart = new google.visualization.BarChart(document.getElementById('index_size_1d_div')):  chart.draw(data, options):  } google.charts.setOnLoadCallback(draw1DSearchHeap):  function draw1DSearchHeap() { var data = google.visualization.arrayToDataTable([ ['Approach', 'Search time heap (MB)', { role: 'style' }], ['Points', 2.12, '#e62739'], ['Numeric Field', 14.4, '#9068be'], ]):  var options = { title: 'Search time heap (MB)', chartArea: { width: '50%' }, hAxis: { title: 'Search time heap (MB)', minValue: 0 }, vAxis: { }, legend: { position: \"none\" }, }:  var chart = new google.visualization.BarChart(document.getElementById('search_heap_1d_div')):  chart.draw(data, options):  } For this particular data and set of queries, points proved 36% faster at query time, 71% faster at index time and used 66% less disk and 85% less memory. Numbers may diffe","locales":"","title":"Searching numb3rs in 5.0"}
{"index":{}}
{"author":"Court Ewing","category":"Releases","publish_date":"2016-08-03T00:00:00.000Z","url":"/blog/kibana-4-5-4-and-4-1-11","seo_title":"Kibana 4.5.4 and 4.1.11 Released","content":" Today we’re releasing Kibana versions 4.5.4 and 4.1.11, which include two security fixes, an increase in the maximum zoom capabilities for tile maps, and some other bug fixes. Since these are security releases, we recommend that users upgrade as soon as possible. You can grab the latest versions from the page. 4.5.4 4.1.11 Conclusion As always, head over to our page to get the latest versions. If you have any questions, please don’t hesitate to reach out to us on our , , or . ","locales":"","title":"Kibana 4.5.4 and 4.1.11 Released"}
{"index":{}}
{"author":"James Baiera","category":"Releases","publish_date":"2016-08-03T00:00:00.000Z","url":"/blog/es-hadoop-2-3-4-released","seo_title":"","content":" I am pleased to announce the release of Elasticsearch for Apache Hadoop (aka ES-Hadoop) . ES-Hadoop 2.3.4 is primarily a bug-fix release, including some new configuration validation steps, smarter parsing of index mappings, and support for the latest and greatest Elasticsearch version . Feedback Looking forward to hearing your feedback on this ! Drop us a line on , Twitter (), , or the . ","locales":"","title":"Elasticsearch for Apache Hadoop 2.3.4 released"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2016-08-02T00:00:00.000Z","url":"/blog/configure-pipeline-ingest-node","seo_title":"","content":" New community Beat: Cassandrabeat Filebeat: Unmarshal JSON inputs to integer by default The standard library in Golang unmarshals the integer values of the json object into floats instead of integers. This leaded to some unexpected behaviour when using the conditions from processors as you couldn’t easily compare a status code from the JSON object as it was translated to float64. The overwrites the Unmarshal behaviour and it tries to convert the numbers from the json objects to integers first, and if it fails then it converts them to floats. This way, unmarshals as int64 and as float64. Metricbeat: Enhance load metrics A new Metricset called is exported by Metricbeat instead of exporting the load statistics inside the CPU statistics. With this   becomes , becomes and becomes . In addition, the load values divided by the number of cores are exported under . Filebeat: Fix state remove and sending empty logs When a very low is set, then it could happen that a state of a finished harvester was overwritten by the prospector and the state is never set to Finished. This is now in that the prospector only sends a state when the state is set to Finished. In addition, there is a to not send empty log lines. Community Beats: Create pure Go binaries in packaging by default The makes the Beat generator assume the Beat is pure Go (doesn’t have C dependencies). This simplifies the packaging process and produces fully static binaries by default. It’s still possible to create packages for the Beats that require Cgo, but you need to adjust the Makefile. Use the as an example of the possible features. Add support for cgroup in gosigar The gives you the ability to ask gosigar for cgroup stats by PID. It returns metrics and limits from the , , , and subsystems. This is part of a larger effort to build a solution on top of Beats to monitor containers. ","locales":"","title":"Brewing in Beats: Configure the Ingest Node Pipeline"}
{"index":{}}
{"author":"Kosho Owa (JP)","category":"Engineering","publish_date":"2016-08-01T00:00:00.000Z","url":"/blog/earthquake-data-with-the-elastic-stack","seo_title":"","content":" INTROThe Elastic Stack is fantastic for analysing time based data, most of which we currently see coming from operational requirements around things like system load, memory or disk use, network flows, application error rates and so on. However, the awesome thing about the stack is that it isn't tied to time based data consisting of logs or metrics. Here we'll take a look at earthquake data from the Northern California Earthquake Data Center, via the , this data set includes naturally occurring quakes, as well as man-made ones from nuclear blasts and quarry explosions. You may not be shaking in your boots with excitement yet, but we'll show a few neat tricks around leveraging custom maps, visualisations and other tips to increase usability for your users and to give things a bit of extra shine. FIRST STEPSTo get this dataset into Elasticsearch we built a very simple Logstash configuration with the CSV, mutate and date filters. We've also created mappings for the fields in the data and converted that into a template. If you want to play along at home, then head over to and either clone it or download the files, then follow the instructions to get the data loaded. The dashboards are all provided, but we'll dig into the bits that make this an interesting use case throughout this post, so don't go anywhere! VISUALISING WITH YOUR EYEBALLS When you open the main Earthquake dashboard (as seen above), you can see we've broken down the data into different groupings, which are also separate dashboards themselves. These are based on Hot Areas around the world, Catalogues, or archives, and finally, by a few specific Quarries and Nuclear Blasts. This is a basic visualisation that's a handy way for introducing your data to your users. Rather than letting them click around and figuring out what the data is all about, you can help frame their discovery process by creating dashboards that link to specific subsets of your data. It's also a great way to introduce concepts such as filters and the ability to build multiple dashboards with different visualisations for different audiences, but all based on the one dataset. One tip we also really like in markdown boxes like this, is having a simple \"return to home\" or \"reset time frame\" link that can help if someone gets lost or makes changes they didn't want. This helps with time based data, as you can have a link to the dashboard with the last 5/10/100 minutes/months/years without them having to jump into the timepicker. We've called our reset link Go back to the world map. and it's at the bottom of markdown box. Moving on, and next to that we have a heat map of all the events, which has been populated thanks to the inclusion of latitude and longitude in the dataset, that was then converted to a single field and then mapped to a using the Elasticsearch template. Note that this is a standard Kibana heatmap, we'll touch on some custom maps goodness later. Then we have a few high level metrics, a histogram of all the magnitudes for the given time period, a breakdown of quake type (natural or man made) and then (ie the network operator code for the detectors) of each of these. Finally we have a bar chart showing a date histogram of all the events for each month, with a count of each magnitude, and below that a Timelion plot showing the average depth of the quakes, broken down per-week as dots, with a moving average of those depths over the past 3 months as a line graph. Let's look at all of this in more detail. SECOND STEPSNow we will open up the dashboard, you can do that by clicking Japan under the Hot Areas links, or finding the dashboard using the Load Saved Dashboard button on the top right of the toolbar. The first thing we notice is that a saved filter has been applied, the little green box labelled \"Japan Territorial Water\". Interestingly. this is actually a query that isn't currently exposed natively in Kibana, luckily for us, Kibana does allow us to specify ","locales":"","title":"Earthquake data with the Elastic Stack"}
{"index":{}}
{"author":"Jason Bryan","category":"Engineering","publish_date":"2016-07-28T00:00:00.000Z","url":"/blog/getting-started-with-elasticsearch-ssl-native-authentication","seo_title":"Getting Started with Elasticsearch and SSL & Native Authentication","content":" When planning to stand up a new Elasticsearch cluster, it is important to make considerations for implementing authentication and SSL/TLS with Shield as early as possible. Getting this going in the planning and testing phases will help ensure a smoother transition to production with security in place. In this article, we walk through setting up Shield's native authentication and SSL/TLS with a wildcard certificate. Native authentication is by far the simplest way to manage users and it will be the default in X-Pack for Elasticsearch 5.x.  There are many ways to setup SSL/TLS, and in this post we will go through one of them that I find simple and easy to use for all my development environments. Prior to proceeding, be sure you have a certificate authority (CA) in place. A private (self-signed) or public CA can be used. Getting Started1. 2. Install the License and Shield plugins bin/plugin install elasticsearch/license/latest bin/plugin install elasticsearch/shield/latest 3. Configure for Shield's file and native authentication. stores users and passwords in file stored locally on each cluster node. With , users are managed with a REST API and centrally stored in the cluster. In Elasticsearch 2.3.x, file-based authentication must be configured prior to using native authentication to gain access to the cluster. This step will not be required in 5.x. shield: authc: realms: file1: type: file order: 0 native1: type: native order: 1 4. Create an admin user with the command that will be used for file-based authentication. bin/shield/esusers useradd admin -p as@m25 -r admin 5. Test authentication to the cluster using a REST client. I like curl, but any REST client will do. curl -s 'http://node1.kle.moc:9200' -u admin:as@m25 { \"name\" : \"roger\", \"cluster_name\" : \"kermy\", \"version\" : { \"number\" : \"2.3.4\", \"build_hash\" : \"e455fd0c13dceca8dbbdbb1665d068ae55dabe3f\", \"build_timestamp\" : \"2016-06-30T11:24:31Z\", \"build_snapshot\" : false, \"lucene_version\" : \"5.5.0\" }, \"tagline\" : \"You Know, for Search\" } Native AuthenticationUse the Shield REST API to configure and manage users for native authentication. To setup the first native admin user, use the file-based credentials. curl -u admin:as@m25 'http://node1.kle.moc:9200/_shield/user/esadmin' -XPOST -d ' { \"password\": \"as@m25\", \"roles\": [ \"admin\" ], \"full_name\": \"Search Admin\", \"email\": \"searchadmin@kle.moc\" }' Confirm the native user was setup successfully with a REST call. curl 'http://node1.kle.moc:9200' -u esadmin:as@m25 { \"name\" : \"roger\", \"cluster_name\" : \"kermy\", \"version\" : { \"number\" : \"2.3.4\", \"build_hash\" : \"e455fd0c13dceca8dbbdbb1665d068ae55dabe3f\", \"build_timestamp\" : \"2016-06-30T11:24:31Z\", \"build_snapshot\" : false, \"lucene_version\" : \"5.5.0\" }, \"tagline\" : \"You Know, for Search\" } Configuring SSL/TLSCreate the Java KeystoreIn this example, a wildcard certificate will be used as it provides ease of administration across a large Elasticsearch cluster. Wildcards permit a single certificate to be used for every node in the cluster. While this is a convenient way to manage cluster certificates, be sure to take into consideration the risk factors associated with using wildcards. It is assumed an issuing certificate authority (CA) is already in place. See the article for help setting up your own CA that can be used with Shield. 1. Create a new Java Keystore by importing the CA certificate that will issue the wildcard certificate. Note: the Java Keystore can be created on any host or workstation with Java installed using the command. keytool -importcert -keystore shield.jks -file ca.cert.pem -trustcacerts -storepass s3cret -alias ca_cert 2. Create a private key in the Java Keystore. keytool -storepass s3cret -genkey -alias es-shield -keystore shield.jks -keyalg RSA -keysize 2048 -validity 3650 -dname \"cn=*.kle.moc\" 3. Create a certificate signin","locales":"","title":"Getting Started with Elasticsearch and SSL & Native Authentication"}
{"index":{}}
{"author":"Michael McCandless","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2016-07-26T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2016-07-26","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top News Wrote a post about combining RestClient with templates for creating and executing queries: — Jettro Coenradie (@jettroCoenradie) Elasticsearch Core Apache Lucene Watch This Space , where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2016-07-26"}
{"index":{}}
{"author":"Andrew Cholakian","category":"Engineering","publish_date":"2016-07-25T00:00:00.000Z","url":"/blog/a-history-of-logstash-output-workers","seo_title":"","content":" SummaryA Brief OverviewThis blog post is meant to clarify the output worker changes in Logstash 2.2+, and to discuss how we’ll be improving output workers in the future. Logstash 2.2 introduced the  which provided a sizable performance improvement. It has created some confusion however. Prior to 2.2 there was a notion of Filter Workers (FWs) and Output Workers (OWs). Filter workers executed everything in the  portion of the config, and each plugin in the  portion could use a user definable number of OW threads. In 2.2 these were both replaced with one concept of Pipeline Workers (PWs). Confusingly, you can still set OWs, and they still do something very important. Let’s compare and contrast. The animation below contrasts the threading model between 2.1 and 2.2.To put it a bit more formally:The upshot of this change is that in 2.2+ , which now controls the pipeline worker setting, controls how many concurrent threads can exist for the filter and output stage combined (PWs). The output worker setting controls how many of those threads can simultaneously work on that output. The takeaways here are:Output workers should never be set to a number > the number of pipeline workers, since that is the maximum number that can be simultaneously used. You may need more PWs than cores on your system since multiple PWs can be blocked on I/O in a single OW object. Increasing the number of OWs often provides a tangible benefit but be careful. For some outputs adding more can either slow them down or cause them to break in subtle ways.A more in-depth exploration of the history and the evolution of pipeline architecture is available in this video by Logstash creator Jordan Sissel. We encourage you to watch this! Why Autoscaling Output Workers in Logstash 2.2 was a MistakeThe NG pipeline was released in 2.2 a few months after 2.0 went out. In Logstash 2.0 we had switched the default protocol for the Elasticsearch output to HTTP. One concern here was that while the plugin was nearly identical in performance / resource utilization to the old default of the Elasticsearch Transport protocol it needed more OWs to reach parity. There was a concern that users upgrading from 1.x to 2.x might feel this slowdown due to the defaults and not tune OW settings. We thought it would also be generally good to, by default, set the number of OWs for every plugin to be equal to the number of PWs. We code reviewed each output we maintain for concurrency issues and set the required options on ones that didn’t support > 1 OW, which meant that the pipeline would leave these at a single OW. This turned out to have some...err… adverse effects.The main problem with this approach is that doing post-hoc code reviews for concurrency is hard. We have a truly massive amount of plugins which made the task even harder. We started getting bug reports in the 2.2.x series related to plugins we’d missed that may have been threadsafe in terms of data structures, but not in terms of logic.We made the decision that it was premature to assume that all outputs would behave well in a concurrent environment and to let those bugs shake out in a minor release, so, we reverted it, setting the default back to one OW. So, users who saw speed gains in 2.2.x might have seen speed losses in 2.3.x out of the box. Improving performance is a top priority for us, but correctness of outputs is of even greater importance. Luckily we can achieve both with the new model we’re incorporating into the upcoming Logstash 2.4 and 5.0 series.All this being said, if you’re having performance problems please read the new . Its an easy to follow set of diagnostic and remedial procedures for managing Logstash performance.The FutureThe future is in just . We call these ‘shared’ outputs. Outputs that can’t be parallelized will be the default, and will only support a single instance with synchronized access. The latest version of the ES output (currently only ","locales":"","title":"A History of Logstash Output Workers"}
{"index":{}}
{"author":"Tudor Golubenco","category":"Brewing in Beats","publish_date":"2016-07-25T00:00:00.000Z","url":"/blog/brewing-in-beats-how-filebeat-works","seo_title":"","content":" Welcome to Brewing in Beats! With this series, we're keeping you up to date with all that's new in , from the details of work in progress pull requests to releases and learning resources. In progress: add cgroup information to gosigarAs part of a larger effort to make container monitoring easy with the Beats, (the library we use in Metricbeat) is getting support for the data from . This gives us not only the CPU and memory usage of each container, but also the configured limits for each. This is a feature requested, for example, by our Cloud team to properly report on the cpu usage as a percentage of the configured limit. Metricbeat: Conversion helpers refactoringThe effort started last week to make most of the parsing and conversion work in Metricbeat declarative was this week. The bulk of the work involved in writing and maintaining a Metricbeat module lies in renaming and converting the metrics, so we hope that doing this declaratively will be an important productivity win. The Metricbeat was also . How Filebeat works documentationGetting confused about what a Prospector or what a Harvester is? Don’t know the difference between and ? We have a explaining the high level Filebeat concepts and docs to explain all the relevant configuration options. Path variables available in the configuration fileIt is now possible to refer to (e.g. path.home, path.data, path.config) in the configuration file. The benefit is that it is a bit more clear to write than just the equivalent . Format string support in libbeatIn several planned features (the lookup feature, , dynamic ) it is useful to be able to format strings based on event fields. This adds supports in libbeat for using a format similar to the one used by Logstash. E.g. `pipeline: %{[fields.type]}`. Metricbeat: add PGID to process dataComing from a , this adds the process group id information to the per process docs. Quickly switch Packetbeat protocols on/off in configWe have standardized on using as a way to disable most things in the configuration file. For example, if you want to disable a Packetbeat protocol without commenting out 10 lines of config, add . Want to disable an output? Add . Filebeat cleanupsThe effort () of cleaning up the Filebeat code continued last week with this doing lots of small improvements. Potential Filebeat file descriptor leak fixedA potential FD leak was and . The file descriptors would have gotten garbage collected eventually, but it’s better to close them as soon as they are not needed anymore. Kibana Dashboard for the Metricbeat network statsThis adds a new sample dashboard that can be used with the networking stats collected by the Metricbeat system module. ","locales":"","title":"Brewing in Beats: How Filebeat works"}
{"index":{}}
{"author":"Haley Eshagh","category":"User Stories","publish_date":"2016-07-25T00:00:00.000Z","url":"/blog/security-and-alerting-for-elasticsearch-a-vandis-story-part-2","seo_title":"Security for Elasticsearch in a logging use case at Vandis.","content":" It’s just another day when Shield, Watcher, Marvel, and Beats help Vandis identify and resolve problems before their customers know anything’s amiss.    : Watcher is instrumental to us in terms of notifying us when things are going awry inside of a customer network. When we picked up Elasticsearch, we also picked up PagerDuty, and Watcher is tightly integrated with it. Each one of our customers has a service with PagerDuty and each one of those services has on-call, tier-one engineers, and tier-two and tier-three escalation engineers. Any time there's an event, we get notified, days, nights, weekends, and really that's because and Elasticsearch are doing their jobs.: I think we're at 1.2 billion documents right now, and that’s not even 30 days’ worth of data. We have one customer who has a network operations center (NOC). They have a voice monitoring solution and because of how fast Elasticsearch is parsing the logs and alerting, we actually were on the phone with them before the NOC even called to say that they were down. That has been deeply instrumental to us. : is its own cluster running in our data center, watching our clusters to make sure that life continues smoothly. was a requirement for us. As we build out our deployment, it's a requirement for me to have role-based access control so that my sales team can go in and look at something and not expose data to customers or people who shouldn’t have access. Whereas the engineering team needs to have full access to all the data.For example, a proof-of-concept customer the other day called to say, \"We have a problem. We don't know what the problem is.\" We built a Kibana dashboard for them that focused on the threat and almost immediately, they knew that they had a machine within their facility that had been compromised and was infecting other hosts. We were able to turn that around for them in about five to ten minutes. They responded with, \"Wow, we knew about the value, but now we really see the value.\" : Oh, absolutely. I was actually having this debate with someone on a bus on the way to , going from the hotel to the venue. He was telling me that they had support previously, and ended up not renewing. I replied, \"You might want to look at that again. It's a whole different world.\" That got us talking about how he's gone and built his own software stack to pretty much do exactly what Watcher and Shield do. He was lamenting about how many cycles he's spending on that and I said, \"That’s another reason you might want to look at a subscription. You don't have to support or maintain any of that. That's actually done for you. It's part of the deal.\"It's awesome. Shield was probably the driving force to get my management to sign off on a subscription. Then, almost immediately after the engagement began and Jason was sleeves rolled up, deep in it with us, they turn around and were like, \"Oh, there's actually a lot more value to this.\" : I’ll leave you with one cool story. This is actually one of my favorite Elastic stories.All of my engineers were out at a conference. We were running very thin. We had a customer who called and said, \"We have this crazy issue. We can't figure it out. We have our firewall vendor involved. We have our load-balancer vendor involved. We have our virtualization platform vendor involved.\" He went through the full list of vendors. He said, \"Everyone is just pointing fingers at each other. No one is telling me anything.\"I happened to have the most recent versions of Elasticsearch and running on my laptop. I went into the office, threw Packetbeat on their network and let it sit there for an hour. We sat and drank some coffee, talked about our kids, what we're doing on the weekend. Meanwhile, in the background, I have Kibana up and it's just running the dashboards.The customer came back and asked, \"So, ","locales":"","title":"Security and Alerting for Elasticsearch: A Vandis Story (Part 2)"}
{"index":{}}
{"author":"Igor Kupczyński","category":"Engineering","publish_date":"2016-07-21T00:00:00.000Z","url":"/blog/cloud-enterprise-the-architecture","seo_title":"Elastic Cloud Enterprise - The Architecture","content":" In today's blog post we would like to give you an overview of and its architecture. IntroductionWe'll start by describing what Elastic Cloud Enterprise is and how it differs from our current Software-as-a-Service offering — . Elastic offers a hosted version of the Elastic Stack named Elastic Cloud. It allows you to run Elasticsearch and Kibana in the cloud. No need to set up the infrastructure or work out the management details. Provisioning and scaling clusters is just a few clicks away. Behind the scene the clusters are hosted on AWS. We've had a lot of great traction with this offering to date — companies of various sizes and profiles love the ease of use, security, and having the latest version of Elasticsearch in a monitored and managed cloud environment. Using a public cloud is a bit trickier for large enterprises. Either because enterprises deal with regulated or sensitive data that cannot leave internal networks:  or because of the investments they have already made in existing on premises infrastructure. Yet, the rapid adoption of the Elastic Stack within different lines of business within these enterprises leads to proliferation of separate clusters managed by different teams. And consequently to a zoo of versions, configurations, and usage patterns. Centralizing the management of these clusters can not only enforce uniform versioning, data governance, backup, and user management policies but also reduce the total cost by increasing the hardware utilization. As we can see, an enterprise with a large number of Elasticsearch installations can hugely benefit from a centralized cloud-like approach, such as the one that is present in Elastic Cloud. We happen to have the right solution to address the challenge — we have decided to package our SaaS platform and to make it available as a product. Enter Elastic Cloud Enterprise. ArchitectureElastic Cloud Enterprise shares most of its codebase with our Elastic Cloud SaaS offering. The key tenets of the architecture are: Let us discuss the points in more detail. ServicesWe have avoided the monolithic approach from day one. The service-oriented architecture has various benefits. It allows us to scale the platform easily. It supports the notion of different services having different reliability and performance requirements as each service can be scaled separately. The services have well-defined behavior accessible via an API. This eases the operational management and allows us to change and improve one service without affecting all the other services. Each service is deployed independently in its own Docker container. This, combined with fine-grained permissions to read and write application state, makes the whole installation more secure. Even if a service is compromised, the damage is contained to a single container plus part of the application state. For example, in our cloud service we assume that any Elasticsearch cluster node can be compromised at any time due to a yet undiscovered vulnerability. Even if attackers can compromise a cluster node they cannot break out of their containers and the host that hosts this container. They can write no application state and they can read only the part of the state that is related to their own cluster. The above diagram depicts the core services and the connections between them. ProxyProxy is the first component that a user's request hits. It maps a cluster id passed in the request url to the container to the actual cluster nodes. The association of cluster id to a container is stored in ZooKeeper, but the proxy caches it. That means that even in the rare event of ZooKeeper downtime the platform can still service the requests to existing clusters. The Proxy is intelligent — if you have a highly available cluster, so that your nodes are spread across two or three availability zones, and if one of the zones goes down then the proxy will not route any requests ","locales":"","title":"Cloud Enterprise - The Architecture"}
{"index":{}}
{"author":"Erdem Ekici","category":"User Stories","publish_date":"2016-07-21T00:00:00.000Z","url":"/blog/how-jj-food-service-uses-the-elastic-stack-for-log-analytics-and-search","seo_title":"How the UK's JJ Food Service Uses the Elastic Stack for Log Analytics and Search","content":" Ltd is a B2B food services company founded in 1988. We provide services to about 60,000 individual accounts, with eight branches across the UK supplying all food industry sectors, from restaurants, pubs and hotels to schools, universities and local authorities. The business offers a wide range of fresh, chilled, ambient and frozen products, a selection of high-quality own-label lines, and packaging and cleaning materials, all competitively priced.  OverviewAt JJ Food Service Ltd we use Elastic products in a number of ways, firstly we use the Elastic Stack - , , and -  to power our logging system. Secondly we use Elasticsearch in a more conventional way, for product search. Our web application is modelled in a fairly unconventional way:  in the front end we use , Web Components talk to JSON APIs via vanilla Ajax and WebSocket requests. We maintain a high level of security on our front facing servers, these just act as brokers between the client and middleware which is running with the support of . This layer does all the heavy lifting. Our backend data comes from , an enterprise ERP system for which we are a reference implementation. This is an eagle’s eye view of our web application. Detecting Errors Made EasyIt is important to know what state a system is in from minute to minute. Knowing when errors occur and how they fit into the context of our system has helped us to react swiftly to issues, change our approach and make our system more resilient, for example are customers getting their passwords wrong? By aggregating and analyzing our logs with the Elastic Stack, it gives us clarity. Not only that but it also helps us to resolve customer issues, track journeys simply by search. An interesting detail to how we use the Elastic Stack is that we do not write logs to files, instead we directly pipe log events into a Redis Pub/Sub, Logstash then consumes the Pub/Sub events directly and feeds them to Elasticsearch. It is faster, better and we love it! There will be more developments on that front in the future. For example we are really excited about and and how it could further improve our system visibility.  Creating a Better Search Experience for CustomersLastly product search. We have separated Elasticsearch instances for this and we developed the infrastructure to feed Elasticsearch from the latest products that emerge from our AX ERP system. This is done at least once a day, we use the excellent NodeJS client for this, making bulk inserts and general management of Elasticsearch easy. Our search on our website is exposed as a Microservice, via Seneca. What this gives us is a really nice decoupling of Elasticsearch and our server and its logic. We can insert our own logic backed by a reliable client such as elasticsearch.js Node client. Service Oriented Architecture done right.  Elasticsearch allowed us to model our Microservices with great flexibility, allowing us to create phased searches that will take into account both exact product matches, store filtering (as our products can be branch specific) and fuzziness.  All of this helps us return the right products to our customers. For example we have greatly decreased the number of zero results that we used to have, by applying fuzziness intelligently, regular expression searches and mutli-field term searches in a layered way will usually return something relevant, it’s a great improvement. Again our work is never done and we will continue to explore the Elastic Stack and its capabilities into the future. , is taking the lead on our cutting edge frontend web development. , a recent addition to our team who was instrumental in testing our Elasticsearch implementation. , is a Senior Engineer at JJ Food Service with 20 years of experience in web development. His passion has always been for high pe","locales":"","title":"How JJ Food Service Uses the Elastic Stack for Log Analytics and Search"}
{"index":{}}
{"author":"Russ Cam","category":"Engineering","publish_date":"2016-07-20T00:00:00.000Z","url":"/blog/spinning-up-a-cluster-with-elastics-azure-marketplace-template","seo_title":"","content":" Last week saw us push out , delivering more features and choices than ever to configure an Elasticsearch cluster deployment within Azure, in a way to suit a multitude of needs. We want to take this opportunity to highlight some of the options available within our offering to demonstrate just how easy it is to get up and running.  For those unfamiliar with ARM templates, there are essentially two components to them:  a UI definition template that defines a step-by-step wizard for gathering all of the inputs required for a deployment and emitting a set of key/value pairs as output, and a deployment template that takes a set of key/value pairs as input and defines all of the resources to create and configure within Azure. It is possible to use the deployment template independently of the UI definition template, for example, with , but the beauty of the UI definition is that not only is it integrated into the Azure portal, it can also take advantage of querying existing resources within an Azure subscription to aid in filling out each step with valid values. Before we dive into the nitty gritty, we’d like to take a moment to pay homage to the from which Elastic’s was forked:  The quickstart templates are a collection of community-contributed templates for provisioning a multitude of different resources and applications on Azure, and Elastic has been happy to contribute features back to the template to continually improve it. Getting startedWhen you want to use the Elastic Stack with existing services running on Azure, it can make sense to also deploy the components to Azure as well, not only to have everything managed from a single dashboard, but also to mitigate egress costs associated with moving data out of Azure data centers. Finding the template on the Azure Marketplace to start a deployment is a simple affair:  simply choose + New and search for Elasticsearch to find the “Elasticsearch and Kibana” template published by Elastic (that’s us!). The template is a Bring-Your-Own-License (BYOL) model:  that is, the template deploys with a 30-day trial license of our commercial X-Pack offering, giving you access to all of the goodness that come with it, including monitoring, security, alerting and graph capabilities. Then, once the trial license expires, you can install your own license to continue using the critical commercial enhancements, or simply uninstall them (although we always recommend having some form of access control on a publicly available cluster!). Deploy into an existing Virtual NetworkSince the initial launch of the template back in December 2015, one of the features most requested has been to allow deployment of a cluster into an existing virtual network and, with the latest template version, we’re pleased to announce this request is now a reality. Whilst configuring the Elasticsearch version and name of the cluster, one can also specify whether to set up a new virtual network in the resource group into which all resources will be created or alternatively, use an existing virtual network within the same subscription and location as the current resource group being created. This is particularly useful in situations where you may already have resources deployed in another resource group, for example, a farm of servers running your website, and wish to make a cluster available on the same network, possibly same subnet, for those web servers to make requests to. Previous incarnations of the template always set up a new virtual network as part of the deployment, deploying master nodes into one subnet and data and client nodes into another subnet. Now, all nodes are deployed into one subnet. One to 50 data nodes (and more!)The portal UI provides the choice to configure anywhere from one to 50 data nodes, , defaulting to three data nodes with three dedicated master nodes. Depending on your use case, it is possible to forgo dedi","locales":"","title":"Spinning up a cluster with Elastic's Azure Marketplace template"}
{"index":{}}
{"author":"Suyog Rao","category":"The Logstash Lines","publish_date":"2016-07-19T00:00:00.000Z","url":"/blog/logstash-lines-2016-07-19","seo_title":"","content":" Welcome back to The Logstash Lines! In these weekly posts, we'll share the latest happenings in the world of Logstash and its ecosystem.Nice progress again this week. On Friday, we were able to plug the various components (producer, disk-backed store, consumers), write test data to disk, successfully read it on the other side and acknowledge it. We now have a meta to track progress, and have a  in Logstash GH repo.Updated all the plugins to use a relaxed version constraint. This allows plugins written for LS 5.0 to be used in the upcoming 2.4 release, so we don't have to maintain 2 separate branches.Work in progress to add support for specifying a as parameters to these APIs. Just like Elasticsearch. For example, will filter and show only jvm and process information. Other to make sure these APIs are implemented as per the specs, and are consistent with ES APIs.More this week surfaced a couple of issues. Tal found and fixed a memory leak when using the compression feature. He also changed the assembly process which packages Java, JRuby and its dependencies (jar libs) to use Gradle instead of jar_dependencies. We released a beta version of this plugin, which can be installed viabin/logstash-plugins install --version 3.1.0.beta1 logstash-input-beatsAdded regex patterns in topics, so you can subscribe to multiple ones. Thanks to Anup Chatterjee () for this contribution. Added extra Kafka based metadata (size, partition, etc) for EventsAaron Mildenstein has been on a speaking marathon this week! He presented a 3 hour crash course on the Elastic stack at the . His other talks were on writing custom LS plugins, managing and performance tuning ES + LS! I'm sure his swag bag was full as well :) ","locales":"","title":"Logstash Lines: Persistent Queue progress, more monitoring API fixes"}
{"index":{}}
{"author":"Clinton Gormley","category":"Engineering","publish_date":"2016-07-19T00:00:00.000Z","url":"/blog/running-site-plugins-with-elasticsearch-5-0","seo_title":"Running site plugins with Elasticsearch 5.0","content":" Way back in Elasticsearch 0.17, Elasticsearch gained the ability to serve static web pages, and were born. Site plugins allowed users to write Javascript applications which provide graphical user interfaces to Elasticsearch.This spawned a flurry of development by Elasticsearch users producing plugins for monitoring Elasticsearch stats, for index management, segment merging visualisations, analyser debugging, clustering, and more.The two most popular site plugins today are and , both of which combine monitoring with index management.All good so far. Now for the bad news… Site plugins are not supported in Elasticsearch 5.0Why are we removing this popular feature from Elasticsearch? Elasticsearch is not designed to be a web server. Serving static files was just a hack that was easy to add on top of the HTTP REST interface that Elasticsearch does provide. What’s the harm in serving static files? Well, it turns out that just serving static files can be harmful. ever discovered in Elasticsearch had to do with site plugins. That’s significant, especially for a non-essential feature. In contrast, two of the other vulnerabilities were due to dynamic scripting, and we wrote a to solve that problem! Elasticsearch now runs under the Java Security Manager. We have locked down the privileges that Elasticsearch core requires to run to the bare minimum. We are moving functionality out of core and into modules to further restrict privilege escalation and file access to the smallest chunk of code possible. We do all of this with the aim of restricting the exploit possibilities open to any hacker who finds a zero day vulnerability. Running a web server for a non-essential feature is not consistent with this goal. On top of that, hosting web applications on Elasticsearch encourages the bad practice of exposing Elasticsearch to the Internet, while it should be running in an isolated, more secure network. Running site plugins with Elasticsearch 5.0While many popular site plugins like , (soon to be replaced by ), and already include instructions for running outside of Elasticsearch, it’s easy to do on your own as well. A site plugin consists of static HTML, Javascript, CSS, and image files, which can be served by the web server of your choice, even (for local use) Python’s SimpleHTTPServer: cd my_plugin/ python -m SimpleHTTPServer 8000 Because site plugins make requests directly to Elasticsearch from the user’s browser, a little configuration is required to instruct Elasticsearch to allow . For example, you could add the following to the config file: http.cors.enabled: true http.cors.allow-origin: /https?:\\/\\/localhost(:[0-9]+)?/ The setting should be a regular expression that matches the address of the web server hosting the site plugin. You can read more about the CORS settings in the . If your Elasticsearch server is using the Security feature in (the replacement for Shield in 5.0), then you will also need to add the following settings: http.cors.allow-credentials: true http.cors.allow-headers: X-Requested-With,X-Auth-Token,Content-Type, Content-Length, Authorization Beyond site pluginsIf you are a plugin author and would like to move beyond the restrictions of writing an application with static web files then consider developing a instead. Kibana comes with a web server and plugins can include server-side functionality. For instance, all calls to Elasticsearch could be made directly from Kibana’s backend server instead of from the user’s browser. There is even a to help you get started with your own plugin, and a describing the process. As always, helpful Kibana devs are and to help with any questions. ","locales":"de-de,fr-fr,ja-jp,ko-kr","title":"Running site plugins with Elasticsearch 5.0"}
{"index":{}}
{"author":"Michael McCandless","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2016-07-18T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2016-07-18","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top News I just published “How we reindexed 36 billions documents in 5 days within the same Elasticsearch cluster” — Fred de Villamil ✌︎ (@fdevillamil) Elasticsearch Core Changes in 2.x: Changes in master: Ongoing: Apache Lucene Watch This Space , where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2016-07-18"}
{"index":{}}
{"author":"Court Ewing","category":"Releases","publish_date":"2016-07-15T00:00:00.000Z","url":"/blog/kibana-4-5-3-and-4-1-10","seo_title":"Kibana 4.5.3 and 4.1.10 Released","content":" Today we’re shipping Kibana versions 4.5.3 and 4.1.10, which use Elastic’s new tile service by default. The tile map settings are also now configurable, so users can use other leaflet-compatible tile services. These releases are available right now on the page. Elastic Tile Service The Elastic Tile Service is brand new, is the default tile service for Kibana, and requires no configuration to use. That said, it does currently have two key limitations that we want to be upfront about. First, the current service only supports zooming up to level 8. With this zoom level, users will be able to see major cities and lakes spread across small countries and US states. We’d love to provide additional zoom levels, but we need to evaluate usage first as each additional zoom level adds considerable bandwidth and scaling requirements for the service. If we do increase the zoom levels in the service, users will be able to take advantage of the additional zoom capabilities with only a configuration change. Second, the current tiles are less detailed than the old tiles. Again, this is something we’d like to improve upon, and if we do, users will get the updated tiles without having to upgrade Kibana. It is also worth mentioning that the Elastic Tile Service is not meant to be a general tile server solution outside of Elastic applications (see ). Custom tile map services If robust map details and zoom capabilities are more your jam, then you can configure Kibana to use other tile service providers instead: For 4.5.3: For 4.1.10: How we got here Since the beginning of time, Kibana has used MapQuest as its tile service provider for map visualizations. MapQuest’s tile service is excellent, and their permissive usage requirements meant that most Kibana users could have beautiful map visualizations without any cost or configuration. On June 15th, MapQuest that they’d be discontinuing the direct tile access API that Kibana was leveraging. On Monday, July 11th, the service was discontinued, and maps in Kibana broke. This was a huge blunder on our part, and it resulted in broken maps across the entire Kibana ecosystem. We sincerely apologize to all of our users, and we promise to do better. We have just begun our internal postmortem process, and that will yield more concrete steps to prevent something like this from happening in the future. For now, moving entirely away from default third party services while also making the provider configurable in the event of any future service outage are our immediate steps. Wrapping up These releases are immediately available on our page. If you have any questions about these changes, please do not hesitate to reach out to us on our , , or . ","locales":"de-de,fr-fr,ja-jp,ko-kr","title":"Kibana 4.5.3 and 4.1.10 released to fix tile map visualizations"}
{"index":{}}
{"author":"Jongmin Kim (KR)","category":"","publish_date":"2016-07-14T00:00:00.000Z","url":"/blog/build-your-own-beat","seo_title":"","content":" Beats is the platform for building lightweight, open source data shippers that send all kinds of data to Elasticsearch for being later analyzed. We have Packetbeat for monitoring the network traffic exchanged between your servers, Filebeat for getting the logs from your servers and the newly released Metricbeat that periodically fetches metrics from external systems. If you need to collect other custom data, you can easily build your own Beat based on the libbeat framework.  There are already 25+ made by the community. We provide the Beat Generator package that helps you create your own Beat. In this blog post, you will see how to create your own Beat by using the Beat Generator. The Beat that we create today for practice is . lsbeat indexes informations of files and directories, similar with the Unix command . This article is based on Unix, so if you are Windows or other OS user, follow the instructions which fits with your OS. Step 1 - Setup your Golang Environment Beats are written in Golang. To create and develop a Beat, Golang must be installed on your machine. Follow the guide here to . Currently Beats require at least Golang 1.6. Make sure you properly setup your variable. Let's see the code that we will use for Lsbeat. This is a simple Golang program that receives a directory as a command line argument and lists all files and subdirectories under this directory. package main import ( \"fmt\" \"io/ioutil\" \"os\" ) func main() { //apply run path \".\" without argument. if len(os.Args) == 1 { listDir(\".\") } else { listDir(os.Args[1]) } } func listDir(dirFile string) { files, _ := ioutil.ReadDir(dirFile) for _, f := range files { t := f.ModTime() fmt.Println(f.Name(), dirFile+\"/\"+f.Name(), f.IsDir(), t, f.Size()) if f.IsDir() { listDir(dirFile + \"/\" + f.Name()) } } } We will reuse the code of the function. Step 2 - Generate To generate our own beat we use the . First you must install . Check out the installation guide . After having installed cookiecutter, we must decide on a name for the Beat. The name must be one word all lowercase. In this example we are using . To create the Beat skeleton, you should get the Beats generator package, available in the repository. Once you installed , you can download the Beats generator package using command. Once you run the command, all source files will be downloaded under the path. $ go get github.com/elastic/beats To work on a stable branch, check out the specific branch. $ cd $GOPATH/src/github.com/elastic/beats $ git checkout 5.1 Now create and move to your own repository under , and run cookiecutter with the Beat Generator path. $ cd $GOPATH/src/github.com/{user} $ cookiecutter $GOPATH/src/github.com/elastic/beats/generate/beat Cookiecutter will ask you several questions. For your project_name enter lsbeat, for github_name - your github id. The next two questions with beat and beat_path should already be automatically set correctly. For the last one your can insert your Firstname Lastname. project_name [Examplebeat]: lsbeat github_name [your-github-name]: {username} beat [lsbeat]: beat_path [github.com/{github id}]: full_name [Firstname Lastname]: {Full Name} This should now have created a directory lsbeat inside our folder with several files. Let’s change to this directory and list up files automatically created. $ cd lsbeat $ tree . ├── CONTRIBUTING.md ├── LICENSE ├── Makefile ├── README.md ├── beater │ └── lsbeat.go ├── config │ ├── config.go │ └── config_test.go ├── dev-tools │ └── packer │ ├── Makefile │ ├── beats │ │ └── lsbeat.yml │ └── version.yml ├── docs │ └── index.asciidoc ├── etc │ ├── beat.yml │ └── fields.yml ├── glide.yaml ├── lsbeat.template.json ├── main.go ├── main_test.go └── tests └── system ├── config │ └── lsbeat.yml.j2 ├── lsbeat.py ├── ","locales":"de-de,fr-fr,ko-kr,zh-chs","title":"Build your own Beat"}
{"index":{}}
{"author":"Haley Eshagh","category":"User Stories","publish_date":"2016-07-14T00:00:00.000Z","url":"/blog/from-poc-to-prod-with-elasticsearch-a-vandis-story-part-1","seo_title":"From POC to Production with Elasticsearch: A Vandis Story","content":" Eight days. That’s how much time Vandis’ Director of Engineering had to go from proof-of-concept to production. There was a job to do — and he wasn’t alone. It's definitely a new support experience for me. The Elastic support model has done a lot in terms of helping me manage my cycles. It's not a I-have-to-call-in-and-speak-to-someone-new-every-time model:  I can call Jason or send him an email with any questions because he knows my system so well. Similarly, Jason also will call us just to say, \"Hey, I was thinking about something. You should look at this as your next option.\" It's been a truly pleasant experience. I jokingly tell my CEO that he's got half an employee he doesn't know about. I like to take a proactive approach to my responses to Ryan and his team. I think responses should be applicable directly to their environment and not just a broad stroke like, \"Here are some things people do.\" I like to provide examples that apply specifically to the Vandis environment, that they can take and use in production.Ryan and his team are ideal customers because we're always discussing things. They trust us, we trust them, and we just have this mutual relationship that carries on through all of our support interactions.I've been supporting open source software for seventeen years. I've managed product development in my past. Every Elastic support engineer is high caliber. We don't sit here and take inbound tier-one support calls. I know Ryan, I know the Vandis use case, I know the environment. I feel like I'm a branch of his team. That's how we do things and that's my mentality when I approach Vandis.To give you a little bit of history, our proof-of-concept cluster has been running since late September. We got funding and had a go-live date within my company for January 1. On December 23, we signed the contract, while I was at my in-laws for Christmas. December 26, 27, 28, 29, 30, Jason worked very closely with me in terms of moving on our proof-of-concept environment into a full-production environment. When a new cluster spun up and started to move data, I was working in my in-laws’ basement and my wife is sleeping next to me in bed, and I just stood up and yelled, \"Yes!\"I woke her up. She asked, \"What? What?\"I said, \"Nothing. The nerdiest thing just happened, don't worry about it.\"I could not have done it without Jason and the support team that I had, especially with the tight turnaround. My hat’s off to everyone at Elastic. It has been an absolute pleasure to work with them. With any of the teams I have one-click access to our engineers, and that translates to one- or maybe two-click access for Ryan and Vandis. Elastic support is part of the Elastic engineering team. We all have the same goal. We want our customers to be successful. There's a lot of comfort in that. Before I had support, I was a lurker on IRC. Yes, all those guys and ladies do a great job of being available there and answering questions. They take a lot of pride in the platform, but at the end of the day, we're all busy and they might not have the time to be there.Having the ability to just shoot a question to Jason that's like, \"I'm going to throw this way out there. You mind just looking it up?\" And having him come back and say, \"Hey man, listen, I talked to the guy who actually wrote the code and it can do that. They want to know your use case a little more and walk through that.\"When I got back from , I told my boss that one of the great things about attending is that I feel like I'm not waiting for a product to develop for the features I want to see. I have more of a direct line within the company to talk about what's important to me. It might not always get done, but I know that I have someone on the inside who's waving my flag and saying, \"Listen, this is what my customer is asking for.” And I know that voice ","locales":"","title":"From POC to Prod with Elasticsearch: A Vandis Story (Part 1)"}
{"index":{}}
{"author":"Court Ewing","category":"Releases","publish_date":"2016-07-07T00:00:00.000Z","url":"/blog/kibana-4-5-2-and-4-1-9","seo_title":"Kibana 4.5.2 and 4.1.9 Released","content":" Today, we’re releasing Kibana versions 4.5.2 and 4.1.9, which have bumped the bundled version of node.js to address a low severity buffer overflow upstream. As with any security fix, we recommend that users upgrade as soon as possible. You can grab the latest versions from the page. ","locales":"","title":"Kibana 4.5.2 and 4.1.9 Released"}
{"index":{}}
{"author":"Tudor Golubenco","category":"Brewing in Beats","publish_date":"2016-07-12T00:00:00.000Z","url":"/blog/brewing-in-beats-set-configuration-options-CLI","seo_title":"","content":" Welcome to Brewing in Beats! With this series, we're keeping you up to date with all that's new in , from the details of work in progress pull requests to releases and learning resources. New community Beat: SaltbeatSalt is an open source automation framework, similar to Ansible for example. is a forwarder for messages from the salt master event bus to Logstash and Elasticsearch. Set configuration options via the CLIThe Beats are , similar to the Elaticsearch -E option, that allows overwriting configuration options from the command line. For example, you can now quickly enable the console output by adding . Combine processor conditions with and/or/notThe Beats processors (a.k.a generic filters), which are new in 5.0, now support . This means you can express complex conditions for filtering out the interesting events. For example, to drop all events that have codes 200 or 404 on a given URL: drop_event.when: and: - equals.request_url: “/test” - or: - equals.http.code: 404 - equals.http.code: 200 Logging verbosity cleanup + more internal metricsWe started with a (thanks Lee) that highlighted how sometimes we fail to provide the operator with important information and started doing a to rethink what we log and with what verbosity level. Part of this, we the numbers of WARN and INFO messages that we produce, so we can switch the default log level . To compensate, we replaced the most verbose warnings/infos with . These internal metrics are dumped periodically in logs and will provide data for the future central monitoring system. Environment variable expansion based on go-ucfgThe previous implementation for environment variable expansion in our configuration files was done via textual replacement before parsing. This was easy and elegant to implement, but could lead issues when the sequence shows up accidentally (e.g. in a password field). By to our new , we can be more selective on where we accept variable expansions. This also comes with a breaking change: we used to replace unresolved variables with an empty string, the shell way. Now unresolved variables result in a configuration error, which should help catching configuration bugs earlier. Specify multiple configuration filesAnother improvement that we owe to go-ucfg is that you can now specify multiple configuration files by . You can use this, for example, for setting defaults in a base configuration file, and overwrite settings via local configs. Filebeat fix for very quick file rotationIn case a file was renamed after the state was read but before the file was opened by the harvester it could happen that the wrong file was opened. This lead to the issue, that the wrong file was read including reporting the wrong state for a file. In general this was rather unlikely to happen, but can happen when scan_frequency is very low and number of files rotated is very high. ","locales":"","title":"Brewing in Beats: Set configuration options from the CLI"}
{"index":{}}
{"author":"Suyog Rao","category":"The Logstash Lines","publish_date":"2016-07-12T00:00:00.000Z","url":"/blog/logstash-lines-2016-07-11","seo_title":"","content":" Welcome back to The Logstash Lines! In these weekly posts, we'll share the latest happenings in the world of Logstash and its ecosystem., which an important security issues in Elasticsearch Output.Persistence QueuesGood progress on Persistence Queues feature this week. Implementation is in full swing now. We added the first end-to-end tests that integrate all the building blocks classes — low level PageIO, in-memory checkpointing etc with the Queue interface. We've also introduced a Queue client class that handles batch of Events as its taken out of the queue by the consumer, manages its acks.Improved pipeline error handling (5.0)Added a configurable flag (pipleine.continue_on_error ) in Logstash to continue processing when there is an error in the filter stage. Sometimes there is a badly written Ruby filter, or a particular data or bug in plugin which triggers an error. For some users the right thing to do here is to crash Logstash so an operator is forced to diagnose and fix the issue. For others, they generally just want it to log an error and keep on running. One long term solution is to have a dead letter queue so bad events can be moved from the main pipeline. In the meanwhile, having this continue option will help some users.Performance Tuning Guide (2.x)We are putting together a performance tuning guide for Logstash 2.x. Until 5.0 GA we will not have any monitoring APIs, so debugging performance issues in 2.x can be hard. Especially since we changed the pipeline work unit (-w) to include both filters and outputs (in 2.2) we've had confusions about how to tune Logstash. In addition, to help with this, we are crafting a blog post to explain in detail the pipeline evolution from pre 2.0 to 2.3 timeframe.Prepping for a 2.4 ReleaseLS 2.4 will be released with ES 2.4 and the rest of the stack. We did some ground work on 2.4 so that plugins developed in 5.x can be installed on top of 2.4. Dependency management in LS plugins is hard. And this involved some back and forth discussion before settling on a solution. Now all plugins will be mass updated to reflect the new version constraints.Others: ","locales":"","title":"Logstash Lines: Persistent Queues, 2.3.4 Release, Pipeline Error Handling"}
{"index":{}}
{"author":"James Baiera","category":"Releases","publish_date":"2016-07-07T00:00:00.000Z","url":"/blog/es-hadoop-2-3-3-released","seo_title":"","content":" I am pleased to announce the release of Elasticsearch for Apache Hadoop (aka ES-Hadoop) . ES-Hadoop 2.3.3 is primarily a bug-fix release, including extended support for interpreting boolean values from Elasticsearch, backporting support for mixed value maps in Pig, and support for the latest and greatest Elasticsearch version . Feedback Looking forward to hearing your feedback on this ! Drop us a line on , Twitter (), , or the . ","locales":"","title":"Elasticsearch for Apache Hadoop 2.3.3 released"}
{"index":{}}
{"author":"Clinton Gormley","category":"Engineering","publish_date":"2016-07-12T00:00:00.000Z","url":"/blog/managing-time-based-indices-efficiently","seo_title":"And the big one said \"Rollover\" — Managing Elasticsearch time-based indices efficiently","content":" Anybody who uses Elasticsearch for indexing time-based data such as log events is accustomed to the index-per-day pattern: use an index name derived from the timestamp of the logging event rounded to the nearest day, and new indices pop into existence as soon as they are required. The definition of the new index can be controlled ahead of time using . This is an easy pattern to understand and implement, but it glosses over some of the complexities of index management such as the following: In this blog post I’m going to introduce the new , and the APIs which support it, which is a simpler, more efficient way of managing time-based indices. ","locales":"","title":"And the big one said \"Rollover\" — Managing Elasticsearch time-based indices efficiently"}
{"index":{}}
{"author":"Michael McCandless","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2016-07-11T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2016-07-11","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top News Just wrote a new blogpost about the new java RestClient. The post shows how to use the client. — Jettro Coenradie (@jettroCoenradie) Elasticsearch Core Apache Lucene Watch This Space , where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2016-07-11"}
{"index":{}}
{"author":"Suyog Rao","category":"Releases","publish_date":"2016-07-07T00:00:00.000Z","url":"/blog/logstash-2-3-4-released","seo_title":"","content":" We are pleased to announce 2.3.4, a bug fix release for Logstash. This release fixes an important security vulnerability with Elasticsearch Output, so we advice our users to read the note below and upgrade to 2.3.4. You can get this release on our page, and the changelog is .Prior to version 2.3.4, Elasticsearch Output plugin would log to file HTTP authorization headers which could contain sensitive information. Users who secure communication from Logstash to Elasticsearch via Basic Auth using or other systems are advised to upgrade to this version. We have created Elastic Security Advisory ESA-2016-02 for this vulnerability and updated our with details.  ","locales":"","title":"Logstash 2.3.4 released"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2016-07-04T00:00:00.000Z","url":"/blog/brewing-in-beats-introduce-processors","seo_title":"Brewing in Beats: Introduce processors","content":" Welcome to ! With this series, we're keeping you up to date with all that's new in , from the details of work in progress pull requests to releases and learning resources. Metricbeat: Use half floatsElasticsearch added support for half floats in the 5.0.0-alpha4 release. The Beats make use of the for the percentage values (a float value between 0 and 1) and for the other float values that are expected to be small. Filebeat: Add more configuration options to help with files managementThere are couple of added in Filebeat to deal with corner cases: Topbeat 1.x: Fix high values of CPU usageThere is a bug in the Topbeat 1.2.x series that reports high values of the CPU usage on Windows. The will be available in the next release of Topbeat 1.x and is already fixed in the 5.0 alphas. libbeat: Rename filters with processorsThe configuration option helps you reduce the fields from the exported event. To align with the naming in the Ingest Node and Logstash, we are the configuration option to . In addition, we are planning to extend its functionality and use it to enhance the event with additional fields. This change breaks the compatibility with the previous version of the Beats. All Beats: Update the definition of the condition in processorWhile renaming with and extending its functionality, we decided to use the keyword to introduce the condition associated to the processor in order to differentiate its borders from the action associated with the processor. processors: - include_fields: equals: proc.pid: 3455 fields: [\"proc.cpu\", \"proc.mem\"] processors: - include_fields: when: equals: proc.pid: 3455 fields: [\"proc.cpu\", \"proc.mem\"] Metricbeat: the system module now compiles on FreeBSDThe system module of Metricbeat is using the library to gather system statistics like the CPU and memory usage, disk usage, file system usage. Thanks to , the gosigar library has support for FreeBSD. ","locales":"","title":"Brewing in Beats: Introduce processors"}
{"index":{}}
{"author":"Dara Gies","category":"Engineering","publish_date":"2016-07-13T00:00:00.000Z","url":"/blog/using-beats-with-elasticsearch-on-aws","seo_title":"Using Beats with Amazon AWS","content":" Ever-increasing quantities and varieties of data are being created and captured. Application, system and user behavior logs, network packets, sensor metrics... you name it. The availability of these data drives a diverse and vast set of use cases ranging from intrusion detection to user behavior analysis, network monitoring, machine monitoring, and remote sensor monitoring, just to name a few. are open source, purpose-built, lightweight, and efficient agents that acquire and feed data natively to Elasticsearch. Optionally, Beats can feed data to Logstash for further refinement before being forwarded to Elasticsearch. As part of the Elastic Stack, Beats is a architecture, enabling the capture and transmission of measurements and other data from remote sources to Elasticsearch for analysis, aggregation, and search. Anywhere there is change, such as production and consumption, growth and demise, acceleration and deceleration, data has significance. GPS-driven tractors capture soil depths, soil fertility, and ground temperatures, all of which influence agricultural strategy. Ocean buoys record wave amplitudes that may help to warn of impending catastrophes or inform of welcome beach conditions. Automobiles capture thousands of metrics ranging from tire pressure to average speed to fuel consumption that facilitate an understanding of a vehicle's abilities and failures, as well as the owner's driving behaviors. Network sensors capture machine communication messages that help with understanding usage patterns that drive security and infrastructure policies. In the previous article we configured a three-node Elasticsearch cluster on AWS EC2. It was fairly straightforward and didn't take much time or effort. We will expand on the example and configure Beats to feed the Elasticsearch cluster running on AWS EC2 instances. This will demonstrate how simple it is to configure a centralized machine resource monitoring solution. First, we'll provide an overview of Beats, review available Beats and the environments in which they run - Part I. Then we'll do a step-by-step example installing, configuring, and verifying Beats output - Part II. Part I - Beats Overview are purpose-built lightweight data shippers, or agents, that run on remote machines and feed Elasticsearch instances. Beats make it easy to get data into Elasticsearch. Beats are available on a number of operating systems such as Debian, Redhat, Linux and Mac. There are several available including Filebeat, Metricbeat, Packetbeat, Winlogbeat and Topbeat. Each Beat has a specific purpose or multiple purposes that are logically related, allowing each Beat to focus on its specific task and do it well. tails logs and can ship data to Logstash for further refinement, or directly to Elasticsearch for analysis and search. Filebeat can be installed on any machine that has applications that generate log data, such as a database or application server. (Alpha) captures operating system metrics as services such as Apache web server and Redis. Metricbeat does everything Topbeat does and much more in that it captures operating system metrics such as per-process CPU, memory, and storage use as well as common application messages, such as web servers. Metricbeat will replace Topbeat, so you might consider using Metricbeat for development. captures web, database, and other network protocols, enabling Kibana real-time analytics. Packetbeat is extensible, enabling the addition of new protocols, metrics and analytics. captures Windows event log system, application and security data, enabling monitoring of Windows machines. captures per-process memory, CPU and disk usage statistics. Topbeat can be installed across a set of machines and indexed into a central Elasticsearch cluster, enabling centralized machine resource monitoring. Topbeat is being replaced by Metricbeat, currently in Alpha, and the change is discussed in this . If you have metric data not address","locales":"","title":"Using Beats with Amazon AWS"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2016-07-07T00:00:00.000Z","url":"/blog/elasticsearch-2-3-4-released","seo_title":"Elasticsearch 2.3.4 released","content":" Today we are pleased to announce the bug fix release of based on . All users of 2.x are advised to upgrade. This release is already available on , our Elasticsearch-as-a-service platform. Latest stable release: Full details of the changes in this release are available in the release notes listed above, but some of the more important changes are mentioned below: ","locales":"","title":"Elasticsearch 2.3.4 released"}
{"index":{}}
{"author":"Jeff Fried","category":"User Stories","publish_date":"2016-07-06T00:00:00.000Z","url":"/blog/betting-on-elasticsearch-for-enterprise-wide-search-with-ba-insight","seo_title":"Betting on Elasticsearch for Enterprise-wide Search with BA Insight","content":" Combining Elasticsearch with SharePointElasticsearch developers may not be too familiar with, but it’s ubiquitous. More than 200,000 organizations use SharePoint today, often as their UI of choice for employee-facing applications, because it is easy for administrators and end users to tailor. BA Insight has been working in the SharePoint ecosystem for 10 years, and has a very tight relationship with Microsoft, so it’s home base for us. When we started working with Elasticsearch we focused first on making a great solution for enterprise search with Elasticsearch as the underlying search engine and SharePoint as the UI. This screenshot is an example of a typical application, designed for investment managers at financial services companies. We’re told by these users that it has become the first thing people check at the beginning of the day, and something they use constantly throughout the day. We’ve set up specific search screens for these financial analysts that encapsulates areas of interest and includes search tabs and embedded Kibana visualizations to drill down into fund performance. It includes: Elasticsearch vs. SharePoint’s built-in searchWe’ve traditionally worked with the search engine included with SharePoint, and still do a lot with it. However, there are several things that we do now that we couldn’t do before we started using Elasticsearch. Most notably: What does the future look like?This is an exciting time. Elastic is growing incredibly fast as a company, and SharePoint is also in a huge resurgence with lots of new releases, so we are sort of holding on to a rocket ship with each hand. I’m particularly excited about: We have also got some cool applications, and interesting case studies in the pipeline. And of course in the near term, we are excited about helping customers adopt and succeed with our products, and with Elasticsearch. (@jefffried) is a longstanding search nerd and CTO of BA Insight. ","locales":"","title":"Betting on Elasticsearch for Enterprise-wide Search with BA Insight"}
{"index":{}}
{"author":"Michael McCandless","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2016-07-04T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2016-07-04","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top NewsI love the new \"How To... Tune for indexing speed / search speed / disk usage\" section in the docs — Clinton Gormley (@clintongormley) Elasticsearch Core Apache LuceneThe Apache Lucene update will be back next week.Watch This Space, where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2016-07-04"}
{"index":{}}
{"author":"Andy Lowry","category":"User Stories","publish_date":"2016-07-04T00:00:00.000Z","url":"/blog/finding-you-the-best-hotel-at-laterooms-com-with-elasticsearch","seo_title":"Finding You The Best Hotel at LateRooms.com With Elasticsearch","content":" At we use Elasticsearch to solve a number of problems. We regularly talk about how we use it for logging, but we don't talk often about how we use it for search. If you visit LateRooms.com the search bar is very prominent on the site, it's our main interface for our customers to find the hotel they want. So it's important we get it right. When you type into the search bar, the autocomplete feature kicks in which is powered by Elasticsearch. Furthermore the search function itself is also powered by Elasticsearch. To quote one of our developers, when asked why we use Elasticsearch: “we spiked it, it met our requirementswe tried it further, it never failed us,we adopted it” AutocompleteLast year we completed a project to rewrite our existing autocomplete feature. Our existing system was slow and the results were not great. We chose to use the Elasticsearch Completion Suggester feature, as we were convinced this would give us the performance we needed. This allowed us to load our destination data and all our hotels into a single index. Each document in the index has a suggest field which we use to match the input text, display text which is what you see in the drop down, and some metadata about the entry. This is our schema: { \"mappings\": { \"destination\": { \"properties\": { \"name\": { \"type\": \"string\" }, \"suggest\": { \"max_input_length\": 50, \"payloads\": true, \"analyzer\": \"standard\", \"preserve_position_increments\": true, \"type\": \"completion\", \"preserve_separators\": true } } } } } Matching is done on the suggest field. Indexing is done on every permutation on the first 5 words of the search text. We needed to do this to allow matches where the words are out of order, so “Manchester City Centre” and “City Centre Manchester” would both match the same results. We also apply stop words for words that are common to hotel names and destinations. This solution resulted in an index of approximately 1GB in size, We have a single cluster for all search and autocomplete with 3 machines all with 24 cores and 80GB RAM. With this, we are getting response times averaging around 15ms. SearchWe have a Search API which powers the data on our Search Results pages on the website, our apps and a few other internal tools. Our existing implementation is based around SQL Server and this has served us well for many years, but we needed something more flexible and better targeted to our problems. So we chose to move most parts of our implementation to Elasticsearch. It is a work in progress and currently we have a hybrid of the 2 systems, however we are focussing on moving the functionality to Elasticsearch where it gives us the most value. Destination SearchWe have 2 main indexes, one for destinations and the other one for hotels. The hotels index includes relatively simple information about the hotel including name, address, facilities and its geo location. Here is the schema for hotel: { \"settings\": { \"analysis\": { \"analyzer\": { \"stopwords_analyzer\": { \"type\": \"standard\", \"stopwords\": [ \"hotel\", \"the\", \"and\", \"in\", \"hotels\" ] } } } }, \"mappings\": { \"hotel\": { \"properties\": { \"id\": { \"type\": \"long\" }, \"name\": { \"type\": \"string\" }, \"typeId\": { \"type\": \"long\" }, \"location\": { \"type\": \"geo_shape\" }, \"address\": { \"type\": \"string\" }, \"brand\": { \"type\": \"string\" }, \"postcode\": { \"type\": \"string\" } } } } } Destinations are places such as cities, towns, counties, points of interest, train stations, airports - basically anywhere someone might be looking for a hotel. Each destination includes a name, a geoshape and some metadata. Here is the schema for destinations - (without the metadata for brevity) { \"mappings\": { \"geoShape\": { \"properties\": { \"name\": { \"type\": \"string\" }, \"location\": { \"type\": \"geo_shape\" }, \"destinationId\": { \"type\": \"integer\" } } } } } We have 1.7 million destinations in our index, most of which a","locales":"","title":"Finding You the Best Hotel at LateRooms.com with Elasticsearch"}
{"index":{}}
{"author":"James Baiera","category":"Releases","publish_date":"2016-06-30T00:00:00.000Z","url":"/blog/es-hadoop-5-0-0-alpha4","seo_title":"","content":" ​I am excited to announce the release of Elasticsearch for Apache Hadoop (aka ES-Hadoop) built against Elasticsearch 5.0.0-alpha4. : This is an alpha release and is intended for purposes only. Indices created in this version . For the sake of your own sanity, we do not advise using this version in production. What’s new? Pig - Mixed Value Maps Maps in Pig with mixed value types are now supported by the connector. No longer will these collection types be processed as an empty map during indexing time. Spark - Auto Create Index Fixed In some cases, Spark integration was all too eager to help you index your data, even when asked to automatically create new indices. After a quick intervention, it’s happy to notify you when the index doesn’t exist. Objects Used in Update Script Parameters Internal request rendering was improved so that the connector would no longer incorrectly treat objects in an update’s parameter list as flat JSON strings. Documentation A handful of documentation improvement contributions have been accepted. Relatedly, we’ve given our personal spell checkers a stern talking to. Feedback Please try this at home! You can ES-Hadoop 5.0.0-alpha4, try it out, find out how it breaks, and let us know what you did on , , or in the . A crisp high five is waiting for all who participate! Not a huge fan of high fives? There’s always the instead! ","locales":"","title":"Elasticsearch for Apache Hadoop 5.0.0-alpha4"}
{"index":{}}
{"author":"Monica Sarbu","category":"Releases","publish_date":"2016-06-30T00:00:00.000Z","url":"/blog/beats-5-0-0-alpha4-released","seo_title":"","content":" Today we are excited to announce the fourth alpha release of the Beats 5.0.0 series. This is an alpha release and is intended for testing purposes only. Please do not deploy in production. Monitor MongoDB with MetricbeatYou are now able to monitor your with Metricbeat. It gives you information about the uptime (ms) of the service, the number of active connections, the number of unused connections, the number of transactions written to the journal, the total size of the heap space (bytes), the number of page faults, the number of inserts, updates, deletes, the RAM size (MB), and . At the moment, Metricbeat has support for monitoring Apache, MySQL, Nginx, Redis, System statistics, Zookeeper, and MongoDB. If you are interested in monitoring another service, please open a in GitHub or contribute to the open source community by . Support for gzip compression in Elasticsearch outputYou can now configure the Beats to the payload when using the Elasticsearch bulk API. The compression level is configurable in the Elasticsearch output section and must be in the range of 1 (best speed) to 9 (best compression). By default the gzip support is off (compression level is set to zero), so you need to explicitly opt in. Ignore Symlinks log filesIn the previous versions, Filebeat had support for symlinks to log files, but it often happened that the same log file was read twice. Now Filebeat is completely Remove topology_expire optionPrior to 5.0, you could configure the topology_expire option in Packetbeat to automatically expire the topology entries from Elasticsearch. Starting with Elasticsearch 5.0, the _ttl index option is removed, so the topology_expire option no longer works. We have the option in 5.0, and the user is responsible for any required cleanup. Kibana Dashboards for Apache Metricbeat ModuleThanks to , Metricbeat comes with a predefined for the Apache Module. Known issue and workaroundThere is a in the 5.0.0-alpha4 release of Kibana which affects the loading process of our sample dashboards. It only affects brand new installations of Kibana 5.0.0-alpha4 that don’t yet have any indices defined. If you are upgrading you can skip this. When following the Beats , after , you will get an error when trying to set the index pattern as the default: If this happens, you can either manually overwrite the index pattern and then run the loading script again, or paste the following in the Kibana Console to force setting the default index (replacing `packetbeat-*` with the name of the Beat you are using): POST .kibana/config/5.0.0-alpha4/_update { \"doc\": { \"defaultIndex\": \"packetbeat-*\" } } Become a PioneerA big \"Thank you!\" to everyone who has tried the previous alpha releases and or . We’d like to also remind you that if you post a valid, non-duplicate bug report during the alpha/beta period against any of the Elastic stack projects, you are entitled to a . ","locales":"","title":"Beats 5.0.0-alpha4 released"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2016-06-30T00:00:00.000Z","url":"/blog/elasticsearch-5-0-0-alpha4-released","seo_title":"Elasticsearch 5.0.0-alpha4 released","content":" Today we are excited to announce the release of based on . This is the fourth in a series of pre-5.0.0 releases designed to let you test out your application with the features and changes coming in 5.0.0, and to give us feedback about any problems that you encounter. Open a bug report today and become an . This is an alpha release and is intended for . Indices created in this version . Elasticsearch 5.0.0-alpha4 delivers some exciting new features which we talk about below. In addition, there are many small changes which you can read about in the release notes above. Also take a look at the release announcements for , , and to read about features like: ","locales":"","title":"Elasticsearch 5.0.0-alpha4 released"}
{"index":{}}
{"author":"Shay Banon","category":"Releases","publish_date":"2016-06-30T00:00:00.000Z","url":"/blog/elastic-stack-release-5-0-0-alpha-4","seo_title":"Elastic Stack Release - 5.0.0-alpha4","content":" Game of Thrones season 6 is wrapped, the Pound has lost a few ounces, and Messi is talking about retirement. It’s been quite a week. But we have something to cheer you up. Say hello to Elastic Stack 5.0 alpha 4. As a reminder, we've also released the to help prepare you for migration from 2.3.x to 5.0. to make the transition to 5.0 more enjoyable.Kibana Head out to the  for details on the awesomeness. But, here's the quick rundown. LogstashFor more detailed information on the latest and greatest, head over to the Logstash .BeatsYou can find more beaty details about these changes in the . ES-HadoopAnd last, but certainly not the least. ES-Hadoop version 5.0.0-alpha4 has also been released . ","locales":"ja-jp","title":"Elastic Stack Release - 5.0.0-alpha4"}
{"index":{}}
{"author":"Marcelo Rodriguez","category":"Engineering","publish_date":"2016-06-30T00:00:00.000Z","url":"/blog/getting-started-with-shield-document-level-security-in-elasticsearch","seo_title":"Getting Started with Shield’s Document Level Security in Elasticsearch","content":" Elastic Shield is capable of filtering documents using query criteria. In this blog post I'll demonstrate how to use this feature by using a simple, two document data set in Elasticsearch where documents will be filtered for the user according to the value of a field and displayed in a simple Kibana visualization.  Users and roles will be created with the Users and Roles API. The Scenario There are three users:  a sales manager and two account representatives. Documents are indexed using a code for one of two regions, EAST and WEST.  Each account representative needs read access to documents in their region and the manager will need access to both regions.  All users will be using the same Kibana visualizations to view the counts of their documents. In Kibana, users will be allowed to create/modify visualizations and dashboards to view their documents. For the the initial steps, you will need to log in using an account with sufficient privileges to create  indices, roles and users.  The following REST API examples can be used directly in Kibana through the Sense plugin. Elasticsearch and Shield STEP 1: Configure Authentication Realm In this example, I'll be using the Native realm to create users so the native realm needs to be configured in Elasticsearch for Shield. Shield configuration example: #-----SHIELD CONFIG------- shield: authc: realms: native: type: native order: 0 STEP 2: Index Sample Data First, we will index two simple documents into one index. PUT myindex/mytype/1 { \"name\" : \"ABC Company\", \"region\" : \"EAST\" } PUT myindex/mytype/2 { \"name\" : \"DEF Company\", \"region\" : \"WEST\" } GET myindex/mytype/_search STEP 3: Create Roles Next, create the roles that will be used to allow users of each region to access their documents. We are specifying two indices: and .  The . index is set up with the \"all\" privilege so that our users can log in, change settings and create visualizations. The index is set up with the \"read\" privilege and we use a query to specify the documents that the users belonging to the role will be able to see. POST /_shield/role/myroleEAST { \"indices\": [ { \"names\": [ \".kibana*\"], \"privileges\": [\"all\"] }, { \"names\": [ \"myindex*\" ], \"privileges\": [ \"read\", \"view_index_metadata\" ], \"query\": \"{\\\"match\\\": {\\\"region\\\": \\\"EAST\\\"}}\" } ] } POST /_shield/role/myroleWEST { \"indices\": [ { \"names\": [ \".kibana*\"], \"privileges\": [\"all\"] }, { \"names\": [ \"myindex*\" ], \"privileges\": [\"read\", \"view_index_metadata\" ], \"query\": \"{\\\"match\\\": {\\\"region\\\": \\\"WEST\\\"}}\" } ] } GET /_shield/role STEP 4: Create Users Now we are ready to create the three users. One user will be assigned the role that allows access to documents marked with the EAST region, another user to the WEST region and the last account will include both roles. POST /_shield/user/myuserEAST { \"password\" : \"mypassword\", \"roles\" : [ \"myroleEAST\" ] } POST /_shield/user/myuserWEST { \"password\" : \"mypassword\", \"roles\" : [ \"myroleWEST\" ] } POST /_shield/user/myuserManager { \"password\" : \"mypassword\", \"roles\" : [ \"myroleEAST\",\"myroleWEST\" ] } GET /_shield/user Kibana In this section, we will configure Kibana to read our new index, , then create a visualization. You will need to log into Kibana with an admin role user to configure the initial dashboard. STEP 1: Set Index Pattern in Kibana STEP 2: Create Visualization in Kibana STEP 3: Test Users The tests below demonstrate how the query clause defined in the role restricts the data for the users.  If there are several visualizations in a dashboard on the same data, the window into the data will also be applied. Repeat the test with the user and you should see only the WEST bar. Repeat the test with the and you should see both EAST and WEST bars. Summary This article provided a base starting point to demonstrate how Shield can be configured with Elasti","locales":"","title":"Getting Started with Shield Document Level Security in Elasticsearch"}
{"index":{}}
{"author":"Court Ewing","category":"Releases","publish_date":"2016-06-30T00:00:00.000Z","url":"/blog/kibana-5-0-0-alpha4","seo_title":"Kibana 5.0.0-alpha4 released","content":" The moment is finally here, and after only 26 days and a bunch of internal delays that we don’t like to talk about, we are stoked to release Kibana 5.0.0-alpha4, or the Alpha to End All Alphas. Folks, if alphas were ears, features would be coming out of this one. And this ball of wax is only one away (ok, two clicks). As usual, become an by opening a bug report today and earn yourself some free stuff. : This is alpha software that will only work with . Please test it, but do not use it in production. Indices created in this version will not be compatible with Elasticsearch 5.0.0 GA. Upgrading 5.0.0-alpha4 to any other version is not supported. Import data from a CSV You read that right. For the first time ever, you can import CSV data into your Elasticsearch cluster via Kibana. Via the new “Upload CSV” tool under the also-new “Management” application, you can simply drag and drop a CSV of your choosing and we’ll let you review a sample of the results, tweak the mappings, name an index pattern, and Kibana does the rest. Monitor your Kibana instance with X-Pack In addition to the new and improved Elasticsearch monitoring capabilities, you now get Kibana monitoring with your free basic license of X-Pack. Keep track of requests, response times, memory usage, and more. Disable visualization buckets, and drag to sort as well Have you ever been in the situation where you meticulously set up a range aggregation on a sub-bucket in Kibana but want to temporarily remove it? Or how about move it to the top of a list of other buckets? Well, in our ongoing effort to single-handedly stop global warming, we decided to embrace micro-optimizations for energy usage by lowering the amount of clicks necessary. Or maybe we just got fed up with the tedium of rebuilding our range aggregations all the time. In any case, you can now sort buckets by dragging and dropping or disable them entirely rather than removing them. Settings is gone. Long live Management! This isn’t really a new feature in its own right - the new Management app still has the old capabilities of the Settings app, but it’s structured to enable a ton of features that we’re planning in the near and long term. Management is your window into the Elastic stack. This is where you’ll go to set up Kibana, configure users and roles in X-Pack security, and even add or manage data in your Elastic stack. Bugs n’ stuff In addition to screenshot-worthy features, we’ve also added a bunch of other improvements and bug fixes, including these highlights: What’s next? Beta, baby! For real this time. We still have a bunch of features we want to get into 5.0.0, and we’re simultaneously focusing more and more on the spit and polish that we all demand of a stable release. For now, download the alpha4 release and help us track down those remaining issues. Bugs can be filed on , and feedback is always appreciated on our . As always, feel free to reach out to us on or as well. ","locales":"","title":"Kibana 5.0.0-alpha4 released"}
{"index":{}}
{"author":"Suyog Rao","category":"Releases","publish_date":"2016-06-30T00:00:00.000Z","url":"/blog/logstash-5-0-0-alpha4-released","seo_title":"","content":" We are pleased to announce that Logstash 5.0.0-alpha4 has been released today. You can review the changes , or jump directly to . Moar Metrics, Moar VisibilityBuilding on the systems level monitoring APIs that were exposed in previous alphas, we have now added per-plugin metrics. The question you've always wanted to ask Logstash: where are events spending the most time? In alpha4, every filter and output plugin has been enhanced to expose wall-clock time spent by events flowing through it. It doesn't stop with timing info — plugins like Grok, Date and GeoIP filters now expose number of failures, successes, cache stats and more. So, yes, debugging your four page Logstash configuration just got a lot easier! curl localhost:9600/_node/stats/pipelines?pretty { \"plugins\":{ \"filters\":[ { \"id\":\"grok_8b1226e2-e714-444c-9774-0135833457d3\", \"name\":\"grok\", \"events\":{ \"in\":122, \"duration_in_millis\":75, \"out\":122 } \"failures\": 3 \"matches\" : 1, \"patterns_per_field\" : { \"message\" : 1 } } ], \"output\":[ { \"id\":\"elasticsearch_713f61e2-6d7a-436c-9159-b3a953b18451\", \"name\":\"elasticsearch\", \"events\":{ \"in\":122, \"duration_in_millis\":175, \"out\":122 } } ] } } Elasticsearch Output Kafka 0.10 SupportVersion 0.10 of Apache Kafka was with many enhancements like rack awareness, better handling for protocol versions and timestamp in messages. Logstash input and output has been updated to work with 0.10. Others FeedbackWe welcome and appreciate all your feedback as we iterate through our pre-releases for 5.0.0! You can open issues on our , or start a conversation on our . Also, do you know about our ?  ","locales":"","title":"Logstash 5.0.0-alpha4 released"}
{"index":{}}
{"author":"Nicolás Bevacqua","category":"Engineering","publish_date":"2016-06-29T00:00:00.000Z","url":"/blog/elastic-stack-primer","seo_title":"An Elastic Stack Primer - log, stream, and visualize data","content":" This article describes my adventures while getting initiated into the Elastic Stack. We'll be building upon the index I've set up for search in an earlier post. We'll upgrade our stack to , incorporate and , so we can :  and we'll also hone and troubleshoot our understanding of the Elastic Stack along the way. Getting started with the can feel a bit overwhelming. You need to set up , Kibana, and before you even can get to the fun parts – maybe even before you fully understand how the three synergize providing you with formidable and formerly untapped insights into your platform. I have been running Pony Foo since late 2012, and, , these kinds of user tracking systems are far from ideal when it comes to troubleshooting. In the years since I launched the blog I tried a couple of instrumentation tools that would provide me with reporting and metrics from the Node.js application for my blog, but these solutions would require me to, well... the Node apps by patching them with a snippet of code that would then communicate with a third party service. Visiting that service I could learn more about the current state of my production application. For a variety of reasons, I ended up ditching every one of these solutions not long after giving them a shot. Not long ago I wrote about , and since I'm already working on the Kibana analytics dashboard team I figured it wouldn't hurt to learn more about so that I could go full circle and finally have a look at some server metrics my way. It helps stream events pulled out of files, HTTP requests, tweets, event logs, or . After processing events, Logstash can output them via Elasticsearch, disk files, email, HTTP requests, or . The above graph shows how Logstash can provide tremendous value through a relatively simple interface where we define inputs and outputs. You could easily use it to pull a Twitter firehose of political keywords about a presidential campaign into Elasticsearch for further analysis. Or to anticipate breaking news on Twitter as they occur. Or maybe you have more worldly concerns, like streaming log events from and into for increased visibility through a Kibana dashboard. That's what we're going to do in this article. Installing the Elastic StackI had used for . This represented a problem when it came to using the Elastic Stack, because I wanted to use the latest version of Kibana As mentioned in the earlier article, we'll have to download and install Java 8 to get Elasticsearch up and running properly. Below is the piece of code we used to install Java 8. Keep in mind this code was tested on a Debian Jessie environment, but it should mostly work in Ubuntu or similar systems. Next, we'll install the entire pre-release Elastic Stack , for lasticsearch, ogstash and ibana. The following piece of code pulls all three packages from and installs them. After installing all three, we should turn on their services so that they run at startup. This is particularly desirable in production systems. Debian Jessie relies on for services, so that's what we'll use to enable these services. You want to reduce your \"hands on\" production experience as much as possible. Automation is king, etc. Otherwise, why go through the trouble? Lastly, we start all of the services and log their current status. We pause for ten seconds after booting Elasticsearch in order to give it time to start listening for connections, so that Logstash doesn't run into any trouble. Great, now we'll need to tweak our server. It's important to note that we'll be expecting the file to have a specific format that Logstash understands, so that we can consume it and split it into discrete fields. To configure with the pattern we'll call metrics, add this to the section of your file. log_format metrics '$http_host ' '$proxy_protocol_addr [$time_local] ' '\"$request\" $status $body_bytes_sent ' '\"$http_referer\" \"$http_user_agent\" ' '$request_time ' '$upstream_response_t","locales":"","title":"An Elastic Stack Primer"}
{"index":{}}
{"author":"Michael McCandless","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2016-06-27T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2016-06-27","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top News “How Airbnb manages to monitor customer issues at scale” by — Joe McCann (@joemccann) Elasticsearch Core Apache Lucene Watch This Space , where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2016-06-27"}
{"index":{}}
{"author":"Suyog Rao","category":"The Logstash Lines","publish_date":"2016-06-27T00:00:00.000Z","url":"/blog/logstash-lines-2016-06-27","seo_title":"","content":" Welcome back to The Logstash Lines! In these weekly posts, we'll share the latest happenings in the world of Logstash and its ecosystem. Moar Metrics, Moar Visibility (5.0)For key filters like Grok, Date and GeoIP, we added per-plugin metrics like number of matches, failures, cache stats, and some more. This, coupled with execution timings we added recently, should make it easier to debug these popular filters. \"name\": \"grok\" \"failures\": 3 \"matches\" : 1, \"patterns_per_field\" : { \"message\" : 1 }, Persistent QueuesThis week saw good progress in this area! Colin and Guy pushed initial commits for the low level data structures and classes that implement ack-able file based queues. Follow the for updates. Beats Input ImprovementBeats input is being rewritten in Java, using the Netty library. This week we are doing and end-to-end testing. Curator v4Big day for Curator! Aaron a new version with tons of enhancements. The headlining feature is adding support for snapshot restore, and a new yaml based config file to manage execution. Others: /tmp/logstash-5.0.0 % time bin/logstash --version logstash 5.0.0 27.79s user 1.34s system 270% cpu 10.773 total .... /tmp/logstash-5.0.0 % time bin/logstash --version logstash 5.0.0 0.01s user 0.02s system 101% cpu 0.029 total ","locales":"","title":"Logstash Lines: More Monitoring Info, Beats Input Improvements"}
{"index":{}}
{"author":"Martijn van Groningen","category":"Engineering","publish_date":"2016-06-27T00:00:00.000Z","url":"/blog/elasticsearch-percolator-continues-to-evolve","seo_title":"Elasticsearch Percolator Continues to Evolve","content":" In 5.0 the percolator is much more flexible and has many improvements. For example, to be able to skip evaluating most queries. All of this is part of since Elasticsearch , which made the percolator scale with the number of shards and nodes in your cluster. However the underlying mechanism of the percolator hasn’t been changed since this feature was back in version . If you didn’t make use of query metadata tagging the execution time of the percolator was always linear to the amount of percolator queries, because all percolator queries had to be evaluated all the time. The main purpose of this refactoring was to address this, so that in many cases not all percolator queries have to be evaluated when percolating a document. The slowest part of percolating is verifying if a percolator query actually matches with the document being percolated. When percolating, the document being percolated gets indexed into temporary in-memory index. Prior to 5.0, all percolator queries need to be executed on this in-memory index in order to verify whether the query matches. So the idea is that the less queries that need to be verified by the in-memory index the faster the percolator executes. Percolator field mapper It is no longer required to index percolator queries in the special type under the field. Any field and type (in any index) can contain percolator queries. Instead before indexing percolator queries, you must configure the in the type you’re going to index percolator queries into. So let’s take a look at how this looks now: We created an index with the name , which has two mappings. The first mapping , is for the percolator query documents, in this case the query must be defined inside the field. The second mapping is for a document being percolated. We need to define this mapping upfront, otherwise the queries that are going to be indexed wouldn’t be analyzed correctly and the process used to skip evaluating percolator queries relies on this analysis. After this we can just index the following document, which holds a query: Besides storing the actual query, the field mapper extracts all terms from the query and indexes them separately into an auxiliary indexed field that is part of field. In the above example the following terms will get extracted and indexed: and . During percolation all the terms from the document to be percolated are extracted. A query is built from these terms, so that the percolator can query this auxiliary field to find candidate percolator queries that may match with the document being percolated. Potentially many percolator queries that don’t match with this query are never evaluated by the in-memory index and thus reducing the time it takes to execute the entire percolate request. It is safe to ignore these percolator queries, because if the queries’ terms don’t appear in the document being percolated then these queries will never match anyway. This is a big win. The query can’t extract terms from all queries and if that happens these percolator queries get marked and will always be evaluated upon percolating. Most term based queries (like , , , and most span queries) and compound queries (like , and queries) are supported. If you’re wondering how you can figure out which queries the field mapper was able to extract the terms for, you can just execute a query as described . Also the percolator will no longer load the percolator queries as Lucene queries into memory as they are instead read from disk. Pre 5.0 if you had thousands of percolator queries they’d take up megabytes of precious JVM heap space, putting pressure on jvm garbage collecting and if not being careful lead to an infamous jvm out of memory error. Back then loading the percolator queries into memory made sense because all the percolator queries were evaluated all the time so we made executing each one as fast as possible. No","locales":"","title":"Elasticsearch Percolator Continues to Evolve"}
{"index":{}}
{"author":"Aaron Mildenstein","category":"Releases","publish_date":"2016-06-24T00:00:00.000Z","url":"/blog/curator_v4_release","seo_title":"","content":" I am very pleased to announce the release of ! So much has changed since version 3. This is a major change in how Curator works. I’ve listened to a lot of feedback, and incorporated many suggestions. I think you’ll find the results compelling. Before we go on, you need to know about the breaking changes from the previous version. Breaking changes Version Support Due to the sweeping nature of the changes, this is the first version of Curator which is not fully reverse compatible with older versions of Elasticsearch. Curator 4 only supports Elasticsearch versions 2.x and the 5.0 pre-releases. It is anticipated that Curator 4 will continue to support Elasticsearch 5.0 releases, though a special Curator 5 may be released which will take advantage of new features set to be released in Elasticsearch 5. API The API is completely different. If you were using the 3.x API, you will perhaps want to stick with that until you’ve tested the new API out. The documentation for the new API is still at  Command line The command line structure is completely different. The new command line only has these few flags: Date Math Dates are all converted to epoch time. Conversions no longer try to pad a full time unit. Either an age is older or younger than the reference epoch time, or it isn’t. More on these changes in a bit! How is it different? Curator 3 and each of its predecessors were designed to be run from cron, so that periodic maintenance could be performed easily. All of the other features added to Curator since the very beginning (which was  index deletion) have been bolted on, resulting in a very complex command-line structure. This was still navigable, but not what I would have called ideal. One of the most requested features was snapshot restore. A look at the configuration flags revealed that 9+ additional flags would have been required to accommodate only most of the options available.  Another frequent request was atomic add and remove alias actions. I puzzled over how to do that with the command-line structure for a long time and realized that it would have resulted in huge, complicated and hard to read command lines. It was time to rethink Curator configuration.  The solution? Configuration files. Configure all the things! One of the design decisions for Curator 4 was to use YAML configuration files–two of them, to be precise:  (and logging options), and . Having a default client configuration allows for multiple, different action configuration files to not need to repeat the client information in each of them. If you store the client configuration file as , then you won’t even have to reference it at the command-line! The action file allows for  and  Filter Stacking If you used Curator before version 4, then you know that Curator had a limited number of ways you could combine filters before performing the desired action. Generally, that was limited to regular expression filtering combined with age-based filtering. With Curator 4, you can combine multiple filters together–as many as you like–to restrict which indices to act on. How might this help you?  Let’s say you want to delete Logstash named indices in excess of 30G of total space consumed. This might represent 30 days worth of data with your normal logging. What if some event caused a torrent of log lines to be produced? You might accidentally delete weeks worth of logs. With filter stacking, you could first filter by pattern, to only count Logstash indices. The next filter would be disk space, 30G worth, sorting by age. The third filter, however, is the magic one: Only delete indices older than 30 days. The total stack would mean, “delete Logstash indices in excess of 30G of storage, but only if they’re also older than 30 days.” Neat, eh? This is what the action file might look like: actions: 1: action: delete_indices description: >- Delete indices. Find which to delete by first limiting the list to logstash- prefixed indices. Next filter by ","locales":"","title":"Curator 4.0 Release"}
{"index":{}}
{"author":"Kosho Owa (JP)","category":"Engineering","publish_date":"2016-06-22T00:00:00.000Z","url":"/blog/running-elasticsearch-on-aws","seo_title":"Instructions for Running Elasticsearch on Amazon Web Services (AWS)","content":" : Elastic Cloud ( can now be added directly your AWS bill through the . If you are looking for hosted & managed Elasticsearch, you can for 14-days at no cost. For deploying and managing yourself on AWS EC2, this is the right article for you:Part I - Provisioning EC2 InstancesWe often talk to customers running Elasticsearch clusters on Amazon Web Services (AWS). AWS is a convenient way to provision and scale machine resources in response to changing business requirements. Elasticsearch takes advantage of EC2's on-demand machine architecture enabling the addition and removal of EC2 instances and corresponding Elasticsearch nodes as capacity and performance requirements change. In this article we will show you how to deploy on Amazon EC2. In this example we will configure a three node Elasticsearch cluster. Step 1: Choose an Amazon Machine Image (AMI)Elasticsearch runs on various operating systems such as CentOS, Redhat, Ubuntu, and Amazon Linux. We suggest using the latest Amazon Linux AMI — \"Amazon Linux AMI 2016.03.0 (HVM), SSD Volume Type\". Step 2: Choose an Instance TypeA reasonable starting instance type is m3.2xlarge which provides 8 vCPUs, 30 GiB of memory, 2 x 80 GB SSD drives and comes with High Network Performance. Solid State Drives are preferred as indexing is IO intensive and High Network Performance is essential for cluster performance and reliability. M3.2xlarge is a baseline recommendation. To determine whether it is an appropriate choice, you should to determine whether it meets performance and scaling requirements. Click the \"Next: Configure Instance Details\" button. Step 3: Configure Instance DetailsEach Elasticsearch node will run on its own dedicated EC2 instance, so set the number of instances to 3. Note that any AWS accounts that have been created after December 4, 2013 only support EC2-VPC, so the \"Network\" option for picking \"Launch into EC2-Classic\" won't be available for those users and should not be enabled anyway. Selecting \"Enable termination protection\" is a good idea as it prevents accidental deletion of nodes and their data. Leave the default values for remaining fields and click the \"Next: Add Storage\" button. Step 4: Add StorageLet's leave the storage Size at 8 GiB. If you happen to know your index storage requirements at this time, you can adjust the storage now. Leave the Volume Type set to General Purpose SSD. Click the \"Next: Tag Instance\" button. Step 5: Tag InstanceIn this field, provide a key and value pair, for example \"name\" and \"esonaws\", to make it easy to recall the ec2 instances. Click the \"Next: Configure Security Group\" button. Step 6: Configure Security GroupThis configuration panel allows you to configure a set of firewall rules for accessing your instance. By default, Elasticsearch exposes TCP port 9200 for REST API access and TCP port 9300 for internal cluster communication. Consider adding rules to allow connecting to TCP port 9200 from desired subnets, typically private subnets, and TCP port 9300 from the subnets where Elasticsearch nodes live. If you plan to change the default port settings in elasticsearch.yml, configure rules for those ports rather than TCP ports 9200 and 9300. Also, add a rule to allow SSH connections on port 22, so you can connect to the instance in the later steps. Click The \"Review and Launch\" button. Step 7: Review Instance LaunchNote any warnings and review the Instance Launch settings and click the \"Launch\" button when ready. At this point you will be prompted to provide a key pair or create a new key pair. This is necessary to enable SSH access to the EC2 instance. If you need help setting up a key pair the article provides an overview and instructions for creating new new key pairs. Start up the EC2 instances and take note of the assigned private IP addresses which we will use in a following step. Part II - Installing ElasticsearchRPMLog into eac","locales":"de-de,fr-fr,ko-kr","title":"Running Elasticsearch on AWS"}
{"index":{}}
{"author":"Suyog Rao","category":"Engineering","publish_date":"2016-06-22T00:00:00.000Z","url":"/blog/just-enough-kafka-for-the-elastic-stack-part2","seo_title":"Just Enough Kafka For The Elastic Stack, Part 2","content":" Welcome to part 2 of our multi-part and Elastic Stack post. In our , we introduced use cases of Kafka for the Elastic Stack and shared knowledge about designing your system for time based and user based data flow. In this post, we'll focus on the operation aspects: tips for running Kafka and Logstash in production to ingest massive amounts of data. Before we dive deep, a reminder that we are mostly talking about Kafka 0.8, and Logstash 2.x which is the current stable version. There are newer versions of Kafka — 0.9 and recently, 0.10, but the core concepts discussed here can be applied to any Kafka versions. Without further ado, let’s start by discussing the different systems at play here: Kafka has a dependency on (ZK) — brokers need it to form a cluster, topic configuration is stored in ZK nodes, etc. Plus, in version 2.x of Logstash, the input offsets are stored in ZK as they get acknowledged. Newer versions of Kafka have decoupled the clients — consumers and producers — from having to communicate with ZooKeeper. In Kafka 0.9 and 0.10, offsets are stored in topics by default instead of in ZK. Either way, you still need ZooKeeper to run Kafka brokers. Our general advice is to run 3 ZK instances to achieve a quorum configuration, and all of them on separate hardware. For more information on operationalizing ZK, refer to in Kafka docs. From our experience, ZK itself does not need much hand-holding once set up. You just have to make sure the instances are up and are monitored. Number of Kafka brokers you need typically depends on data retention and replication strategy. The more brokers you add, more data you can store in Kafka. In terms of resources, Kafka is typically IO bound. Performance will be limited by disk speed and file system cache — good SSD drives and file system cache can easily allow millions of messages/sec to be supported per second. You can use to monitor these information. How many Logstash instances do you need to process the data in Kafka? It is really hard to magically place a number for this, because frankly, it depends on a lot of variables. Questions like: how many filters do you have? How expensive are your filters, needs to be answered. Remember, it is really easy to end up with a complex Grok pattern with multiple conditionals to process your data! What is your volume of data you expect? What are all your outputs? As you see, there's a lot of information we need to gather before providing a number. Often times, it is the outputs (external systems) where you have to focus your capacity planning, not Logstash itself! That being said, you can easily scale Logstash and Elasticsearch horizontally. Therefore, our advice is to start small, and continue to add nodes or new LS instances as your data needs grow. In particular, for data in Kafka that Logstash consumes, you can group multiple instances into consumer groups. Each group shares the load and instances will handle data exclusively, i.e. messages will be consumed only once by one client in the group. This design lends very cleanly to our original proposition — start small and scale iteratively. Using topics, you can design your workflow such that data which needs more complex transformations, or data that needs to be stored in a slower output is isolated from other fast-moving data. Remember, in Logstash, a single slow output can block all other outputs which are configured to run after it. As we've mentioned before, Elasticsearch is truly elastic in that you can scale up easily. Capacity planning for Elasticsearch is an entire blog post by itself, and beyond the scope of this article. We recommend you read the following posts which cover the concepts of scaling and sizing Elasticsearch -- , , and .    If your Kafka instance is running out of disk space, chances are that your retention time for Kafka logs is too high. I","locales":"de-de,fr-fr,ja-jp,ko-kr","title":"Just Enough Kafka For The Elastic Stack, Part 2"}
{"index":{}}
{"author":"Maarten Roosendaal","category":"User Stories","publish_date":"2016-06-22T00:00:00.000Z","url":"/blog/finding-a-scalable-data-model-for-search-at-bol-com","seo_title":"","content":" started in 1999 and has grown from an online bookstore to an online superstore with a wide variety of products, including books, segways, shoes, saunas, swimming pools, and much more. Since 2010, Bol.com has also opted to become a platform for other sellers to (re)sell their products and/or sell the same products bol.com offers but with different conditions. Right now we have over 11 million products available, 6.2 million active customers, and about 230,000 active sellers a month. What's the problem? Like any e-commerce site, we want to help our customers find what they are looking for and part of that comes in the form of a search engine. Currently we use Endeca for site search and we have a 'flat' document model for products, which means that we join data relevant for search for each product in one flat document and for each product we select the offer (which contains information like price, availability, seller) from a set of offers we think is most relevant based on some rules. This means and are part of the product document and we can only use one offer: Functionally this limits the customers. Let's say I have a product with an offer from bol.com for 20 Euros and an offer from Seller A for 18 Euros and our offer selection says bol.com has the . The price-facet is based on the 'best buy' price so if a customer selects a price between 15-19 Euros the product will disappear because it does not apply to the filter. This is bad for our customers because there actually is a relevant offer, and it’s also bad for sellers because their offer isn't shown. We want the customer to be in more control. One of the major things we need to solve is that we need a scalable way to model all offers for each product. Other challenges We've been working with and the Elastic Stack for a while and recently also for an application for our professional sellers. Elasticsearch seems to be very scalable and flexible so we wanted to see if Elasticsearch could solve our modelling challenge and whether it comes close to the performance we get out of our current search engine. To give an indication, we had about 2,400 requests a second on our search engine during the holiday season, and expect a lot more this year. Another crucial challenge is the number of updates due to the amount of products, offers, and sellers — about 500K offer updates (price or availability changes) and 200K content updates a day (and mostly during office hours). Sometimes we have peaks of 1.5 million product updates and 20 million offer updates a day. On average this will increase in the coming years because we are growing, with more products, more sellers, more offers, and more updates and inserts. So choosing the model is not just about IF we can model product offers correctly, it's also thinking about how we can set up Elasticsearch in such a way that it can handle all those updates in near real-time while processing the number of requests. For this article we'll stick mostly to modelling and performance from a request perspective. Challenge recap So to recap our challenges: I'll get back to 'complex query requirements' in a moment but in general we have come up with a principle which states 'although we value performance at index time, we value performance at query time more'. We used this principle in the decision process. Elasticsearch and data modelling Elasticsearch has a few modelling options for our case: Approach Option 3 is the situation we have now so we skipped that one, option 2 was also quickly discarded because you lose the relation between the data of an offer if you have more than one offer. That left us with three models (1, 4 and 5). But how can you test which one is best? We first started by actually creating representative indices for all these options to test against. We used a Node.js script for creating random data: Product Offer We also want to test with different index sizes so we f","locales":"","title":"Finding a Scalable Data Model for Search @ bol.com"}
{"index":{}}
{"author":"Michael McCandless","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2016-06-20T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2016-06-20","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top News Sat on this draft for a week, and well, it's Friday. Yolo. ElasticSearch at petabyte scale on AWS: — Jamie Alquiza (@jamiealquiza) Elasticsearch Core Apache Lucene Watch This Space , where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2016-06-20"}
{"index":{}}
{"author":"Antonio Bonuccelli","category":"User Stories","publish_date":"2016-06-21T00:00:00.000Z","url":"/blog/jobrapido-powering-the-search-for-employment","seo_title":"","content":" Searching for a job in general can quickly become a full-time job, we all know.... Allowing others to find their dream vacancy in 58 different countries with a state-of-the-art search solution that scales and is able to understand up to 18 different languages is a different matter, and the team at Jobrapido knows it very well. 58 countries. 20 million monthly job listings. 60 million registered users.Headquartered in Milan, Jobrapido allows millions of people to quickly find relevant results when searching for a job. Elasticsearch is powering its searches — providing great search throughputs and delivering contextualised relevant results in the blink of an eye. Since the end of 2014, Jobrapido has migrated its global infrastructure to rely on Elasticsearch to handle 20,000+ requests per second for its 60 million globally distributed registered users. They make use of a wide range of features, including custom language analyzers, stopwords, and stemming for all the major languages, as well as percolations and aggregations to improve search results — helping users find jobs that are tailored to their history and preferences, and providing a seamless search experience overall. \"They're one of those customers that have a strong interest and passion for the product. I'm lucky to have them.\"Supporting Jobrapido and its awesome search team, led by Salvatore Vadacca, is one of my tasks and surely it's always been a pleasant one. Their use case is one of the most feature rich and advanced for free text search that I have had the chance to directly support. I am impressed and inspired by the passion and dedication of the Jobrapido search team, always striving for perfection and actively exploring new ways to make your life better when you're on a quest for a job. At Elastic{ON}, Salvatore and I got a chance to talk about what it's been like working together for the past year and a half. We recorded it in this video: There's still quite some time ahead, but I can already say that I look forward to seeing Salvatore and company again next year at Elastic{ON}, our epic yearly user conference in San Francisco ( to receive Elastic{ON} updates!). That's where we had the chance to meet, know each other better and also have beers and play some table football. : -) ","locales":"","title":"Jobrapido: Powering the Search for Employment"}
{"index":{}}
{"author":"Suyog Rao","category":"Releases","publish_date":"2016-06-17T00:00:00.000Z","url":"/blog/logstash-2-3-3-released","seo_title":"","content":" Bugs Fixed ","locales":"","title":"Logstash 2.3.3 Released"}
{"index":{}}
{"author":"Nik Everett","category":"Engineering","publish_date":"2016-06-17T00:00:00.000Z","url":"/blog/refreshing_news","seo_title":"","content":" When you send Elasticsearch a request that modifies or creates documents and it replies with or it has synced the changes to disk on all active shards . That means that the changes will survive catastrophic system shutdown but it doesn't mean that the changes are available for search. The process that makes changes available for search is called a \"refresh\" and it is the topic of this post. Refreshes are performed periodically ( ), when the is full, and on demand (). On demand refreshing is rarely used outside of testing because it creates small index segments which are inefficient to create and search and must later be merged into larger segments. Waiting for the indexing buffer to be full is unpredictable so we can't rely on it either. That means that we mostly think of the index as being refreshed every , which defaults to 1 second. The problem Refreshing every second is fine if you are indexing something like logs where you expect to be some amount of time behind real time, but if you are indexing blog posts or comments or calendars then it can be a bit difficult. For anything where a user might expect to make a change and immediately be able to search for that change (blog, forum, scheduling app) your application needs some way to that the change is visible for search. This is doubly true for applications that want to use search for something interesting after the user's change (think scheduling or aggregations). In those cases you have a few options all of which have interesting tradeoffs: Wait for the refresh You could just wait for the refresh interval to pass. This has the advantage of being something you can do totally asynchronously. The disadvantage is that you have to wait for the whole one second and even then it is not guaranteed. Refresh isn't instant. Usually it is pretty quick but some refreshes will be slower than others so you can't really predict it. For applications where you can tolerate not knowing for sure if something is available for search then this is totally the right choice. But this blog post really isn't about those applications. So, for the sake of this blog post, we're going to assume this option isn't good enough for you. Force a refresh with You could force an immediate refresh. This has the advantage of being pretty quick. Like I said a few paragraphs up, it has the disadvantage of creating small segments that are inefficient to create, search, and merge. For plenty of use cases this inefficiency is worth the speed. Don't be afraid to force a refresh if it makes sense for your use case. For example, say you are loading something into Elasticsearch and plan to analyze the results. This search index is just for you so you know when you are done loading documents. At that point you shouldn't hesitate to refresh the index. Waiting isn't going to help. I should mention that adding to an index, update, delete, or bulk request is subtly different than performing a API call. Refresh API calls will refresh all the shards on the index. will only refresh the shards that have been modified. So for index, update, and delete requests that is just the shard to which the document was routed. For bulk requests that is all shards to which any document was routed. might also be a bad choice because it affects other indexing in the same index. Say you have a bulk loading process that works quite well. But now you want to start inserting a few documents into the same index interactively. If you do it with then, suddenly, you've started refreshing documents outside of whatever refresh interval you were using for the bulk load. If you do that frequently enough that'll change the search and index performance of the bulk loading process. Wait for the refresh with   Elasticsearch 5.0 brings a hybrid approach between the two options. Adding to index, update, delete, or bulk request will cause the request to wait until its changes have been made visible for search before returning to the user. This has","locales":"","title":"Where are my documents?<br>Refreshing news..."}
{"index":{}}
{"author":"Tudor Golubenco","category":"Brewing in Beats","publish_date":"2016-06-15T00:00:00.000Z","url":"/blog/weekly-beats-logstashbeat-nginxupstreambeat-lambdabeat","seo_title":"","content":" Welcome to Weekly Beats! With this series, we're keeping you up to date with all that's new in , from the details of work in progress pull requests to releases and learning resources. New Community Beat: Logstashbeat Logstash now has a monitoring API, so it can be monitored with Beats :-). has created a new community Beat called that gathers stats (events, jvm and process) from a running Logstash instance and ship them to Elasticsearch. New Community Beat: Nginxupstreambeat Nginxustreambeat is a Beat used for Nginx upstream server status monitoring. The Nginx , by the same ,  reads the statistics periodically and indexes them in Elasticsearch. This is very useful when using Nginx as a reverse proxy to load-balance among multiple upstream servers. New Community Beat: Lambdabeat , one of our Elastic colleagues, created  to collect various AWS Lambda metrics and index them in Elasticsearch. Lambdabeat fetches data from the Cloudwatch API, so you don't need to deploy it anywhere in particular, but it does require valid AWS credentials. New Generators, now in the main Beats repository The Beat generator proved to be extremely useful in quickly bootstrapping new Beats and keeping them up-to-date with our changes. As we’re adding different types of generators, like the one that generates a , or one for generating a Beat that , we figured it makes more sense to in a single place. This place is inside the main Filebeat: symlinks are no longer followed Following symlinks to log files in Filebeat was asking for trouble, for example it could happen that the same file was read twice. We will be with the next alpha release. Symlinks are now completely ignored from now on. Fixed backoff logic in outputs Coming from a community contributor, this fixes a bug in the backoff retry code in libbeat. Metricbeat: automatically document events Every Metricbeat module so far comes with Dockerized tests to verify it against a real instance of the monitored system. We now take and automatically generate samples of the events by running the integration tests. MongoDB Metricbeat module A for Metricbeat is currently in progress. ","locales":"","title":"Brewing in Beats: Logstashbeat, Nginxupstreambeat, Lambdabeat"}
{"index":{}}
{"author":"Jordan Zimmerman","category":"Engineering","publish_date":"2016-06-15T00:00:00.000Z","url":"/blog/zookeeper-backup-a-treatise","seo_title":"","content":" Introduction is an open source distributed coordination service originally developed at Yahoo and now at Apache. It is the core coordination service that we use here at . Critical data is stored in ZooKeeper and having a reliable backup system for it is vital. However, backing up and restoring a ZooKeeper cluster can be very tricky to do correctly. Data types A ZooKeeper database contains two broad categories of data: persistent and ephemeral. Persistent data is generally the same type of data stored in traditional datastores (RDBMSes, etc.). It is data that is accessed via traditional CRUD activity. Ephemeral data, however, is unique to ZooKeeper and is usually associated with state machine semantics. The presence of the ephemeral data implies a specific state. Consistency ZooKeeper in production is deployed on multiple processes (typically 3 or 5) each of which maintains its own database. ZooKeeper’s consistency guarantee is that processes will receive writes. The important implication of this is that, at any given point in time, it must be assumed that one or more of the individual ZooKeeper databases are in a different consistent state than the others. Data Implications Regardless of whether the data is persistent or ephemeral, data stored in ZooKeeper can be: Note: this is different than traditional datastores where data is nearly always source of truth. Sessions ZooKeeper clients maintain a session with the server. The server implements this session as a special type of transaction that is stored in its database. Ephemeral nodes are tied to sessions and expire when sessions expire. ZooKeeper sessions are durable and fault tolerant. They are managed by the internal ZooKeeper leader instance. Distributed State Machine The transient and stateful data in ZooKeeper extends to data structures held in client processes to form a distributed state machine composed of ZooKeeper servers, client applications, etc. For example, five clients might be involved in a leader election meaning they all have created ephemeral nodes in ZooKeeper and the leader is executing some action. Thus, there are ephemeral nodes in ZooKeeper and objects in client memory that form a single state machine. Backup/Restore Implications With source of truth datastores backups usually are done by periodic copying of a transaction log or some similar type of on-disk representation of the data. These datastores have built-in mechanisms to help/support backup and restore. In the best cases, restores can be achieved with no transaction losses. In the worst cases, only the most recent transactions are lost. ZooKeeper creates a new class of backup/restore issues due to its use for transient and stateful data. Further, it has no built in support for backup and restore. As stated above, at any point in time transient or stateful data combine with client data structures to create a distributed state machine. Thus, . Back In Time An improperly restored ZooKeeper . Imagine the following scenario: This is just one example. Copious other scenarios can be contemplated. Recommendations Backup of ZooKeeper by copying its transaction and snapshot logs is a reasonable method. It must be understood that data loss will occur for writes that happened past the time of last log copy. These logs should be restored without filtering. All ephemeral node transactions should appear to be deleted nodes from the logs before the ensemble is restored. This can be accomplished by removing all session transactions from the logs. Dealing with transient persistent nodes is more difficult as it’s not possible to automatically identify these nodes. They could be identified by known path prefixes or some other method. Or, hopefully, there are no cases and this can be ignored. Filtering of ZooKeeper’s transaction log is very easily done. There are existing classes in the ZooKeeper library that can be used for this (e.g. ). Alternatively, if possible, all clients should be c","locales":"","title":"Apache ZooKeeper Backup, a Treatise"}
{"index":{}}
{"author":"Joshua Rich","category":"User Stories","publish_date":"2016-06-14T00:00:00.000Z","url":"/blog/behind-the-elastic-stack-working-with-ebay","seo_title":"Behind the Elastic Stack: Working with eBay","content":" \"I know that platform. They're my customer. That's the way we work at Elastic.\"At Elastic, we like to build a relationship with our customers. We are assigned to a support contact as opposed to being in a rotation, so we often feel like an extension of the customer's team. We build relationships in that team just like at any workplace and the information exchange flows both ways. One of the first customers I started working with when I joined Elastic is eBay. I've built up a knowledge of their project and use case just as Sudeep and the eBay team's knowledge of the Elastic Stack has increased. We're definitely past the general questions and issues — now we can focus on solving the fun problems! I have been involved with Sudeep and his team since they first became a support customer. However, although we've exchanged a ton of support tickets and many conference calls, it wasn't until our in February, some six months after I started working with Sudeep, that we were finally able to meet face-to-face. That was great to finally see each other, sit down, and just chat – talk shop and joke around a little. We also spent some time in front of the camera sharing what it's been like working together: The best part about working with eBay is working at scale. They have a huge amount of data and a massive range of projects using the Elastic Stack. It's a deployment of a size that not many people get to work with. It's great to be able to contribute to their success. At the end of the day, it's really cool to think that I may have helped, even if just in a small way, to support something that millions of people all around the world are using. \"It feels less like working with colleagues and more like collaborating with friends on a project.\" Working at Elastic has been a fantastic experience. Everyone across the board is so committed to everything we do and supporting the wider community around our products. It feels much less like working with colleagues to meet some business goals and much more like just a bunch of friends coordinating together on a fun project. The depth and width of knowledge within the company is amazing, and it's an environment in which you can both constantly learn and definitely contribute back. Support is probably one of the most distributed teams within the company, so it's really important to us all that we have a sense of togetherness and team spirit. I think that is definitely the case and shines through strongly on those rare occasions where we are all standing in the same room. It's really great to be part of a passionate and fun team and it's always exciting to interact with our customers who continually surprise and inspire us with their use cases around . ","locales":"","title":"Behind the Elastic Stack: Working with eBay"}
{"index":{}}
{"author":"MyGene.info Development Team","category":"User Stories","publish_date":"2016-06-09T00:00:00.000Z","url":"/blog/uncoiling-data-in-dna-elasticsearch-as-a-bioinformatics-research-tool","seo_title":"Uncoiling the Data in DNA: Elasticsearch as a BioInformatics Research Tool","content":" Awash in a sea of gene and variant informationThe availability of genetic and genomic information has exploded in the last decade following decreasing costs in sequencing technology:  however, much of this information exists scattered over many different resources. For example, different resources on the same gene often have different identifiers, formats, and information. The fragmented data landscape makes creating and maintaining bioinformatics pipelines challenging, frustrating, and time consuming.As part of (Associate Professor) computational biology research group at the , our team is interested in solving big data challenges like the aforementioned fragmented gene/variant data landscape.  (Associate Professor) spearheaded the endeavor to create easy-to-use gene and genetic variant annotation services so that researchers can spend more time making new discoveries and less time on dealing with the fragmented data landscape. By using our free* service, users can obtain up-to-date gene and variant information in a consistent format (JSON), from any of the two endpoints used for each service.Building the solution with ElasticsearchMyGene.info was the first of the two annotation services we built. In building our services, we knew there were several issues we needed to consider:Given these constraints, we employed Elasticsearch in our Indexing Engine. Our previous experience with CouchDB for a different resource, enabled us to smoothly transition into using Elasticsearch and we were early adopters of Elasticsearch (circa v0.5.x). Even at the earlier stages of development, Elasticsearch has been a valuable tool in our arsenal, and we had no doubt it would be able to suit our needs.Applying our success in building MyGene.info into a highly scalable service, we followed by building MyVariant.info to address the even more fragmented data landscape of genetic variant information. MyVariant.info currently has more than 334 million unique gene variants from over 14 databases.By using Elasticsearch in our services, users would be able to search for one or thousands of gene or variant-specific JSON object(s) using flexible query terms and return just the information of interest to them. If they were only interested in variant annotations from dbSNP or gene annotations from worms, they would be able to specify those filters in their search. Most importantly, users could get their results quickly. According to our recent paper released in , MyGene.info can handle traffic from >5000 concurrent users for approximately 10,000 requests per minute:  and over 95 % of actual user requests take less than 30 ms to process. MyGene.info receives requests from over 4000 unique IP addresses on a monthly basis, while MyVariant.info caters to roughly 1,500 unique IP’s each month.Tracking our success with KibanaWe already had , a well-used, user-friendly resource, which originally utilized CouchDB (v1). As we migrated the service over to utilize MyGene.info, we wanted a way to distinguish the MyGene.info traffic coming from BioGPS.org from our various clients (python, R, etc). We utilized Kibana to help visualize the different sources and volumes of traffic for MyGene.info and MyVariant.info. Both MyGene.info and MyVariant.info consist of two endpoints each, and Kibana was an easy way for us to inspect the usage of our service endpoints.Scaling towards other BioThingsMyGene.info currently has 10 shards spread across two web nodes, three master nodes, and three data nodes. Scaling up from 13 million genes to cover 334 million variants, MyVariant.info is made up of 20 shards spread across three web nodes, three master nodes, and five data nodes. We use load balancers to handle the queries coming into our web nodes to ensure fast and stable processing. Given the les","locales":"","title":"Uncoiling the Data in DNA: Elasticsearch as a BioInformatics Research Tool"}
{"index":{}}
{"author":"Robert Muir","category":"Culture","publish_date":"2016-06-07T00:00:00.000Z","url":"/blog/elastic-love-for-the-adopt-a-character-program-and-the-unicode-consortium","seo_title":"Elasticsearch Loves the Unicode Adopt-a-Character Program","content":" The is a non-profit corporation founded in 1991. Its goals include standardizing and supporting the languages of the world and allowing people to use any language on their computers and smartphones. This work is essential for the software we build at Elastic. Unicode does more than just list out all the characters. They describe how to parse text, how to sort in different languages, and so much more to support all human languages. The Unicode Consortium's raises money to support a variety of important missions. Conserving the world's living languages is a huge task, and includes working with language experts, technologists, and cultural leaders, all in order to support minority languages on computers. According to Unicode, close to 98 percent of our world's living languages are digitally disadvantaged. This means that operating systems, web browsers and mobile applications don't support them. So the Adopt-a-Character donations help Unicode — a neutral organization interested in language conservation and technological standardization — to drive the work to correct this. They plan to focus use of the funds on adding characters for both modern and historic disadvantaged languages, and to support internationalization for those languages ( and ). When Elastic Founder and CTO found out about the adoption program, he had a cool idea: why not allow every engineer at Elastic (as well as other teammates within the company) to choose and adopt a character? Besides supporting a good cause, it tells us a bit about every engineer on our team! Here's what we chose: td {vertical-align:top: }figcaption {font-size:10px:  text-align:left:  line-height:130%: } Soon to be added: The Elastic  Team will be adopting the owl (U+1F989) from Unicode 9.0 at bronze level. As seen in the , the owl is one of a set of brand-new emojis coming soon to a screen near you! The Unicode Consortium is close to allowing adoption of Unicode 9.0 characters with their website update in the coming weeks. Check back and you can be first to adopt one of the awesome new emojis! In case you're wondering, adopting wasn't required and all adoption fees were eligible for reimbursement by Elastic. And here's an example of the cool certificates that a Bronze-level sponsor may choose to receive: Elastic commends the Unicode Consortium for all the work it is doing, and we encourage everyone reading this to visit the site and its , and make a donation to sponsor your favorite character today! ","locales":"","title":"Elastic <3 for the Adopt-a-Character Program and the Unicode Consortium :-)"}
{"index":{}}
{"author":"Michael McCandless","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2016-06-06T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2016-06-06","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top NewsSupporting customers is encoded into our DNA. explains how — elastic (@elastic) Elasticsearch Core Apache Lucene Watch This Space, where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2016-06-06"}
{"index":{}}
{"author":"Nicolás Bevacqua","category":"Engineering","publish_date":"2016-06-06T00:00:00.000Z","url":"/blog/setting-up-elasticsearch-for-a-blog","seo_title":"Setting Up Elasticsearch for a Blog","content":" I've been experimenting with  — and working at the company behind it for a while — so it only made sense to incorporate it as 's search provider. This article describes in detail the steps I took in setting up Elasticsearch as the search provider for Pony Foo. I start by explaining what Elasticsearch is, how you can set it up to make useful searches through the Node.js API client, and how to deploy the solution onto a Debian or Ubuntu environment. A while back I started working at Elastic — the , a search engine & realtime analytics service powered by Lucene indexes. It's an and I'm super happy here —  😉 🎉 Thrilled to announce I've started working at !⛹ Working on Kibana (ES graphs)👌 Great fun/team! Hiring!🎢 — Nicolás Bevacqua () Possible use cases for Elasticsearch range from indexing , analyzing in real-time, , all the way to and back to providing search for a lowly blog like Pony Foo. We also build , a dashboard that sits in front of Elasticsearch and lets you perform and graph the most complex queries you can possibly imagine. Many use Kibana across those cool service status flat screens in hip offices across San Francisco. But enough about me and the cool things you can do with Elastic's products. Let's start by talking about Elasticsearch in more meaningful, technical terms. What is Elasticsearch, even? 🔎 is a REST HTTP service that wraps around , a Java-based indexing and search technology that also features spellchecking, hit highlighting and advanced analysis/tokenization capabilities. On top of what Lucene already provides, Elasticsearch adds an HTTP interface, meaning you don't need to build your application using Java anymore:  and is distributed by default, meaning you won't have any trouble scaling your operations to thousands of queries per second. Elasticsearch is great for setting up blog search because you could basically dump all your content into an index and have them deal with user's queries, with very little effort or configuration. Here's how I did it. Initial SetupI'm on a Mac, so — — I just installed using . brew install elasticsearch If you're not on a Mac, just go to the download page and , unzip it, run it in a shell, and you're good to go. Once you have the executable, you can run it on your terminal. Make sure to leave the process running while you're working with it. elasticsearch Querying the index is a matter of using , which is a great diagnostics tool to have a handle on:  a web browser, by querying ( ):  the , which provides a simple interface into the Elasticsearch REST service, or the , which is similar to Sense. There are client libraries that consume the HTTP REST API available to several different languages. In our case, we'll use the Node.js client: . npm install --save elasticsearch The API client is quite pleasant to work with, they provide both Promise-based and callback-based API through the same methods. First off, we'll create a client. This will be used to talk to the REST service for our Elasticsearch instance. Creating an Elasticsearch IndexWe'll start by importing the package and instantiating a REST client configured to print all logging statements. import elasticsearch from 'elasticsearch':  const client = new elasticsearch.Client({ host: 'http://localhost:9200', log: 'debug' }):  Now that we have a we can start interacting with our Elasticsearch instance. We'll need an index where we can store our data. You can think of as the rough equivalent of a database instance. A huge difference, though, is that at once —  I'll create an index named . Since returns a , we can on it for our code to stay easy to follow. If you need to brush up on / you may want to read \"\" and the as well. await client.indices.create({ index: 'ponyfoo' }):  That's all the setup that is . Creating an Elasticsearch Mappingin addition to creating an index, you can create . Type mappings aid Elastic","locales":"","title":"Setting Up Elasticsearch for a Blog"}
{"index":{}}
{"author":"Martin Smith","category":"User Stories","publish_date":"2016-06-02T00:00:00.000Z","url":"/blog/elasticon-16-one-hundred-days-later","seo_title":"Elastic{ON}<sup>16</sup> — One hundred days later","content":" It’s been almost exactly one hundred days since I gave a talk about the  at in San Francisco, California. Since February, the new names and logos we saw for the first time have become commonplace, and many other changes announced at Elastic{ON} are faint memories. But since February, I’ve also seen some really awesome new things come out — like — and major updates to projects like .As a conference attendee this year, I got a real sense that Elastic was picking up the pace on product development, and the release bonanzas have continued to show that commitment. has also supported my continuing efforts to maintain the Chef cookbook with updates for new versions, functional enhancements, Chef software updates, and packaging bug fixes. For the first time this year, I was also able to be part of the ‘international attendance’ at Elastic{ON}. I’ve been living as a digital nomad and traveling with since February 1st, 2016. I traveled to San Francisco from Montevideo, Uruguay. I’ve since lived a month each in Uruguay, Argentina, Bolivia, and Peru. At the conference, I met up with Chef cookbook collaborators who had traveled equally far —  from Prague, from Austin, and from the Bay Area. I also had the pleasure of meeting some Elastic staff like and , both traveling to San Francisco from Europe. I learned that Elastic is actually a distributed company, which I very much respect as a digital nomad. Looking back on the trip, there was a spark of excitement in the air at the venue. It was a very different conference than the ones I’m used to — a mixed focus of product and open source, all rolled into one. I gained a ton of practical knowledge, especially about the Beats architecture and the recent Kibana changes to support plugins like . In the talk I gave, I enjoyed explaining the challenges we faced in building automation around Elasticsearch, and inviting new contributors to join me:  we even had some attend that I hadn’t met in person before! Since that talk, I’ve started to help maintain the popular Kibana and Logstash cookbooks in the Chef community as well. With the unification of things like plugin commands in the Elastic Stack, maintaining Chef automation across the products is starting to become easier too.If you weren’t able to attend Elastic{ON}, be sure you about next year’s conference. And if you get a chance, check out the videos () from this year. All talks are , and for free. I hope to see you next year at Elastic{ON}! Martin Smith is a DevOps Engineer at Rackspace Hosting, where he works with customers and colleagues on automation engineering and building IT infrastructure as code. With more than ten years of experience as a systems administrator and software developer, his interests also include load testing, performance tuning, and software testing in general. He strongly believes in open source and actively contributes to open source automation projects using Chef. When Martin isn’t writing code or reviewing pull requests, he enjoys sharing his skills with others, volunteering in his local community, and mentoring minority college students in STEM fields. ","locales":"","title":"Elastic{ON}<sup>16</sup> — One hundred days later"}
{"index":{}}
{"author":"Chad Pryor","category":"Engineering","publish_date":"2016-06-01T00:00:00.000Z","url":"/blog/building-cloud-sandbox-with-sample-data-v2","seo_title":"Elastic Cloud Tutorial: Getting Started with a sample dataset","content":" Getting an Elasticsearch environment up and running has never been easier. With Elastic Cloud, you can launch your cluster and start ingesting data in literally minutes. See how in this . This step-by-step set of instructions will walk you through setting up an Elastic Cloud account, creating and securing a Elasticsearch cluster, importing data, and visualizing it in Kibana. So, let's get started. Log into Elastic Cloud  Create your first hosted Elasticsearch cluster Secure your Cluster Next, let's configure cluster access and security. You can update your passwords or add additional users using the same process. You may also use the new security API that is included in 2.3.1 by following the .  Enable Kibana Elasticsearch EndpointOnce you are logged into Kibana, you will first see the Discovery tab. However, there is no data to visualize. Next, we will work on ingesting data into Elasticsearch. Let's gather some information so we can be successful.  Import Data Now, let’s get some data into our Elasticsearch cluster to see the Elastic Stack in action. If you don’t have a sample dataset handy, use one from the various data samples in our  I will be using the  and  (download your system version). To ingest the logs into our hosted Elasticsearch cluster, we will need to modify the elasticsearch output of the . 1. Download the repository, and change to the directory that contains the  file. Be sure to replace hosts endpoint in the config with your own cluster endpoint (copied in the previous step) 2. Modify username and password to the user account with write access configured Secure Elasticsearch section. I will be using user elasticsearch { hosts => \"https://e66e6e11692c749cc8e09f25e1af4efa.us-west-1.aws.found.io:9243/\" user => \"sa_admin\" password => \"my_f@ncy_p@55w0rd\" index => \"apache_elastic_example\" template => \"./apache_template.json\" template_name => \"apache_elastic_example\" template_overwrite => true } 3. Run the following command to index the data into Elasticsearch via Logstash: cat ../apache_logs | <Logstash_Install_Dir>/bin/logstash -f apache_logstash.conf 4. You can verify your data exists in Elasticsearch by going to , where is the Elasticsearch endpoint URL. You should see the count as 10000. {\"count\":10000,\"_shards\":{\"total\":1,\"successful\":1,\"failed\":0}} 5. You can verify the health of your cluster by going to . You should see your index listed along with its statistics: health status index pri rep docs.count docs.deleted store.size pri.store.size yellow open .kibana 1 1 2 0 19.1kb 19.1kb yellow open apache_elastic_example 1 1 10000 0 7.3mb 7.3mb Visualize DataNow let's access your Kibana instance and continue with the example instructions to visualize our data. 2. You can look at your data by selecting index on the Discovery tab. 3. Import the examples dashboard by clicking on  >  >  and selecting the  file. You can view this dashboard by clicking on the view button (eye icon) or by going to the Dashboards tab and clicking the Load Saved Dashboard button. Now you have some sample Apache log data in Elasticsearch and you can begin to get some insight and more importantly value from your logs. You can continue exploring with other sample datasets from the   and the   or start sending your own data by using Logstash or Beats. Here are some other useful links to help you on the journey of using the Elastic Stack on Cloud. Or, you can continue your Training with some official classes by some world class Education Engineers:  ","locales":"","title":"Tutorial: Getting Started with Elastic Cloud with a Sample Dataset"}
{"index":{}}
{"author":"Marty Messer","category":"Culture","publish_date":"2016-06-02T00:00:00.000Z","url":"/blog/elastic-support-speaking-code-and-human","seo_title":"Elastic Support: Speaking Code and Human","content":" I've worked in support for over twenty years — specifically in open source — and I can confidently say that support at Elastic is unlike anything else that exists out there. I mean, I don't know if you know, but typically, support sucks. And being in support sucks even more (just ask any support engineer you know). Why do I say this? Well, some software you buy doesn't always do what you were told it would. It's not as good as the packaging claims. This happens for a number of reasons: egos, cathedral building (when customer feedback doesn't make it back to engineering), marketing and sales being pressured to hit certain numbers no matter what it takes, etc. This gives customers certain expectations of how a product should perform, and when it doesn't work the way it's advertised … you know who has to absorb all that pain? Yeah, you know the answer. At Elastic, things are different. It starts with having empathy for our users. How can I explain it? I'll take you frame-by-frame it (bonus points if you get this reference). It starts with our very first support engineer. Do you know who that was? This guy named Shay Banon, Elastic co-founder and creator of Elasticsearch. Back in 2010 when Shay created Elasticsearch, he was the only person 'responsible' for supporting the developers who were using the software — answering questions on IRC and mailing lists (check out this exchange between Shay and Clinton Gormley, who joined Elastic two years later and ended up co-authoring ), reviewing and merging pull requests on GitHub … and, you know, continuing to build the software, too. NBD. That's open source, right? But just keep in mind that's our roots. That's where we came from. That's where we were born. And things evolved along the same path from there. Speaking Code and HumanAfter Elastic the company was founded, all the engineers we hired early on had to have the same skillset as Shay: a mix of being technically bad ass to continue building out our software, combined with the ability, desire, and skill to talk to people in order to support our customers. It was a fundamental part of their job. Fast forward to 2014, two years after Elastic the company was founded. That's when I came onboard to build a dedicated Customer Care team. Which has been really, well, incredible, and kind of f*cking awesome (can I say that? Well, I just did). Here's why. Our developers care. A lot.The people that created the open source projects that make up the — Elasticsearch, Kibana, Logstash, and Beats — all work here. Like Shay, these all started as passion projects they created and maintained ‘on the side'. And again like Shay, they served as their sole support engineers before this whole Elastic company became a thing. They care deeply about user feedback and take it seriously. Hang out in our and . Follow them on Twitter and you'll see exchanges like this one with Logstash creator Jordan Sissel. you were busy so I didn’t want to interrupt. But wanted to again say thanks for all your community work with — Chris (@tebriel) A lot of the developers we hire were avid users of the Elastic Stack before they got here… which is ultimately what brought them to Elastic. Many were contributors and active in the community, many were deploying our software in their companies or startups, and many had pre-existing relationships with our own developers. They're here because they not only have passion and love our software, but they want to continue to make it even better. The support team is fully integrated with the engineering teamBecause of what I already explained about our history — that supporting our users and making them successful is encoded into our DNA — the support team is part of the overall engineering organization. We are very closely aligned to product and development. We're constantly working together on Slack, and getting together virtually via video con","locales":"","title":"Elastic Support: Speaking Code and Human"}
{"index":{}}
{"author":"Costin Leau","category":"Releases","publish_date":"2016-05-31T00:00:00.000Z","url":"/blog/es-hadoop-5-0-0-alpha3","seo_title":"","content":" ​I am excited to announce the release of Elasticsearch for Apache Hadoop (aka ES-Hadoop) built against Elasticsearch 5.0.0-alpha3. : This is an alpha release and is intended for purposes only. Indices created in this version . What’s new? Upgrade to Storm 1.x Since ES-Hadoop 5.0.0-alpha3, the Storm support has been upgraded to 1.0.x. As this version is backwards compatible with Storm 0.9.x, support for these versions had to be dropped. (Do note that one can still use Storm 0.9.x with ES-Hadoop 2.x accordingly.) Changelog layout The release changelog is now aligned closer to that of Elasticsearch proper. Feedback Please [download] ES-Hadoop 5.0.0-alpha3, try it out and let us know what you think on , , or in the . ","locales":"","title":"Elasticsearch for Apache Hadoop 5.0.0-alpha3"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2016-05-31T00:00:00.000Z","url":"/blog/elasticsearch-5-0-0-alpha3-released","seo_title":"Elasticsearch 5.0.0-alpha3 released","content":" Today we are excited to announce the release of based on . This is the third in a series of pre-5.0.0 releases designed to let you test our your application with the features and changes coming in 5.0.0, and to give us feedback about any problems that you encounter.Open a bug report today and become an . This is an alpha release and is intended for . Indices created in this version . Upgrading 5.0.0-alpha3 to any other version is not supported.Elasticsearch 5.0.0-alpha3 is close to being feature complete, and builds on the work released in 5.0.0-alpha2. There are many small changes which you can read about in the release notes above, but some of the more interesting ones are mentioned below.Also take a look at the release announcements for and to read about features like: ","locales":"","title":"Elasticsearch 5.0.0-alpha3 released"}
{"index":{}}
{"author":"Suyog Rao","category":"Releases","publish_date":"2016-05-31T00:00:00.000Z","url":"/blog/logstash-5-0-0-alpha3-released","seo_title":"","content":" We are happy to announce the third pre-release (alpha3) in the 5.0.0 series! Please check the release notes for details, or jump directly to our if you can't wait to try it out!  This release is loaded with goodies that are highlighted below:Logstash release packages (Debian, RPM) has been given an overhaul in this release. Most importantly, this is a breaking change, so please read ahead! Previously, Logstash used directory to install the binaries, whereas Elasticsearch used , and . To make user experience consistent across our products — a theme you'll hear a lot in our 5.0.0 releases — we've to reflect Elasticsearch RPM and DEB layouts. Secondly, we added systemd and upstart support to run Logstash as a. Today, a lot of Linux distributions (Debian Jessie, Ubuntu 15.10+, and many of the SUSE derivatives) use systemd as the default service manager, and we received many requests from our users to support systemd. Oh, and all the JVM options have been extracted to a separate file, à la Elasticsearch, so you can easily override or pass in new JVM options by modifying a single clean file, instead of digging through BASH scripts. Say hello to file. Yep, this is a new one-stop-place to configure all application-level settings for the Logstash process. With all the new features we've been adding, our list of CLI options kept growing! To stop cluttering the CLI, and to make bootstrap easier, we've introduced a new settings file which mirrors the CLI. Most of the have been renamed to adhere to the yml dot notation, so this is filed under as well. Short form options have remained the same. If you installed an RPM or DEB package, look for this file in , or otherwise in . Just to be clear, the pipeline configuration where you specify the input, filters and outputs is separate from the settings file. Remember we released a new feature called Java Event — a rewrite of Event handling in pure Java — in version 2.3? Say you were using the Ruby Filter:  in some cases this change could have been backward incompatible because this filter allows users to manipulate the Event object directly using Ruby’s hash paradigm. To mitigate this, we rolled back this feature when we released version 2.3.1. Now, in 5.0.0 — a major release — we've handled this correctly by introducing non-ambiguous APIs to interface with the Event object. This change mostly affects plugin developers who write and maintain custom plugins — all the default packaged plugins have been updated to use the new APIs. If you maintain a custom plugin, or plan to write a new one, this old style of accessing Events directly using the Ruby hash convention will not work anymore. Please use these if you need access to data inside of the Event object. Kibana. Beats. And now Logstash has it too! We're talking about a plugin generator tool that makes it easier to develop new plugins for Logstash. Previously we've recommended developers to clone/fork the, but now you can simply do: bin/logstash-plugin generate --type input --name xkcd --path ~/ws/elastic/plugins This subcommand bootstraps a new plugin logstash-input-xkcd with the right directory structure and all the required files (templates) for you to start developing this plugin right away. So, go on, create that input to stream those fine comic strips to Kibana! Wait, you though we'd just talk about and not drop a strip here?! We said this was a loaded release before.. \"process\" : { \"peak_open_file_descriptors\" : 48, \"max_file_descriptors\" : 10240, \"open_file_descriptors\" : 48, \"mem\" : { \"total_virtual_in_bytes\" : 5274738688 }, \"cpu\" : { \"total_in_millis\" : 20792844000, \"percent\" : 23 } } Please try and let us know what you think! You can even for Elastic{ON} '17 when you help test our pre-releases! Your feedback and contribution is really important as we continue to iterate on 5.0.0. You can create issues on our, find us on our, or hang out with us on IRC (#logstash). ","locales":"","title":"Logstash 5.0.0-alpha3 released"}
{"index":{}}
{"author":"Michael McCandless","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2016-05-31T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2016-05-31","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top NewsUsing #Elasticsearch for #geohazards & working w' @esa to map ground deformation @terradue — elastic (@) Elasticsearch Core Apache Lucene Watch This Space, where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2016-05-31"}
{"index":{}}
{"author":"Shay Banon","category":"Releases","publish_date":"2016-05-31T00:00:00.000Z","url":"/blog/elastic-stack-release-5-0-0-alpha-3","seo_title":"Elastic Stack Release - 5.0.0-alpha3","content":" Alpha 1 has come and gone. Alpha 2 included a variety of new features. But, the release train continues.Or, the hits keep on coming.Perhaps, some other product release related catch-phrase. It's here! Say \"Heya\" to Alpha 3. Before you get too excited, keep in mind that this is still an Alpha, so don’t put it into production. And, since it is an Alpha, it is not available on . But, because Elastic Cloud is the official hosted Elasticsearch and Kibana offering on AWS, you’ll be able to deploy the 5.0 releases on Elastic Cloud the day of 5.0 GA.* (We wouldn’t want you to wait for all this goodness!) If you open a bug report, today, you too can become an . And now, without further ado, some highlights from Alpha 3. Elasticsearch For more detailed information, and many other features, peruse the Elasticsearch . From 2.3.x to 5.0. The magical journey. The undiscovered country. The next frontier. Other coined phrases. As a reminder, we've also released the , which runs on your existing 2.3 cluster.  Use this site plugin to prep for your migration. Kibana For more detailed information, and all the PR links in one place, visualize the future in the Kibana . Logstash All the information, in one central location. Parse the . Beats Explore the edge, in the Beats . ES-Hadoop ES-Hadoop v 5.0.0-alpha3 has also been released today. Peruse all the information in the . ","locales":"ja-jp","title":"Elastic Stack Release - 5.0.0-alpha3"}
{"index":{}}
{"author":"Monica Sarbu","category":"Releases","publish_date":"2016-05-31T00:00:00.000Z","url":"/blog/beats-5-0-0-alpha3-released","seo_title":"","content":" We are over-the-moon excited to announce the 5.0.0-alpha3 release, a major milestone on our road to Beats 5.0. IMPORTANT: This is an alpha release and it is intended for testing purposes only. Bye bye Topbeat, hello Metricbeat We are proud to see that in just 6 months since we launched libbeat 1.0, there are already around created by the community. If you have a closer look at the community Beats, you notice that many of them are used to collect metrics from various services. For example, queries the MySQL server for metrics, queries Nginx for metrics, queries Apache for metrics, and so on. Our own works in a similar way: it periodically queries the various operating system APIs for system statistics like CPU usage, memory usage, per process statistics, and indexes them to Elasticsearch. We’ve seen great adoption of Topbeat and have gotten constantly good feedback about it. Metricbeat is designed from the beginning to be modular so you can easily add new modules that collect data from external systems. For now Metricbeat includes the following modules: system, apache, mysql, nginx, redis, and zookeeper. We will keep extending this list during the alpha and beta phases of the 5.0 release. I would like to give special thanks to the community contributors (creator of Apachebeat), (Nginxbeat), and (Redisbeat) for converting their Beats to Metricbeat modules. In the default Metricbeat configuration, only the module is enabled, so running Metricbeat with the default configuration is equivalent with running Topbeat. It exports system statistics like CPU usage, memory, swap, per process statistics, per core statistics, and filesystem statistics. In addition, it also exports IO and network statistics, which were a popular feature request for Topbeat. This means that if you are currently using Topbeat, migrating to Metricbeat is easy. Here is the default Topbeat 1.x configuration: input: period: 10 procs: [\".*\"] stats: system: true process: true filesystem: true And here is the default Metricbeat configuration, exporting the same data: metricbeat.modules: - module: system metricsets: - cpu #- core #- diskio - filesystem #- fsstat - memory - process enabled: true period: 10s processes: ['.*'] So, if you are running the default Topbeat configuration, all you need to do is upgrade to Metricbeat and use its default configuration. You will get the same data, but in a slightly different format. Add conditions to filtering Filtering is a new feature added in 5.0.0-alpha2, and it’s available to all the Beats through . With filtering, you can reduce the number of fields that are exported by defining a list of filter actions that are applied to each event before it’s sent to the defined output. The filter actions are executed in the order that they are defined in the config file. Starting with alpha3, we introduce conditions to filter the exported fields only if the condition is fulfilled. We also add the operation to drop entire events. For example, if you are using Packetbeat to monitor your HTTP transactions, you can decide not to index the successful transactions in Elasticsearch by dropping all events that have the HTTP response 200 OK. The configuration file should include the following filters section: filters: - drop_event: equals: http.code: 200 Configuration files, how you like them The Beats use YAML for configuration and have a tradition of putting all the available options commented out in the configuration file together with a short description for each of them. This means that the configuration file also acts as a reference, so you almost don’t have to read the manual (you still should, it contains useful guides and more details for some of the options). The downside of this is that as we add more options to the Beats (notably the Redis and Kafka outputs), the configuration files tend to become very large, which contradicts the lightweight nature of the Beats. With the alpha3 release, each Beat comes with two versions ","locales":"","title":"Beats 5.0.0-alpha3 released"}
{"index":{}}
{"author":"Court Ewing","category":"Releases","publish_date":"2016-05-31T00:00:00.000Z","url":"/blog/kibana-5-0-0-alpha3","seo_title":"Kibana 5.0.0-alpha3 released","content":" This is a tale of triumph. Of intrigue and mystery, and an unwavering fortitude in the face of incredible odds. For in this tale, we release Kibana’s third alpha in two months. They said this could never happen. They said we’d never make it. Well, we made it. And you can get Kibana 5.0.0-alpha3 . This is alpha software that will only work with . Please test it, but do not use it in production. Indices created in this version will . Upgrading 5.0.0-alpha3 to any other version is not supported. With 30 pull requests in 28 days, we focused on making this the most stable pre-release yet. This is also an awesome release for the X-Pack UI, which includes two huge new features: X-Pack users and roles management Security UI, meet Users and Roles. Of course, you can still manage your users and roles via the X-Pack API, but why bother when you have that same power at your fingertips from within Kibana itself. X-Pack reports, now with history Reports now get queued and processed in the background, so you can generate a lot of reports without worrying about it having a negative impact on Kibana. The reports are then stored and appear in the UI, so you don’t need to regenerate the same report over and over again. And Kibana core, of course What’s next? Beta 1 is still on the horizon, but we want to get one more alpha out before then. You see, when we do launch beta 1, we want to shift our focus away from features and entirely toward stability and upgradability, but we don’t yet feel like we’ve jammed enough features into 5.0. Keep your eyes peeled for alpha4. In the meantime, grab alpha3 and help us find all of the bugs. Every. Last. One. We’d love your feedback on our and any bug reports on . As always, feel free to reach out to us on or as well. ","locales":"","title":"Kibana 5.0.0-alpha3 released"}
{"index":{}}
{"author":"Marcelo Rodriguez","category":"Engineering","publish_date":"2016-05-31T00:00:00.000Z","url":"/blog/quick-start-guide-configuring-elasticsearch-with-shield-and-active-directory","seo_title":"Quick Start Guide - Configuring Elasticsearch with Shield and Active Directory","content":" When learning a new system, I always find it useful to have instructions on how to install and configure a feature with minimum steps in a cookbook style format.  It allows me to get up and running quickly without having to reference several pages with the multitude of optional settings.  From that point, I can take a look at the reference manuals and the more advanced options to configure the components according to my end architecture requirements. In this post, I’ll go through installing and configuring Elastic Shield with Elasticsearch to use Windows Active Directory domain authentication for Elasticsearch Administrators.   (Repeat steps below for every node in the cluster) 1. Download and install the public signing key: rpm --import https://packages.elastic.co/GPG-KEY-elasticsearch 2.  Create the new yum repo file for Elasticsearch: vi /etc/yum.repos.d/elasticsearch.repo 3.  Copy and paste the following entries in the new repo file: [elasticsearch-2.x] name=Elasticsearch repository for 2.x packages baseurl=https://packages.elastic.co/elasticsearch/2.x/centos gpgcheck=1 gpgkey=https://packages.elastic.co/GPG-KEY-elasticsearch enabled=1 1.  Create a user who will be the Elasticsearch administrative user in Active Directory Users and Computers (ADUC). See example below for user named Elasticsearch Administrator. 2.  Create an Active Directory security group for the Elasticsearch Admins.  This group will be used to hold our new user account.   (Repeat steps below for every node in the cluster) 1.  Install the license plugin.  *The license plugin is required to run commercial plugins from Elastic.  /usr/share/elasticsearch/bin/plugin install license 2. Install the Shield plugin. /usr/share/elasticsearch/bin/plugin install shield 1.  Open the elasticsearch.yml file. vi /etc/elasticsearch/elasticsearch.yml 2.  Add the following entries at the bottom of the file, replacing the sample domain entry with your specific domain: #-----SHIELD CONFIG------ shield: authc: realms: active_directory: type: active_directory order: 0 domain_name: mydomain.local url: ldap://mydomain.local:389 unmapped_groups_as_roles: false 3.  Repeat Steps 1 and 2 for all nodes in the cluster. 4.  Open the role_mapping.yml file. vi /etc/elasticsearch/shield/role_mapping.yml 5.  Add the following entry at the bottom of the file, replacing the sample DN location with the location noted above in Section B, Step 2. admin: - \"cn=ESAdmins, ou=Groups, ou=Elasticsearch, dc=mydomain, dc=local\"6.  Copy the role_mapping.yml file to all nodes in the cluster as in the examples below replacing sample node names with your node names: scp /etc/elasticsearch/shield/role_mapping.yml root@esnode2:/etc/elasticsearch/shield/ 7. Restart elasticsearch service on each node. service elasticsearch restart You can test by simply performing the following query replacing the sample node name with yours: curl -XGET ‘esnode1:9200’ -u esadmin Command and output should look something like this: [root@esnode1 ~]# curl -XGET 'esnode1:9200' -u esadmin Enter host password for user 'esadmin': { \"name\" : \"esnode1\", \"cluster_name\" : \"marscluster1\", \"version\" : { \"number\" : \"2.3.2\", \"build_hash\" : \"b9e4a6acad4008027e4038f6abed7f7dba346f94\", \"build_timestamp\" : \"2016-04-21T16:03:47Z\", \"build_snapshot\" : false, \"lucene_version\" : \"5.5.0\" }, \"tagline\" : \"You Know, for Search\" } There are many options and combinations that are possible. Once you are able to set up this basic Shield and Elasticsearch combination, you can find additional resources below to customize for your architecture requirements and needs. For additional information on:  Active Directory authentication options: Role-based access: Mapping groups to roles: Encrypting Active Directory communication with SSL/TLS: Installing an","locales":"","title":"Quick Start Guide - Configuring Elasticsearch with Shield and Active Directory"}
{"index":{}}
{"author":"Tudor Golubenco","category":"Brewing in Beats","publish_date":"2016-05-30T00:00:00.000Z","url":"/blog/brewing-in-beats-getting-metricbeat-ready-for-release","seo_title":"","content":" Welcome to Brewing in Beats! With this series, we're keeping you up to date with all that's new in , from the details of work in progress pull requests to releases and learning resources. New community Beat: CloudTrailBeat An interesting new Beat was created by . relies on a combination of SNS, SQS and S3 to create a processing 'pipeline' to process new log events quickly and efficiently. The Beat polls the SQS queue for notification of when a new CloudTrail log file is available for download in S3. Each log file is then downloaded, processed and sent to the configured receiver (logstash, elasticsearch, etc). Metricbeat: export CPU usage fields as percentages With , the system module in Metricbeat exports all CPU usage fields (user, system, idle, etc.) as percentages from the total CPU time. This makes it easier to make sense of the values. The old “ticks” units are also available, but disabled by default. Metricbeat: export network IO stats This new provides network IO metrics collected from the operating system. An event is generated for each network interface. This metricset is available on Darwin (aka OSX), Windows, Linux, and FreeBSD. Filebeat new registry file format The previous format for the registry file was a dictionary with the file paths as keys. This had the potential to lead to overwrite and conflicts on file rotation. As the file path is also stored inside the state object this information was duplicated. Now which makes the format more flexible and brings it close to the format used by Logstash. The migration from the old format happens automatically on the first run. Lumberjack code extracted in a library We now have a library that implements both the client and the server side of the Lumberjack protocol, which we use between Beats and Logstash. The unit tests for logstash output now use , simplifying test code quite a bit. Custom incomplete lumberjack server has been removed from output plugin. The above referenced PR also does the first steps towards replacing the default JSON encoding with custom code that does JSON encoding without reflection and with fewer allocations. Generator for Metricbeat modules/metricsets Adding a MetricSet requires creating several files with a predefined file structure for automation. To simplify the task of creating a MetricSet and make sure the correct structure is used, this task is now . Normalizing Metricbeat fields We’ve continued to normalize the fields exported by Metricbeat to use a common format. This week the and the field names were refactored. Configuration files improvements Continuing the theme from the last weeks, the configuration files were improved to the “full” versions. Our system tests were to use a similar structure for configuration as the files that we ship. Also, to use the same style in configuration samples. Packetbeat NFS fixes and refactoringCommunity contributor has done a great job improving the NFS module in packetbeat with fixes and refactorings to make the code cleaner. ","locales":"","title":"Brewing in Beats: Getting Metricbeat ready for release"}
{"index":{}}
{"author":"Tyler Fontaine","category":"Engineering","publish_date":"2016-05-31T00:00:00.000Z","url":"/blog/lost-in-translation-boolean-operations-and-filters-in-the-bool-query","seo_title":"Boolean Operations and Filters in the Bool Query in Elasticsearch","content":" With on the horizon, a number of query types deprecated in 2.x will be removed. Many of those are replaced by functionality of the bool query, so here’s a quick guide on how to move away from :  , , queries:  and a general look into how to parse boolean logic with the bool query.For the examples used in this article, let's assume this scenario: A school surveyed its students on their preferences for fruit snacks. Now you're putting together an application so administrators can view this data. Because of the many grades, categories, and other features of the data, you may find you have some arbitrarily complex boolean logic to deal with.Matching Boolean Operations with the Bool Query FieldsLet's get to the heart of these boolean operations and how they'd look without the and, or, not queries. In the bool query, we have the following fields: is analogous to the boolean , is analogous to the boolean , and is roughly equivalent to the boolean . Note that isn't exactly like a boolean , but we can use it to that effect. And we’ll take a look at later on.Boolean AND and NOT are easy, so let's look at those two first. If you want documents where preference_1 = Apples AND preference_2 = Bananas the bool query looks like this:{ \"query\" : { \"bool\" : { \"must\": [{ \"match\": { \"preference_1\": \"Apples\" } }, { \"match\": { \"preference_2\": \"Bananas\" } }] } } }If you want documents where preference_1 != Apples:{ \"query\" : { \"bool\" : { \"must_not\": { \"match\": { \"preference_1\": \"Apples\" } } } } }But what about OR? That's where the parameter comes in. If you want the set of documents where preference_1 = Apples OR preference_1 = Raspberries:{ \"query\" : { \"bool\" : { \"should\": [{ \"match\": { \"preference_1\": \"Apples\" } }, { \"match\": { \"preference_1\": \"Raspberries\" } }] } } }So these, then, can all be combined into much more complex boolean logic, because we can easily nest bool queries. So let's look at this boolean logic: So in this case, you are searching for documents that match this set of rules, so documents where preference_1 is Apples and term 2 is either Bananas or Cherries, OR preference_1 is grapefruit, regardless of what term 2 equals.That logic translates into a query that looks like this:{ \"query\": { \"bool\": { \"should\": [{ \"bool\": { \"must\": [{ \"match\": { \"preference_1\": \"Apples\" } }, { \"match\": { \"preference_2\": \"Bananas\" } }] } }, { \"bool\": { \"must\": [{ \"match\": { \"preference_1\": \"Apples\" } }, { \"match\": { \"preference_2\": \"Cherries\" } }] } }, { \"match\": { \"preference_1\": \"Grapefruit\" } }] } } }To break this down a bit further:Note that the whole of this query is wrapped in a which satisfies the three OR clauses, and each individual piece is its own nested bool query.So the piece is{ \"bool\": { \"must\": [{ \"match\": { \"preference_1\": \"Apples\" } }, { \"match\": { \"preference_2\": \"Bananas\" } }] } }And because this is all wrapped in a should, the next in the chain would be an ORSo the piece would be another bool:{ \"bool\": { \"must\": [{ \"match\": { \"preference_1\": \"Apples\" } }, { \"match\": { \"preference_2\": \"Cherries\" } }] } }And then finally the single term:{ \"match\": { \"preference_1\": \"Grapefruits\" }The query can be arbitrarily complex, to fit your particular boolean requirement. Each piece can be broken down and turned into its elementary boolean expressions, then chained together as shown above, to make sure you're retrieving the right documents. It’s also worth noting here that you can set to a value you choose. This is the prime difference of the function from the boolean . By default , defaults to 1, but if you would like for more than one should clause to match for a document to be returned, you can increase this value.Filtered QueriesBecause filtered queries have also been deprecated in 2.x, the new method is the field in the bool query. So let's take our boolean logic from before: . ","locales":"","title":"Lost in Translation: Boolean Operations and Filters in the Bool Query"}
{"index":{}}
{"author":"Emmanuel Mathot","category":"User Stories","publish_date":"2016-05-27T00:00:00.000Z","url":"/blog/elastic-earth-science-for-global-monitoring","seo_title":"Elastic Earth science for global monitoring","content":" Our living planet is continuously monitored by a growing number of Earth observation satellites that produce terabytes of data daily. Europe is taking a lead role in the mission and is stimulating the use of this large amount of public and private satellite data. At we are developing platforms and applications for researchers and practitioners in Earth Science to help them extract information from these massive amounts of data. We are also bringing the community together with the European Space Agency to serve all the major research institutes in Europe, and aid international cooperation abroad. Hereafter, we share a look at a specific domain, the , for which Terradue developed a query engine powered by Elasticsearch aimed at selecting the best satellite acquisitions for the application, a technique for mapping ground deformation using radar images of the Earth's surface. In a few words, InSAR (Interferometric Synthetic Aperture Radar) consists in comparing phase information from images taken at different times, by the radar instrument on board of a satellite. This technique can measure the smallest terrestrial displacements down to a centimeter! Not clear enough? © DLR/EOC The above image is a 3D view of an area near the boundary of the Indian and Eurasian tectonic plates over Nepal. The colored zone corresponds to displacement of the Earth ground after the 7.8 magnitude earthquake that struck Nepal on 25 April 2015. To obtain this result, SAR experts compared two satellite images using a complex processing technique that depends on one crucial step - data selection. One image was taken just after the earthquake (post-event) and one before the earthquake (pre-event). The latter one was not selected randomly among the large archive of satellite acquisitions, but must be chosen based on analysing and minimizing the “baseline” between the two acquisitions. Because a satellite flies an orbital pattern the sensor is at a different position every time it images the same area. This is how SAR imagery is collected from slightly different viewing angles. ","locales":"","title":"Elastic Earth Science for Global Monitoring"}
{"index":{}}
{"author":"Tudor Golubenco","category":"Brewing in Beats","publish_date":"2016-05-26T00:00:00.000Z","url":"/blog/brewing-in-beats-conditional-filtering","seo_title":"","content":" Welcome to Brewing in Beats! With this series, we're keeping you up to date with all that's new in , from the details of work in progress pull requests to releases and learning resources. Last week we have released Beats 1.2.3 containing a few important . Besides that, we have worked on a set of improvements that we plan to release with 5.0.0-alpha3: Conditionals for generic filtering It is now to use conditionals when filtering out fields and events in libbeat. This makes it really easy to remove unneeded data even before sending it over the network. Here is an example filter that drops the HTTP header fields if the code is 200 and the status is OK: - drop_fields: fields: [\"http.request_headers\", \"http.response_headers\"] equals: http.code: 200 status: OK Metricbeat gets an Nginx module KS Chan, the person behind , has contributed an to Metricbeat. Metricbeat progress As anticipated last week, the work has started to normalize the metric names so that users get an uniform experience regardless of the system they monitor. For example, here is the and here is the one . Other Metricbeat improvements include: Configuration refactoring The work on reorganizing the options in our configuration files is almost . We now distribute two versions of the configuration files: the default one which contains only the most common options and a “full” one, that contains all non-deprecated options with longer comments. The full versions can be almost used as a reference and it is easy to copy and paste between them. Another configuration related improvement is that all now accept the same specifiers (e.g 10s, 1ms). This change was done in a backwards compatible way. Static builds We now produce for Filebeat and Winlogbeat (which don't use CGo) by using simple cross compilation, without Cgo enabled. The advantage of the statically linked binaries is that they tend to be more portable. The Beats that use CGo continue to be dynamically linked against libc and not much else. Package names We have updated our to be consistent with the other Elastic stack projects. Filebeat to Logstash performanceThe work on figuring out why the Filebeat to Logstash communication is slower than expected is now resumed. The current intention is to rewrite the beats input plugin in Logstash to use Netty. It’s really early, but there are some very promising results already. ","locales":"","title":"Brewing in Beats: Conditional Filtering"}
{"index":{}}
{"author":"Mark Harwood","category":"Engineering","publish_date":"2016-05-25T00:00:00.000Z","url":"/blog/using-elastic-graph-and-kibana-to-analyze-panama-papers","seo_title":"Using the Elastic Graph on Panama Papers Analysis","content":" The new Elastic Graph capabilities allow you to analyse connections in data. Whether it is chasing down the tangled web of offshore financial arrangements in the Panama Papers or a high-level overview of click behaviour on a busy ecommerce website, Graph technology helps bring these relationships into focus. The Graph capability is bundled as part of the commercial X-Pack plugins for the Elastic Stack and includes a Kibana app and a new Elasticsearch API. In this first Graph blog we'll take a brief look at what the combination of the Kibana app and the API can offer. Forensic analysis: Panama Papers from the offshore law firm Mossack Fonseca is one of the most explosive news stories of 2016. The records reveal that many politicians, members of royalty, the rich and their families are exploiting networks of shell companies established in secretive offshore tax regimes. Journalists and financial institutions are now intently focused on this data, but unravelling the connections can be both difficult and time consuming. The Kibana Graph app makes this process simple for anyone: Above we see the companies and individuals connected to Vladimir Putin's close friend, .  This picture was built up from a few simple steps: Selecting the datasource Initially we select \"panama\" from our list of indices and then select one or more fields whose values we want to show in the diagram. Each field can be given an icon and colour for the \"vertices\" that will appear in the diagram. Running a search Now we can run a regular free-text search to match documents containing the name of Putin's friend, \"Roldugin\". The terms found in the matching documents are shown as a network — each line representing one or more documents that connect a pair of terms.  The journalists at the ICIJ who are curating the data have tried to give each real-world entity (person/company/address) a unique ID that is attached to every document that references them.  Unfortunately people names and addresses can be awkward to match — the journalists correctly identified three documents that are connected to Person entity 12180773 but we can see that there are two other people with similar names, but they have been assigned different identity codes. Equally there are two addresses that look similar but have been assigned different identity codes. In future blog posts, we will talk about using the Graph API for automated entity resolution. For now let's fix this manually with the grouping tool. Grouping vertices Using the advanced mode tools we can select, then click the group button to merge vertices. This gives us a cleaner picture. If we wanted, we could further group already grouped items e.g. merging people with multiple identities into single vertices and then merging those into company vertices. \"Spidering out\" Now what if we wanted to see what else was connected with these entities? We can continue to explore the connections in the data using the \"+\" button on the toolbar to pull in other related entities. We can expand out the picture further by pressing \"+\" repeatedly and use selections to focus on expanding only certain areas of the graph. The undo and redo buttons are important parts of backing out of any uninteresting results. Additionally, delete and blacklist buttons allow control over which vertices are currently visible or can return. Snippets of example documents behind selected vertices can also be shown. If you'd like to explore the Panama Papers data yourself, grab a copy of , , and the then follow the index setup at Wisdom of crowds The Panama Papers are an example of a detailed \"forensic\" type investigation where each single document may represent a highly important connection. However, where the Elastic Graph technology can really shine is in its ability to summarise mass user behaviour such as the data found in ","locales":"de-de,fr-fr,ja-jp,ko-kr","title":"Using Elastic Graph + Kibana to Analyze Panama Papers"}
{"index":{}}
{"author":"Andrew Kroh","category":"Engineering","publish_date":"2016-05-26T00:00:00.000Z","url":"/blog/monitoring-windows-logons-with-winlogbeat","seo_title":"","content":" Windows event logs can provide invaluable insight into your Windows based infrastructure. The Windows operating system has many event log channels, each dedicated to a specific category of events. In this blog post we are going to look at how to visualize logon and logon failure events from the Security event log. Winlogbeat is our lightweight shipper for Windows event logs. It installs and runs as a Windows service and ships event log data to Elasticsearch or Logstash. We will install Winlogbeat 5.0 on all machines in our example domain. Winlogbeat 5.0 has a that enables it to ship the raw data that was used in logging the event. Having these raw event data fields makes filtering and aggregating much easier than in earlier versions of Winlogbeat. Setup Below is the configuration file being used with Winlogbeat to ship data directly to Elasticsearch. For more information on how to install Winlogbeat please see the . winlogbeat.event_logs: - name: Security ignore_older: 168h output.elasticsearch: hosts: [\"elasticsearch.elastic.local:9200\"] template.name: \"winlogbeat\" template.path: \"winlogbeat.template.json\" template.overwrite: false On domain controllers I am adding an additional line to the configuration file as shown below. This will tag all events from the domain controllers with “dc”. The tag will we be used for filtering. tags: [“dc”] Monitoring for Successful Logons The reason for monitoring successful logons is to look for compromised user credentials. The number of successful logons can be a major indicator that compromised credentials are being used for system crawling or other malicious activity. An event with event ID is logged by Windows for every successful logon regardless of the (local, network, remote desktop, etc.). If we simply created a data table visualization in Kibana showing all events with event ID 4624 we would be overwhelmed with noise and it would not be easy to spot abnormal user logon patterns. So there are several filtering steps we are going to apply to remove the noise. Each of the filters is described below. The data table visualization shown above was created using this list of filters. The data table uses aggregations to count the total number of logons per user, the number of unique computers the user logged on to, and the number of unique source IPs that were used in those logons (if the user was remote). The table can be used to spot accounts that are potentially compromised. If an account has been used to logon to an abnormally high number of computers within your organization, it would warrant further investigation to see if the account is being used to crawl the network. Monitoring Logon Failures Monitoring failed logons can be useful for a number of purposes. From a security perspective, the obvious use cases are to detect unauthorized access attempts and brute-force credential attacks. From an infrastructure management perspective, failed logon attempts can be used to detect problems. For example, if a service account starts generating failures then it may indicate a configuration issue. Windows uses event ID when logging failed logon attempts. To visualize the failed logons we are going to use an area chart and simply filter for . To show the different types of logons being used we split the area based on the field. An example is is shown above. If anomalies are spotted in the chart, the user can select a more specific time period or logon type and use the Discover tab to view more details. Visualizing the Origin of Remote Logons (requires Logstash) If you allow Remote Desktop connections from the Internet it can be useful to plot the origins of those connections of a map. Both successful and failed logons report the IP address of the client in the field. By using a GeoIP filter to enrich the event with the location associated with the source IP address, we can visualize the events on a map in Kibana. This type of visualization can be used t","locales":"","title":"Monitoring Windows Logons with Winlogbeat"}
{"index":{}}
{"author":"Suyog Rao","category":"Engineering","publish_date":"2016-05-24T00:00:00.000Z","url":"/blog/logstash-lines-2016-05-24","seo_title":"","content":" Welcome back to The Logstash Lines! In these weekly posts, we'll share the latest happenings in the world of Logstash and its ecosystem.Following projects are in progress: Monitoring: New System Stats Following up on monitoring APIs that got added in alpha1, we've enhanced to include detailed, process level information like file descriptors, cpu and memory stats. All this can be found in As part of exposing these stats, we've refactored this module for maintenance — removed boilerplate code, simplified classes to mirror API structure, etc. Plan is to merge this into alpha3 (). Release Packages This is a breaking change to make the directory structure in LS release packages (RPM, DEB)  to ES. As part of this work, we are also doing a much needed upgrade to our service scripts to be able to use systemd and upstart. Exploring Use Of Log4j To make improvements to logging framework used by Logstash, we are exploring a Log4j. This will allow us to do component based logging (think per-plugin), log rotation, dynamic log level setting etc. Tricky bit is to come up with a solution that works well with existing structured logging APIs used in JRuby and pure Java (). Improve Beats Input Collaborating with the Beats team to improve performance of LS beats input. A POC of rewriting this input using an async Netty based approach has showed promising . Logstash Settings File Say hello to . This week we merged a feature to master and 5.0 which allows users to configure Logstash bootstrap settings using a yml file instead of doing it via CLI options. Please be aware that this is a breaking change in that most long form CLI options have been changed to mirror the yml dot notation. Using a settings file will un-clutter the already crowded CLI options and allow us to introduce more configurations for future features. Most importantly, we can now ship experimental features using feature flags. (). Others: ","locales":"","title":"Logstash Lines: New Settings File, Release Packages Improvements"}
{"index":{}}
{"author":"Michael McCandless","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2016-05-23T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2016-05-23","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top NewsUnwittingly using deprecated features in #elasticsearch? Maybe not for much longer — Chris Earle (@) Elasticsearch Core Apache Lucene Watch This Space, where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2016-05-23"}
{"index":{}}
{"author":"Greg Marzouka","category":"Engineering","publish_date":"2016-05-24T00:00:00.000Z","url":"/blog/ingest-node-a-clients-perspective","seo_title":"Elastic Ingest Node: A Client's Perspective","content":" With the of Elasticsearch 5.0 comes a ton of new and awesome features, and if you've been paying attention then you know that one of the more prominent of these features is the new shiny . Simply put, ingest aims to provide a lightweight solution for pre-processing and enriching documents within Elasticsearch itself before they are indexed. We aren't going to dive into the details of how ingest node works in this blog post (it is recommended that you  as a prerequisite), but instead we're going to showcase how to consume the ingest APIs from an Elasticsearch client. In these examples we're going to use , the official .NET client, but keep in mind that these concepts apply to any of the . So whether you're C# inclined or not, transferring this knowledge to the language of your choice should be trivial. Creating an ingestion pipelineIn Elasticsearch 5.0, all nodes are ingest nodes by default, so there's nothing to do in terms of setting up Elasticsearch. Thus, the first step to enriching documents via an ingest node is to create an ingestion pipeline. Let's use the classic Tweet example (my apologies) and create a index in our Elasticsearch cluster with the following mapping: { \"tweets\": { \"mappings\": { \"tweet\": { \"properties\": { \"lang\": { \"type\": \"keyword\" }, \"message\": { \"type\": \"text\" }, \"retweets\": { \"type\": \"integer\", \"coerce\": false } } } } } } Here, contains the actual content of the tweet, represents the language code (, , etc...), and is the number of times the tweet has been re-tweeted. Now imagine we have the following C# type to model our documents: public class Tweet { public string Message { get:  set:  } public string Lang { get:  set:  } public string Retweets { get:  set:  } } Notice that is of type , and perhaps for some reason we cannot to change the type. Well, since we set in our mapping of the field, if we try to index one of these documents, Elasticsearch is going to throw a parse exception since the incoming value will be a . So, what can we do? Let's create a pipeline that converts our field to an before indexing it. While we're at it, let's also uppercase our language codes since they are of type in our mapping and so case-sensitivity matters. Here's what this looks like using NEST: var client = new ElasticClient():  client.PutPipeline(\"tweet-pipeline\", p => p .Processors(ps => ps .Convert<Tweet>(c => c .Field(t => t.Retweets) .Type(ConvertProcessorType.Integer) ) .Uppercase<Tweet>(u => u .Field(t => t.Lang) ) ) ):  So what we've done here is used the  to create a pipeline with the id \"tweet-pipeline\" that has two processors. A for converting from to , and an for, you guessed it, upper-casing the value of our field. Indexing documentsNow that we've created our pipeline, let's index some documents using the bulk API and enrich them through the pipeline. client.Bulk(b => b .Index(\"tweets\") .Pipeline(\"tweet-pipeline\") .Index<Tweet>(i => i .Document(new Tweet { Retweets = \"4\", Message = \"Hello, Twitter!\", Lang = \"en\" }) ) .Index<Tweet>(i => i .Document(new Tweet { Retweets = \"32\", Message = \"Bonjour, Twitter!\", Lang = \"fr\" }) ) .Index<Tweet>(i => i .Document(new Tweet { Retweets = \"\", Message = \"Hallo, Twitter !\", Lang = \"nl\" }) ) ):  Business as usual, except notice we specified a pipeline. This tells Elasticsearch we want to pre-process each document using the \"tweets-pipeline\" we created earlier before indexing. Here we specified the pipeline for all index commands, but we could specify a different pipeline for each individual index command if we had multiple pipelines. Handling errorsIf we inspect the response from our bulk request: { \"took\" : 33, \"ingest_took\" : 4, \"errors\" : true, \"items\" : [ { \"index\" : { \"_index\" : \"tweets\", \"_type\" : \"tweet\", \"_id\" : \"AVSl-cCKVD5bKRQTTXNo\", \"_version\" : 1, \"_shards\" : { \"total\" : 2, \"successful\" : 1, \"failed\" : 0 }, \"created\" : true, \"status\" : 201 } }, { \"index\" : { \"_index\" : \"tweets\", \"_","locales":"","title":"Ingest Node: A Client's Perspective"}
{"index":{}}
{"author":"Tudor Golubenco","category":"Releases","publish_date":"2016-05-18T00:00:00.000Z","url":"/blog/beats-1-2-3-released","seo_title":"","content":" We’re happy to announce that the 1.2.3 bug fix release is now available for all the Elastic Beats: Filebeat, Packetbeat, Topbeat and Winlogbeat. You can download the new versions from the usual . You can find the full release notes . Fix Filebeat sending duplicate logs of rotated files on restart This fixes an where Filebeat didn’t record the file offset on shutdown of rotated files. This could cause resending of already shipped files on restart. Note that while the bug is fixed with the 1.2.3 release, the duplicates can still happen one time only on the upgrade from 1.2.2. Fix Topbeat high CPU usage on Windows Before this release, Topbeat could cause on Windows when using a custom regular expression pattern to filter processes. The root cause was that the caching that we were using for command lines (getting the command line is fairly expensive on Windows) wasn’t working when most of the processes were filtered out. Fix Winlogbeat panic on large events Messages larger than 32K could cause Winlogbeat to crash on Windows XP or Windows 2003. There are more details in this . Easy as 1, 2, 3Many thanks to our dear community members who tested the Beats, reported and helped with the troubleshooting of the above issues. If you find any issues with the new release or there are features that you miss, please interact with us on Discuss or GitHub. ","locales":"","title":"Count with us: Beats 1.2.3 released"}
{"index":{}}
{"author":"Daniel Palay","category":"User Stories","publish_date":"2016-05-18T00:00:00.000Z","url":"/blog/elastic-yale-and-the-quest-to-cure-cancer","seo_title":"Elastic, Yale, and the Quest to Cure Cancer","content":" The date was December 4, 2014, and we had just launched the Call for Papers for our first ever Elastic{ON}. I didn't really know what to expect and then boom, an email hits my inbox from Dr. Wade Schulz of the Yale Department of Laboratory Medicine: Hold up — did I just read that our software was being used to help to find causes and treatments for cancer?! This realization was and continues to be one of my proudest moments working for Elastic. (Now I couldn't sleep because I needed to know these answers.)So we invited Wade to join us at Elastic{ON}. His pinpoint exact DNA variants that led to higher cancer risk or treatment possibilities was the perfect fit for the conference. The standing-room only crowd got a great mix of technical, biological, and medical knowledge and I walked away wanting more.Luckily Elastic{ON} was only the beginning of a beautiful friendship with Wade. Over the next several months we stayed in touch hoping to see how their Elastic Stack use case evolved and what their final research produced. Then, in November 2015, Wade invited me and Elastic videographer Ben Ferrer — my partner-in-storytelling-crime — to Yale New Haven Hospital. Wade gave us front-row seats to see firsthand how the Elastic Stack is helping power the next generation technology they are using to help find a cure for cancer. We had the camera rolling for nearly four hours, our attention and curiosity entranced by their work.As a science nerd, I spent the day geeking out about the machines we saw in action and the discoveries that I saw possible. But even more, as a human — and one who has lost a mother to cancer — I was humbled by the work being done and the hope that emanated from those halls. It's the kind of hope that our world needs right now: hope for treatments that give patients their lives back, that give parents more time with their kids, friends more time with their loved ones, and hope that once and for all, the power of human ingenuity will win, and cancer will most definitely lose.This is not the end of the journey (nor my friendship with Wade). But for now, I hope you enjoy the story (so far) of how the Elastic Stack is powering the search for cancer's cure… ","locales":"","title":"Elastic, Yale, and the Quest to Cure Cancer"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2016-05-18T00:00:00.000Z","url":"/blog/elasticsearch-2-3-3-released","seo_title":"Elasticsearch 2.3.3 released","content":" Today we are pleased to announce the bug fix release of based on . All users of 2.3.x are advised to upgrade. This release is already available on , our Elasticsearch-as-a-service platform.Latest stable release:Full details of the changes in this release are available in the release notes listed above, but some of the more important changes are mentioned below: ","locales":"","title":"Elasticsearch 2.3.3 released"}
{"index":{}}
{"author":"Michael McCandless","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2016-05-17T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2016-05-17","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top NewsVideo of my \"Ingest Node: (re)indexing and enriching documents within #elasticsearch\" talk given at #DevCon16 is up — Luca Cavanna (@) Elasticsearch Core Apache Lucene Watch This Space, where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2016-05-17"}
{"index":{}}
{"author":"Costin Leau","category":"Releases","publish_date":"2016-05-18T00:00:00.000Z","url":"/blog/es-hadoop-2-3-2-released","seo_title":"","content":" I am pleased to announce the release of Elasticsearch for Apache Hadoop (aka ES-Hadoop) . ES-Hadoop 2.3.2 is a bug-fix release, addressing handling on Hive, introducing a couple of options for Kerberos configurations for repository hdfs plugin and also upgrading the Elasticsearch dependency to . Feedback Looking forward to hearing your feedback on these ! You can find us on , Twitter () or the . works too. ","locales":"","title":"Elasticsearch for Apache Hadoop 2.3.2 released"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2016-05-17T00:00:00.000Z","url":"/blog/brewing-in-beats-disk-io-status","seo_title":"","content":" Welcome to Brewing in Beats! With this series, we're keeping you up to date with all that's new in , from the details of work in progress pull requests to releases and learning resources. Last week we mostly worked on improving Metricbeat and reorganizing the configuration files of all our Beats. Here are the most interesting pull requests that were merged in master: Monitor disk IO with Metricbeat Adding disk IO information was a very requested feature in Topbeat and is finally available in the system module of Metricbeat. Currently it works on Linux, Windows, and FreeBSD (amd64). Standardize the exported fields We decided to use the same format of the exported fields for all our Beats and the dot notation in Metricbeat `moduleName.metricSetName` (the dot is actually a sub-document) instead of `moduleName-metricSetName`. We are planning also to standardize the metric names and follow the rules: Refactor the configuration files The Beats are now following the format of the Elasticsearch yml configuration file in using the that makes the configuration file more robust as it doesn’t rely so much on indentation. The change affects all the configuration files, but it doesn’t break compatibility as the old way of writing is still accepted. The configuration files are containing all the options that can be configured, so they are bit too long. We decided to add a of the configuration file for each Beat. The short version would become the default configuration file and the long version would serve as a guide for more configuration options. Other Metricbeat improvements Proper shutdown in Filebeat Clean shutdown for all Beats is an important prerequisite for features like configuration reloading and running multiple beats in a single process. It is also important for our test suite. This  refactors the Filebeat code to make sure it shutdowns correctly. Deprecate GeoIP support One of the older Packetbeat features is that you can visualize the location of your clients on a map in Kibana based on the GeoIP information of the source IPs of the incoming HTTP requests. While this is a very appreciated feature, doing GeoIP lookups in Packetbeat is not needed, because you can better do it using Logstash or the Ingest Node in Elasticsearch (new in 5.0). So, we the GeoIP in 5.0. Switch to Perl like regular expressions We found some limitations in the current use of our regular expression engine () due to using the POSIX subset only, so we decided to closer in 5.0 to support same general regular expression syntax used by Perl, Python and other languages. ","locales":"","title":"Brewing in Beats: disk IO status"}
{"index":{}}
{"author":"Kevin Moore","category":"User Stories","publish_date":"2016-05-23T00:00:00.000Z","url":"/blog/from-splunk-to-elastic-in-4-weeks-challenge-accepted","seo_title":"From Splunk to Elastic in 4 weeks – Challenge Accepted!","content":" Like a lot of companies, invested heavily in log aggregation and analysis, which quickly became an absolutely critical tool, both from an operational perspective and also for the development teams. And Splunk became the go-to place to find answers to many questions relating to performance and what was going on in the systems. Paddy Power sees double digit growth year on year and not surprisingly we rather quickly hit our daily license limits in what we were indexing in Splunk. We got a temporary increase while a budget request was submitted for a license increase. That request was rejected so I was set a challenge to build an alternative to Splunk that not only provides the functionality we were so used to, but do it for a fraction of the cost – thankfully the fraction was not specified! And to have production logs in this new system within six weeks, which was more like four weeks as it was just before Christmas. My team had previously looked into other log analysis platforms and had built and evaluated the various different tools. We had concluded that the only product that would meet our needs was the Elastic Stack. This meant we could use our limited time in building a solution to start with the straight away, and not need another POC phase. We knew we could set up an Elastic Stack ourselves, but we also knew this would not be fit for Paddy Power's purpose and wouldn’t have a chance of scaling, so we needed help from the experts. I decided to go directly to the source and contacted Elastic. We had a few calls to discuss what we wanted as a solution and signed on the dotted line for a 10-node cluster. The goal was to scale this as we migrated more and more to the Elastic Stack. We also took an eight-day consultancy package with the aim of having our cluster production ready by the end of it. I had one full-time employee dedicated to work with the Elastic consultant and an extra person available to help out when needed. Even though we ran into some challenges in the build phase, at the end of those eight days, we were indexing production logs for the specified application in parallel in both the Elastic Stack and Splunk. We confirmed that searches were returning the same results in both stacks and replicated the dashboards the users were so fond of. The build can only be described as manic. We helped Elastic re-write Puppet modules, found bugs in the latest Filebeat and were running on pretty much Betas of every stack. But it was a hugely entertaining and rewarding two weeks. Due to the tight timelines, we didn’t have time to procure dedicated hardware so we plonked the entire stack on virtual machines. We knew there would be performance issues with this but we ended up seeing massive lag during our peak times. We engaged with Elastic as running on virtual machines was not the only contributing factor. We identified performance bottlenecks in our implementation of Logstash and other various components in the Elastic Stack. I have never been so impressed with Elastic support as I was when we had calls with the original authors of Filebeat, Logstash, and other Elastic Stack components. We had new agents custom written and these improvements were merged back to the master branch so hopefully people reading this are enjoying those improvements and bug fixes!Due to the migration, the second phase of the project was dedicated to hardware and migrating other applications into the Elastic Stack, but since our merger with Betfair, we are about to look into the Elastic Stack as a solution for the entire combined organisation! I can’t give a higher compliment than that! , Systems Engineering Manager in PaddyPower Betfair. I have been with Paddy Power for 5 years and started as a Senior Linux Admin. My team is responsible for the almost 8000 *nix systems that run the Paddy Power website, mobile apps as","locales":"","title":"From Splunk to Elastic in 4 weeks – Challenge Accepted!"}
{"index":{}}
{"author":"Court Ewing","category":"Releases","publish_date":"2016-05-18T00:00:00.000Z","url":"/blog/kibana-4-5-1-and-4-1-7","seo_title":"Kibana 4.5.1 and 4.1.7 Released","content":" Today, we’re releasing Kibana patch versions 4.5.1 and 4.1.7 that include an updated version of Node.js with a fix for a high severity vulnerability in OpenSSL (). We recommend any users that have configured Kibana to use TLS/SSL to upgrade as soon as possible. You can grab the latest versions from the page. ","locales":"","title":"Kibana 4.5.1 and 4.1.7 Released"}
{"index":{}}
{"author":"Suyog Rao","category":"The Logstash Lines","publish_date":"2016-05-16T00:00:00.000Z","url":"/blog/logstash-lines-2016-05-16","seo_title":"Logstash Lines: Java Event updates, 5.0 features","content":" Welcome back to The Logstash Lines! In these weekly posts, we'll share the latest happenings in the world of Logstash and its ecosystem.JSON Logging is backBack in the day, Logstash used to emit its logs in JSON format. Because of endless encoding issues and crashes in logger framework, we had to roll it back. We've worked through most of these issues, so we're bringing structured logging back. For 5.0, you can tell LS to switch to JSON logging by using the CLI flag ().Java EventThe new have been merged to master. This seems trivial, but it required a delicate dance of making sure existing plugins used the old API while at the same time updating plugins to use the new, unpublished core code. Plus, we had to untangle the dependency requirements between plugins. Our main goal was to minimize failures in CI and not to unnecessarily publish gems while all this was happening. What followed was a plugins . 80+ plugins (bundled in LS core) were updated with the new setter/getter APIs in about 3 days. Plugins tests are passing locally. We are now mass publishing these plugins so its compatible with 5.0.Next up, is phase 2, where we update the remaining non-packaged plugins and then we call this project doneRelease Packages ImprovementsWork is in progress to make improvements to our release artifacts. Firstly, we are changing the of rpm, deb packages to match Elasticsearch packages. This will provide a consistent user experience across our stack. Next, we'll be using and to LS service. They have more advanced features than , and make service scripts easier. While we're at it, we're creating a separate JVM options file — to centralize all the JVM flags — just like ES did ().Persistence Investigating the use of for serializing/deserializing Event objects into the disk backed queue. Initial work on rewriting the mmap based queues (which was once written in JRuby) in Java using the new .Dynamic Config Reload BugA user reported symptoms of “resource leak” while using the dynamic config reloading feature. After much tricky debugging and chasing, turns out the problem was in usage of ivars in Logstash. The JRuby team explained internal details of how ivar is implemented and how it can grow — in our usage pattern — to ultimately cause a slowdown. Another good example of collaboration between JRuby team and us. Fix is in progress on our side and a new release (2.3.3) will be cut soon.RabbitMQ EnhancementsOthers ","locales":"","title":"Logstash Lines: Java Event updates, 5.0 features"}
{"index":{}}
{"author":"Aaron Mildenstein","category":"Engineering","publish_date":"2016-05-16T00:00:00.000Z","url":"/blog/how-to-centralize-logs-with-rsyslog-logstash-and-elasticsearch-on-ubuntu-14-04","seo_title":"How To Centralize Logs with Rsyslog, Logstash, and Elasticsearch on Ubuntu 14.04","content":" IntroductionMaking sense of the millions of log lines your organization generates can be a daunting challenge. On one hand, these log lines provide a view into application performance, server performance metrics, and security. On the other hand, log management and analysis can be very time consuming, which may hinder adoption of these increasingly necessary services. Open-source software, such as , , and provide the tools to transmit, transform, and store your log data. In this tutorial, you will learn how to create a centralized rsyslog server to store log files from multiple systems and then use Logstash to send them to an Elasticsearch server. From there, you can decide how best to analyze the data. GoalsThis tutorial teaches you how to centralize logs generated or received by syslog, specifically the variant known as . Syslog, and syslog-based tools like rsyslog, collect important information from the kernel and many of the programs that run to keep UNIX-like servers running. As syslog is a standard, and not just a program, many software projects support sending data to syslog. By centralizing this data, you can more easily audit security, monitor application behavior, and keep track of other vital server information. From a centralized, or aggregating rsyslog server, you can then forward the data to Logstash, which can further parse and enrich your log data before sending it on to Elasticsearch. The final objectives of this tutorial are to: PrerequisitesIn the same data center, create the following servers with : You will also need a non-root user with sudo privileges for each of these servers. To maximize performance, Logstash will try to allocate 1 gigabyte of memory by default, so ensure the centralized server instance is sized accordingly. Step 1 — Determining Private IP AddressesIn this section, you will determine which private IP addresses are assigned to each server. This information will be needed through the tutorial. On each server, find its IP addresses with the command: sudo ifconfig -a The option is used to show all interfaces. The primary Ethernet interface is usually called . In this case, however, we want the IP from , the IP address. These private IP addresses are not routable over the Internet and are used to communicate in private LANs — in this case, between servers in the same data center over secondary interfaces. The output will look similar to: eth0 Link encap:Ethernet HWaddr 04:01:06:a7:6f:01 inet addr:123.456.78.90 Bcast:123.456.78.255 Mask:255.255.255.0 inet6 addr: fe80::601:6ff:fea7:6f01/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:168 errors:0 dropped:0 overruns:0 frame:0 TX packets:137 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:18903 (18.9 KB) TX bytes:15024 (15.0 KB) eth1 Link encap:Ethernet HWaddr 04:01:06:a7:6f:02 inet addr:10.128.2.25 Bcast:10.128.255.255 Mask:255.255.0.0 inet6 addr: fe80::601:6ff:fea7:6f02/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:6 errors:0 dropped:0 overruns:0 frame:0 TX packets:5 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:468 (468.0 B) TX bytes:398 (398.0 B) lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:16436 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) The section to note here is and within that . In this case, the private network address is . This address is only accessible from other servers, within the same region, that have private networking enabled. Be sure to repeat this step for all 3 servers. Save these private IP addresses somewhe","locales":"","title":"How To Centralize Logs with Rsyslog, Logstash, and Elasticsearch on Ubuntu 14.04"}
{"index":{}}
{"author":"Jack Lawton","category":"User Stories","publish_date":"2016-05-12T00:00:00.000Z","url":"/blog/using-elastic-graph-to-analyze-trump-data","seo_title":"Using Elastic Graph to Analyze #Trump Data","content":" In this blog, I will discuss how I've used , a new Elastic X-Pack plugin for Elasticsearch and Kibana, to explore relationships in Twitter data based on Donald Trump's political campaign. Straight away Graph shows us popular hashtags associated with the Trump presidential campaign and links those which are frequently seen together. In contrast searching the hashtag “#Cruz” for the dataset gives a completely different impression. Using the advanced features of Graph, not only can we assess links between hashtags and other hashtags but also links between hashtags and other data fields. For my case, a good example of this is to categorise America by time-zone, splitting our sample into three groups we can see what hashtags are most commonly associated with each of these time-zones. The data fields provided by Logstash have a vast potential for gaining insight on topics that may be very difficult to understand otherwise, like I have studied hashtags here we can also look at the raw content of the tweets and common associated words, as well as the words Twitter users use to describe themselves! However, one place where the standard Logstash Twitter feed falls short is Twitter followers. What if I want to know what users are popular in a subject and who people who follow them also tend to follow? Fortunately, Logstash can read in pretty much any file, so it’s pretty easy to use the Twitter API to bring down the data we’re interested in and pass it to Logstash. Straight away, we can build up a graph showing common following trends, the thickness of the lines shows the strength of the links. Clearly Graph is a powerful tool for identifying key, associated and similar users quickly and easily. A subtle issue that can come about in graph analysis is results being dominated by repeated data from one source. For example, if one user tweets one hundred times more than another then their description will be weighted one hundred times stronger, such as when I search for the term “republican” in user descriptions: Here at first glance the results seem good, but a couple of results seem a bit strange to see, are all Republicans cowgirls? I think it is more likely there is one Republican cowgirl who tweets very frequently. Fortunately, Graph has a solution to this issue, using the diversity field setting:  one can limit the consideration given to content with the same value of a certain field. For our case it makes sense to limit this to one tweet per user ID. Repeating the analysis with this new setting gives a very different picture: Another one of the key advantages of the Graph algorithm is its ability to filter out popular “super-connected” terms and identify only “significant links”. This feature can be disabled if needed. For example, repeating the search for “republican” without significant links, graph pulls back loads of connecting words, “and”, “of”, “the” etc. these words do appear frequently with the world “republican”, however they also appear frequently in descriptions without the word “republican” and are hence not reflective of descriptions featuring the word. All the relationships displayed in the Kibana app are as a result of querying the Elasticsearch Graph API. The app allows you to view the raw JSON, making it easy to integrate Graph into a custom application.To get an even better idea of the power of Graph watch  on the Elastic site, this video demonstrates the features of Graph as applied to other datasets such as Wikipedia and LastFM data. is a Technical Consultant at Aiimi, an Elastic partner specialising in information management consultancy. After studying Physics at Manchester, Jack is now exploring cutting edge data science technology to evolve Aiimi’s services and meet rising demands. ","locales":"","title":"Using Elastic Graph to Analyze #Trump Data"}
{"index":{}}
{"author":"Tudor Golubenco","category":"Brewing in Beats","publish_date":"2016-05-11T00:00:00.000Z","url":"/blog/brewing-in-beats-metricbeat-progress","seo_title":"","content":" Welcome to Brewing in Beats! With this series, we're keeping you up to date with all that's new in , from the details of work in progress pull requests to releases and learning resources. Last week we released 5.0.0-alpha2, with several important . Apart from that, here is what we are busy with: Per process stats in Metricbeat We had a lot of back and forth on this , debating how we should best represent in Elasticsearch the metrics that are per “dynamic entity”. A good example of such entities are the processes in an operating system, but can also be mounted file systems or tables in an SQL database. After considering nested objects or creating dynamic fields, we went for the simpler solution of having individual objects for each entity. These objects are grouped in their own metricset. Metricbeat API refactoring In order to have a clean and stable API before adding more modules, Andrew did a lot of refactoring and documenting work on the . Configuration migration tool Between the 1.x and 5.0 there are a few configuration file changes that break compatibility. We now have a that should help with migrating existing configuration files to 5.0 format. The script doesn’t do real YAML parsing, meaning that it won’t work on any possible configuration file, but it should do the job in most cases. It also requires no dependencies besides python itself and preserves the comments. Zookeeper module in Metricbeat , created by Erik Redding, was into a Metricbeat module. It uses the Zookeeper command to get simple stats. Thanks Erik! Kafka output deadlock fix Due to a bug in the Go library we use for outputting to Kafka, a is possible in case infinite retries are used. In time for alpha2, a of were to this issue. We now avoid asking for infinite retries from the library and instead we simulate it in our code. Topbeat: fix high CPU usage on windows We had a couple of interesting that indicated that on Windows Topbeat uses more CPU when it is configured to monitor less processes. It turns out that the reason was failing to use the command line cache (getting the full command line is expensive on Windows) for the processes that were filtered out. The fixing PR the logic so that the command line is not read at all if the process is filtered out. Filebeat refactoring Heavy is in the Filebeat code, making the code more readable and easier to maintain.  Previously the file state was loaded from disk every time a new file was found. The state from the registry file once during startup and from the one the prospector internal in memory state is used. This is more efficient and prevents race conditions. It also makes the Registrar and the Harversters . This refactoring work is crucial for us to be able to stay on top of possible races due to all the file rotation and file systems variations. Normalize new line character after multiline In order for the regular expressions to work in a consistent way on multiline events, this makes sure is used as a line separator after multiline stitching. ","locales":"","title":"Brewing in Beats: Metricbeat progress"}
{"index":{}}
{"author":"Megan Wieling","category":"","publish_date":"2016-05-09T00:00:00.000Z","url":"/blog/witwies-codemotion-amsterdam-devops-days-kiel-oscon-austin","seo_title":"Where in the World is Elastic? - DevOpsDays Austin and PuppetCamp AU","content":" Welcome to Find out which Elastic events and meetups are happening near you in the next two weeks.Upcoming EventsMay 9-10: May 11-12: May 12-13: May 18-20: May 19-20: May 14-15: May 16-19: May 18-20: May 12-14: Upcoming MeetupsMay 10: May 11: May 12: May 17: May 18: May 10: May 11: May 17: May 19: May 15: May 15: May 18: That's it for this week. Stay tuned next week for more Elastic happenings.- The Elastic TeamP.S.  if you're interested in hosting a meetup or are giving a talk about Beats, Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic? - CodeMotion Amsterdam, DevOps Days Kiel and Oscon Austin"}
{"index":{}}
{"author":"Martijn Laarman","category":"Engineering","publish_date":"2016-05-04T00:00:00.000Z","url":"/blog/nest-2-3-1-released","seo_title":"","content":" We recently released that contains an important fix for a memory leak NEST users are very likely to hit. We urge everyone on any version to upgrade to 2.3.1. If you are making the jump from , please jump straight to which will work against any release. The memory leak happened in the caching of s which are used throughout the NEST API to have strongly typed expressions to CLR type properties relating to fields in a document. For example, var response = client.Search<Project>(s => s .Query(q => q .Term(c => c .Field(p => p.Name) .Value(\"project description\") ) ) ) Here we have a reference to the \"name\" field in Elasticsearch using the Lambda expression, . Because of a bug in our 's implementation, the dictionary in which resolved expressions are cached would grow unbounded during the lifetime of the . We profile extensively and are halfway through adding profile results and their differences in our CI pipeline, but unfortunately this did not catch this leak in longer running applications. The low level client, , was not affected by this bug. Nearly three years ago through a GitHub issue reporting a regression, and today we've been served another. We would like to thank , ,  and  for raising and confirming the issue. If you have any questions, please don't hesitate to reach out to us either on or . We sincerely apologize for any inconvenience this may have caused! ","locales":"","title":"There are two hard problems in .NET, caching expressions and naming things"}
{"index":{}}
{"author":"Costin Leau","category":"Releases","publish_date":"2016-05-03T00:00:00.000Z","url":"/blog/es-hadoop-5-0-0-alpha2-and-2-3-1-released","seo_title":"","content":" I am pleased to announce the release of Elasticsearch for Apache Hadoop (aka ES-Hadoop) and As mentioned in the version, 5.0.0 is software that will only work with . Please test it, but do use it in production. Now that we have that out of the way, let’s see what these releases bring to the table. What’s new? Version alignment ES-Hadoop 5.0.0-alpha2 joins the 5.0 Elastic release train and add support for Elasticsearch 5.0.0-alpha1 and alpha2 while maintaining support for Elasticsearch 1.x and 2.x. Attentive users might notice that there was no ES-Hadoop 5.0.0-alpha1 release and this was on purpose:  to minimize confusion between products compatibility, ES-Hadoop is aligning itself with the global versioning of the Elastic stack. Removed support for ‘old’ library versions In ES-Hadoop 5.0.0-alpha2, the requirements for the various libraries have been raised to clean-up the code base and remove cruft. This includes (but is not limited to): Eliminate integration for Spark SQL 1.0-1.2 SparkSQL was released in Spark 1.0 through 1.2 as an component which became stable in Spark 1.3. In doing so however the Spark SQL API has changed significantly (moving away from to ). In ES-Hadoop 5.0.0-alpha2, support for Spark SQL 1.0-1.2 is being removed. The core/ support for Spark 1.0 is still present however with the iminent release of Spark 2.0, it is likely the version requirement will be raised (probably to Spark 1.2 or 1.3). Remove HDFS plugin repository As the HDFS repository plugin is now part of Elasticsearch proper, it has been removed from the ES-Hadoop project. Users of Elasticsearch 2.x can still use it as part of ES-Hadoop 2.x. Note that the HDFS plugin in Elasticsearch 5.x is not just conveniently packaged but also better integrated (there is no need to disable the JVM for example - an option that is anyway not available anymore). Bump Hive compatibility to 1.0 Hive 1.0 has been released for quite a while and the majority of distros have already moved to it. As such, support for Hive 0.13 and Hive 0.14 (two releases that were plagued by snafus) has now been dropped cleaning up the code base. Keep compatibility with JVM 1.7 Currently ES-Hadoop 5.0.0-alpha2 can still be used on JVM 1.7. This means users using old Hadoop distros or using Scala 2.10 can upgrade to ES-Hadoop without concern. Note that Elasticsearch 5.0 itself does require JDK 1.8 however as ES-Hadoop is a client, there are no hard JVM dependencies between the two - decoupling FTW! What about 2.3.1? ES-Hadoop 2.3.1 accompanies the 5.0.0-alpha2 release, introducing a few but important enhancements: Rework field escaping A bug report in the Spark module triggered a review and subsequent rework of the way internally mapping fields are being passed on. No API have changed however, at least in Spark, users should be now able to use field names with rare characters (such as ). HDFS repository upgrade The HDFS repository plugin has been upgraded to Elasticsearch 2.3.2. Better error messages Some of the error messages at start-up have been improved to provide more guidance especially for new users. Feedback Looking forward to hearing your feedback on these ! You can find us on , Twitter () or the . works too. ","locales":"","title":"Elasticsearch for Apache Hadoop 5.0.0-alpha2 and 2.3.1 released"}
{"index":{}}
{"author":"Michael McCandless","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2016-05-02T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2016-05-02","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top NewsWe “unlocked” some indexing performance in #elasticsearch Coming to 2.4.0 and 5.0.0! — Jason Tedor (@) Elasticsearch Core Apache Lucene Watch This Space, where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2016-05-02"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2016-05-03T00:00:00.000Z","url":"/blog/elasticsearch-5-0-0-alpha2-released","seo_title":"Elasticsearch 5.0.0-alpha2 released","content":" Today we are excited to announce the release of based on . This is the second in a series of pre-5.0.0 releases designed to let you test our your application with the features and changes coming in 5.0.0, and to give us feedback about any problems that you encounter. Open a bug report today and become an .  This is an alpha release and is intended for . Indices created in this version . Upgrading 5.0.0-alpha2 to any other version is not supported. Elasticsearch 5.0.0-alpha2 is close to being feature complete, and builds on the work released in 5.0.0-alpha1. There are many small changes which you can read about in the release notes above, but some of the more interesting ones are mentioned below. Also take a look at the to read about features like: Migration HelperThe Elasticsearch Migration Helper is a site plugin designed to help you to prepare for your migration from Elasticsearch 2.3.x to Elasticsearch 5.0. It comes with three tools: Instructions for  on Elasticsearch 2.3.x. Lucene 6This release is based on Lucene 6.0.0 and uses the new for , , and fields. Besides faster indexing, faster range queries, and smaller indices, the use of point fields also means that fields support IPv4 and IPv6 out of the box. Percolate QueryWe sat back and had a long hard think about how we could improve the Percolator with requested features like highlighting, scoring, and pagination. What we realised was that percolation is just a special form of search and we could benefit from all the features of the search API by replacing the percolate API with the new . The query is just a new query that can be used along with the rest of the Query DSL. It queries the special , which replaces the document type. The old percolate API is now deprecated but will continue to work through Elasticsearch 5.x. Deleted Index TombstonesHave you ever restarted an old node and found that previously deleted indices popped back to life like annoying, hard-to-kill zombies? We now maintain tombstones for deleted indices in the cluster state to keep these zombies where they should be. The tombstone list is capped at 500 indices by default, which equates to about one season of The Walking Dead. Indexed Scripts/Templates are now StoredPreviously, scripts and templates could be indexed into the special index and referenced by name in search and other requests. This complicated index recovery because the index had to be recovered before other indices. We have renamed these indexed scripts/templates to scripts/templates, and they are now stored in the cluster state instead of in an index. You will need to migrate any existing indexed scripts or templates that you have to the cluster state when moving to 5.0. Instructions for doing so are available . So long Environment Vars and thanks for all the fishIn previous versions, JVM configuration options could be scattered across multiple configuration files and environment variables. Now, all JVM options are centralized into a single that can be checked into your version control. Importantly, you can no longer use the environment variable but instead should . Safety MeasuresWe’ve added more checks to Elasticsearch to keep your cluster running healthily. At we check to ensure that you have set the correctly, and that your node has access to enough . This last one is particularly important because Elasticsearch is switching to use for all Lucene files, instead of the / hybrid file system that was used before. There is also a new circuit breaker to limit the . This is to prevent users from overloading your cluster by sending too many large bulk or search requests at the same time, which can overload nodes and even cause them to run out of memory. ConclusionPlease , try it out, and let us know what you think on Twitter () or in our . You can report any problems on the . ","locales":"","title":"Elasticsearch 5.0.0-alpha2 released"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2016-05-02T00:00:00.000Z","url":"/blog/brewing-in-beats-mysqlbeat-hwsensorbeat","seo_title":"","content":" Welcome to Brewing in Beats! With this series, we're keeping you up to date with all that's new in , from the details of work in progress pull requests to releases and learning resources. ","locales":"","title":"Brewing in Beats: Mysqlbeat and HWsensorsbeat from the community"}
{"index":{}}
{"author":"Shay Banon","category":"Releases","publish_date":"2016-05-03T00:00:00.000Z","url":"/blog/elastic-stack-release-5-0-0-alpha-2","seo_title":"Elastic Stack Release - 5.0.0 alpha 2","content":" In our post , we discussed the background of the release and the reason behind the Elastic Stack. So, this time, we will get right into the detail. Before you get too excited, keep in mind that this is still an alpha so don’t put it into production. And, since it is an alpha it is not available on . But, because Elastic Cloud is the official hosted Elasticsearch and Kibana offering on AWS -- the only hosted Elastic Stack offering with 5.0 upon release will be Elastic Cloud. If you open a bug report, today, you too can become an . And now, without further ado, some highlights from alpha 2. ElasticsearchFor more detailed information, and quite a few other features, peruse the Elasticsearch . From 2.3.x to 5.0. The magical journey. The undiscovered country. The next frontier. Other coined phrases. In addition to alpha 2, we’ve also released the , which runs on your existing 2.3 cluster.  Use this site plugin to prep for your migration. KibanaFor more detailed information, and all the PR links in one place, visualize the future in the Kibana . In the last release we mentioned that we hadn’t lost our sense. In alpha 2, we officially found our Console. Make sense? No? It doesn’t? Sense is now known as Console and it ships with Kibana. Logstash Download the Logstash packages from .BeatsThe Redis output in Beats just got a whole lot better, supporting the same level of guarantees as the other outputs, SOCKS5 proxies, and encryption. The downloadable Beats packages also got a new directory layout which is more consistent with the one used by Elasticsearch and they now also include sample Kibana dashboards. You can find more details about these changes in the . X-Pack ES-HadoopBut wait, there’s more! ES-Hadoop v 5.0.0-alpha2 has also been released . ","locales":"ja-jp","title":"Elastic Stack Release - 5.0.0 alpha 2"}
{"index":{}}
{"author":"Megan Wieling","category":"","publish_date":"2016-05-02T00:00:00.000Z","url":"/blog/witwies-devopsdays-austin-puppetcamp-au","seo_title":"Where in the World is Elastic? - DevOpsDays Austin and PuppetCamp AU","content":" Welcome to Find out which Elastic events and meetups are happening near you in the next two weeks.Upcoming EventsMay 9-10: May 11-12: May 12-13: May 2-3: May 14-15: May 12-14: May 3: Upcoming MeetupsMay 3: May 5: May 5: May 10: May 11: May 12: May 3: May 3: May 4: May 4: May 4: May 10: May 11: May 7: May 15: May 15: That's it for this week. Stay tuned next week for more Elastic happenings.- The Elastic TeamP.S.  if you're interested in hosting a meetup or are giving a talk about Beats, Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic? - DevOpsDays Austin and PuppetCamp AU"}
{"index":{}}
{"author":"Suyog Rao","category":"Engineering","publish_date":"2016-05-12T00:00:00.000Z","url":"/blog/just-enough-kafka-for-the-elastic-stack-part1","seo_title":"Just Enough Kafka for the Elastic Stack, Part 1","content":" The and  share a tight-knit relationship in the log/event processing realm. A number of companies use Kafka as a transport layer for storing and processing large volumes of data. In many deployments we've seen in the field, Kafka plays an important role of staging data before making its way into Elasticsearch for fast search and analytical capabilities. Through a series of blog posts, we'd like to shine more light on how to set up and manage Kafka when integrating with the Elastic Stack. Specifically, we'll discuss our experiences operating Kafka and Logstash under high volume. For the purposes of these posts, we refer to Kafka's version. Some of the functionality has changed in Kafka's latest version, but 0.8.x is still very popular and widely used. The BasicsLet's get some basic concepts out of the way. From Kafka's : Kafka was created at LinkedIn to handle large volumes of event data. Like many other message brokers, it deals with publisher-consumer and queue semantics by grouping data into . As an application, you write to a topic and consume from a topic. An important distinction, or a shift in design with Kafka is that the complexity moves from producer to consumers, and it heavily uses the file system cache. These design decisions, coupled with it being distributed from scratch, makes it a winner in many high volume streaming use cases. Logstash integrates natively with Kafka using the . It provides both and plugins so you can read and write to Kafka from Logstash directly. The configuration to get started is pretty simple: kafka { zk_connect => \"hostname:port\" topic_id => \"apache_logs\" ... } Kafka has a dependency on , so if you are running Kafka, you'll need access to a ZooKeeper cluster. More on that later. When To Use Kafka With The Elastic Stack? Log data or event based data rarely have a consistent, predictable volume or flow rates. Consider a scenario where you upgraded an application on a Friday night (why you shouldn't upgrade on a Friday is for a different blog :) ). The app you deployed has a bad bug where information is logged excessively, flooding your logging infrastructure. This spike or a burst of data is fairly common in other multi-tenant use cases as well, for example, in the gaming and e-commerce industries. A message broker like Kafka is used in this scenario to protect Logstash and Elasticsearch from this surge. In this architecture, processing is typically split into 2 separate stages — the and stages. The Logstash instance that receives data from different data sources is called a  as it doesn't do much processing. Its responsibility is to immediately persist data received to a Kafka topic, and hence, its a producer. On the other side, a Logstash instance — a beefier one — will consume data, at its own throttled speed, while performing expensive transformations like Grok, DNS lookup and indexing into Elasticsearch. This instance is called the . While Logstash has traditionally been used as the Shipper, we strongly recommend using the suite of  products available as specialized shippers. Filebeat, for example, is a lightweight, resource friendly agent which can follow files and ship to Kafka via a Logstash receiver. At this time, Filebeat cannot write directly to Kafka, but starting with 5.0.0 (currently in ), you'll be able to configure Kafka as one of the outputs. This enhancement further simplifies the above architecture in use cases that ingest data using beats. Please try out this and other awesome new features in our , and let us know what you think! Word on the street is that you can even win a to Elastic{ON} by helping us test these!  Consider another scenario. You're planning to upgrade your multi-node Elasticsearch cluster from 1.7 to 2.3 which requires a full cluster restart. Or, a situation where Elasticsearch is down for a longer period of time than you ","locales":"de-de,fr-fr,ja-jp,ko-kr","title":"Just Enough Kafka for the Elastic Stack, Part 1"}
{"index":{}}
{"author":"Michael McCandless","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2016-05-09T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2016-05-09","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top NewsDetecting Geo-Temporal anomalies with #Elasticsearch pipeline aggs Blog post: #gis — Dave Erickson (@) Elasticsearch Core Apache Lucene Watch This Space, where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2016-05-09"}
{"index":{}}
{"author":"Tudor Golubenco","category":"Releases","publish_date":"2016-05-03T00:00:00.000Z","url":"/blog/beats-5-0-0-alpha2-released","seo_title":"","content":" Around a month ago we released the of the next major release for the Elastic Stack. Today we’re proceeding according to the plan and announce the 5.0.0-alpha2 release. It includes bug fixes and new features for the data collectors you know and love: Filebeat, Packetbeat, Topbeat, and Winlogbeat. What’s new?Improved Redis outputThe Beats had a Redis output from the earliest days, but we neglected it for a while as we thought we will remove it to focus on the Logstash and Elasticsearch outputs. By popular demand we reverted that decision and, after introducing the new Kafka output in alpha1, we have now completely reworked the Redis output for alpha2. It now supports the guaranteed mode needed by Filebeat and Winlogbeat, meaning that events won’t be lost in case of network unavailability. It also supports authentication, SOCKS5 proxies, and even . Redis itself doesn’t natively support encryption, but you can make use of to secure the communication between the Beats and Redis. Kibana dashboards per BeatOur proved to be quite popular, so we worked on improving the experience around them. Part of this effort was to split them per Beat, so they are easier for us to maintain and more convenient for you to use. This means, for example, that if you only use Topbeat, you can load only the Topbeat dashboard along with all the required saved visualizations and searches. And since the dashboards are now per Beat, we include them in the , so they are there when you need them. New directory layoutWe also reorganized a bit where each Beat looks for and creates its files. This makes it easier to upgrade the Beats that store state and it makes it easier for us to create and maintain features like automatic template loading in a cross-platform way.  The new will be familiar to you if you know how the Elasticsearch paths are organized. Another notable change is that the Beats now log by default to rotating files instead of syslog. This makes the experience more consistent across the various platforms that we support. Bug fixes since Alpha1The change log also contains a few fixes for issues discovered since the 5.0.0-alpha1 release. These include bugs related to , to the automatic , and to using . Become a PioneerA big Thank You to everyone that has tried the alpha1 release and has or . We’d like to also remind you that if you post a valid non-duplicate bug report during the alpha/beta period against any of the Elastic stack projects, you are entitled to a . ","locales":"","title":"Beats 5.0.0-alpha2 released"}
{"index":{}}
{"author":"Court Ewing","category":"Releases","publish_date":"2016-05-03T00:00:00.000Z","url":"/blog/kibana-5-0-0-alpha2","seo_title":"Kibana 5.0.0-alpha2 released","content":" Today marks a huge milestone in the history of Kibana, software development, and the world. Brace yourself. Be proud all ye who contribute here, for on this day in May 2016, we scraped some features together, fixed a bunch of bugs, and released Kibana 5.0.0-alpha2. Pick up your very own, generic copy of Kibana 5.0.0-alpha2 from the page. This is software that will only work with . Please test it, but do use it in production. Enough with the warnings, let’s start breaking some new stuff. Features, you say? Sense is now Console, and it ships with every copy of Kibana Need I say more? Console is the same great Kibana application you’ve come to rely on, only it now supports the ES 5.0 APIs and is free of any pesky maintenance burden for you or your swarm of devops minions. Oh, you never used Sense before? Well, feast your eyes on the easiest way on Gaia’s green earth to query Elasticsearch: And there’s more! Some features just aren’t very screenshot friendly, but that doesn’t mean we love them any less. We fixed some bugs as well: What’s next? It’s really quite hush hush, but if you promise not to tell anyone, I’ll let you in on a little secret. Kibana 5.0.0-beta1 is coming. I know, right? I was as shocked as anyone when I heard. It won’t be this week, and it won’t be next week, but I have it from a reliable source that beta1 will be release during week sometime in the future. So what are you waiting for? Grab the release and give it a whirl. We’d love to get your feedback on our , , or even , and please post any bugs you find directly to our . ","locales":"","title":"Kibana 5.0.0-alpha2 released"}
{"index":{}}
{"author":"Suyog Rao","category":"The Logstash Lines","publish_date":"2016-05-02T00:00:00.000Z","url":"/blog/logstash-lines-2016-05-02","seo_title":"Logstash Lines: GeoIP filter enhancements","content":" Java EventLast week, we made to expose the new setter and getter APIs to the Event object. To propagate this change to all the 200+ plugins, we've taken a phased approach. First up, we've updated all the plugins that are used in the core unit tests with the new API, and tests are now passing. Next step, which will be this week, we move to those plugins (70+) which are packaged in Logstash artifacts. We have a to track that, and work will be split by the team. Once these plugin tests are passing, we can start publishing gems for all the updated plugins, and move on to non-packaged pluginsPlugin ManagerAdded option to command. This allows us to preserve gem options which are already specified in which would have been previously overwritten. This will also help with the unified build effort by avoiding the unnecessary publication of core plugins snapshots. When running any plugin related command you can use , which will give the user a bit more information about what bundler is doing.Acceptance TestingWork continues on building the framework for acceptance tests on multiple artifacts/platforms using Vagrant. At this point, we've written plugin manager validation tests using the new framework. This week, we'll focus on deb/rpm package tests and running these on Jenkins (thanks to our infra team).PluginsSupporting MaxMind's database has been a longstanding enhancement request! Over the past couple of weeks we've been working with our community members and on a to bring these changes to this filter. This plugin now directly uses the GeoIP2 Java API, adds support for IPv6 lookups and brings in all the goodness that is GeoIP2. Many thanks to Gary and Thomas! These changes will be packaged with 5.0.0-alpha2.This output now supports ingest pipelines while indexing docs. Very simply, you can:{ \"hosts\" => \"localhost\", \"pipeline\" => \"apache-logs\", ... }Others ","locales":"","title":"Logstash Lines: GeoIP filter enhancements"}
{"index":{}}
{"author":"Shaunak Kashyap","category":"Engineering","publish_date":"2016-04-29T00:00:00.000Z","url":"/blog/kibana-under-the-hood-object-persistence","seo_title":"Kibana under the hood: object persistence","content":" Users of Kibana create saved searches, visualizations, dashboards, and other such objects. Kibana, of course, has to persist these objects somewhere so they can be loaded up next time users start it up. As Kibana connects to Elasticsearch for querying users’ data anyway, it conveniently uses Elasticsearch to store its objects as well. This blog post explores some of these objects, how they are stored in Elasticsearch, and when they are created. The goal is for this information to serve as a useful tool not only for debugging but also for administrators who might want to deploy Kibana in an automated, repeatable fashion today. Please note, however, that these objects are internal data structures that may evolve over time. Future versions of Kibana may provide REST APIs to programmatically — and safely — manipulate these objects. The special index Kibana stores its objects as documents in the index in Elasticsearch. The name of this index can be changed via the configuration setting (starting with Kibana 4.2:  prior to that this setting was named ). Let’s start at the beginning When you start with a completely fresh install of Kibana there is, of course, no index in Elasticsearch yet. This index is created when you start the Kibana server. At this point the index contains two document types: Index patterns When a user loads up Kibana in their browser for the first time, they are required to create an index pattern. Of course, they can add more index patterns at a later time as well. Whenever users create a new index pattern, a document is created in the index, under the document type. Depending on the choices the user makes when creating the index pattern, this document will have the following fields: Also, when users create the very first index pattern, it becomes the default index pattern for Kibana to use as well. This bit of information — specifically the of the index pattern document — is recorded in the field of the type document introduced in the section above. Saved searches After creating an index pattern, users will typically start on the Discover page. Here they will explore the data in their index pattern by searching through it, focussing on certain fields in the results, etc. Sometimes users will want to save their search on the Discover page for later use. They might want to open it later on the Discover page or add it as a panel to a dashboard. When users save a search, a new document is created in the index under the search type. It contains the following fields: Saved visualizations When users create a new visualization via the Visualization page, a new document is created in the index under the visualization type. It contains the following fields: Saved dashboards When users create a new dashboard via the Dashboard page, a new document is created in the index under the (surprise, surprise!) dashboard type. It contains the following fields: This post covered persistence for the fundamental objects in Kibana. In a future post we’ll cover more advanced objects and lifecycle operations. As always, if you have questions about Kibana feel free to ask them in our discussion forums at or chat live with the Kibana team and community in the #kibana channel on Freenode IRC. ","locales":"","title":"Kibana under the hood: object persistence"}
{"index":{}}
{"author":"Suyog Rao","category":"Releases","publish_date":"2016-04-26T00:00:00.000Z","url":"/blog/logstash-2-3-2-released","seo_title":"","content":" Bugs Fixed In Logstash CoreBugs Fixed In Plugins ","locales":"","title":"Logstash 2.3.2 Released"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2016-04-26T00:00:00.000Z","url":"/blog/elasticsearch-2-3-2-released","seo_title":"Elasticsearch 2.3.2 released","content":" Today we are pleased to announce the bug fix release of based on . Users who are affected by the issues mentioned in the release notes are advised to upgrade. This release is already available on , our Elasticsearch-as-a-service platform.Latest stable release:Full details of the changes in this release are available in the release notes listed above, but some of the more important changes are mentioned below: ","locales":"","title":"Elasticsearch 2.3.2 released"}
{"index":{}}
{"author":"Michael McCandless","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2016-04-25T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2016-04-25","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top NewsForbes: Amazing Big Data At NASA with Elasticsearch: Real Time Analytics 150 Million Miles From Earth — dbaldassano (@) Elasticsearch Core Apache LuceneNews about Apache Lucene will be back next week.Watch This Space, where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2016-04-25"}
{"index":{}}
{"author":"Tudor Golubenco","category":"Engineering","publish_date":"2016-04-28T00:00:00.000Z","url":"/blog/structured-logging-filebeat","seo_title":"","content":" The idea behind structured logging is simple: instead of having applications write logs that need to be parsed via regular expressions into JSON objects that you index into Elasticsearch, you make the application write JSON objects directly. To exemplify, let’s say you are writing a Python web application, and you are using the standard library for logging. When a user signs in, you might use a logging statement that looks like this: logging.debug(\"User '{}' (id: {}) successfully logged in. Session id: {}\" .format(user[\"name\"], user[\"id\"], session_id)) And then when the user gets verified: logging.debug(\"User '{}' (id: {}) changed state to verified.\" .format(user[\"name\"], user[\"id\"])) This results in log lines like this: DEBUG:root:User 'arthur' (id: 42) successfully logged in. Session id: 91e5b9d DEBUG:root:User 'arthur' (id: 42) changed state to verified. This way of logging is popular ever since the good old printf. Back then, you would typically have a single server, and you would tail and grep the log files and everything was great. Times have changed, however, and today it is more likely that you have tens, hundreds, or thousands of servers / virtual machines / containers creating massive amounts of logs, so you centralize them in Elasticsearch and use its magical (that’s just how it feels) search and aggregation features to navigate through them. This works best when the logs are pre-parsed in a structured object, so you can search and aggregate on individual fields. So before indexing, you would typically use Logstash’s amazing Grok filter to parse the application logs into a JSON object, but this means that you have to write and maintain Grok patterns and spend CPU cycles to do the parsing. Now let’s try the same example with structured logging. While usually not in the standard library, all major programming languages have libraries that make structured logging easy. James Turnbull created a list in this , which also goes into detail about how to do this for a Rails application. In Python, there’s the library, which we will use here: log = log.bind(user='arthur', id=42, verified=False) log.msg('logged_in') log.msg('changed_state', verified=True) Which results in log lines like this: verified=False user='arthur' session_id='91e5b9d' id=42 event='logged_in' verified=True user='arthur' session_id='91e5b9d' id=42 event='changed_state' One thing to notice is that the code is less repetitive and it encourages the developer to include all the data rather than only what seems important while writing the code. Also note that in this format, the log lines are still reasonably easy to follow for a human during development. When moving to production, however, it makes more sense to use the JSON renderer: log = wrap_logger(PrintLogger(), processors=[JSONRenderer()]) This results in log lines like this: {\"verified\": false, \"user\": \"arthur\", \"session_id\": \"91e5b9d\", \"id\": 42, \"event\": \"logged_in\"} {\"verified\": true, \"user\": \"arthur\", \"session_id\": \"91e5b9d\", \"id\": 42, \"event\": \"changed_state\"} This is less readable to human eyes, but has the advantage that the data is already structured in the format that Elasticsearch likes. is an open source log shipper, written in Go, that can send log lines to Logstash and Elasticsearch. It offers “at-least-once” guarantees, so you never lose a log line, and it uses a back-pressure sensitive protocol, so it won’t overload your pipeline. Basic filtering and multi-line correlation are also included. Starting with version 5.0 (currently in alpha, but you can give it a ), Filebeat is able to also natively decode JSON objects if they are stored one per line like in the above example. Here is a sample configuration file that configures Filebeat to pick up the files and send the JSON objects to Elasticsearch: filebeat.prospectors: - input_type: log paths: [\"test/*\"] json.message_key: event json.keys_under_root: true fields: planet: Magrathea service: ${SERVICE_NAME} t","locales":"","title":"Structured logging with Filebeat"}
{"index":{}}
{"author":"Megan Wieling","category":"","publish_date":"2016-04-25T00:00:00.000Z","url":"/blog/witwies-elastic-meetups-beijing-portland-dallas-st-louis-new-york","seo_title":"/blog/witwies-qcon-devoxx-percona-aws","content":" Welcome to Find out which Elastic events and meetups are happening near you in the next two weeks.Upcoming EventsMay 2-3: May 3: Upcoming MeetupsApril 25: April 25: April 27: April 28: April 28: April 29: May 3: May 5: May 5: April 26: April 26: May 3: May 3: May 4: May 4: May 4: April 30: May 7: April 26: That's it for this week. Stay tuned next week for more Elastic happenings.- The Elastic TeamP.S.  if you're interested in hosting a meetup or are giving a talk about Beats, Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic? - Elastic Meetups in Beijing, Portland, Dallas St. Louis and New York"}
{"index":{}}
{"author":"Daniel Mitterdorfer","category":"Engineering","publish_date":"2016-04-19T00:00:00.000Z","url":"/blog/announcing-rally-benchmarking-for-elasticsearch","seo_title":"","content":" Today we’re excited to announce the first public release of , the benchmarking tool we have been using internally in the Elasticsearch development team for a few months now. We want to share it with the community to help you reproduce performance numbers that we publish in your own environment and to help you write your own benchmarks without worrying about lots of tiny details. Rally’s origins Rally originates from Python scripts that drive the and also . The benchmarking infrastructure is a great help to avoid boiling frog problems (a.k.a. slowly decreasing performance). We are constantly improving Elasticsearch in all areas and we also care deeply about performance. Wouldn’t it be great if developers could run a benchmark by themselves to see the impact of their changes early during development instead of waiting until a feature is merged to the master branch? Developers are typically creative, so you can always write a quick and dirty script in the language of your choice, run it for your specific use case and forget about it. But how often do we really verify these numbers? So I tried the next best thing, which is to use the existing benchmark scripts locally, but the setup involved lots of manual steps. I’ve decided to simplify installation and usage and . What can Rally do? Over the last months Rally gradually supported more and more features: We can attach so-called telemetry devices for detailed analysis of the benchmark candidate behavior. For example, Java flight recorder has already helped us to spot different problems. Here are a few examples of what you can do with Rally and the Java flight recorder telemetry device: Allocation Profiling Inspecting hot classes in Elasticsearch We have also added a JIT compiler telemetry device where we can inspect JIT compiler behavior, which allows us, for example, to analyze warm-up times during the benchmark. The graphics below shows the number of JIT compiler events during the benchmark: Evaluating the performance of such a complex system as Elasticsearch is also a very multi-dimensional problem. Whereas performance could improve in one scenario - say for searching log data - it could have a negative impact on full-text search. Therefore, we can define multiple benchmarks (called “tracks” in Rally). They are currently directly implemented in the Rally code base, but as the API is more stable, we want to , so it is easier to define your own ones. As Rally stores all metrics data in Elasticsearch, we can easily visualize data with Kibana, such as the distribution of CPU usage during a benchmark: In the beginning we add the benchmark data set to the index. After that we run search benchmarks. I bet you can clearly see the point in time where indexing is complete. We have also started to run the nightly benchmarks in parallel now and provide the results as . Roadmap There is still a lot of work to do: One major topic is to remove restrictions. We want to separate the benchmark definitions (called “tracks”) from Rally itself and also allow more flexibility in the steps that Rally performs during the benchmark. Currently we support only a very limited scenario: first all documents are bulk-indexed, then we run a track-dependent number of queries. By default, we run a benchmark based on data from but further tracks are available (just issue ). The second major topic is improving correctness of the measurements. We take correctness seriously and are already aware of a couple of topics that need improvement. One of the major issues that . This basically means that requests that take a long time to process prevent the benchmark driver to send further requests in the meantime, so we lose measurement samples. This means that the reported latency percentile distribution appears to be better than it actually is. Third: Rally is currently limited to single machine benchmarks, but and ea","locales":"de-de,fr-fr,ja-jp,ko-kr","title":"Announcing Rally: Our benchmarking tool for Elasticsearch"}
{"index":{}}
{"author":"Alex Brasetvik","category":"Engineering","publish_date":"2016-04-21T00:00:00.000Z","url":"/blog/elastic-cloud-outage-april-2016","seo_title":"","content":" Summary ","locales":"","title":"Elastic Cloud Outage: Root Cause and Impact Analysis"}
{"index":{}}
{"author":"Michael McCandless","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2016-04-18T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2016-04-18","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top News 5.0 will use the new Lucene 6 points API to index numeric, date and ip fields— elastic (@elastic) Elasticsearch Core Apache Lucene Watch This Space, where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2016-04-18"}
{"index":{}}
{"author":"Tudor Golubenco","category":"Releases","publish_date":"2016-04-26T00:00:00.000Z","url":"/blog/beats-1-2-2-released","seo_title":"","content":" We’re happy to announce that the 1.2.2 bug fix release is now available for all the Elastic Beats: Filebeat, Packetbeat, Topbeat and Winlogbeat. You can download the new versions from the usual . Bug fixes affecting all Beats Bug fixes affecting Filebeat Many thanks to our dear community members who tested the Beats, reported and helped with the troubleshooting of the above issues. If you find any issues with the new release or there are features that you miss, please interact with us on or . ","locales":"","title":"Beats 1.2.2 Released"}
{"index":{}}
{"author":"Tudor Golubenco","category":"Brewing in Beats","publish_date":"2016-04-25T00:00:00.000Z","url":"/blog/brewing-in-beats-tomcat-jmx-dnssec-monitoring","seo_title":"","content":" Welcome to Brewing in Beats! With this series, we're keeping you up to date with all that's new in , from the details of work in progress pull requests to releases and learning resources. New community Beat: JmxProxyBeat Created by , who also wrote Apachebeat and Elasticbeat in the past, JmxProxyBeat gets JMX metrics from Tomcat via the JMX Proxy Servlet. Great companion for Metricbeat :-). DNSSEC support in Packetbeat DNS monitoring is already one of the most popular use cases for Packetbeat. Thanks to this , Packetbeat is now also able to understand EDNS and DNSSEC resource records. Clean shutdown procedure It used to be that you couldn’t stop the Beats if the output queue was not empty (for example, because Elasticsearch/Logstash was not available). After this fairly , the Beats can signal to the publisher that they want to shutdown and that the outputs can drop the output queue if needed. Dropping the inflight messages is fine, because for the Beats that offer at-least-once-guarantees (Filebeat & Winlogbeat), the registry makes sure they will be resent after the Beat restart. Besides solving the bug mention above, the clean shutdown semantics are one of the prerequisites for configuration reloading and running multiple Beats in a single binary. Metricbeat, meet generic filtering The new (since alpha1) libbeat functionality of generic filtering can now be used , making it possible to flexibly control which fields (metrics) are exported and which not. Metricbeat system module Metricbeat now borrows from Topbeat (every Beat is also a library!) the , so any host running it will report CPU/mem statistics. Topbeat and Packetbeat now work on OpenBSD We don’t yet consider OpenBSD a supported platform for Beats, but thanks to the efforts of Jasper Lievisse Adriaanse, a , Topbeat and Packetbeat can now be compiled and executed on OpenBSD. Jasper went as far as fixing libpcap on OpenBSD and to us so we can test OpenBSD more regularly. Filebeat was already working on BSDs via simple cross-compilation, because it's pure Go. Load the Elasticsearch template on every connect One of the known issues with 5.0.0-alpha1 was that the Beats only attempted to load the Elasticsearch template once, meaning that if they were started before Elasticsearch the template was not loaded. With this , the template is loaded immediately after a successful connection is established. The error handling was also improved, so we can now make sure that we don’t insert any documents before the template is loaded. Filebeat’s JSON decoder can now parse @timestamp fields One of the bugs reported against alpha1 was that if a field was supplied as JSON field, Filebeat would crash. This is because our output plugins expect the field to have a particular type. Filebeat now that the timestamp and type (could also be affected by this) have the right types. Besides not crashing, a benefit of this is that the application can now provide its own timestamp to overwrite the one added by Filebeat. Added path.logs option and log to files by default The Beats used to log to syslog by default. With the introduction of the , and to be more similar with the other products in the stack, it makes more sense to log to files by default. So this . The files are automatically rotated and old files are removed. When loading the default dashboards, you can now select the index name Thanks to another contribution by , you can now rename the index names before loading the dashboards in Elasticsearch. Other merges: ","locales":"","title":"Brewing in Beats: Tomcat JMX monitoring and DNSSEC monitoring"}
{"index":{}}
{"author":"Suyog Rao","category":"The Logstash Lines","publish_date":"2016-04-25T00:00:00.000Z","url":"/blog/logstash-lines-2016-04-25","seo_title":"","content":" Welcome back to The Logstash Lines! In these weekly posts, we'll share the latest happenings in the world of Logstash and its ecosystem.Event API This week, we spec'd out APIs for interacting with the Event class. Previously, there was no formal, documented API which meant that plugin developers and especially Ruby Filter users could use the internals (hash map) directly to modify data. Not a great idea! We defined setters and getters interfaces which will be used by all plugin developers. This breaking change affects almost plugins, and we need to review/modify 200+ plugins, but this is the right thing to do in 5.0 and document properly. This will protect us against future implementation changes (e.g. plugging in Java Event implementation). See for new APIs -- we would love any feedback you have! All core plugin tests are passing locally with these changes. Next up, we need to split the work to update all plugins with these new calls. Acceptance Tests Work continues on to add much needed acceptance tests. Progressing well on validating plugin manager workflow, CLI switches, testing release packages on CentOS and Debian flavors. Also, working with our infra team to add these to the new Jenkins CI server. Other Fixes/In-Progress StuffUpcoming Releases Prepping for a 2.3.2 bug fix release this week and 5.0.0-alpha2 next week.  ","locales":"","title":"Logstash Lines: New Event API"}
{"index":{}}
{"author":"Tyler Langlois","category":"Engineering","publish_date":"2016-04-25T00:00:00.000Z","url":"/blog/elasticsearch-docker-plugin-management","seo_title":"","content":" If you're running Elasticsearch within Docker containers, there are some important operational considerations to bear in mind. This is especially true when managing stateful services and daemons - when persisting data outside of an ephemeral container becomes important.Using Elasticsearch plugins within containers is an example of this, both in terms of them in a repeatable, trackable manner and plugin configuration and data.In this post, we'll explore some of the options to achieve sane plugin management within the context of Docker and Elasticsearch.: In this guide we will reference the Elasticsearch image found on the Docker Hub. The development and production of this Docker image is not affiliated with Elastic, Inc.A Docker Persistence PrimerLike a file change in a version control system, changing the filesystem in a running container introduces differences in the running image. Steps need to be taken if data and changes need to persist permanently outside the scope of an impermanent container.Although one could leverage some more complex storage schemes to achieve container persistence, the basic Docker mechanism of is the most illustrative. This is achieved by, for example, keeping Elasticsearch indices long-term in by passing an option like to a command, which effectively stores your Elasticsearch data in the (not container's) directory.If the changes are more permanent, that is, they are expected to be there without changing over time, codifying changes in a may make more sense. Both approaches are useful in different cases.Managing Basic PluginsIn the following examples, I refer to \\\"basic\\\" plugins as those that do not require licenses or any other special components. Straightforward plugins like just need to place some files on the system to work.In a case like this, managing the presence of the plugin is simplest by just extending the image using a . For example, considering the following :FROM elasticsearch:2 RUN /usr/share/elasticsearch/bin/plugin install --batch cloud-awsThis starts with the Elasticsearch image provided by maintainers at the Docker hub and runs a simple command. This image can then be built and referenced by a tag:$ docker build -t elasticsearch-aws .When run in the same directory as the , the image name is built and can now be referenced in future Docker commands when starting new containers with behavior inherited the original.More Complex PluginsSome plugins may require the presence of additional files (such as certificates when using ) for certain features. The aforementioned technique of building a custom image can handle the installation of these plugins, but managing configuration is a task better left to a different approach. This helps keep images generic for deployment re-use and maintains tighter control over secrets.: Some commercial plugins require the presence of a license. In the following examples, we simply rely on the temporary trial license present by default. When deploying in production, license management is performed through the Elasticsearch REST API, which stores the license in Elasticsearch's data path. As long as your data is persisted appropriately through a volume mount or otherwise, your license will be saved within your cluster.Example: ShieldAs outlined in the , installing the and plugins is a prerequisite, which we can achieve by using the previous strategy to build a derived image:FROM elasticsearch:2 RUN /usr/share/elasticsearch/bin/plugin install --batch license RUN /usr/share/elasticsearch/bin/plugin install --batch shieldThen build the image to use for future steps:$ docker build -t elasticsearch-shield .At this point, if we volume mount a config directory into the container, Shield will pick up our settings. As an example configuration, consider the following directory structure:$ tree config config ├── elasticsearch.yml ├── logging.yml ├── scripts └── shield ├── roles.yml ├─\\xE2\\x94","locales":"","title":"Effective Elasticsearch Plugin Management with Docker"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2016-04-18T00:00:00.000Z","url":"/blog/weekly-beats-add-path-and-redis-output","seo_title":"","content":" Highlights of the week:Enhance Redis output supportLast week we were on improving the Redis output by adding support for failover and loadbalancing if multiple Redis hosts are configured. In addition it adds support for TLS, SOCKS5 and backoff strategy in case Redis is unresponsive. ","locales":"","title":"Brewing in Beats: Add data path support and enhance Redis output"}
{"index":{}}
{"author":"Matt Roman","category":"User Stories","publish_date":"2016-04-19T00:00:00.000Z","url":"/blog/how-jamplify-is-rocking-the-music-tour-industry-with-elasticsearch","seo_title":"","content":" collects and analyzes concert ticket sales data and provides venue and routing information for leading talent agencies, both large and boutique. For our clients, we are collecting and analyzing sales data for hundreds of touring artists as they perform in venues around the world. Data collection, visualization, and analysis around live event ticket sales have not advanced in parallel with other analytics tools in entertainment. With our and tools, Jamplify steps in not only by removing the time and headache of tracking down updated ticket sales data from venues, but also by turning otherwise stagnant sales data points into valuable, actionable insights for our clients. How we use ElasticsearchWe began using the Elastic Stack as a DevOps tool. It helped us get our log data in one place so that we could analyze problems with our servers and applications in context with each other. Once we had the Elastic Stack in place, we became accustomed to adding logging to diagnose what was happening with our application in production. It didn’t take long to realize that we could use the Elastic Stack as an analytics platform to understand user behavior and feature viability on a deeper level. We were initially using Google Analytics for this purpose, but Elasticsearch provided much more flexibility and only required adding more logging.Eventually, we began using Elasticsearch to enhance our products as well. Previously, our clients collected and managed ticket sales data via emails and spreadsheets, but by using the Elasticsearch aggregation framework and D3 we were able to give our clients a new look at their sales data and provide them with actionable insights.Since dealing with Elasticsearch aggregation queries and results in your application can be a bit clumsy, we created a Node.js wrapper () which handles these tasks with a small DSL for retrieving relevant data from the results. This allowed us to simplify our application code and externalize parsing and processing as much as possible. Live IntelOur latest product release, Live Intel, heavily utilizes Elasticsearch’s geographic search capabilities. Live Intel is a venue intelligence platform that helps booking agents route tours by providing them with a combination of geographic and term-based searching.Common challenges in tour routing include finding new venues that fit your artist profile and filling in open days during a tour. For instance, if there are two booked dates for a tour, one in New York on a Wednesday and another in Philadelphia on a Saturday, an agent would need to find potential shows and venues between these two cities and dates. To further complicate matters, live-event contracts often include radius clauses that prevent the artist from performing within a specified distance from the venue several months after the show date (music festivals have particularly onerous radius clauses that can span hundreds of miles and an entire year).Historically, agents would try to work with radius clauses by cross-checking venue/promoter directories with Google Maps. This solution is limited, time-consuming, and cumbersome - an agent might spend an entire work day filing one or two available dates.  Live Intel quickly resolves this issue by using Elasticsearch to combine the map and the directory in a single application, while also ranking the search results based on performance metrics like the venues’ ticket sales histories. Now they can reach out to every relevant venue within minutes and get to a booked show in an hour. By streamlining processes around both ticket sales data collection and tour routing, Jamplify helps agents do their jobs faster and better. ","locales":"","title":"Datapalooza: How Jamplify is Rocking the Music Tour Industry with Elasticsearch"}
{"index":{}}
{"author":"Megan Wieling","category":"","publish_date":"2016-04-18T00:00:00.000Z","url":"/blog/witwies-qcon-devoxx-percona-aws","seo_title":"/blog/witwies-qcon-devoxx-percona-aws","content":" Welcome to Find out which Elastic events and meetups are happening near you in the next two weeks.Upcoming EventsApril 18: April 19: April 19: April 19: April 23-24: April 21-23: Upcoming MeetupsApril 18: April 19: April 19: April 19: April 20: April 20: April 25: April 25: April 27: April 28: April 28: April 29: April 19: April 19: April 20: April 20: April 20: April 21: April 26: April 26: April 30: April 21: April 26: April 21: That's it for this week. Stay tuned next week for more Elastic happenings.- The Elastic TeamP.S.  if you're interested in hosting a meetup or are giving a talk about Beats, Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic? - QCon Beijing, Devoxx France, Percona Live & AWS Summit Chicago"}
{"index":{}}
{"author":"Samir Bennacer","category":"Engineering","publish_date":"2016-04-15T00:00:00.000Z","url":"/blog/docker-networking","seo_title":"","content":" In this blog we’ll talk about network considerations when using Docker with an Elasticsearch cluster. Note: In this blog we will reference the Elasticsearch image found on the Docker Hub.The development and production of this Docker image is not affiliated with Elastic.You can create your own image by following our recommendation in the blog . There are different ways to setup networking in Docker and by default three network types are presented. We can list all of them using the following command: $ docker network ls NETWORK ID NAME DRIVER d610d782daa0 bridge bridge 16a982d835f8 none null 7d80e0e91caf host host None NetworkIt completely disables networking, which is not useful when running an Elasticsearch cluster. Host Network If you use then the container will use the host network and this can be dangerous. It will allow you to change the host network from within the container and if you have an application running as root and it has a vulnerability, there is risk of unsolicited remote control of the host network via the the Docker container. In general we recommend against using it for security reasons, but it can be useful when you need to get the best network performance because it is as fast as normal host networking. Bridge NetworkThe bridge network is the default network in Docker. We can check the details of the default bridge network using the following command: $ docker network inspect bridge [ { \"Name\": \"bridge\", \"Id\": \"b38c312777a0f3890034c9b396669842947b80c9051d10a283c9d43937910578\", \"Scope\": \"local\", \"Driver\": \"bridge\", \"IPAM\": { \"Driver\": \"default\", \"Options\": null, \"Config\": [ { \"Subnet\": \"172.17.0.0/16\" } ] }, \"Containers\": {}, \"Options\": { \"com.docker.network.bridge.default_bridge\": \"true\", \"com.docker.network.bridge.enable_icc\": \"true\", \"com.docker.network.bridge.enable_ip_masquerade\": \"true\", \"com.docker.network.bridge.host_binding_ipv4\": \"0.0.0.0\", \"com.docker.network.bridge.name\": \"docker0\", \"com.docker.network.driver.mtu\": \"1500\" } } ] The bridge network name here is . When you add a container, each of them will have its own virtual Ethernet interface connected to the docker bridge  and it will have an IP address allocated to the virtual interface. This bridge will automatically forward packets between any other network interfaces that are attached to it and also allow containers to communicate with the host machine as well as with the containers on the same host. By default, Docker containers can make connections to the outside world, they connect via the interface but the outside world cannot connect to containers. External connectivity is provided by IP forwarding and rules. You can achieve that using the port mapping. When running Elasticsearch, you will need to ensure it publishes to an IP address that is reachable from outside the container:  this can be configured via the setting . For the discovery between the nodes you have to configure Zen Discovery via the settings and . docker run -d -p 9200:9200 -p 9300:9300 elasticsearch:2 \\ elasticsearch \\ -Des.discovery.zen.ping.unicast.hosts=192.168.99.100,192.168.99.101 \\ -Des.discovery.zen.minimum_master_nodes=2 \\ -Des.network.publish_host=192.168.99.100 docker run -d -p 9200:9200 -p 9300:9300 elasticsearch:2 \\ elasticsearch \\ -Des.discovery.zen.ping.unicast.hosts=192.168.99.100,192.168.99.101 \\ -Des.discovery.zen.minimum_master_nodes=2 \\ -Des.network.publish_host=192.168.99.101 By default, a Docker container is configured to use IPv4 only. It is possible to configure IPv4/IPv6 by starting the Docker daemon with the flag. When creating a container it will get a link-local IP address. You can also assign a globally routable IPv6 addresses to your containers. Using routable IPv6 addresses allows you to realize communication between containers on different hosts. The following article provides a lot more information on this subject: .Overlay NetworkIn recent versions of Docker they introduced a new type of network called overlay network which ","locales":"","title":"Docker Networking"}
{"index":{}}
{"author":"Michael McCandless","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2016-04-11T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2016-04-11","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top NewsThe alpha-1 of the is here! If you’re exploring it, you’re an Elastic — & we’ve got good news: Details: — elastic (@elastic) Elasticsearch Core Apache Lucene Watch This Space, where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2016-04-11"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2016-04-11T00:00:00.000Z","url":"/blog/weekly-beats-manage-dashboards-per-beat","seo_title":"","content":" Welcome to Brewing in Beats! With this series, we're keeping you up to date with all that's new in , from the details of work in progress pull requests to releases and learning resources. Last week we released the long-expected Beats 5.0.0-alpha1 () and the patch release Beats 1.2.1 (). The highlights of the week are: Require braces in the environment variable expansion Version 1.2.0 introduced the possibility of using environment variables in the configuration file by replacing strings like or with the value of the environment variable. Together with this great feature, we also introduced a bug that replaces the environment variable when it’s not wanted if the dollar sign shows up in passwords or in regular expressions. This restricts to replace the environment variable only when the form is encountered. Change exported fields for Packetbeat flowsThe re-organizes the exported fields for flows. The ip, ipv6, port, statistics and the associated location are grouped per and . In the case of flows, the is considered the one that sends the first SYN packet. Manage Kibana dashboards per Beat In the current version, the sample Kibana dashboards are available in a single for all the Beats together with a bash and powershell script to load them all in Kibana. In most of the cases, you don’t need all the Kibana dashboards and only the ones for a single Beat. Last week we adjusted the bash and the powershell scripts to be able to import and export the Kibana dashboards together with visualizations, searches and index patterns to Kibana only for a single Beat. They will be available in elastic/beats/dev-tools with this Now it’s easier to contribute with Kibana dashboards to the project. You can just create your own dashboard in Kibana for any Beat and export all the Beat dashboards together with visualizations, searches and index patterns by using the python script  Customize Discovery page for Packetbeat Starting with 5.0.0-alpha1, Packetbeat exports two different types of data: transactions of various protocols and flows. Both are available in the index pattern, but with different values depending if it’s a HTTP transaction or a flow. The creates two new searches to customize the transactions view and the flows view with the most important fields. They are already available in and you can start using them by following the .  Loading Elasticsearch template is now on by default With this , when the Beat starts the Elasticsearch template is loaded automatically by default, if it was not previously loaded. You can force to overwrite the template by enabling the option. This change is already available in 5.0.0-alpha1. If you want to learn more about this, please check the . ","locales":"","title":"Brewing in Beats: Manage dashboards per Beat"}
{"index":{}}
{"author":"Costin Leau","category":"Releases","publish_date":"2016-04-08T00:00:00.000Z","url":"/blog/es-hadoop-2-3-0-and-2-2-1-released","seo_title":"Elasticsearch for Apache Hadoop 2.3.0 and 2.2.1 released","content":" Joining the release train this week, Elasticsearch for Apache Hadoop and are now out containing compatibility improvements and bug fixes. Users are recommend to upgrade as soon as possible to take advantage of these. As always, the artifacts are available at the and or . Important fixes HDFS repository compatibility with Elasticsearch 2.3.0 For those that missed it, Elasticsearch 5.0.0 alpha1 was a few days back and among its bundle of features, ships out of the box with the repository hdfs plugin. As such, pending any unforeseen events, ES-Hadoop 2.3 will be the last release cycle containing the HDFS plugin repository. Optimized network transfer for fixed routing When using a fixed or predefined routing, the connector optimizes the network request to hit only the target shards (whether it is for reads or writes). Improved indexing of Spark s The check for empty Spark s has been tweaked to avoid triggering loading of the content, especially important when using disk persistence or no caching. Better detection of shards overlap The algorithm for checking overlapping shards has been improved (thanks to a user ) to use significantly less memory and thus, increasing the limit of indices it can work on. Last 2.2. release Alongside 2.3, ES-Hadoop 2.2.1 is released as the last planned maintenance release in the 2.2.x line. It contains a series of backported bug-fixes for those with conservatory upgrade paths. However even if you are on ES 1.x, upgrading to ES-Hadoop 2.3 is highly recommended. Feedback Looking forward to hearing your feedback on ! You can find us on , Twitter () or the . works too. ","locales":"","title":"Elasticsearch for Apache Hadoop 2.3.0 and 2.2.1 released"}
{"index":{}}
{"author":"Monica Sarbu","category":"Releases","publish_date":"2016-04-07T00:00:00.000Z","url":"/blog/beats-1-2-1-released","seo_title":"","content":" Today we are pleased to announce the bug fix release of Beats 1.2.1. Change the behaviour of environment variables expansion Version 1.2.0 introduced the possibility of using . This is a great feature because it allows you to inject settings via environment variables, but the way it was initially implemented was a bit too simplistic. The way it worked was that before parsing the configuration file, the code simply replaced strings like or with the value of the environment variable. However, we didn't account for the usage of as a literal in configurations files. For example, if a dollar sign shows up in a password or in a regular expression, it will get removed together with the word next to it. This can break existing configuration files. In Beats 1.2.1 we restrict to one form and replace only with the value of the environment variable. We started our own to improve the environment variables expansion so that it works only on selected options. Add username to Topbeat We were planning to add support for reporting the user name of a process in Beats 1.2.0, but we discovered in the late stages of QA that it doesn’t actually work, so we removed it from the release notes. It is now fully working in Beats 1.2.1. Other fixes coming with Beats 1.2.1 Send us feedback!A big Thank You to everyone who contributed code, reported issues, created a new Beat or just tried the Beats. Start your experience with  and let us know what you think on , , or open an issue on . ","locales":"","title":"Beats 1.2.1 released"}
{"index":{}}
{"author":"Jason Dickson","category":"News","publish_date":"2016-04-13T00:00:00.000Z","url":"/blog/five-star-videos-and-more-top-10-elasticon-16-presentations","seo_title":"Five-Star Videos from Elastic{ON}16 Presentations","content":" Elastic{ON} was a rad experience. This was my first time at our annual user conference. It was so much fun and an amazing opportunity for meeting other users, of our amazing dev team, and learning more about the Elastic Stack. For those of you who attended, I can't read all of your minds. So I can't say for sure that it was as much of an overwhelming and cool experience for you, but I did have a unique vantage point as one of the coordinators of the conference's . One thing that stood out in the mobile app comment feed again and again is that there were many great presentations happening across three stages, plus the Spotlight Theater, and the Birds of a Feather community chats in the lunch area. It was often hard to choose which talk to attend. We have proudly shared videos of all of the great , and now want to provide you with a roundup of the most popular talks. We had two criteria for selecting videos for this top 10 list: 1) top views on Elastic.co and 2) top-rated video (1 to 5 stars) by users from our mobile app. A handful even achieved the impressive average rating of 5 out of 5 — there are diamonds in the rough here! So we narrowed the list down to the following 10 vids. We hope you find a presentation or two that you haven't watched, or maybe didn't even realize existed. Without further delay, enjoy! 1) Logstash's creator Jordan Sissel and team lead Suyog Rao wowed the crowd when they presented “,” but this later talk — a technical deep dive with core developers Colin Surprenant and Andrew Cholakian — has been very popular on demand, earning a place on our list. 2) “Space… the final frontier.” Oh wait, this isn't about that awesome Star Trek vs. Star Wars string wall at Elastic{ON}? My bad. This geo aggregations and visualizations talk is all about geo and Elasticsearch, and was given by core Elasticsearch developer Nick Knize, who self-describes himself as our “spatial agent.” 3) Probably the most anticipated and talked about new feature announced at Elastic{ON}, is now available for . Elastic's Mark Harwood and Steve Kearns revealed Graph's power to discover and explore the relevant connections in your data sets. 4) Another superstar among on-demand recordings! Elastic software engineer Spencer Alger and Director of Product Management Tanya Bragin discussed Kibana's extensibility, the goals of Kibana plugins, and resources like the plugin generator. 5) \"Here be math.\" The first talk on this list that was rated 5 stars out of 5 by you, attendees of Elastic{ON}! Elasticsearch software engineer Britta Weber spoke about BM25, the scoring algorithm of Apache Lucene 6 that will soon be the new default scoring algorithm in Elasticsearch. 6) This is another talk — featuring Elastic's Martijn van Groningen, Tal Levy, and Jim Unger — that attendees rated 5/5 stars! 7) Also rated 5/5 is this gem of a talk on choosing the best size and configuration of your cluster. Presented by Elastic solution architect Christian Dahlqvist and Elastic technical trainer Ryan Schneider, this features the answers to important questions that so many of you have about sizing your cluster. 8) The first talk on our list that took place in the smaller Spotlight Theater location at Elastic{ON} is all about securing Elasticsearch. Software engineer Igor Motov and security engineer Jay Modi discuss common pitfalls for securing data, document- and field-level security, and why will make integrated security across the Elastic Stack so grand. 9) You're not alone. That's the message that was sent loud and clear by Elastic's Chris Earle and Mark Walkom as they presented on the most interesting trials and tribulations our customers have shared with our . This talk was given five stars by in-app voting! 10) In one of the most popular talks from the first day of Elastic{ON}, Elastic's Clinton Gormley and Simon Willnau","locales":"","title":"Five-Star Videos and More: Top 10 Elastic{ON}16 Presentations"}
{"index":{}}
{"author":"Megan Wieling","category":"","publish_date":"2016-04-11T00:00:00.000Z","url":"/blog/witwies-elastic-los-angeles-user-group-meetup","seo_title":"Where in the World is Elastic? - Elastic Los Angeles User Group Meetup","content":" Welcome to Find out which Elastic events and meetups are happening near you in the next two weeks.Upcoming EventsApril 18: April 19: April 20-22: April 19: April 19: April 23-24: April 21-23: Upcoming MeetupsApril 13: April 14: April 18: April 19: April 19: April 19: April 20: April 20: April 12: April 12: April 14: April 14: April 14: April 16: April 19: April 19: April 20: April 20: April 20: April 21: April 21: April 21: That's it for this week. Stay tuned next week for more Elastic happenings.- The Elastic TeamP.S.  if you're interested in hosting a meetup or are giving a talk about Beats, Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic? - Elastic Los Angeles User Group Meetup"}
{"index":{}}
{"author":"Henrik Nordvik","category":"Engineering","publish_date":"2016-04-08T00:00:00.000Z","url":"/blog/how-to-make-a-dockerfile-for-elasticsearch","seo_title":"","content":" How to make a Dockerfile for ElasticsearchDocker containers gives you a way to ship and run applications with their environment in an isolated and repeatable way. While there are a myriad of Docker images out there , creating your own Dockerfile allows you to customize it, for instance by installing plugins, changing the base image, strip out what you don't need, etc. Dockerfiles also act as a way to document how an application gets installed and deployed. In this introductory post we will go through how to create a Dockerfile from scratch for running Elasticsearch, and discuss a few things that you need to consider when creating your own. Building the imageA Dockerfile is a recipe with steps describing how to build your Docker image. You start from a base image, which gives you the basics needed for running applications, then run steps on top of that, which results in a new image. If you want you can also use the resulting image as a base image for another image. The simplest Dockerfile you can create is something like this: FROM ubuntu:14.04 Put this in a file called . You can now build it by running: docker build -t my-es-image . The image has been built and can be run with: docker run --rm -it my-es-image /bin/bash While this created an image that is not very useful, we have now learned how to build and test an image. Now let’s create a more useful one. Since Elasticsearch requires Java to run, let’s install it first. FROM ubuntu:14.04 ENV DEBIAN_FRONTEND=noninteractive RUN apt-get install -y --no-install-recommends software-properties-common && add-apt-repository -y ppa:webupd8team/java && \\ apt-get update && \\ (echo oracle-java8-installer shared/accepted-oracle-license-v1-1 select true | sudo /usr/bin/debconf-set-selections) && \\ apt-get install --no-install-recommends -y oracle-java8-installer && \\ rm -rf /var/cache/oracle-jdk8-installer && \\ echo \"networkaddress.cache.ttl=60\" >> /usr/lib/jvm/java-8-oracle/jre/lib/security/java.security && \\ apt-get clean && rm -rf /var/lib/apt/lists/* ENV JAVA_HOME /usr/lib/jvm/java-8-oracle This one installs Oracle JDK 8. If you build and run it with the  command above then you can test that is works by running . At this point we’re ready to install Elasticsearch. Let’s use the apt package. RUN groupadd -g 1000 elasticsearch && useradd elasticsearch -u 1000 -g 1000 RUN apt-key adv --keyserver pgp.mit.edu --recv-keys 46095ACC8548582C1A2699A9D27D666CD88E42B4 && \\ add-apt-repository -y \"deb http://packages.elastic.co/elasticsearch/2.x/debian stable main\" --keyserver https://pgp.mit.edu/ && \\ apt-get update && \\ apt-get install -y --no-install-recommends elasticsearch WORKDIR /usr/share/elasticsearch RUN set -ex && for path in data logs config config/scripts:  do \\ mkdir -p \"$path\":  \\ chown -R elasticsearch:elasticsearch \"$path\":  \\ done Before we run it, we should add an elasticsearch.yml file. Create a file named  in the same directory as the Dockerfile, with this content: cluster.name: \"docker-cluster\" network.host: 0.0.0.0 Also, to get logging to work with docker we should add a simple  file: rootLogger: INFO,console appender: console: type: console layout: type: consolePattern conversionPattern: \"[%d{ISO8601}][%-5p][%-25c] %m%n\" Note: When running with logging to stdout/stderr Docker stores the log in a json file, and it is recommended to specify a max size for the log file to rotate, and a max number of files to keep. E.g.  To add these files to the container we add the following to the Dockerfile: COPY logging.yml /usr/share/elasticsearch/config/ COPY elasticsearch.yml /usr/share/elasticsearch/config/ This will bake the files into the image when running .  What’s left now is to actually make the container run Elasticsearch at startup. USER elasticsearch ENV PATH=$PATH:/usr/share/elasticsearch/bin CMD [\"elasticsearch\"] EXPOSE 9200 9300 To run this, first build it as before, and then run with . This opens port 9200 from ","locales":"","title":"How to make a Dockerfile for Elasticsearch"}
{"index":{}}
{"author":"Michael McCandless","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2016-04-07T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2016-04-05","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top News release w/ 6, ingest node, & more! Details: — elastic (@elastic) Elasticsearch Core Apache Lucene Watch This Space, where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2016-04-05"}
{"index":{}}
{"author":"Scott Fingerhut","category":"Culture","publish_date":"2016-04-06T00:00:00.000Z","url":"/blog/elasticsearch-in-3","seo_title":"Describe Elasticsearch in Three Words","content":" During our annual conference in San Francisco, our roving reporter captured some great insights from attendees. If you didn't catch \"\", check that one out. We filmed that at the Elastic{ON} party where alcohol may have played some role.  This time, on the conference floor, we asked attendees to describe Elasticsearch in just three words. There were certainly plenty that told us Elasticsearch was fast and scaled. But you'll have to watch the video to see some of the other ones. ","locales":"","title":"Video: Describe Elasticsearch in 3 Words"}
{"index":{}}
{"author":"Michelle Carroll","category":"News","publish_date":"2016-04-07T00:00:00.000Z","url":"/blog/elastic-pioneer-program","seo_title":"","content":" This week, we released the alpha 1 of the . You can test the new goodness in the Elastic Stack 5.0 today, by . This pre-release period is incredibly important. As much as we test the software before release, there are users with environments we haven’t conceived of, and folks using the Elastic Stack in ways we’ve never seen. We want to know the limits of our stack, and do everything we can to improve it before general release. We’re launching the Elastic Pioneer Program to encourage folks to try the latest version. What is the Elastic Pioneer Program?We’re very excited about these upcoming Elastic Stack Version 5.0 releases, but we want to make sure they’re perfect (or as perfect as software gets) before they ship more broadly. Everyone who reports a legitimate bug on the pre-release of the software will be recognized for the wider release, and receive a special Elastic gift package as our thank you. If you find something particularly tricky, you could also earn a free ticket to , coming up next March 7 – 9 in San Francisco. How to participateTo join the program, just try out the pre-release of any (preferably, every) part of the Elastic Stack, and open issues as you find them in the appropriate repo (, , , ) or forum (). When you open an issue, mention that you found the bug in 5.0.0-alpha1/2/beta1, and we’ll add a “Pioneer Program” label. While we appreciate the information, duplicate issues won’t enter you into the program.: This is an alpha release and is intended for purposes only. There is no guarantee that any of the 5.0.0-alpha1 versions will be compatible with other pre-releases, or the 5.0.0 GA. We strongly recommend that you keep this far, far away from production. We hope you’ll join us in trying the pre-release versions of the Elastic Stack 5.0! , or . ","locales":"","title":"Explore the Elastic Stack 5.0 with the Elastic Pioneer Program"}
{"index":{}}
{"author":"Jason Dickson","category":"User Stories","publish_date":"2016-04-07T00:00:00.000Z","url":"/blog/usaa-security-analytics-journey-to-the-elastic-stack","seo_title":"USAA, Security Analytics and the Elastic Stack","content":" Is your company getting the most out of its SIEM and log management solutions? Allow us to share a story with a happy ending. It's about a company that needed the best way to analyze their ever-growing data, in order to protect personal information and other sensitive files. The analysts now enjoy faster, easier data management, while predicting and averting cyber threats all along the way … all while USAA saves money. USAA is a financial institution serving the U.S. military community, and Neelsen \"Nelly\" Cyrus has been a part of that institution for almost two decades. As a senior security analyst in the company's Cyber Threat Operations Center (CTOC), his primary focus is infrastructure support. At in San Francisco, we were honored to host Nelly and five other attendees from the USAA CTOC.USAA's 93-year history stretches back to a group of U.S. Army officers who saw a need for auto insurance when other insurers had classified military officers as \"high risk.\" Today, employs over 26,000 people, boasts a multi-billion dollar annual net income, and has been consistently named one of the over the past 11 years by . At Elastic{ON}, Nelly presented on USAA's transition to the Elastic Stack from a security information and event management (SIEM) solution — a transition that saved the company money and improved productivity among the company's security analysts. The presentation included a recipe for \"hunting\" — the practice of information security analysts proactively seeking out malicious activity and vulnerabilities before harm is done. Hunters have to think like attackers and block off routes before they can be exploited. As the volume of these attacks is ever-increasing, analysts like Nelly and the team at USAA CTOC rely on technology like the Elastic Stack for effective logging and constant monitoring for malicious activity. USAA traditionally invested in large enterprise solutions. Advocating the Elastic Stack, an open source product, up the chain of command for use at the size and scale needed by the CTOC team was challenging. However, the available from Elastic's world-class engineers made all the difference. \"We know that they're there, and they've proven it time and time again,\" Nelly said. One manager assured Nelly that he made the right move. The cost was easily justifiable based on seeing analyst productivity improvements.In production, USAA's Elastic Stack deployment has grown to seven clusters, grouped by feed type — feeds change often but include \"almost all of the major security appliances,\" Unix and Windows server events, etc. — after they broke up their single, monolithic cluster (and upgraded ) about two months before Elastic{ON}16. They send 24 feeds into Elasticsearch, with between 2 billion and 4 billion security events daily and an average of about 52,700 events per second. They have 53.11 billion documents in their store. About moving to the multi-cluster setup, Nelly noted that they cut the time it takes to create a snapshot of their data from 20+ hours to 10 hours, with the snapshots executed in parallel. It's important for all companies to back up data, but keenly important for those in financial and military spaces like USAA. The great thing about Elasticsearch's snapshot API is that after the first backup process, subsequent snapshots save the delta change between the existing snapshots and new data. Transmitting far less data means snapshots takes less valuable time away from CTOC personnel.Bottom line: USAA's old SIEM and old log management solution weren't giving them the same bang for the buck that the Elastic Stack does today. Elastic quickly became an integral part of USAA's cyber threat prevention process, and the speed and scale of Elastic helps these analysts ask questions (and find answers) that they couldn't ask before — and if this interests you, definitely watch","locales":"","title":"USAA, Security Analytics, and a Journey to the Elastic Stack"}
{"index":{}}
{"author":"Andrew Cholakian","category":"Releases","publish_date":"2016-04-07T00:00:00.000Z","url":"/blog/logstash-2.3.1-and-2.2.4-released","seo_title":"Logstash 2.3.1 and 2.2.4 Released","content":" Hot on the heels of 2.3.0 and 2.2.3, we’ve released Logstash 2.3.1 and Logstash 2.2.4 which contain important compatibility and security updates. We highly recommend that all users upgrade to either version immediately. You can read the detailed release notes , or jump directly to our if you can't wait to test drive this release! Additionally, we've released an update to the 2.2.x series, with a bunch of important bugs packaged in 2.2.4. You can read the changelog for 2.2.4 .Regression in Regex handling in JRubyThe upgraded version of JRuby we included in 2.3.0 contained an important fix for Windows users, but introduced a dangerous thread safety bug for regular expressions used within Logstash. Environment Variable Support Now Deactivated By DefaultWe are very excited to have introduced support for environment variables in configuration files in Logstash 2.3.0! However, we didn’t anticipate that some existing configs would be incompatible with this change. The environment variable support we added treats or characters as part of our new variable syntax. This creates problems for people using the characters for other purposes, such as in password values. As such, we have disabled environment variable interpolation by default in 2.3.1 to make the upgrade path easier. You can enable it in 2.3.1 with the flag on the CLI. In addition, we have removed support for environment variables completely. Only the syntax will be supported going forward as it minimizes the potential for conflicts. This change was completed in Fixed Broken ConditionalsLogstash 2.3.0 includes our new dynamic reloading feature. A change to the pipeline internals required by this feature caused conditionals (anywhere an was used in a config) to not work correctly. This was fixed in . Logstash versions prior to 2.3.0 were not affected by this bug. Reverting the new Java Event In Logstash 2.3.1The new pure Java implementation of the Event class Logstash 2.3.1 is lightning fast, but unfortunately not as compatible as we’d have liked for a minor release. In particular, it could cause problems with some custom Ruby filter scripts and custom plugins from the community. We take our commitment to compatibility, and versioning semantics, seriously. Though we have reverted to the prior Ruby Event implementation, the Java version remains the correct technical direction and we will most likely be reintroducing it in Logstash 5.0 if not sooner. If any breaking changes need to be made, we will ensure these changes are communicated as clearly and broadly as possible. Passwords Printed in Log Files under Some ConditionsIt was discovered that, in Logstash 2.1.0+, log messages generated by a stalled pipeline during shutdown will print plaintext contents of password fields. While investigating this issue we also discovered that debug logging has included this data for quite some time. Our latest releases fix both leaks. You will want to scrub old log files if this is of particular concern to you. This was fixed in issue Fixed Config Test Flag in Logstash 2.3.0The Logstash 2.3.0 release inadvertently broke the option, this has been fixed in Logstash 2.3.1. This was fixed in issue Other IssuesThe accounting of issues fixed in the Logstash 2.3.1 / 2.2.4 release is tracked on issue . FeedbackWe are super excited for this release of Logstash and look forward to your feedback. You can reach us at our , open issues in our , or tweet at us . Happy 'stashing! ","locales":"","title":"Logstash 2.3.1 and 2.2.4 Released"}
{"index":{}}
{"author":"Tudor Golubenco","category":"Releases","publish_date":"2016-04-05T00:00:00.000Z","url":"/blog/beats-5-0-0-alpha1-released","seo_title":"","content":" At almost precisely a year after the Packetbeat team has joined Elastic, we’re excited to reveal the first alpha release of the Filebeat, Packetbeat, Topbeat, and Winlogbeat next major versions. One version to rule them allYou might be wondering why we’re jumping from version 1.2 directly to 5.0. To make our software suddenly more stable and to one-up our competition, of course. In seriousness, all the projects in the Elastic stack are doing releases in sync and will use the same version numbers from now on. As Kibana is currently at 4.5, we’re all going with 5.0 as the next major. This is to avoid the support matrix from hell, for example now you need to know that Beats 1.2 were tested against Elasticsearch 2.3, Logstash 2.3 and Kibana 4.5.  Starting with 5.0, you’ll know that if Beats and Elasticsearch have the same version number, they were released at the same time and we have tested them together. It simplifies communication all around. New FeaturesBeats 5.0-alpha1 comes packed with new features and you can expect more of them to land in during the alpha and beta phases. Here are some of the highlights from Alpha 1: Custom fields and generic filteringYou now have more freedom over how the documents created by the Beats look like. On one hand, you can now add custom fields and tags per Beat and module. On the other hand, you can use the newly introduced to remove the fields that you don’t want. These features are implemented at the libbeat level, meaning that all automatically benefit from them as soon as they upgrade. JSON support in FilebeatFilebeat can now natively objects from log lines. This is useful for structured logging, where the logging library writes the metadata directly formatted as JSON. This can also be used as a convenient way of collecting logs from Docker hosts, because Docker uses JSON to wrap the log lines from the application. Integration with Ingest NodeThe new functionality, released with Elasticsearch 5.0.0-alpha1, is big news because it gives users processing capabilities similar with Logstash directly in Elasticsearch! This makes it really easy to get started with the Elastic stack. For simple logging usecases, for example, you only need Filebeat and Elasticsearch. All Beats can work with the Ingest Node, simply set the in the Elasticsearch output configuration. Packetbeat IP/TCP flowsSo far Packetbeat was focused on the application layer protocols, giving you visibility into the business transactions as seen in the network. Packetbeat now also reports statistics like packet count and byte count about IP and TCP flows, regardless of the upper layer protocols. This opens Packetbeat to a new set of use cases, giving insights into how the traffic is flowing through the network. Kafka outputWe listened to your feedback and we’ve added Kafka output support in Beats, at the same time removing the deprecation mark for the Redis output. This means that if you are passing all messages through a Kafka queue anyway, you won’t need a Logstash instance to convert between Beats and Kafka. Winlogbeat improvementsWinlogbeat now extracts all the fields from Windows event log records including the EventData and UserData fields and includes them in the documents it indexes. In addition, now it is possible to select events by event ID, level, and provider. Winlogbeat efficiently implements this event selection by using a query with Windows APIs so that only the requested events are returned. Don’t fear the alphaGetting your feedback early is key for us to make the necessary adjustments in time for the 5.0 GA release, so please test early and often. You can find us on discuss for question and discussion and on Github for issues and enhancement requests. ","locales":"","title":"Beats 5.0.0-alpha1 released"}
{"index":{}}
{"author":"Shay Banon","category":"News","publish_date":"2016-04-05T00:00:00.000Z","url":"/blog/elastic-stack-release-5-0-0-alpha-1","seo_title":"Elastic Stack Release - 5.0.0 alpha 1","content":" At Elastic{ON} we announced the Elastic Stack. Before diving into detail about the 5.0 alpha release, let us review why this release is so important. When we say “,” we are making a powerful statement about how we develop our products and how our community and customers will consume those products. The Elastic Stack is more than just a name. It is an investment in building, testing, and releasing all products together. Recognizing this, we incremented the version number to . This is not just about our release cycles. It is a commitment to make it easier for developers to add new functionality not just to a single product, but to the entire Stack. Which brings us to the notion of “packs.” Packs are bundled extensions for the whole Elastic Stack - and they’re key in making your life easier. Say heya to the X-Pack. Naming things is the greatest challenge in computer science:  perhaps only surpassed by exactly-once delivery, guaranteed messages of order, and exactly-once delivery. Marvel, Shield, and Watcher will be product names no longer. (Quick, can you tell us which product performed which feature?) Rather, all commercial capabilities are combined into X-Pack, which includes security, alerting, monitoring, Graph, and reporting features. This ensures a consistent experience during installation and usage. In addition, features (like security) will apply to the entirety of the Stack. In addition, we will soon be announcing the Elastic Pioneer Program to recognize community participation. The feedback garnered during the 2.0 release was invaluable. Help us make 5.0 the most scrutinized release ever. Happy and testing! Keep in mind, it is an alpha so don’t put it into production. Given that this is an alpha release, it is not available on . We expect a release candidate version of 5.0.0 in the coming months which we’ll make available on Elastic Cloud, the best hosted Elastic Stack. Let’s explore a few of the alpha details at a high-level. Elasticsearch For more detailed information, and quite a few other features, peruse the . Kibana For more detailed information, view the . As a note:  No, we haven’t lost our senses.  But 5.0.0 alpha 1 has temporarily lost its Sense. Sense will remain open source and will be built into Kibana, directly, as “Console”. But it is not yet available in this release. Logstash For more detailed information, grok the . Beats For more detailed information, a lightweight (at the edge). X-Pack X-Pack is a single extension for Elasticsearch and Kibana that brings together the functionality of Shield, Watcher, Marvel, Graph, and adds new Reporting capabilities. All of the products now install together, have the sleek new Kibana 5 design, and it’s easier than ever to ! Among the new features and improvements, Reporting is a highlight: ","locales":"ja-jp","title":"Elastic Stack Release - 5.0.0 alpha 1"}
{"index":{}}
{"author":"Court Ewing","category":"Releases","publish_date":"2016-04-05T00:00:00.000Z","url":"/blog/kibana-5-0-0-alpha1","seo_title":"Kibana 5.0.0-alpha1 released","content":" Kibana 5 is the best thing since sliced bread. There, I’ve said it. You know what? It’s better than sliced bread. Kibana 5 is going to dethrone bread as the universal standard of goodness. And you can take a bite out of this bad boy right now: today we’re sharing with the world the first alpha release of Kibana 5. If you’re already salivating, pick up the Kibana 5.0.0-alpha1 from the page. This is software that will only work with . Please test it, but do use it in production. So what’s new in Kibana 5? A new design That’s right, we redesigned it. Where Kibana 4 was dreary, Kibana 5 is bright and colorful. Where Kibana 4 wasted countless pixels on unnecessary navigation and chrome, Kibana 5 lets the borders fall away and brings a new focus on your data. Where Kibana… ah forget it. Just see for yourself: First-class applications Since the plugin system was launched in 4.2, people have been building entire applications on top of Kibana. In Sense and Timelion, we created two such applications ourselves, but even they have been relegated to a tiny “app switcher” that is mostly hidden away in the toolbar. In Kibana 5, all applications are first-class citizens. Applications added by any plugin will appear in the main navigation alongside the Kibana favorites: discover, visualize, and dashboard. Packs, and a new plugin installer I’m sure you all loved installing every single plugin by hand as well as always remembering the exact plugin version that was compatible with your current version of Kibana. And off the top of your head, I bet you can recite exactly which organization name we arbitrarily chose to use for any given plugin of our own. But despite those “charms” of the existing plugin installer, we decided to make it a bit easier this time around anyway. In Kibana 5, one or more plugins can be bundled and installed as a single pack. Want to install a third party pack? Just give it a url: Or how about one of our own - perhaps timelion’s your cup of tea: Want security, monitoring, reporting, and graph? Grab them all as a single pack: What is it not? Great question, me! Kibana 5 is not a rewrite from the ground up. We ripped that band-aid off long ago, and while necessary at the time, a massive overhaul of the entire application is no longer required to take a huge step forward like we have in Kibana 5. This alpha release is also not production ready. There are a bunch of known issues, and there are no doubt many more that we haven’t discovered yet. There will be tons of commits coming into 5.0 over the next few weeks and months, and we’ll probably break a few new things as well. This release isn’t even feature-complete. We have a ton of features that we’re still working on that we want to get into 5.0. Sense, for example. We’re bringing that whole plugin into Kibana core, but that work isn’t finished yet, so alpha1 is Senseless. What’s next? Well, alpha2 of course! We’re already working on it, and it’s going to be even better than this. When Kibana 5.0 stable ships, these Kibana pre-releases will have bogarted the whole goodness leaderboard, and there will be no bread in sight. Too far with that analogy, eh? Anyway, we hope that you’ll download this alpha and try it out. We’d love to get your feedback on our , , or even , and please post any bugs you find directly to our . ","locales":"","title":"Kibana 5.0.0-alpha1 released"}
{"index":{}}
{"author":"Megan Wieling","category":"","publish_date":"2016-04-05T00:00:00.000Z","url":"/blog/witwies-lone-star-php-meetups-worldwide","seo_title":"Where in the World is Elastic? - Lone Star PHP and Meetups Worldwide","content":" Welcome to Find out which Elastic events and meetups are happening near you in the next two weeks.Upcoming EventsApril 7-9: Upcoming MeetupsApril 7: April 13: April 14: April 5: April 5: April 6: April 7: April 7: April 7: April 14: April 14: April 14: April 9: April 16: April 6: April 11: That's it for this week. Stay tuned next week for more Elastic happenings.- The Elastic TeamP.S.  if you're interested in hosting a meetup or are giving a talk about Beats, Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic? - Lone Star PHP and Meetups Worldwide"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2016-04-05T00:00:00.000Z","url":"/blog/elasticsearch-5-0-0-alpha1-released","seo_title":"Elasticsearch 5.0.0-alpha1 released","content":" Today we are excited to announce the release of based on . This is the first in a series of pre-5.0.0 releases designed to let you test out your application with the features and changes coming in 5.0.0, and to give us feedback about any problems that you encounter.  This is an alpha release and is intended for . Indices created in this version . Upgrading 5.0.0-alpha1 to any other version is not supported. Elasticsearch 5.0.0-alpha1 is jam packed with awesome new features, with more to be added before we release 5.0.0 GA. ","locales":"","title":"Elasticsearch 5.0.0-alpha1 released"}
{"index":{}}
{"author":"Pier-Hugues Pellerin","category":"Releases","publish_date":"2016-04-05T00:00:00.000Z","url":"/blog/logstash-5-0-0-alpha1-released","seo_title":"","content":" We are excited to announce the availability of the first pre-release version for Logstash 5.0.0. Wait, what? 5.0.0? Yep, the next major version will be 5.0.0. In case you missed our during Elastic{ON} 2016, we've decided to align all the components of the Elastic Stack to a single version. You can find the release notes or head straight to the page if you can't wait to try it out. IMPORTANT: This is an alpha release and is intended for testing purposes only. There is no guarantee that Logstash 5.0.0-alpha1 will be compatible with other pre-releases, and 5.0.0 GA. Monitor All The Things!Over the last couple of releases, we've been working hard to make Logstash easier to manage operationally. Our goal is to make it a breeze to configure and deploy multiple Logstash instances, and in 5.x we'll continue to add features to support this initiative. In this release, we are pleased to introduce the first set of which will provide more visibility into the Logstash pipeline. Event StatsWant to measure the number of events processed by Logstash? We've got an API for that now - curl localhost:9600/_node/stats?pretty{ \"events\" : { \"in\" : 15000, \"filtered\" : 14875, \"out\" : 14000 }, \"jvm\" : { \"timestamp\" : 1459393492170, \"uptime_in_millis\" : 18731, \"mem\" : { \"heap_used_in_bytes\" : 245625232, \"heap_used_percent\" : 11, .... Hot ThreadsIf you've used the in Elasticsearch, you know how useful it can be for debugging hotspots in your cluster. Well, now, you can do the same thing in Logstash! The new provides stacktrace from top 3 Java threads that are consuming the most CPU.curl localhost:9600/_node/hot_threads?human Hot threads at 2016-03-30T20:08:22-07:00, busiestThreads=3: 5.22 % of of cpu usage by waiting thread named '[main]>worker3' java.lang.Object.wait(Native Method) java.lang.Object.wait(Object.java:460) org.jruby.RubyThread$SleepTask.run(RubyThread.java:1050) org.jruby.RubyThread.executeBlockingTask(RubyThread.java:1066) org.jruby.RubyThread.wait_timeout(RubyThread.java:1414) org.jruby.ext.thread.Queue.pop(Queue.java:152) org.jruby.ext.thread.Queue.pop(Queue.java:127) org.jruby.ext.thread.SizedQueue.pop(SizedQueue.java:111) org.jruby.ext.thread.SizedQueue$INVOKER$i$pop.call(SizedQueue$INVOKER$i$pop.gen) org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:134) 2.44 % of of cpu usage by timed_waiting thread named '[main]-pipeline-manager' java.lang.Object.wait(Native Method) .... We are big fans of APIs here at Elastic — we love providing access to all kind of metrics into the working of our software. Next up, in Logstash, we are targeting stats at a plugin level granularity. For example, it would be great to know how long (on average) an event spends in grok filter, geoip filter etc. You'll be able to do this soon. How 'bout a monitoring UI, you ask? A Kibana app to visualize all these metrics across a period of time is in the works! So, stay tuned!Kafka 0.9 SupportApache Kafka had 0.9 couple of months ago which brings in new security features (SSL, client based auth, access control), improved consumer API, and much more. This Logstash release provides support for SSL encryption and client auth features in Kafka. Some of the configuration options in the consumer have changed — as such this plugin is not backward compatible. To use these new security features, you'd need to upgrade the Kafka broker to 0.9bin/plugin is now bin/logstash-pluginWe'd like to note that bin/plugin command — which is used to manage plugins — has been renamed to bin/logstash-plugin command. The main reason was to prevent PATH being polluted when other components of the are installed on the same instance. Also, this new command will be enhanced in upcoming versions to work with cross-component plugin packs.Elasticsearch 5.0.0 CompatibilityThis alpha1 release works out of the box with Elasticsearch 5.0.0-alpha1! Also, you'll be able to use any 2.x Elasticsearch version with Logstash 5.0.0-alpha1. So, go ahead and give it","locales":"","title":"Logstash 5.0.0-alpha1 released"}
{"index":{}}
{"author":"Adam Torman","category":"User Stories","publish_date":"2016-04-05T00:00:00.000Z","url":"/blog/integrating-salesforce-event-monitoring-with-elastic-stack-an-elasticon-reflection","seo_title":"","content":" Over a year ago, Abhishek Sreenivasa and I went to Strata + Hadoop World in San Jose to learn more about processing large amounts of Big Data using systems like Hadoop. Abhishek and I worked on a product called Event Monitoring, part of the Salesforce Shield product line.While at Strata + Hadoop World, Abhishek and I attended an awesome tutorial called hosted by . Why this particular tutorial? Well, as a product manager, I hear lots of requests from customers to work with event and log data and one of those requests was to visualize it using the Elastic Stack and Kibana. And as a kick-ass engineer, Abhishek loves building inspiring applications that demonstrate the power of our product, Event Monitoring. So we attended the tutorial, which became the inspiration for an Elastic Stack that Abhishek created with our intern, Mohammed Islam.Fast forward a year, Abhishek and I were at Pier 48, standing on stage at our first Elastic{ON} conference. We were talking about ', which was about the integration between Salesforce Event Monitoring and the Elastic Stack. The speaking engagement was stupendous — Abhishek crushed his explanation of how he designed and built the plug-in with Mohammed.But the most remarkable connection that was made was in discovering that our host and emcee for the session was none other than Kurt Hurtado:  the same instructor who gave us our introduction to the Elastic Stack. I wouldn’t say that the students became the masters, but I would absolutely emphasize the amazingness of the people who work at Elastic to make open source a viable and supported solution for the enterprise.And that was really the experience we had with everyone we met at the conference, from the Elastic AMA (Ask Me Anything) genius bar to the spontaneous conversations struck up in the heart of the pier, where we connected with people who shared our respect for large scale event management and visualizations, all while eating artisan food truck-catered lunch.Connections and network effects were definitely the theme for the conference. There was a string wall where attendees could provide input about ourselves, while comparing Star Wars to Star Trek.Elastic{ON} was a uniquely enjoyable conference experience. Connecting with people spontaneously, sharing what we built on top of the Elastic Stack, and learning more about how we can continue to work with Elastic were all highlights of this fantastic conference.Adam has worked at Salesforce for the past 10 years, both in professional services and as a platform product manager. In that time, among other achievements, he introduced a new way of layering user access controls called Permission Sets and built a product called Event Monitoring that easily integrates low level server application logs with a customer's SIEM or business intelligence reporting tools. On his , Adam provides tips and tricks for building security into every customer organization.Abhishek Sreenivasa is a software developer at Salesforce. He works on Platform Monitoring team that develops self-service Event Monitoring feature. Events from this feature are used by Salesforce customers for security audits and measuring application performance and feature adoption. ","locales":"","title":"Integrating Salesforce Event Monitoring with the Elastic Stack — An Elastic{ON}<sup>16</sup> Reflection"}
{"index":{}}
{"author":"Jason Tedor","category":"Engineering","publish_date":"2016-04-04T00:00:00.000Z","url":"/blog/a-heap-of-trouble","seo_title":"","content":" A Heap of Trouble Engineers can except giving their processes more resources: bigger, better, faster, more of cycles, cores, RAM, disks and interconnects! When these resources are not a bottleneck, this is wasteful but harmless. For processes like Elasticsearch that run on the JVM, the luring temptation is to turn the heap up:  what harm could possibly come from having more heap? Alas, the story isn't simple. Java is a . Java objects reside in a runtime area of memory called . When the heap fills up, objects that are no longer referenced by the application (affectionately known as ) are automatically released from the heap (such objects are said to have been ). The maximum size of the heap is specified at application startup and fixed for the life the application:  this size impacts allocation speed, garbage collection frequency, and garbage collection duration (most notably the dreaded stop-the-world phase which pauses all application threads). Applications have to strike a balance between small heaps and large heaps:  the heap . Too Small If the heap is too small, applications will be prone to the danger of out of memory errors. While that is the most serious risk from an undersized heap, there are additional problems that can arise from a heap that is too small. A heap that is too small relative to the application's allocation rate leads to frequent small latency spikes and reduced throughput from constant garbage collection pauses. Frequent short pauses impact end-user experience as these pauses effectively shift the latency distribution and reduce the number of operations the application can handle. For Elasticsearch, constant short pauses reduce the number of indexing operations and queries per second that can be handled. A small heap also reduces the memory available for indexing buffers, caches, and memory-hungry features like aggregations and suggesters. Too Large If the heap is too large, the application will be prone to infrequent long latency spikes from full-heap garbage collections. Infrequent long pauses impact end-user experience as these pauses increase the tail of the latency distribution:  user requests will sometimes see unacceptably-long response times. Long pauses are especially detrimental to a distributed system like Elasticsearch because a long pause is indistinguishable from a node that is unreachable because it is hung, or otherwise isolated from the cluster. During a stop-the-world pause, no Elasticsearch server code is executing: it doesn't call, it doesn't write, and it doesn't send flowers. In the case of an elected master, a long garbage collection pause can cause other nodes to stop following the master and elect a new one. In the case of a data node, a long garbage collection pause can lead to the master removing the node from the cluster and reallocating the paused node's assigned shards. This increases network traffic and disk I/O across the cluster, which hampers normal load. Long garbage collection pauses are a top issue for cluster instability. Just Right The crux of the matter is that undersized heaps are bad, oversized heaps are bad and so it needs to be . Oops!...I Did It Again The engineers behind Elasticsearch have long advised keeping the heap size below (some docs referred to a 30.5 GB threshold). The reasoning behind this advice arises from the notion of compressed ordinary object pointers (or ). An ordinary object pointer (or ) is a managed pointer to an object and it has the same size as a native pointer. This means that on a 32-bit JVM an oop is 32-bits in size and on a 64-bit JVM an oop is 64-bits in size. Comparing an application that runs on a 32-bit JVM to an application that runs on a 64-bit JVM, the former will usually perform faster. This is because 32-bit pointers require half of the memory space compared to 64-bit pointers:  this is friendlier to limited memory bandwidth, precious CPU caches, and leads to fewer garbage collection cycles as t","locales":"","title":"A Heap of Trouble: Managing Elasticsearch's Managed Heap"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2016-03-30T00:00:00.000Z","url":"/blog/elasticsearch-2-3-0-and-2-2-2-released","seo_title":"Elasticsearch 2.3.0 and Elasticsearch 2.2.2 released","content":" Today we are pleased to announce the release of based on , along with the bug fix release of based on . Both of these new releases are already available on , our Elasticsearch-as-a-service platform. This week’s release bonanza also includes new versions of , , and .Latest stable release:Bug fixes in 2.2:Elasticsearch 2.3.0 delivers three of the most-asked-for features in the history of Elasticsearch: the , the , and the , along with to help you to prepare for Elasticsearch 5.0. ","locales":"de-de,fr-fr,ja-jp,ko-kr","title":"Elasticsearch 2.3.0 and 2.2.2 released"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2016-04-04T00:00:00.000Z","url":"/blog/elasticsearch-2-3-1-released","seo_title":"Elasticsearch 2.3.0 and Elasticsearch 2.2.2 released","content":" Today we are pleased to announce the bug fix release of based on . This new release is already available on , our Elasticsearch-as-a-service platform.This release fixes a thread deadlock in Shield which can prevent nodes from joining the cluster. All users of Elasticsearch 2.3.0 with Shield are advised to upgrade.Latest stable release: ","locales":"","title":"Elasticsearch 2.3.1 released"}
{"index":{}}
{"author":"Spencer Alger","category":"Releases","publish_date":"2016-03-30T00:00:00.000Z","url":"/blog/kibana-4-5-0-released","seo_title":"Kibana 4.5.0 released","content":" Welp folks, the release bonanza has begun and with it comes Kibana 4.5.0. If you're ready to jump in, or just plain impatient, it's already available on  along with the latest releases for Elasticsearch, Logstash, and Beats. You can also download Kibana from our  page. As you have probably come to expect, Kibana 4.5.0 has been upgraded to take advantage of the latest features in Elasticsearch and therefore requires at least . Below are some of the other changes you'll find in Kibana 4.5.0. It's a smaller release, as we have been working hard to get Kibana 5.0 ready. Expect more on that soon! Features Bug FixesIn  we fixed a bug where pre-flight requests that failed would cause a full-screen \"fatal\" error. These requests happen just before Kibana sends queries to Elasticsearch in order to find out which indices should be searched, but were not using the same error handling logic as the actual search request. When a search fails it just shows an error at the top of the page with a dismiss button, and now eventually disappears. Now, errors that occur in these pre-flight requests will exhibit the same behavior. Another bug was caused by legend values that had funky characters. These values were being used to generate element selectors and were not being properly escaped before doing so. This meant that simply moving your mouse over the legend could sometimes result in the not-so-lovely \"fatal\" error screen. That was fixed in . Some other bugs that we squashed: ","locales":"","title":"Kibana 4.5.0 released"}
{"index":{}}
{"author":"Tudor Golubenco","category":"Brewing in Beats","publish_date":"2016-03-29T00:00:00.000Z","url":"/blog/brewing-in-beats-json-support-filebeat","seo_title":"","content":" Welcome to Brewing in Beats! With this series, we're keeping you up to date with all that's new in , from the details of work in progress pull requests to releases and learning resources. JSON support in Filebeat Since merging this last week, Filebeat can natively decode JSON objects from log lines. This is useful for structured logging, where the logging library writes the metadata directly formatted as JSON. Of course, this was already possible with Logstash, but  this enables people to take the direct Filebeat -> Elasticsearch path when they have their logs in JSON already. Another interesting use case for the JSON decoding is that it can be used to ship the logs from a Docker host. When writing the logs to files, Docker wraps the log lines of the application in JSON to add some meta-data. Because Filebeat decodes the JSON before applying line filtering and multiline rules, it is able to unwrap the JSON and then apply these rules, so these features combine well in the Docker and other similar use cases. Apachebeat and Redisbeat merged into Metricbeat Radovan Ondas, the creator of , and Chris Black, the creator of , have contributed modules for Apache and Redis to Metricbeat. It’s a great sign for Metricbeat that the Beats community devs are embracing it. Metricbeat progress Speaking of Metricbeat, Nicolas continued to shape it over the last few weeks, adding a , a common way of handling , adding metadata, so its functionality can be shared in Metricbeat, adding , and many others. New 5.0 Elasticsearch templates After making sure that Elasticsearch 5.0 is able to upgrade automatically the mapping templates used by the Beats, Adrien opened a to upgrade our templates with the the options accepted by 5.0. These are the templates that we will ship with Beats 5.0-alpha1. Winlogbeat - select events by level, event_id, and provider Winlogbeat is now able to by these key fields. It does this efficiently by adjusting its Windows API query to only return the events needed. The details are in the . Packetbeat - Split real_ip_header to only have one valueThis fixes an issue, where Packetbeat’s Geoip resolving didn’t work if the X-Forwarder-For (or similar) header had multiple IP addresses inside. With this fix, Packetbeat takes the first IP address when there are multiple defined in the header.  The  fix will be available in 1.2. ","locales":"","title":"Brewing in Beats: JSON support in Filebeat"}
{"index":{}}
{"author":"","category":"User Stories","publish_date":"2016-03-30T00:00:00.000Z","url":"/blog/how-big-sparks-levels-up-the-price-comparison-for-mobile-tech-services-with-elasticsearch","seo_title":"","content":" is the publisher of some of the biggest tech websites in the Netherlands, among which , and . Every month, more than two million consumers visit these websites on which we inform about and inspire with up-to-date news and in-depth articles on subjects like mobile tech, apps, games and devices. Furthermore, we advise them on specific purchases using our product and price comparison service. Our mission is to inspire consumers to get the most out of technology and to help them to make better choices. “”, says Peter Geurts - Founder of BigSpark. Dominant player in the NetherlandsIn just three years, BigSpark managed to become a dominant player on the Dutch market for product and price comparison services for mobile tech devices. Every month, the telecom experts of BigSpark advise customers on 70,000 purchases of smartphones, tablets, wearables and plans. Our price comparison service is powered by Elasticsearch, which makes it possible for BigSpark to claim a unique position in the market and continuously compare three million product prices of among others: Unique featuresThanks to Elasticsearch, consumers can easily search the complete product offerings of Dutch telecom providers and shops. They can use facets to filter on product features such as color, storage, data allowances, provider, as well as conditions such as time of delivery and user reviews of shops. Where we come fromOriginally, the setup of our price comparison service was based on Mongo DB and Node.js, but they did not keep up with our requirements anymore. Our databases were getting too big, aggregations became too slow and the performance of the service became unpredictable. A new setup in PostgreSQL, connected with Elasticsearch, proved to be the right solution. Suddenly, it became easy to quickly search the database, taking different groups and facet preferences into account. “”, says one of BigSpark’s Development engineers. Under the hoodElasticsearch has been installed on a cluster of two instances with 8GB RAM each. The index with offers is around 3GB and has a very good performance in combination with a low load. With Elasticsearch, the average query takes around 50ms which is significantly faster than previous queries. We are very impressed by the way Elasticsearch allows us to still deliver big performance with relatively limited resources.The future with ElasticsearchElasticsearch has several interesting out-of-the-box features with which we can improve the power of our price comparison service. These are for example function scores, which enables us to add a higher value to certain factors (like conversion, changes in search volume or the profile of the consumer) in order to influence the search results. Furthermore, we are going to give our users the possibility to intelligent suggestions on free text queries such as \"did you mean \"Galaxy S7\"?\" when someone accidentally typed \"Galaxie S7”.The success of BigSpark is partly thanks to the possibilities and accessibility of Elasticsearch: “”, concludes Peter Geurts. ","locales":"","title":"How BigSpark levels up the price comparison for mobile tech services with Elasticsearch"}
{"index":{}}
{"author":"Paul Sorensen","category":"User Stories","publish_date":"2016-03-29T00:00:00.000Z","url":"/blog/how-hoteltonight-finds-the-best-hotels-in-the-moment-elasticon-reflection","seo_title":"","content":" Elastic{ON} was a blast! The and the venue had so much to offer. I had never been to a conference with an AMA (Ask Me Anything) booth before, and I thought that was really one of the more valuable parts of the whole setup (I had been itching to ask questions about filter cache). I also really enjoyed the . It was inspiring to hear a recollection of each of their careers and how they ended up where they are now.Speaking at Elastic{ON} was an unforgettable experience. It was my first opportunity to speak at a conference, and I could not have been happier with the venue, coordination and the outcome. The atmosphere on stage was really great, and all the equipment was extremely easy to use. Everything went smoothly, and it was great to connect with so many people at the end.I once heard that stories make for good talks, and this is a . Behind the words and slides, smiles were had, tears shed, and glasses clinked throughout our journey. Hope you enjoy, and next time you’re feeling spontaneous, try HotelTonight!Paul Sorensen is a Platform Engineer at HotelTonight. We plucked him from Chicagoland, where he was building Rails web apps, digging deeper into Elasticsearch, and rooting hard for the Bears. He now keeps busy scaling HT’s infrastructure to support massive data demands, within a constantly changing real-time marketplace servicing millions of requests per day. You can find Paul taking his breaks at the ping-pong table, HT Bar, or playing soccer with the HotelTonight team. ","locales":"","title":"How HotelTonight Finds the Best Hotels in the Moment: An Elastic{ON} Reflection"}
{"index":{}}
{"author":"Tyler Hannan","category":"Releases","publish_date":"2016-03-30T00:00:00.000Z","url":"/blog/release-bonanza-elasticsearch-graph-shield-watcher-marvel-logstash-2-3-beats-1-2-and-kibana-4-5-are-now-available","seo_title":"","content":" .full-bleed-data h2 {font-size:22px: } The returns! Today, we are pleased to release Elasticsearch 2.3, Kibana 4.5, Beats 1.2, and updated versions of all the commercial products. But wait, there's more. Graph. Several thousands of you saw the presentation at . Several thousands more tuned into a recent webinar. Now, it is here. That's right, Graph is a GA product available for download. And, it just wouldn't be a Release Bonanza if we didn't have all the products available, now, on . Graph Existing products get bullets. New products get a paraGraph. (See what we did there?) When you store data in Elasticsearch, this data often contains references or properties that represent connections between objects, entities, people, machines, etc. We built the Graph product to allow you to ask a whole new type of question — focusing on these relationships. The best way to explore these connections is to see them, which Graph provides via a Kibana plugin. Like everything at Elastic, this UI is built on a simple, but powerful API, which leverages the same statistics that power search relevance (like bm25) to bring relevance to the relationship exploration process. Whether discovering and visualizing relationships through the Graph UI or directly integrating with the Graph API (recommendations anyone?), we are excited to see how you use Graph! Want to learn more? , to see it in action and learn more. Elasticsearch For detailed information, check out the post. Kibana For detailed information, take a look at the post. ","locales":"ja-jp","title":"Release Bonanza! Elasticsearch, Graph, Shield, Watcher, Marvel, Logstash 2.3, Beats 1.2, and Kibana 4.5 are Now Available!"}
{"index":{}}
{"author":"Monica Sarbu","category":"Releases","publish_date":"2016-03-30T00:00:00.000Z","url":"/blog/beats-1-2-0-released","seo_title":"","content":" Today we announce a new Beats release along with new versions of Kibana, Logstash, and Elasticsearch. Here are the highlights of the Beats release: Expand environment variables in configuration filesIt is now possible to use environment variables in the configuration file of any of the Beats. Here is an example for injecting the Elasticsearch host: output: elasticsearch: hosts: [“${ELASTICSEARCH}:9200”] Or if you would like to provide a default value in case the variable is not set: output: elasticsearch: hosts: [“${ELASTICSEARCH:localhost}:9200”] You can find more details in the . Managing the Elasticsearch templateThe mapping templates for Elasticsearch that we provide for each Beat need to be loaded before the Beat starts for the first time, otherwise Elasticsearch cannot know how to correctly assign types to the received data. So far we were asking our users to run a curl command to load the template, but that was easily forgotten and led to issues later on. With this release, the Beats themselves can load the Elasticsearch template by setting it in the configuration file, for example: template: name: filebeat path: filebeat.template.json overwrite: true By default the Elasticsearch template is not loaded. For more details check the . Topbeat: export command line for a processTopbeat used to only collect the process name, but this information was not enough to differentiate between two processes of the same application. Starting with the 1.2 version, Topbeat exports the full command line used to start the process on all supported platforms: Linux, Windows and OS X. Filebeat: introduce close_older settingThe ignore_older setting in Filebeat used to do two things: ignore files with a modification time larger than a given value but also close the open files that had no activity for the given amount of time. The problem with having the two combined was that sometimes there’s no single value that can be used for both. So we now introduced a configuration option, with a default of 1 hour, and we modified the option to no longer close the open files. Send us feedback!A big Thank You to everyone who contributed code, reported issues, created a new Beat or just tried the Beats. Start your experience with and let us know what you think on , , or open an issue on . ","locales":"","title":"Beats 1.2.0 released"}
{"index":{}}
{"author":"Uri Boness","category":"Releases","publish_date":"2016-03-30T00:00:00.000Z","url":"/blog/shield-2-3-released","seo_title":"","content":" It is with great pleasure that we announce the 2.3 release of Shield - the security extension for the Elastic Stack.This release, like previous releases, introduces bug fixes and enhancements to existing features (see for a complete change list). In addition, in this release we also introduce two important new features: A new native API-driven realm and the role management API.PrologueWhen Shield was first released (back in January 2015), we focused on getting the basic security model right and wanted to deliver the minimum required capabilities for you to safely secure your cluster. With this in mind, we introduced the realm for simple deployments, and a set of Enterprise grade realms (e.g. LDAP and Active Directory) for the more complex requirements out there.The realm was a simple authentication service that required defining the users in configuration files (very similar to unix’s users/groups or Apache’s ). While it certainly did the job when it comes to protecting the Elasticsearch cluster, when working with this realm, you never truly felt at home - it wasn’t aligned with the “Elasticsearch way”. From day one, Elasticsearch was built around its APIs - you create an index, add data, run search, monitor the cluster, all of these were exposed via a set of well defined and dedicated APIs. And here came Shield, which required you to manipulate a set of files - clearly an outlier. served all of us well, but the intention was never to stop there. From the early days of Shield we were open about it and promised our users a better way for managing users. Today we’re thrilled to deliver on that promise.Good Morrow, Sir The realm is a built-in realm in Shield that uses Elasticsearch itself to store all of its users. It’s dynamic by nature and exposes a set of APIs for user management. Lets see how you can use it.First thing first, assuming you have Elasticsearch 2.3 installed, you’ll need to install the License and Shield plugins:bin/plugin install licensebin/plugin install shieldNext, we’ll add an admin (super) user using the tool:bin/shield/esusers useradd elastic -p changeme -r adminI know what you’re thinking - “What? esusers? But…”. Yes! Remember, with Shield, all APIs in Elasticsearch are protected. In order to manage users via APIs, you will need to be and authenticate as an admin user. We use the tool to enable that. Besides… having an admin (super) user defined in is not such a bad idea, but we’ll get to that later.OK, you’re all set! If you haven’t done it yet, start up Elasticsearch:bin/elasticsearchAnd add your first user:curl -XPOST -u elastic -p changeme 'localhost:9200/_shield/user/romeo' -d '{ \"password\" : \"minejuliet \", \"roles\" : [ \"power_user\" ], \"full_name\" : \"Romeo\", \"email\" : \"romeo@montague.it\", \"metadata\" : { <5> \"moto\" : \"thus with a kiss I die\" } }'That is it. “romeo” is now a user in Elasticsearch. No need to run command-line tools, edit files or copy files from one node to another. In addition to that, as the example above shows, with the introduction of we also extended the notion of a user to include its full name, email and arbitrary metadata.Having a role, grants romeo the permission to create an index and add data. Running the following command will execute on behalf of the newly added user and will index a new document into the auto-created the index. curl -XPUT -u romeo -p minejuliet 'localhost:9200/shakespeare/tragedy/1' -d '{ \"title\" : \"The Tragedy of Macbeth\", \"quote\" : \"All fair is foul, and foul is fair:  Hover through the fog and filthy air\" }The realm exposes a full set of CRUD APIs to manage its users.You can get the users:curl -XGET -u elastic -p changeme 'localhost:9200/_shield/user/romeo'Update users:curl -XPUT -u elastic -p changeme 'localhost:9200/_shield/user/romeo' -d '{ \"password\" : \"minejuliet \", \"roles\" : [ \"user\" ], \"full_name\" : \"Romeo\", \"email\" : \"romeo@montague.it\", \"metadata\" : { \"moto\" : \"Love is a smoke raised with the fume of sighs\" } }'And de","locales":"","title":"Shield 2.3.0 Released"}
{"index":{}}
{"author":"Suyog Rao","category":"Releases","publish_date":"2016-03-30T00:00:00.000Z","url":"/blog/logstash-2-3-0-and-2-2-3-released","seo_title":"","content":" : It's highly recommended to move directly to the as they include various important bug fixes.Logstash 2.3.0 has been released today, packed with extremely useful new features, an awesome performance boost and a few bug fixes. You can read the detailed release notes, or jump directly to our page if you can't wait to test drive this release! Additionally, we've released an update to the 2.2.x series, with a bunch of important bugs packaged in 2.2.3. You can read the changelog for 2.2.3.Today, any changes made to Logstash configuration files requires the entire process to be restarted, which is not ideal when you manage a multi-instance deployment. Logstash is often run as a centralized service to process data from multiple sources, departments and users. In such deployments it is fairly common for operators to onboard new data sources, and update Grok patterns to extract fields from new, unstructured data. Therefore, each iteration means you'd have to make config changes locally, push them to all Logstash instances and restart every instance to apply those changes. Starting, you can set Logstash to detect and reload configuration changes automatically! This feature also reduces the feedback loop as you develop and debug new configurations.To enable automatic config reloading, start Logstash with the (or) command-line option specified. For example:By default, Logstash checks for configuration changes every 3 seconds. To change this interval, use the - option, where seconds specifies how often Logstash checks the config files for changes.Alternatively, if you would like to manually force a reload and pipeline restart you can send a (signal hangup) to the process.Using environment variables is a common pattern to parameterize server instances, and many times it is convenient to use these variables to inject values into the Logstash configuration. Maybe you'd like to use the EC2 instance ID to tag the input, or use environment variables from your Dockerized Logstash instance to populate the hosts settings for Elasticsearch Output, and so on.. The possibilities are endless here.Prior to this release, users had to use m4 (or any templating system) to achieve this before launching Logstash. Thanks to an amazing contribution from our community member, you can now reference environment variables directly from Logstash configuration. Simply use the syntax and you're done. For exampleinput { tcp { port => \"${TCP_PORT:54321}\" } }A new chapter in Logstash implementation begins in 2.3. Some of you may know that Logstash is written in JRuby, and runs on JVM. In this release, we've completely rewritten the Event representation — a core component which encapsulates the data flow — in pure Java. So what does this mean for users? In our we've seen consistent throughput increases across multiple configurations. In some cases, we've seen up to 75% increase in events processed through Logstash. This change also provides the foundation for future persistence work where events will be persisted to disk while being processed by the pipeline. The Java implementation will make serialization to disk faster. Using the excellent interoperability between Java and JRuby, we were able to make this change 100% backward compatible — all the existing plugins and configuration will work seamlessly (). The graph below illustrates the performance throughput between the 2.3.0 and 2.2.0 release:    All benchmarks were performed using the tool, default settings, and configurations available . Machine specifications can be found .We'd like to note that bin/plugin command — which is used to manage plugins — has been deprecated in favor of bin/logstash-plugin command. bin/plugin will be removed in the next major version mainly to prevent PATH being polluted when other components of the are installed on the same instance. Also, this new command will be enhanced in future versions to work with cross-component plugin packsAdditionally, t","locales":"","title":"Logstash 2.3.0 and 2.2.3 Released"}
{"index":{}}
{"author":"Megan Wieling","category":"","publish_date":"2016-03-28T00:00:00.000Z","url":"/blog/witwies-code-palousa-lone-star-php","seo_title":"Where in the World is Elastic? - Code PaLOUsa and Lone Star PHP","content":" Welcome to Find out which Elastic events and meetups are happening near you in the next two weeks. Upcoming Events March 28-30: April 7-9: Upcoming Meetups March 28: March 29: March 29: March 30: March 31: April 7:  March 29: March 29: April 5: April 5: April 6: April 7: April 7: April 7: March 30: March 29: March 31: April 9: March 28: March 29: April 6: That's it for this week. Stay tuned next week for more Elastic happenings. - The Elastic Team P.S.  if you're interested in hosting a meetup or are giving a talk about Beats, Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic? - Code PaLOUsa and Lone Star PHP"}
{"index":{}}
{"author":"Nik Everett","category":"Engineering","publish_date":"2016-03-25T00:00:00.000Z","url":"/blog/reindex-is-coming","seo_title":"","content":" and are coming to Elasticsearch 2.3.0 and 5.0.0-alpha1! Hurray! reads documents from one index and writes them to another index. It can be used to copy documents from one index to another, enrich documents with fields, or recreate the index to change settings that are locked when the index is created. reads documents from an index and writes them back to the same index. It can be used to update fields in many documents at once or to pick up mapping changes that can be made online. copies documents The API is really just a convenient way to copy documents from one index to another. Everything else that it can do is an outgrowth of that. If all you want to do is to copy all the documents from the index into the index you invoke like this: curl -XPOST localhost:9200/_reindex?pretty -d'{ \"source\": { \"index\": \"src\" }, \"dest\": { \"index\": \"dest\" } }' If you want to be a little more selective and, say, only copy docments tagged with you invoke like this: curl -XPOST localhost:9200/_reindex?pretty -d'{ \"source\": { \"index\": \"src\", \"query\": { \"match\": { \"tags\": \"bananas\" } } }, \"dest\": { \"index\": \"dest\" } }' If you want to copy documents tagged with but you want to add the tag to all copied documents you invoke like this: curl -XPOST localhost:9200/_reindex?pretty -d'{ \"source\": { \"index\": \"src\", \"query\": { \"match\": { \"tags\": \"bananas\" } } }, \"dest\": { \"index\": \"dest\" }, \"script\": { \"inline\": \"ctx._source.tags += \\\"chocolate\\\"\" } }' That requires that you have dynamic scripts enabled but you can do the same thing with non- scripts. Recreating an index to change settings that are locked at index creations is a bit more involved but still simpler than before : # Say you have an old index that you made like this curl -XPUT localhost:9200/test_1 -d'{ \"aliases\": { \"test\": {} } }' for i in $(seq 1 1000):  do curl -XPOST localhost:9200/test/test -d'{\"tags\": [\"bananas\"]}' echo done curl -XPOST localhost:9200/test/_refresh?pretty # But you don't like having the default number of shards # You can make a copy of it with the new number of shards curl -XPUT localhost:9200/test_2 -d'{ \"settings\": { \"number_of_shards\": 1 } }' curl -XPOST 'localhost:9200/_reindex?pretty&refresh' -d'{ \"source\": { \"index\": \"test\" }, \"dest\": { \"index\": \"test_2\" } }' # Then just swing the alias to the new index curl -XPOST localhost:9200/_aliases?pretty -d'{ \"actions\": [ { \"remove\": { \"index\": \"test_1\", \"alias\": \"index\" } }, { \"add\": { \"index\": \"test_2\", \"alias\": \"index\" } } ] }' # Then when you are good and sure you are done with it you can curl -XDELETE localhost:9200/test_1?pretty modifies documents The simplest way to invoke update by query isn't particularly useful on its own: curl -XPOST localhost:9200/test/_update_by_query?pretty That will just increment the document version number on each document in the index and fail if you modify a document while it is running. A more interesting example is adding the tag to all documents with the tag: curl -XPOST 'localhost:9200/test/_update_by_query?pretty&refresh' -d'{ \"query\": { \"bool\": { \"must\": [ {\"match\": {\"tags\": \"bananas\"}} ], \"must_not\": [ {\"match\": {\"tags\": \"chocolate\"}} ] } }, \"script\": { \"inline\": \"ctx._source.tags += \\\"chocolate\\\"\" } }' Like the last version this will fail if any documents are changed while it is running, but it is written in such a way that you can just retry it and it'll pick up from where it left off. If you've already modified whatever application is making the concurrent updates to add the tag whenever it sees then you can safely ignore version conflicts in the . You can tell it to do so by setting . It will just count the version conflicts and continue performing updates. Now the command looks like this: curl -XPOST 'localhost:9200/test/_update_by_query?pretty&refresh&conflicts=proceed' -d'{ \"query\": { \"bool\": { \"must\": [ {\"match\": {\"tags\": \"bananas\"}} ], \"must_not\": [ {\"match\": {\"tags\": \"chocolate\"}} ] } }, \"script\": { \"inline\": \"ctx._source.tags += \\\"chocolate\\\"\" } }' Finally, you can use to suck","locales":"ja-jp","title":"Reindex is coming!"}
{"index":{}}
{"author":"Marko Iskander","category":"User Stories","publish_date":"2016-03-22T00:00:00.000Z","url":"/blog/adding-context-to-queries-the-story-behind-adobes-api-and-ui-elasticon-reflection","seo_title":"","content":" Elastic{ON} just wrapped up and I am still trying to wind down from the excitement. Not only because I was invited to speak at this year’s conference, but because it’s one of the more engaging conferences with product announcements, user stories, and great networking. Meeting people like , the founder of the very tool that brought us all together, was surreal. Then add on top of that the Elastic Team being all under one roof for those few days -- it made for a lot of search power! Of course meeting people like , the Ruby and the Rails SDK core contributor, who I’ve talked with over Github issues and pull requests, is always a plus. There is no other event where you can have access to so much Elastic knowledge and expertise.With talks like  and  it’s hard not to geek out. Of course, the user story sessions do a great job of balancing out the reality of business use cases and the real problems we are working to solve. Stories from , , and all offered up a great insight into different ways Elasticsearch is used in the wild. However, the data nerd in me couldn’t wait to see how the stored over 160 years of its stories in Elasticsearch.Speaking at Elastic{ON} was not only an honor but a great joy. Sharing the Adobe Typekit story was something that I’ve wanted to do for a while now. It’s a bit different than most of the talks given at this conference since our story is about a small cluster that handles small amounts of data, but at a very high throughput with complex queries. The introduction of Elasticsearch to our stack was not for log analysis, big data storage, and evaluation but rather for its robust query interface and stability under high throughput. I felt that many could benefit from hearing such a story and Elastic{ON} was the stage on which to do just that. With a diverse speakers list and even more diverse attendees, finding the right balance of content was key. The questions at the end of the talk not only affirmed that our story was one that others can relate to, but that it had answered some questions for people struggling through the same obstacles.If you are interested in the Adobe Typekit story, stop on over and enjoy . In it, I share how we are able to use advanced scripting to make real-time, high-throughput queries to power our browse UI and API.Marko Iskander is a Senior Computer Scientist at Adobe on the Typekit team. He has implemented large scale implementation of Elasticsearch at various Fortune 500 clients before joining the Typekit team. After joining, his first initiative was to switch the core search and filtering of data from SQL to Elasticsearch for faster query times, simpler implementations, and support of complex business queries. Next up was the API. Currently, Marko is working on pioneering the next-level implementation of user-specific prepared data. ","locales":"","title":"Adding Context to Queries: The Story Behind Adobe’s API and UI: An Elastic{ON} Reflection"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2016-03-21T00:00:00.000Z","url":"/blog/weekly-beats-packagebeat-lmsensorsbeat-soundbeat","seo_title":"","content":" Last week we released Beats 1.1.2. The most interesting news and enhancements of the week are: Packagebeat Joe Hillenbrand from Elastic has started Packagebeat to gather Linux distribution package information and index them into Elasticsearch. The Beat is used internally by the infra team and it is actively developed and improved. LmsensorsbeatShane Connely from Elastic created Lmsensorbeat to monitor a variety of I2C/SMBus sensors, such as CPU/motherboard temperatures, fan speeds, voltages, etc. This is another example that creating a new Beat is easy and fun when using the beat-generator to generate all the boilerplate code for you. SoundbeatSoundbeat is another Beat developed inhouse by David Pilato from Elastic and it shows that Beats can have applications outside of the operations domain. It reads the MP3 files to extract the sound level for left and right channels using a given period for each sample. He wrote a nice blog post with all the detailed steps that he followed for writing the Beat. Packaging for community BeatsComing from a community Beat author, this PR adjusted the Makefile in our Beat packer so that it can be easily used by all community Beats. This means that a single make command can be used by any of the community Beats to get cross-compiled RPMs, DEBs, etc. The Beat generator was also updated to support for this, so it’s really easy to use by the community Beats authors. Generic filteringGeneric filtering has now support for include_fields and drop_fields actions by merging the pull request in master. The include_fields action specifies a whitelist of fields to export. The drop_fields action defines the fields that are dropped if they exist. By default all fields are exported. The only fields that cannot be dropped via generic filtering are @timestamp and type, because they are required by the outputs. Both actions can receive in the fields argument full nested maps. For example to keep only the percentages of the cpu load and remove the cpu ticks, the configuration file looks like: filter: - include_fields: fields: [“cpu”] - drop_fields: fields: [“cpu.user”, “cpu.system”] Authenticate MySQL connection in Metricbeat/Mysql moduleThis adds authentication support for connecting to the MySQL server in the mysql module of Metricbeat. You can configure the username and password as different options or you can define them in the DSN connection string: [username[:password]@][protocol[(address)]]. Enhancements in WinlogbeatFew improvements are done in Winlogbeat to provide the data from the event log messages in a structured format. In addition, more information are exported like activity_id, process_id, thread_id and others. Add support for double in templatesWith this pull request double values are also supported in the Elasticsearch templates for Beats. Remove count field for FilebeatAll Beats are exporting the count field that was meant to be used for sampling, but never used so far. To cleanup the exported fields, this removes the count field from the exported fields in Filebeat. ","locales":"","title":"Brewing in Beats: Packagebeat, Lmsensorsbeat and Soundbeat"}
{"index":{}}
{"author":"Suyog Rao","category":"The Logstash Lines","publish_date":"2016-03-21T00:00:00.000Z","url":"/blog/logstash-lines-2016-03-21","seo_title":"","content":" Welcome back to The Logstash Lines! In these weekly posts, we'll share the latest happenings in the world of Logstash and its ecosystem.Support for Kafka v0.9 Apache Kafka had 0.9 couple of months ago which brings in new security features (SSL, client based auth, access control), improved consumer API, and much more. The biggest asks from Logstash users were support for SSL encryption and client auth features. Over the last few months we've been working on implementing these features into the input and output plugins, and this week, we beta versions. The new consumer library from Kafka has been greatly simplified - much of the logic (like rebalancing), has been pushed to the broker side. This meant we could directly use the Java APIs. While we're at this, added more integration tests running on travis, and cleaned up some configs. that these features need an upgrade to the Kafka broker -- 0.8 producer/consumer will not work with 0.9 broker and Logstash plugins are not backward compatible. To install this version of plugin on your Logstash (> 2.0.0): bin/plugin install --version 3.0.0.beta3 logstash-input-kafkabin/plugin install --version 3.0.0.beta1 logstash-output-kafka Elasticsearch Output memory issue Users have recently ran into a when using the feature in Elasticsearch Output. This leak was caused by ES output frequently instantiating — the underlying http library used in Logstash — while tearing down and reconnecting, upon hosts being updated from sniffing. Manticore lib has been patched to be more efficient in this scenario and ES Output version 2.5.3 released. To install this on Logstash 2.2: Plugin Installation Bug While validating the fix for memory leak described above, we ran into a plugin installation issue when executing . Turns out we didn't know of hidden files produced by jar-dependencies library (use to package jars) and weren't correctly packaging dirs like in our gem building process. Fix is in, and life is good in plugins-land () Beats input certificate verification Continue to make progress on adding on the Beats input side. We found a while adding support for chained certs, but root CA verification works fine. Released a new version which includes certificate validation. Java Event Timestamp Fixes This bug surfaced in Gelf input when a timestamp conversion from JSON string was loosing precision. The Java BigDecimal type conversion to a proper Ruby BigDecimal was not handled, and has been fixed now. This issue of losing precision also showed up in JRuby Event implementation previously () Packs Installation Support Preliminary work of installing packs in Logstash has begun. This will use the offline install feature which creates an intermediate plugin state file and can be pointed to use this as source instead of RubyGems. Adding integration tests to validate this feature (). In progress:Plugins changes: ","locales":"","title":"Logstash Lines: Kafka 0.9 beta support, ES output memory fix"}
{"index":{}}
{"author":"Megan Wieling","category":"","publish_date":"2016-03-21T00:00:00.000Z","url":"/blog/witwies-breizhcamp","seo_title":"Where in the World is Elastic? - Great Wide Open & FOSSASIA 2016","content":" Welcome to Find out which Elastic events and meetups are happening near you this week.  Upcoming EventsMarch 23-25:  Upcoming MeetupsMarch 21: March 21: March 22: March 23: March 23: March 24: March 24: March 23: March 24: March 23: That's it for this week. Stay tuned next week for more Elastic happenings.- The Elastic Team P.S.  if you're interested in hosting a meetup or are giving a talk about Beats, Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic? - BreizhCamp"}
{"index":{}}
{"author":"Jason Dickson","category":"News","publish_date":"2016-03-21T00:00:00.000Z","url":"/blog/why-elastic-cloud-is-the-best-choice-for-your-hosted-elasticsearch-needs","seo_title":"Why Use Elastic Cloud - #1 Choice for Hosted Elasticsearch","content":" Allow me to ask you a couple of questions that might seem silly. Do you only eat food that you've grown? No? Did you build your own home or apartment? No? Why not? Was it easier to simply buy or rent a place that suitably addresses your needs for shelter and comfort? Sometimes it's wise to just take the path of least complexity. That's pretty much what makes — the official hosted Elasticsearch service — a perfect choice for many companies, organizations, and individuals. Choosing us for your Elasticsearch needs simply makes good sense. We understand that every team and system has a set of unique requirements. However, if you think about hosting the Elastic Stack on your own and consider the time needed to deploy, setup, get started, and maintain — you or your boss may be thinking, \"How can I get this taken care of with a minimum of added stress?\" That's where Elastic Cloud comes in. It's the only hosted Elasticsearch offering from the creators of Elasticsearch, Kibana, Beats, and Logstash. Whether your team needs 1GB of memory or 4TB of storage, Elastic Cloud is the best hosted Elasticsearch experience. Fully hosted and providing the real-time search and analytics magics of , Elastic Cloud is always running the most current version of Elasticsearch and incorporates all of the latest and greatest . You can also get Elastic's world-class tech and security, alerting, and monitoring. And you can go try it out right now with a free . Don't take my word for it, experience it for yourself. For , over 1,800 users converged on San Francisco's Pier 48 to hear from the creators and engineers of the Elastic Stack. This included a fantastic presentation on the architecture and history of Elastic Cloud by Software Engineer Njal Karevoll and Site Reliability Engineer Erik Redding, as well as a live demo. In the recording below, watch Erik and Njal discuss the history and early architectures of what is now Elastic Cloud, a fascinating section about \"living with systems that fail\", an in-depth look at Elastic Cloud's current architecture, and the demo, in which they install Elastic Cloud Enterprise on three servers, all while shining a big flashlight to illuminate how it all works. If you enjoyed this presentation, you may also want to check out the Elastic{ON} , in which Njal and Elastic Cloud Team Lead Michael Basnight presented a demo of Elastic Cloud Enterprise, beginning at the 85-minute mark. For even more Elastic Cloud coolness: ","locales":"","title":"Why Elastic Cloud is the Best Choice for Your Hosted Elasticsearch Needs"}
{"index":{}}
{"author":"Tudor Golubenco","category":"Brewing in Beats","publish_date":"2016-03-17T00:00:00.000Z","url":"/blog/weekly-beats-sock5-to-logstash","seo_title":"","content":" Welcome to Weekly Beats! With this series, we're keeping you up to date with all that's new in , from the details of work in progress pull requests to releases and learning resources. Merged this week: DNS eTLD+1 support in PacketbeatAs Andrew , “the effective top level domain plus one more label is really useful for clustering DNS requests (hostnames). For example, the eTLD+1 for  is . This was the basis for the aggregations used in .\" The adds a field to the Packetbeat DNS transactions.SOCKS5 proxy for the connection to Logstash makes it possible for the Beats to connect to Logstash via SOCKS5 proxy. This can useful when sending the data between different Internet domains. It also opens the possibility of using username/password for authenticating to Logstash. Cleaner separation between the Go unit tests and integration testsOur Go  files contain both unit and integration tests (defined as tests that require other services). We used to separate them via the “short” flag on a test by test basis. This also meant that in order to run only the unit tests, one had to call . With this , the unit and integration tests are strictly separated in different files and we use build flags to select between them. From now, to run only the unit tests you simply write . To execute the integration tests, you now need . Fix Topbeat CPU time computation on Windows avoids some floating point arithmetic when converting from a Windows structure to a 64 bits value. We’re hoping that this fixes a where huge numbers were reported for CPU usage. Track unavailability in MetricbeatIf a monitor system is not available at all, an error document is to Elasticsearch so it is easier to track downtime. And from the currently in progress pull requests: Generic event representationThe way we currently represent internally an event in Beats is in the form of . Translating from Go, that means a map from string to anything (because anything implements the empty interface). The only requirement is that all types inside are JSON serializable, but then again, pretty much everything in Go is. This makes it quite easy to write new Beats (just throw anything have into a map), but it means that we have to rely on reflection when working with the event in libbeat, for example for the . Reflection code is slow and error prone. Benchmarking also showed that the JSON serialization often dominates the performance of the Beats, and it is slow because it has to do reflection. So we want to move away from this “anything goes” events. For the transition phase (we don’t have control over the community Beats) and for convenience later, we’ve written that takes a  event and uses reflection (no way around that) to transform it in a nested map that only contains a few accepted types. This makes the rest of the libbeat code easier, especially the filtering code. JSON support in FilebeatWe now have a l for implementing JSON support in Filebeat. The advantage of this one over the one I in last week’s update is that you can combine it with multiline and line filtering in a more meaningful way. This makes it a good way from shipping logs from a Docker host, for example, while still being able to use multiline on the application logs. Dashboards per Beat & moduleWe to move the Kibana configuration for the inside the main repo and organize it per Beat and even per module (in Metricbeat). Nicolas wrote Python scripts to split the dashboards into “snippets” and combine them back. ","locales":"","title":"Brewing in Beats: SOCKS5 support when sending to Logstash"}
{"index":{}}
{"author":"Monica Sarbu","category":"Releases","publish_date":"2016-03-15T00:00:00.000Z","url":"/blog/beats-1-1-2-released","seo_title":"","content":" Today we are pleased to announce the bug fix release of Beats 1.1.2. The release notes are available .Latest stable Beats releases:Send us feedback!A big Thank You to everyone who contributed code, reported issues, created a new Beat or just tried the Beats. Start your experience with Beats 1.1.2 and let us know what you think on , , or open an issue on . ","locales":"","title":"Beats 1.1.2 released"}
{"index":{}}
{"author":"Doug Turnbull","category":"User Stories","publish_date":"2016-03-16T00:00:00.000Z","url":"/blog/opensource-connections-the-ghost-in-the-search-machine-an-elasticon-recap","seo_title":"OpenSource Connections: The Ghost in the Search Machine - An Elastic{ON}16 Recap","content":" I'm trying to put my finger on why I enjoyed Elastic{ON} so much. Certainly the talks were top-notch. I especially enjoyed the talk — which walked through the math behind relevance scoring. I loved learning how using Elasticsearch. The walk through of had me spinning with ideas (which I ) and Mark Harwood’s seeded my colleagues and me with many ideas.  But I think my favorite part was how Elastic{ON} enabled amazing conversations. See I just finished on search relevance. So our company was primed to have a ton of awesome conversations about how to improve search. We weren't disappointed. Lunch, the coffee line, walking around, and our sponsor booth seeded innumerable intersecting discussions. We got to troubleshoot with engineers from Apple, Facebook, and Roku about their challenges and to share our enthusiasm for the Elastic Stack. This \"hallway track\" as it's called makes being a speaker even more inviting. You know your talk will trigger a dozen followup conversations. Personally, I don't want to speak to sound smart. I want to speak to be stumped by the audience and learn. I want to be asked hard questions — and most importantly to find the folks better at what I'm talking about than me! This made being a \"spotlight theater\" speaker feel especially attractive. I enjoyed how speaking was very one-on-one and conversational. It let others share ideas that I hadn't thought of — which only created a snowball of additional ideas and conversations. The small, 50-seat \"spotlight\" venue enabled this back and forth, which I loved. Being a speaker at Elastic{ON} is also refreshing due to the amount of prep Elastic puts into you as a speaker. They meet with you, help you refine your slides, and make sure you're comfortable. You also meet with your stage manager — thanks Auberie! So for anyone uncomfortable with speaking, this is a great first \"major conference\" experience where you get a tad bit of extra guidance on the road to success. Oh, so what did I talk about? My talk  highlights all the crazy and weird ways you can use Elasticsearch's inverted index to find *anything* — images, at risk students etc.  And if you'd like to see it or meet me live, catch me at any one of these Elastic   or) is a search relevance consultant at OpenSource Connections. Author of Relevant Search. Doug crafts search/recommendation solutions that “get” users. To do this, Doug uses Elasticsearch, sprinkling a little natural language processing and machine learning on top for good measure. EndFragment ","locales":"","title":"OpenSource Connections: The Ghost in the Search Machine - An Elastic{ON}16 Recap"}
{"index":{}}
{"author":"Michael McCandless","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2016-03-14T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2016-03-14","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top NewsHow does build, test, & analyze DNA mods to microbes at scale? — elastic (@elastic) Elasticsearch Core Apache Lucene Watch This Space, where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2016-03-14"}
{"index":{}}
{"author":"Travis Smith","category":"User Stories","publish_date":"2016-03-17T00:00:00.000Z","url":"/blog/bro-ids-elastic-stack","seo_title":"Integrating Bro IDS with the Elastic Stack","content":" Cyber attacks are continually increasing in scope and complexity. Advanced persistent threats are becoming more difficult to detect, leading to what the calls a detection deficit. has found that the average time to detection for attacks is 205 days. The core of this detection deficit is the fact that the cost, complexity, and volume of data to be analyzed increases with the maturity of the security organization. Most organizations are collecting logs from systems, applications, and network devices to generate operational statistics and/or alert on abnormal behavior. Software engineers write the code that determines what gets logged within their applications. Unfortunately, a lot of valuable data is not written to logs, making it improbable for log management systems administrators to detect attacks quickly. The best method to detect attacks is to analyze the sessions and full packet capture data within the environment. To detect a cyber attack in real-time using packet-level inspection, as well as provide historical analysis, a network security monitoring application should be used. One such option is the . Bro is an open source network security monitor that has been around since 1995. Bro can inspect network traffic in real-time or look into previously captured packet capture files. Bro looks for known attacks in the same way a typical intrusion detection system would. The benefit of Bro is that all connections, sessions, and application level data are written to an extensive set of log files for later review. This blog will take a deep look into using Elasticsearch, Logstash, and Kibana for managing and analyzing log data from Bro. Log CollectionElasticsearch, Logstash, and Kibana are for collection, normalization, storage, visualization and analysis of log data. You can either install the Elastic Stack on the same system as Bro or you can run it on a separate server and forward logs from Bro via syslog. This blog assumes that Elastic Stack and Bro are install on same server. By default, all Bro logs are written to and are rotated on a daily basis. The Logstash configuration below shows how to tail the Bro log files and index data into a local instance of Elasticsearch. The full Logstash config file can be found . In the next few section, we do a step-by-step walkthrough of the configuration. input { file { path => \"/opt/bro/logs/current/*.log\" } } filter {…} output { elasticsearch { host => localhost cluster => \"elasticsearch \" } } Logstash: FiltersNext, filters need to be added to normalize the logs and extract the metadata such as IP addresses, file names, ports, etc. We will apply the filter plugin which uses regular expression to parse through the logs and add structure. Logstash ships with a set of grok expressions, however custom expressions are needed to parse Bro logs. We can directly embed regular expressions in the section of the config file. However, I have found it better to simplify the configuration by moving complex regular expressions to a separate rule file in the pattern directory. Below is an example configuration that shows how. filter { grok { match => { patterns_dir => \"/path/to/patterns\" \"message\" => \"%{291009}\" } } } The regular expressions for custom pattern  are stored in rule file in the directory mentioned above. Patterns for different devices can be stored in separate rule file, for example bro.rule, linux.rule, apache.rule, etc. A sample file will contain rule such as shown below. 291009 (?<start_time>\\d+\\.\\d{6})\\s+(?<uid>\\S+)\\s+(?:(?<evt_srcip>[\\d\\.]+)|(?<evt_srcipv6>[\\w:]+)|-)\\s+(?:(?<evt_srcport>\\d+)|-)\\s+(?:(?<evt_dstip>[\\d\\.]+)|(?<evt_dstipv6>[\\w:]+)|-)\\s+(?:(?<evt_dstport>\\d+)|-)\\s+(?<fuid>\\S+)\\s+(?<file_mime_type>\\S+)\\s+(?<file_description>\\S+)\\s+(?<seen_indicator>\\S+)\\s+(?<seen_indicator_type>[^:]+::\\S+)\\s+(?<seen_where>[^:]+::\\S+)\\s+(?<source>\\S+(?:\\s\\S+)*)$ Logstash: Conditionals To further enhance the data","locales":"","title":"Integrating Bro IDS with the Elastic Stack"}
{"index":{}}
{"author":"Tanya Bragin","category":"Engineering","publish_date":"2016-03-15T00:00:00.000Z","url":"/blog/recreating-kibana-3-dashboards-in-kibana-4","seo_title":"","content":" In my role I speak with many Kibana 3 users in the process of re-creating their dashboards in Kibana 4. In these conversations, I find that one of the biggest struggles is wrapping your head around how Kibana 4 approaches UI organization and navigation differently from Kibana 3.  \"Before\" screenshot: \"After\" screenshot:   There are many Kibana 4 features this video doesn't cover — for more ideas refer to some of our , peruse the , or simply , install, and play with Kibana 4.  If you have questions about Kibana 3 to 4 transitions, please join in on discussions on IRC (#kibana on irc.freenode.net) or on  forums. Also, let us know what other Kibana video tutorials you'd like to see! ","locales":"","title":"Re-creating Kibana 3 dashboards in Kibana 4"}
{"index":{}}
{"author":"Megan Wieling","category":"","publish_date":"2016-03-14T00:00:00.000Z","url":"/blog/witwies-great-wide-open-fossasia","seo_title":"Where in the World is Elastic? - Great Wide Open & FOSSASIA 2016","content":" Welcome to Find out which Elastic events and meetups are happening near you this week.  Upcoming EventsMarch 18-20: March 16-17: Upcoming MeetupsMarch 16: March 16:  March 15: March 15: March 15: March 16: March 16: March 17: March 18: March 14: March 14: March 16: March 15:That's it for this week. Stay tuned next week for more Elastic happenings.- The Elastic Team P.S.  if you're interested in hosting a meetup or are giving a talk about Beats, Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic? - Great Wide Open & FOSSASIA 2016"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2016-03-15T00:00:00.000Z","url":"/blog/elasticsearch-2-2-1-released","seo_title":"Elasticsearch 2.2.1 released","content":" Today we are pleased to announce the bug fix release of based on . This release is already available on , our Elasticsearch-as-a-service platform. Users are advised to upgrade if they find themselves affected by any of the bugs fixed in this version.Latest stable release:You can read the full release notes in the pages linked above, but some of the more important bug fixes are as follows:Please , try it out, and let us know what you think on Twitter () or in our . You can report any problems on the . ","locales":"","title":"Elasticsearch 2.2.1 released"}
{"index":{}}
{"author":"Joe Fleming","category":"Releases","publish_date":"2016-03-10T00:00:00.000Z","url":"/blog/kibana-4-4-2-and-4-3-3-and-4-1-6","seo_title":"Kibana 4.4.2, 4.3.3, and 4.1.6 Released","content":" Last month, we dropped some patch releases to address security fixes in Node. This month, we’re doing the same thing again. Thankfully, the version bumps here address the OpenSSL CacheBleed issue. Since we’re cutting releases again, we decided to roll in some bugfixes to the 4.3 and 4.4 versions while we were at it. We also made a major version bump to node to help address some memory leak issues when connecting to Elasticsearch over SSL. For more details on that, see . You can download the latest and greatest version of Kibana at our page, and for more details about the bugfixes that made it in, see below. 4.4.2 Changes 4.3.3 Changes 4.1.6 Changes ","locales":"","title":"Kibana 4.4.2, 4.3.3, and 4.1.6 Released"}
{"index":{}}
{"author":"Robin Moffatt","category":"User Stories","publish_date":"2016-03-09T00:00:00.000Z","url":"/blog/visualising-oracle-performance-data-with-the-elastic-stack","seo_title":"Visualising Oracle Performance Data with the Elastic Stack","content":" IntroductionOne of the great things about the Oracle database is the level of diagnostics and performance data that is available from it. Used in conjunction with good instrumentation from the applications generating the workload on the database, it’s a brilliant way – arguably, the only way – to accurately and efficiently get to the bottom of any performance problems that arise. By analysing what the database is doing at a point in time we can understand more about the load that it is under, and by looking at how a given session executes we can identify optimisation opportunities for it. The data is available natively through some of the V$ system views, and exposed through Enterprise Manager, and tools such as SQL Developer. What I’m going to show in this article is how you can make use of the open-source Elastic Stack – comprising Elasticsearch, Logstash, and Kibana – to extract this data and visualize it as an alternative to the aforementioned options. Why the Elastic Stack?Because is a great data visualization and exploration tool. Because is a great datastore that is dead easy to work with, and because makes getting data from a an absolute doddle. It’s a stack I frequently use for monitoring and diagnosing applications including OBIEE, and including Oracle performance data in that analysis was an obvious thing to do. LicensingElasticsearch, Logstash, and Kibana are all open-source tools that are free to use. You can pay for a that gives you direct access to product support, as well as additional tools including for monitoring, security and alerting. The is required if you’re using functionality in Oracle including (but not limited to) Active Session History (ASH) – if in doubt, speak to your Oracle Account Manager. VersionsThis article was written using Elasticsearch 2.2, Logstash 2.2, Kibana 4.4, pulling data from Oracle 12c (12.1.0.1.0). I used the excellent to generate load against the database. OverviewThe Elastic Stack gives you great flexibility to analyze exactly the data that you need, in exactly the way that you want to. In this article I’m going to show how to get Active Session History (ASH) data streamed into Elasticsearch. You might want to then enrich it further with log data from your applications, or OS metrics from the servers – or anything else that might be useful for a full-stack view of your application’s performance. Using Logstash’s JDBC input, we pull data from the Oracle table, loading the data directly into Elasticsearch. From there we can analyze it with Kibana. It really is as simple as that. Setup You should see some output showing that Elasticsearch has started up successfully: If you Ctrl-C or close the terminal, you’ll kill Elasticsearch. For this – and a multitude of other purposes – I’d always use  in order to run multiple sessions in a single window/SSH connection. If you installed kopf in step 4 above you can now go to  to see the status of Elasticsearch.  After a moment go to and you should see a config screen like this:  Getting the Data into ElasticsearchSo far, so easy. Download and run a handful of binaries. But, we’ve not got any data yet. Enter . Logstash enables you to pull in data from , , and then output it to Elasticsearch and . Here we’re using the Logstash input plugin. Note that this is relatively new (in the open-source sense) and previously was often done as a “River” – something now deprecated in Elasticsearch but I mention it here as you may still find articles through Google that reference it. To use Logstash we need to build a configuration file. We’ll build this up step by step to make sure it’s all working along the way. First Steps with Logstash JDBC InputAll Logstash configuration files have the same simple structure : input, filter, output. And of those, filter ","locales":"","title":"Visualising Oracle Performance Data with the Elastic Stack"}
{"index":{}}
{"author":"Kristina Frost","category":"Culture","publish_date":"2016-03-08T00:00:00.000Z","url":"/blog/women-of-elastic-a-look-back-and-our-steps-forward","seo_title":"","content":" For those of you who couldn’t join us at , Thursday, February 18 was a pretty special day for the women of Elastic:  we hosted our first ever  at the conference. This marks our first attempt to make a space for the women of our community to come together, share experiences, and talk about the kinds of things we care about, from the best and most ergonomic women’s backpacks (hey, that struggle is real), to career sponsorship, to further opportunities for meetups and hackathons. At the end of the breakfast, we asked for your feedback, information I promised our attendees I’d publish out to the world as we think about what future diversity events look like and how this community is going to grow and thrive. Now here we are. Almost 50 different suggestions came in, including some feedback that was overwhelmingly positive and meant the world to all of us. I’m sure more is on the way through post-conference surveys, and some we got in real time via the Elastic{ON}16 app.I’m writing this as I sit on a flight to Denver, reflecting on the notes we’ve received so far. Among these responses, the word “more” appeared regularly in the things you wrote. More conversation. More events. More opportunities to meet women and exchange ideas. Relationships and networking were a huge theme. wrote one attendee, a comment echoed by other women who want or who think we need to keep spending time together Two attendees suggested we work with other companies to partner and sponsor these kind of events in the future:  another, in writing about the possibility of a Bay Area meetup, gave a shoutout to our very own as a great host for these kinds of events.Training was a big theme, too. You asked us if we could get more hands-on in the future, if we could provide walk-throughs for newbies, things to get those of you who are approaching the Elastic Stack for the first time off the ground quickly. You want to see us participate in protecting and developing the interests of young girls, teaching them how to code. “Girls 4-10 are being turned off from tech,” one of you wrote, encouraging us to think about how we can encourage the women of the future to participate in technology, to “see how cool it is!\" We are proud of several of the talks that were delivered by women at Elastic{ON}:  in particular, I highly recommend the . Social scientist Dr. Sherry Forbes shared the ways in which Giant Oak is using Elastic to stop human trafficking and to find the individuals who endanger the world’s rhino population by trading rhino horns on the black market. Of the talks that I attended at the conference, it was far and away the nearest and dearest to my heart as a woman who’s crossed continents to be with women who’ve been trafficked and who wants women everywhere to be able to flourish without fear. But other women speakers included of Cisco Talos, Netflix's , from Goldman Sachs, as well as Elastic's own , , and , and the talks are all for anyone who wasn’t able to attend.Others wanted to see women executives come and speak. One attendee wrote, and one asked for session with women from all levels, all backgrounds, speaking as panelists and offering their insight. Another great suggestion we received was around crafting a track at the conference specific to women in tech, or even, perhaps, specific to diversity.As an interesting aside, a question that came up several times prior to the conference, including once in the Elastic{ON}app, was whether or not men were welcome to come and join us, and this desire to include them was reflected in one of my favorite comment cards from the entire morning: I’ll be thinking about how to make it more clear in the future that we want and value every person who wants to participate in our conversations about Women in Tech, and how we broaden this movement to engage in all kinds of diversity conversations from a variety of perspectives.We look forwar","locales":"","title":"Women of Elastic: A look back and our steps forward"}
{"index":{}}
{"author":"Tudor Golubenco","category":"Brewing in Beats","publish_date":"2016-03-08T00:00:00.000Z","url":"/blog/beats-weekly-community-beats-custom-fields-performance","seo_title":"","content":" Welcome to Weekly Beats! With this series, we're keeping you up to date with all that's new in , from the details of work in progress pull requests to releases and learning resources. Lots of positive energy in the team after Elastic{ON}, thank you everyone that attended and provided us with feedback. Custom fields for all Beats A common request was to add Filebeat-like custom fields to all other Beats. The tags we had in the common section of the configuration files just weren’t enough. So all Beats have fields  tags in the common section and it is also easy to add them per Beat-specific element (e.g. prospector in Filebeat, protocol in Packetbeat, etc.). New Community Beats More goodies from the community: Better configuration file handling Up to now we were using a generic YAML parser for reading our configuration files. This is very clean and convenient from the code perspective, but there are several things about it that annoyed us: Those were the bad news. The good news is that Steffen is on fixing them, and not only for the Beats but for other Go programs that use YAML as well. He is writing ucfg (universal configuration) that adds a layer above the YAML parser (or any other parser) that will eventually be able to do configuration validation, accepts dots in field names (so we don’t have to always rely on white space) and make the configuration definitions pluggable. This is already used for the output modules in libbeat as well as the   truly self contained. Topbeat performance improvement When monitoring a large number of processes, the cost of getting their command line on each poll was significant, especially on Windows. This PR fixes it by the command line strings. Telling benchmarks for Windows and OS X are in the PR description. Improved shutdown logic in libbeat Shutting down is never easy when dealing with lots of channels. McStork, a community contributor, stepped up and helped i we have for shutting down the publisher. Metricbeat progress Nicolas is making good progress on getting the Metricbeat infrastructure ready before adding more modules. This includes a reworking of the handling and getting more metricsets in the existing , testing if and starting the to new Metricbeat modules. Filebeat fix for duplicates on restarting We discovered a that could cause Filebeat to re-read a complete file if it was restarted at the wrong time. It is now fixed and we’re preparing a 1.1.2 release for next week to include this. Other notable merges since the last update And from the newly opened / in progress / discussion PR ","locales":"","title":"Brewing in Beats: New community Beats, custom fields, performance improvements"}
{"index":{}}
{"author":"Kiyan Ahmadizadeh","category":"User Stories","publish_date":"2016-03-08T00:00:00.000Z","url":"/blog/optimizing-the-design-of-new-microbes-using-elasticsearch-elasticon-reflection","seo_title":"","content":" At , we are combining biology, software, and automation to design new microbes that will revolutionize chemical and material production. To do this, we are creating an industrial biofactory that allows us to build, test, and analyze DNA modifications to microbes at scale. Technology like Elasticsearch has helped us take advantage of the data produced during this process. Thanks to Elasticsearch, our scientists can ask questions of our data that help us explore the vast space of possibilities in DNA.I attended Elastic{ON} as a speaker, sharing our experience at Zymergen using Elasticsearch to help optimize the design of new microbes. The best technical conferences introduce you to new ideas you can apply to tough engineering problems. Elastic{ON} was this kind of conference. For example, minutes after arriving on the first day I met Marko Iskander from Adobe, who is to improve search results by adding user context to queries. Although font discovery and microbe design are as different as use cases can be, my conversation with Marko and his talk immediately got me thinking about how this technique could be used to improve search results at Zymergen. Learning about Elasticsearch libraries (like the Hadoop, Beats, and Kibana Plugin frameworks) also stimulated exciting new ideas about ETL and analytics. Elastic{ON} was rewarding not just for the chance to speak, but also for the chance to learn from fellow engineers developing exciting solutions.My experience as a speaker was equally rewarding. I was impressed by the support the Elastic team gave as I was preparing my talk for the conference, and their advice was crucial in developing a talk that reached the audience. The lighting, seating, and screen setup at Stage A was energizing and made me feel like an announcer at a sporting event. I appreciated the audience’s attention during my talk, and was delighted by the variety of questions as attendees looked to explore techniques they could apply to their own solutions. Others shared their experiences applying Elasticsearch to problems in bioengineering. This chance to share my experience at Zymergen and learn about the experiences of others was the primary reason I wanted to speak at Elastic{ON}.I hope you enjoy . It was a pleasure to share our experiences with such a bright and engaging audience.Kiyan Ahmadizadeh builds software that helps organizations in complex industries (like utilities infrastructure or industrial bio-laboratories) make intelligent operational decisions. His work often utilizes cloud computing and NoSQL technologies. His hobbies include running, reading, music, and meditating. ","locales":"","title":"Optimizing The Design of New Microbes Using Elasticsearch: An Elastic{ON} Reflection"}
{"index":{}}
{"author":"Michael McCandless","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2016-03-07T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2016-03-07","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top NewsWondering why queries don't always work? dives into the details of phrase-matching in : — elastic (@elastic) Elasticsearch Core Apache Lucene Watch This Space, where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2016-03-07"}
{"index":{}}
{"author":"Gabriel Moskovicz","category":"Engineering","publish_date":"2016-03-07T00:00:00.000Z","url":"/blog/phrase-Queries-a-world-without-stopwords","seo_title":"Phrase Queries: a world without Stopwords","content":" Analysis and query processing can be confusing when you start out with Elasticsearch. It’s one of the most common problems that users run into, and one clear example is phrase matching. In this article we will dig into the process of understanding some of the complexity of Phrase queries, while working with languages and stopwords. The key of working with Elasticsearch is to have a better understanding of what a query is doing. Furthermore, we need to understand that the art of searching starts when a document is indexed, and all its field content is analyzed and finally indexed into Lucene. Most of the time, we are only thinking about how to query our data source, but in Elasticsearch we should first think about the insight that we want to get from our data, so we may then discover the best way to accomplish this. To achieve our goals in Elasticsearch we need to think about the entire picture: indexing and querying. Indexing means not only adding the structured document in Elasticsearch, but also including the analysis of each field. Each field is analyzed, and then the result of the analysis is indexed by Elasticsearch. But what is analysis? By definition it is the process of breaking a - complex - topic (or substance) into smaller parts in order to gain a better understanding of it. This word comes from an Ancient Greek word ἀνάλυσις (analysis, “a breaking up”, from ana- “up, throughout” and lysis “a loosening”). In concrete terms, Elasticsearch analysis is a process that consists of the following steps: For more information and details about the full analysis process please visit . Once the document is indexed, we can execute queries to retrieve the results based on different conditions. The query execution and matching will be strongly related to the way that we index data, hence why we say that Elasticsearch is not only about querying, but about creating a solution that suits user needs. Languages and a world with Stopwords The stopword definition is very simple: they are the most common words in a language. The important fact about stopwords is that, since they are very common, these are words that are going to frequently appear in our language, phrases and probably most of our text fields. Some examples of stop words are: “a”, “and”, “but”, “how”, “or”, and “what”. Usually, when we search for something, we want to exclude them. This can be tricky, because excluding words is easy, but it will impact the results of certain queries. The art of Phrasing In linguistic analysis, a phrase is a group of words (or possibly a single word) that functions as a constituent in the syntax of a sentence: a single unit within a grammatical hierarchy. The phrase is composed of different words, one followed by the other, with a certain order. While two phrases can have the same meaning, the order of the words will create a completely different phrase. As an example: “This fox is brown” is totally different to “Is the fox brown”. Why is this? Because each word has a position within the phrase that causes the meaning of the sentences to be similar, but the word chain to be completely different. As we were explaining in the introduction, the indexing process in Elasticsearch will execute the analysis process. Analysing phrases in Elasticsearch is simple, and can help to explain why two phrases are different. In Elasticsearch, we provide the Analyze API that can be used with a simple set of parameters to understand what a specific analysis process is doing. In the following example we will be using the English analyzer that is predefined in Elasticsearch to understand why the following phrases are different. GET _analyze?text=This fox is brown&analyzer=english { \"tokens\": [ { \"token\": \"fox\", \"start_offset\": 5, \"end_offset\": 8, \"type\": \" GET _analyze?text=Is the fox brown&analyzer=english { \"tokens\": [ { \"t","locales":"","title":"Phrase Queries: a world without Stopwords"}
{"index":{}}
{"author":"Megan Wieling","category":"","publish_date":"2016-03-07T00:00:00.000Z","url":"/blog/witwies-big-data-paris-javaland-2016","seo_title":"Where in the World is Elastic? - Big Data Paris & Javaland 2016","content":" Welcome to Find out which Elastic events and meetups are happening near you this week.  Upcoming EventsMarch 7-8: March 8-10: Upcoming Meetups March 8: March 8: March 8: March 10: March 11: March 10: March 10: That's it for this week. Stay tuned next week for more Elastic happenings. - The Elastic Team P.S.  if you're interested in hosting a meetup or are giving a talk about Beats, Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic? - Big Data Paris & Javaland 2016"}
{"index":{}}
{"author":"Suyog Rao","category":"The Logstash Lines","publish_date":"2016-03-07T00:00:00.000Z","url":"/blog/logstash-lines-2016-03-07","seo_title":"","content":" Welcome back to The Logstash Lines! In these weekly posts, we'll share the latest happenings in the world of Logstash and its ecosystem.Fallout from 2.2 changes In 2.2, we changed the pipeline to combine filters and output stages into one worker unit/thread. Some users upgrading to 2.2 complained about drop in throughput, mostly because of misconfiguration in pipeline workers setting. In 2.2, it is safe to bump the pipeline workers to say 2x * number of cores because there will be IO idle time resulting from filters and workers on the same execution unit. Although it seems that pipeline needs more threads, its also doing the additional output work. To help users upgrade, we released a  details this info. Another side effect of this change was that certain outputs like ES now use more resources like file handles, sockets etc because the outputs were never optimized for running in so many threads. This week we worked on making ES output (and underlying elasticsearch-ruby client) to be thread-aware and to share resources like connections () Expanding Environment Variables inside LS Config has been a popular enhancement request by users, but since there's a workaround using config management tools and/or things like sed, awk we had been pushing it down the road. Recently, we decided to bite this bullet, and have been working with to implement this enhancement. It just got merged to master and 2.x. So now you can do: input { tcp { port => \"${LS_TCP_PORT:9999}\" } } output {# file { path => \"${HOME}/file.log\" } } Pretty sweet and convenient. The syntax is This behavior is also consistent with Beats product. Many thanks to Fabien for contributing this enhancement! Beats input certificate verification Beats input and its predecessor Lumberjack input had never actually verified client certificates against the CA (on the server side). We are fixing , and also adding end to end Filebeat and Logstash integration tests with SSL. Dynamic Config Reload This feature has been backported to 2.3. With this, you don't have to restart Logstash when any configuration changes have been made. Stay tuned for the release of version 2.3.0 soon. Other fixes: ","locales":"","title":"Logstash Lines: Environment variables in Config"}
{"index":{}}
{"author":"Guy Boertje","category":"Engineering","publish_date":"2016-03-02T00:00:00.000Z","url":"/blog/the-evolving-story-about-the-logstash-file-input","seo_title":"","content":" In this post, we explore the popular file input, and its many use cases. We offer a peek into the evolution of this input and contrast it with alternatives like Filebeat. Some History (before version 2.2.0) The file input is a popular way in Logstash to ingest contents of a file. From the onset, it was designed to solve the tailing file use case where the file is being constantly updated. Many users have also tried to use it for uploading static file content into Logstash, and we'll explore this dichotomy in this post. File input saves information of how far along in the file Logstash has processed so Logstash can resume when it is restarted. For this, it uses sincedb, a file-based persistent store that records the number of bytes read against the file inode (*nix) or identifier (windows). Most of the real work behind the file input is done by the filewatch library, which is responsible for watching path globs and tailing files. Filewatch has two phases of operation, discovery and processing. During the first discovery, the watch pattern glob detects files, and each file is put into an internal store. In later discovery phases, only newer files will be put in the store. In the first processing phase, reading: for each stored file, we open, seek to the last byte read position, and read it in 32K chunks. We extract each line from the chunk, send it to the file input, and record the line byte count to the sincedb. Note: The file stays open, and we compare its inode with what we have from the sincedb to decide whether we have seen the file before. In the subsequent processing phase, tailing: if the file has grown, it is read in chunks as above. Some problems with this are: The concept of identity mapping was introduced to the file and other inputs, to ensure that data from the same origin went to the same codec instance, so each time a new identity is seen (path or socket connection), we add a mapping to a clone of the codec specified. To avoid boundless growth of the map, we set the limit to 20,000 identities, because we did not imagine that there would be setups that have more than 20,000 active connections or files being actively . Internally there is a mechanism to evict identities that have not been used for a while (1 hour). Eviction is non-destructive. However in the true setup, all the files may be \"hot\" and their identity (path) may be \"hot\" too — meaning that the identity may never be evicted. However, if the number of discovered files is greater than 20,000 — they are all opened — then the hard limit will be reached before the eviction mechanism can do anything. If the number of open file handles hits the limit, then Logstash starts failing to read/write other files e.g., sincedb, templates, and sockets. In version 2.1.0 we implemented: In this version the behaviour bypasses reading and tailing if a file was last modified more than the setting seconds ago. The behaviour is to close a file if it was last modified more than the setting seconds ago. We found two problems with this release: for , files were opened then closed (because the open function did the sincedb update) but this consumed file handles unnecessarily and, for , we should have used a time to decide if a file can be closed. File Input version 2.2.0 We introduced a tracking object to hold state on each file we discover and process. In discovery for each file path, a tracking object is stored. This object holds the current state and the previous state history as well as many other attributes e.g., path, inode, , stat info, and some timestamps. Files satisfying the setting are put into the 'ignored' state during discovery and all others are put in the 'watched' state. Files in the ignored state are not opened but are monitored for changes. We changed the behaviour to use time to decide if a file can be closed. Files in the closed state are monitored for changes. We have added a config option (default 40","locales":"","title":"The evolving story about the Logstash File Input"}
{"index":{}}
{"author":"Michael McCandless","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2016-02-29T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-core-changes","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top NewsWhat would life be like without ? attendees answer: — elastic (@elastic) Elasticsearch Core Apache Lucene Watch This Space, where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - Core Changes"}
{"index":{}}
{"author":"Benjamin Speckmann","category":"User Stories","publish_date":"2016-03-02T00:00:00.000Z","url":"/blog/process-transparency-and-error-categorization-at-1-1-by-using-the-elastic-stack-and-etl","seo_title":"Process transparency and error categorization at 1&1 by using the Elastic stack and ETL","content":" What operating drivesSo what do we operate? We are working on 10 BPMN processes with approx. 50,000 process instances a week. The logs generated by these processes are about 50GB a day. The log files contain technical information as well as business information. The Elastic Stack gives us the ability to analyze these logs in real-time and provides transparency on customer orders. As an operations team we are driven by errors and incidents that attack customer orders. Our primary goal is customer satisfaction which we are trying to maintain by observing process SLAs like “in time provisioning” of customer orders. For being meaningful about the health of our processes we needed an overview as well as a categorization of errors in our processes. The questions we needed to answer were: Extending the Elastic StackNot all of the information can be taken from our process logs. To completely answer our questions we needed to enrich our application logs with information from other sources like issue tracker, databases, etc. The data sources in detail are:  For conditioning this information we use the ETL tool Pentaho Kettle. Dashboard as a toolOur Kibana dashboard helps us working with our processes every day. (1) Number of errors by process (pie and table) (2) Number of errors by issue tool ticket (pie and table) (3) Number of errors by description text (pie and table) (4) Full list of errors In order to make operations more efficient, the dashboard integrates other tools for example linking to related bugs in our issue tracker. In detail the dashboard provides: (1) Link to issue tracker (2) Link to central support tool showing detailed contract and customer information (3) Link to Kibana dashboard showing all log information concerning that order (for drilldown)(4) Link to information tool showing detailed order and customer information OutcomeWe are now using the Elastic Stack for 2 months. The outcome is significant. More focus on what is better now e.g. increased error detection and addressing rate by 30 to 90 percent.  In the future……we will concentrate on using the Elastic Stack for real-time monitoring purposes. For example a visualization of order content will help us to cluster our orders for estimation of campaigns (e.g. new product release). Therefore we need to deal with Apache SPARK for data processing and computations. Spark will calculate different KPIs (Key Performance Indicators) for us e.g. the duration of different process periods. is Business Process Manager at 1&1 Internet SE. Previously he was working as a consultant at NTT DATA Deutschland GmbH. Since the beginning of his career Benjamin deals with executable BPMN processes (Business Process Model and Notation). Monitoring, Data-Analysis and Reporting of KPI’s was always a part of his work. He holds a M.S. in Computer Science from Eastern Michigan University as well as the University of Applied Sciences in Karlsruhe.  is Advanced System Analyst at 1&1 Internet SE. Previously Christian was working as a Requirements Engineer delivering business concepts for offshore developed business process monitoring solutions. He is experienced in JBoss EAP and BPM dominated infrastructure. In 2015 Christian implemented the Elastic Stack for efficient monitoring and analysis of technical and business data in Order Management. ","locales":"de-de","title":"Process transparency and error categorization at 1&1 by using the Elastic Stack and ETL"}
{"index":{}}
{"author":"Andrew Cholakian","category":"Engineering","publish_date":"2016-03-01T00:00:00.000Z","url":"/blog/upgrade-guide-for-logstash-2-2","seo_title":"","content":" Hello Logstashers! In this post, we’ll talk about considerations when upgrading to . Specifically how to avoid situations where some new behavior (and a bit of soon to be fixed misbehavior by the Elasticsearch output plugin), can cause a larger than desired number of file handles to be used. I will also discuss some of the new tuning considerations for 2.2.0. To help users with upgrading to 2.2.0, we’ve also added a new documentation section . The good news is that Logstash 2.2 brings with it some very significant performance improvements! The down side is that one of the changes we made to the default settings in Logstash cause issues for some configurations. For example, we are hearing reports of Logstash taking up many more file handles than it previously would. Lets dive deeper into these changes. Worker Units There are two types of worker in Logstash 2.2, pipeline workers (formerly filter workers) and output workers. In previous versions of Logstash the filters and outputs ran in their own threads. In 2.2 we unified our threading model, putting both filter and output worker threads in one ‘pipeline’ worker thread type. This has given us some significant performance gains. The output worker concept remains, but rather than mapping directly to a thread it maps to a pool of available output objects. During execution these are grabbed by a pipeline worker for use and returned when done. You will find that you need to set higher than before, and in fact Logstash may require more total threads than before. However, this is by design! You will find that you have more threads doing less work. Sure, a fair number of them may be idle in IO wait, however the work they do is now much more efficient, and with less context switching. To tune the setting just keep increasing it, even to a multiple of the number of cores on your system (remember, those threads are often idle) until you see throughput go down. By default, is set to match the number of cores in your system. Batching Events There is also a new batch size setting which you can tune in a similar way. Please that explains these options in more detail. This batch size is now also the default flush size for the Elasticsearch output. The option on that now only changes the maximum flush size and will no longer set the minimum flush size. New Pipeline and Outputs In our performance testing we found that increasing the number of output workers to stay in lock step with pipeline workers yielded some nice performance gains. However, for our Elasticsearch output there’s an undesired behavior that stayed hidden until now. If you use multiple Elasticsearch output workers they won’t share the same backend connection pool! That means if you have 5 backend ES instances and 5 workers you may have up to 5*5=25 connections! If you have 40 ES nodes the effect is amplified! We understand the current situation isn’t ideal in some cases. The fix isn’t to change the Logstash core behavior, but to make the Elasticsearch output handle connections responsibly in the new pipeline model. We are targeting a new release in the 2.2 series that addresses this. We’re currently working on a for this that will only require a plugin upgrade, but until that’s released you’ll want to stick with a relatively low number of ES output workers. The workaround in 2.2 is to just set your count low explicitly, try only 1 or 2 workers. Feedback I hope we’ve clarified some aspects about our new pipeline architecture and upgrading to 2.2. Please let us know if you have any feedback! You can always reach us on Twitter (@elastic), on our , or report any problems on the GitHub page. ","locales":"de-de,fr-fr,ja-jp,ko-kr","title":"Upgrade Guide for Logstash 2.2"}
{"index":{}}
{"author":"Asawari Samant","category":"News","publish_date":"2016-02-29T00:00:00.000Z","url":"/blog/elastic-at-rsa-conference-2016","seo_title":"","content":" On the heels of our annual user conference, , this week we are at RSA Conference showcasing how the touches a variety of security use cases.Like parsing through massive rows and columns of data (which we love), if you are on the show floor, we’d love to have you stop by to see our collection of demos ranging from finding events of interest in honeypot data, ingesting and spotting trends in variety of security logs, and using the power of graph analysis to track interesting connections and relations. Oh but one thing, this year, we didn’t sign up for a booth on-time, but thanks to our partner, , we’ve joined them at their Booth #444 in South Moscone. So if you’d like to meet Elastic experts and talk about how our products Elasticsearch, Logstash, Beats, Kibana and our plugins for security, alerting, and monitoring can help you, please come visit us.Additional Elastic Security Resources: ","locales":"","title":"Elastic @ RSA Conference 2016"}
{"index":{}}
{"author":"Nik Everett","category":"Engineering","publish_date":"2016-02-20T00:00:00.000Z","url":"/blog/loading-wikipedia","seo_title":"Loading Wikipedia's Search Index For Testing","content":" A month ago I published a post testing query speeds with data from English Wikipedia and mentioned that the loading process deserved its own blog post. Well here it is!Step 0: WhyThis gets you a copy of the search index used to power on site search for Wikipedia. The index is quite large because it's used to support all the funky things that Elasticsearch is used for there. It may have mistakes, old data, strange leftovers from previous versions of the indexing code that have yet to be cleaned up. In short: it's imperfect, just like any real production deploy.Step 1: Download a smaller wikiThe Wikimedia Foundation makes everything about Wikipedia and the other wikis it powers public. Well, everything that is safe to make public anyway. That includes a dump of the search index. Head and you'll get a list of dates when the dump runs began. Click on the latest date and download the file that looks like . The naming scheme is where: By now you've figured out that I didn't have you download English Wikipedia's search index. I had you fetch English Wikiquote. You can follow my instructions below to load English Wikiquote and then start over to load any other wiki you'd like from the dumps page. If one particular wiki caught your eye, go and start downloading it now while you follow along to the end of the blog post using English Wikiquote. If all goes well it'll be ready by the time you want it.Step 2: Get the index readyWhile the dump downloads you can setup Elasticsearch to handle it the index. You'll need the plugin. You can do that by this into your bash shell:You'll have to restart Elasticsearch after installing that for it to take.Then you'll need for some of the json-foo you'll do next. For me it's just but your command may vary.Finally you can create you index with these bash shell commands:export es=localhost:9200 export site=en.wikiquote.org export index=enwikiquote curl -XDELETE $es/$index?pretty curl -s 'https://'$site'/w/api.php?action=cirrus-settings-dump&format=json&formatversion=2' | jq '{ analysis: .content.page.index.analysis, number_of_shards: 1, number_of_replicas: 0 }' | curl -XPUT $es/$index?pretty -d @- curl -s 'https://'$site'/w/api.php?action=cirrus-mapping-dump&format=json&formatversion=2' | jq .content | sed 's/\"index_analyzer\"/\"analyzer\"/' | sed 's/\"position_offset_gap\"/\"position_increment_gap\"/' | curl -XPUT $es/$index/_mapping/page?pretty -d @- Let me walk you through that: Clear as mud? Ok.Step 3: Prepare the wiki for loadingCrack open the file with zless and have a look at it. Don't worry, you can do that before it finishes downloading. Its contents are conveniently in the format that Elasticsearch uses for bulk loading. Hurray! We'll unzip and cut this file into smaller chunks so it'll work properly with curl and the API. It'll also you a way to pause and resume the process. After it has finished downloading you can do that with these bash shell commands:export dump=enwikiquote-20160201-cirrussearch-content.json.gz export index=enwikiquote mkdir chunks cd chunks zcat ../$dump | split -a 10 -l 500 - $index For English Wikiquote that should finish in a few seconds. English Wikipedia takes longer than a coffee break.Step 4: Load the wikiThe last step is to load the actual data with these bash commands:export es=localhost:9200 export index=enwikiquote cd chunks for file in *:  do echo -n \"${file}: \" took=$(curl -s -XPOST $es/$index/_bulk?pretty --data-binary @$file | grep took | cut -d':' -f 2 | cut -d',' -f 1) printf '%7s\\n' $took [ \"x$took\" = \"x\" ] || rm $file done The first three lines should be familiar from above. The loop loads each file and deletes it after it's loaded. If the file fails to load it isn't deleted and the loop moves on to the next file.I find setting the to will speed this process up some. You can apply it by running this in a different terminal:curl -XPUT \"$es/$index/_settings?pretty\" -d '{ \"index\" : { \"refresh_inte","locales":"","title":"Loading Wikipedia's Search Index For Testing"}
{"index":{}}
{"author":"Chris Rimondi","category":"User Stories","publish_date":"2016-02-29T00:00:00.000Z","url":"/blog/reflections-from-elasticon-tapping-out-security-threats-at-fireeye","seo_title":"Elastic{ON}16: Tapping Out Security Threats at FireEye","content":" FireEye is a security company that provides real-time threat protection to enterprises and governments against cyber-attacks. Its real-time threat protection platform operates without the use of signatures to protect an organization across the primary threat vectors and across the various stages of an attack life cycle.Elasticsearch has been a critical part of the Threat Analytics Platform (TAP) at FireEye for the past two years. Our engineering and operations teams had the opportunity of watching the maturity of Elasticsearch through multiple versions. The conferences have become mile markers in the product’s progress. FireEye sent attendees to the conference in 2015 and was privileged to have a speaking spot in 2016.One of the most valuable aspects of the conference is that Elastic sent its entire company — including engineers, product managers, and executives — to the conference this year. They laid out the vision of aligning their products and how these improvements will impact features and versioning. As an organization that is looking to leverage different Elastic products within its development roadmap, it was valuable to get the insight directly from the source. Various product management from Elastic were there including . I shared some of FireEye’s objectives and queried Tanya about Elastic’s roadmap related to analytic capabilities.. As a speaker, the best part of presenting was getting to talk to attendees afterwards about their use cases and compare war stories. I easily learned as much (and probably more) from chatting afterward with attendees as I did from other sessions. It was a healthy exchange of what worked and what didn’t when deploying Elasticsearch.I hope you enjoy hearing how FireEye built a security analytics platform that indexes hundreds of thousands of events per second and allows its enterprise customers to find evil in their organizations across petabytes of data.  Chris Rimondi runs the Site Reliability Engineering team for the Cloud business unit at FireEye. He started at Mandiant, three years ago, prior to its acquisition by FireEye. Since then he is focused on building and supporting the next generation of FireEye applications in its public and private cloud infrastructure.  ","locales":"","title":"TAP(ping) Out Security Threats at FireEye: An Elastic{ON} Reflection"}
{"index":{}}
{"author":"Scott Fingerhut","category":"Culture","publish_date":"2016-02-19T00:00:00.000Z","url":"/blog/life-without-elasticsearch-elasticon16","seo_title":"Video: Life Without Elasticsearch?","content":" \"My life would cease to exist as I know it today if I did not have Elasticsearch.\" \"I would cry, there'd be tears.\" \"Without Elasticsearch, life would be a lot slower.\" Amidst the science experiments, colored lights, and pulsing dance music of San Francisco's Exploratorium at the kickoff party, many folks were faced with the apocalyptic question: \"What would your life be like without Elasticsearch?\" See what many more had to say (and laugh about), in this short video from the Elasticsearch user conference. ","locales":"","title":"Video: Life Without Elasticsearch?"}
{"index":{}}
{"author":"Suyog Rao","category":"Releases","publish_date":"2016-02-18T00:00:00.000Z","url":"/blog/logstash-2-2-2-released","seo_title":"Logstash 2.2.2 released with important security bug fix","content":" A short post to announce that Logstash version 2.2.2 has been released today with an important security bug fix! Jump to the page for the binaries, where you can also find the full list of . Elasticsearch Output SSL Configuration Issue Logstash version 2.2.1 is vulnerable to a man in the middle attack when used with Elasticsearch output. In version 2.2.1, the config which enables SSL/TLS default has been disabled inadvertently, so a malicious user could access payload data sent via HTTP during the initial handshake. This has been fixed in 2.2.2. User who do not wish to upgrade immediately to 2.2.2 can use prefix in their configuration. For example, replace value of to . Please restart Logstash after you make this change. ","locales":"","title":"Logstash 2.2.2 released"}
{"index":{}}
{"author":"Shay Banon","category":"News","publish_date":"2016-02-17T00:00:00.000Z","url":"/blog/introducing-elastic-cloud-and-elastic-cloud-enterprise","seo_title":"Introducing Elastic Cloud and Elastic Cloud Enterprise","content":" We “Found” A New Name! string theStory = “Start at the Beginning” At last year’s Elastic{ON}15, we announced the acquisition of Found, the company. Our goal, at the time, was to provide the simplest, most complete Elasticsearch as a service offering that was available in market. Of course, the team behind Found was a key element in the decision making process. We are proud that, as a company, we have made major strides in achieving our stated objective. Today, more than 1,000 companies are running a portion of their business on the hosted offering that we provide. So what, you may ask, have we been doing behind the scenes over the last year? string theStory = “Naming is Hard!” If you’ve been following the announcements from Elastic{ON}, you will have heard about the renaming of our commercial products — Shield (security), Marvel (monitoring), and Watcher (alerting) — as X-Pack, a single extension that bundles these meaty features and more. As we began to align the name of products with the functionality that they provide it was necessary to reconsider what we call our hosted offering as well. Elasticsearch as a Service. It seems fairly straightforward. But the reality is that Found provides much more than just Elasticsearch. Each cluster includes a free Kibana instance, integration with Shield (now called security). Premium users can leverage the monitoring and alerting capabilities of the X-Pack. And many of them ingest using Beats, Logstash, or a combination of both. So what do we call a thing that is the entire Elastic Stack hosted and maintained so that you don’t have to? Welcome, Elastic Cloud. string theStory = “A Rose by Any Other Name…” The Elastic Cloud is everything about Found that you’ve come to know and love. Or, perhaps, don’t know is available to make your life easier. You can setup a cluster and have it running in a matter of minutes (with a 14-day free trial) and it includes 16GB of storage for every 1GB of memory. Additionally, it includes updates to the latest release of the Elastic Stack and is tightly integrated with the X-Pack features like security, alerting, monitoring, and even Re{Search} features like Timelion. But, most importantly, the team that has built — and maintains — Elastic Cloud is a part of Elastic. We release on the same schedule. We test together. We solve issues together. It is the Elastic that you know and trust…only hosted. string theStory = “But Wait, There’s More!” The benefit of being a company driven, in large part, by listening to the community is that we are able to make decisions for a multitude of reasons. Several of our customer (those with whom we have a commercial relationship) have asked about “how” we run Elastic Cloud. Or, have asked about best practices for attempting to build their own Elastic Cloud. I’m pleased to introduce Elastic Cloud Enterprise. It is the same product that is powering our hosted offering, available for installation on the hardware – or in the environment – that you choose. If you manage multiple deployments be that across multiple teams or geographies, you can leverage the same technology that we do to centralize and manage the provisioning, monitoring and management, scaling, replication, and upgrades of your Elasticsearch clusters. Elastic Cloud and Elastic Cloud Enterprise. One product that allow you to choose your own deployment adventure. And yes, we treat Elastic Cloud as a product. We don’t simply offer a service. Rather, we build a product that can be consumed as a service or installed to manage multiple clusters or offer a service inside of your organization. Want to be part of it? Learn more, or sign-up for the private Beta of Elastic Cloud Enterprise. If you happen to be at Elastic{ON} join the session by Njal Karevoll and Erik Redding entitled ","locales":"ja-jp,ko-kr","title":"Introducing Elastic Cloud and Elastic Cloud Enterprise"}
{"index":{}}
{"author":"Apu Kumar","category":"User Stories","publish_date":"2016-02-16T00:00:00.000Z","url":"/blog/lotadata-will-neighborhood-shine-red-blue","seo_title":"LotaData Studies Your Neighborhood - Will It Shine Red or Blue","content":" The past few years have brought about a renaissance in American neighborhoods with strong economies, healthy workforce and diverse communities. Nextdoor has done well to recognize this trend. The simplicity and elegance of Nextdoor's local community network provides a convenient and useful way for residents to stay in touch with their neighbors. While most people intuitively understand the colloquial definition of \"neighborhood\", it is an intriguing exercise to research how neighborhoods manifest in spatial and temporal dimensions, along sociological, philosophical and cultural vectors. What makes a neighborhood? How exactly are neighborhoods defined and who defines them? Do the boundaries change over time? How does one find the information for the thousands of neighborhoods and communities across the US? Is there such a thing as a \"neighborhood search engine\"? Deep knowledge about neighborhoods can set the tone and context for businesses and brands trying to serve the needs of the local market. As published in a , the data scientists at  have studied the composition, characteristics, trends and correlations across multiple location-based datasets to understand the physical attributes, the social structure and the digital fabric of local communities. The resulting profiles represent the active identity for each neighborhood, constructed from hundreds of geo-temporal variables, including: Marketers and advertisers look at neighborhoods as a collection of unique people with distinct practices. Neighborhoods provide a sense for the types of audiences one can expect to find in the area, thereby influencing advertising campaign decisions and marketing budget allocations. The increased focus on location-based campaigns in 2016 is starting to put the spotlight back on our neighborhoods. Neighborhood profiles, structured in the form of APIs, can make it possible for marketers to seamlessly search through unwieldy datasets, to unearth meaningful and actionable intelligence for designing hyper-targeted campaigns. As an example, it would be a marketer's dream to be able to “create and monitor geo-temporal zones, 100 feet in radii, around all café locations in neighborhoods that voted against Prop 8, with median home value above $400K and mean household income over $100K, with an average of 3 members per household, located near venues scheduled to host +5 music events with projected attendance of +1000 per event, with <5% chance of precipitation, over the next 16 weeks”. To enable complex queries like these,  has published the detailed profiles for +6800 neighborhoods and made them accessible and searchable through the  and the . LotaData's technology platform is powered by . Our machine learning recipes extract, collect, cleanse, de-dupe, structure, classify and publish geo-temporal data from tens of thousands of sources, across +80 countries, +35,000 cities and towns, +680,000 local deals and promotions, +145,000 performers, musicians, actors, athletes, teams, +1,800,000 venues, +9,000,000 local events, activities, and an ever growing list of businesses and brands. The speed, scalability and performance of Elasticsearch are the foundation for our platform. The massively distributed architecture with cluster resiliency allowed us to scale horizontally. To get the full story, find our Co-founder and CTO  at , offer to buy him a Cortado and he'll spill the beans faster than you can collect. Neighborhood insights are of tremendous value to businesses and brands looking to reach the right audience at the right place, the right time and in the right mood. The New York Times recently wrote about a national brand with an effective local campaign that significantly exceeded the industry average for mobile ad engagement.  profiled neighborhoods that have a high density of bars in order to reach liquor consumers ag","locales":"","title":"LotaData Asks, Will Your Neighborhood Shine Red or Blue?"}
{"index":{}}
{"author":"Antonio Bonuccelli","category":"Engineering","publish_date":"2016-02-18T00:00:00.000Z","url":"/blog/elasticsearch-and-siem-implementing-host-portscan-detection","seo_title":"","content":" Intro: using a SIEM approach Effectively monitoring security across a large organization is a non-trivial task faced everyday by all sorts of organizations.The speed, scalability and flexibility of the Elastic stack can play as a great asset when trying to get visibility and proactively monitoring large amounts of data. The traditional SIEM approach relies on normalization of the data from raw, based on a schema. For example a failed login, be it from a Linux Nov 26 12:15:04 zeus sshd[19571]: Failed password for ciro from 10.0.4.23 port 57961 ssh2 or a Windows host, Log Name: Security Source: Microsoft-Windows-Security-Auditing Date: 27/11/2015 2:07:33 PM Event ID: 4625 Task Category: Logon Level: Information Keywords: Audit Failure User: N/A Computer: minerva Description: An account failed to log on. Subject: Security ID: NULL SID Account Name: - Account Domain: - Logon ID: 0x0 Logon Type: 3 Account For Which Logon Failed: Account Name: gennaro <.....> will be indexed observing a common structured format: Using a field naming convention allows to build correlation logic abstracting from which source the event originated from, be it a Windows or a Linux failed login. Also some tagging or categorization of the data can be performed, grok{ match => { \"message\" => [\"%{SSH_AUTH_1}\",\"%{SSH_AUTH_2}\"] } add_tag => [ \"auth_success\" ] } grok{ match => { \"message\" => [\"%{SSH_AUTH_3}\",\"%{SSH_AUTH_4}\"] } add_tag => [ \"auth_failure\" ] } where SSH_AUTH_X are our custom defined grok patterns to match success/failure events. Using this approach, correlation logic can be applied to all the events, regardless of the datasource from which the event originated from. Following the same approach, we will show how to use the Elastic stack to cover a basic network security use case, TCP host portscan detection, for which we'll implement alerting via email. Implementation I: datasource When trying to detect whether a portscan against a given host on your premises was carried on , network traffic data becomes relevant. For this use case we will want to monitor all events indicating a new TCP connection being initiated from source to target host, in short all TCP packets with SYN=1, ACK=0. While we impatiently wait for to be released and allow more out-of-the-box network protocol level capture capabilities, we'll use tcpdump capture using the below command for the purpose of this blog: sudo tcpdump -i eth0 -n -tttt tcp[13] == 2 | nc localhost 5001 the above command will listen on the eth0 network interface of the monitored host and capture all and only the TCP packets indicating that a new TCP connection handshake was initiated, also avoiding resolving IP to hostnames for faster execution:  then we pipe the results to netcat to send them to our Logstash instance for event processing, which we assume here to be running locally. For convenience, we can launch the above command using a all time favourite linux CLI utility, . !/bin/bash screen -d -m /bin/bash -c 'sudo tcpdump -i eth0 -n -tttt tcp[13] == 2 | nc localhost 5001' This is what the captured raw data looks like 2016-02-09 13:51:09.625253 IP 192.168.1.105.60805 > 192.168.1.1.80: Flags [S], seq 2127832187, win 29200, options [mss 1460,sackOK,TS val 259965981 ecr 0,nop,wscale 7], length 0 Implementation II : event processing We'll use logstash to mangle the data and extract the information relevant to this use case, namely timestamp, src_ip and dst_port. grok{ match => {\"message\" => \"%{TCPD_TIMESTAMP:timestamp} IP %{IP:src_ip}\\.%{INT:src_port} > %{IP:dst_ip}\\.%{INT:dst_port}(?<payload>[^$]+)\"} add_tag => [\"network\",\"tcp_connection_started\"] } where TCPD_TIMESTAMP is a custom defined grok pattern to match . As we have extracted the information we were after (,,) we can decide to trash and fields: mutate{ remove_field => [\"message\",\"payload\"] } Next we send these events to Elasticsearch index elasticsearch { hosts => \"e","locales":"","title":"Elasticsearch and SIEM: implementing host portscan detection"}
{"index":{}}
{"author":"Shay Banon","category":"News","publish_date":"2016-02-17T00:00:00.000Z","url":"/blog/heya-elastic-stack-and-x-pack","seo_title":"","content":" string theStory = \"ELKB, BELK, what?\" string theStory = \"The Elastic Stack, more than a name, 5.0.0\"I'm excited to announce that, unveiled today at Elastic{ON}16, all of Elastic's open source products — , , , and — will be called the Elastic Stack. This is more than just a name:  going forward, we will be building, testing, and releasing all components of the Elastic Stack together and they will all share the same version number — . Our goal is that this evolution will speed your deployments, simplify compatibility testing, and make it even easier for developers to add new functionality across the stack. , the first alpha release of 5.0.0 will be shipping soon. string theStory = \"Introducing Packs — Extensions for the Elastic Stack\"Each of the products in the Elastic Stack have always had great support for extensions. With Elasticsearch, plugins can add language analyzers, extend snapshot/restore, and even create new APIs. The Logstash plugin ecosystem is a wonderful example of how powerful extensions can be, and Kibana introduced plugin infrastructure in recent releases, which we have used to create extensions like Timelion. However, as we use these extension points to build our own products, we realized that there is a tremendous benefit to thinking bigger:  extending the whole stack. Today, we announced the introduction of Packs — extensions that apply to the whole stack. On the surface, this is a simple concept — a single zip that contains extensions for one or more of the products in the Elastic Stack. With aligned releases and versions across the stack, packs make it easy to build, test, and release extensions that span the stack. We saw this need ourselves, when building product like Marvel, which plug into Elasticsearch to capture telemetry and metrics, while also installing into Kibana to provide the UI for monitoring your infrastructure. We will be releasing our own pack (read on!), but we are most excited to see how you extend the Elastic Stack, and what new use-cases you uncover. string theStory = \"X-Pack: Shield, Watcher, Marvel and more in a single extension\"Today, we also announced our own pack, known as X-Pack — delivering security, alerting, monitoring to the entire Elastic Stack through a single extension. And with the release of X-Pack, we're super excited to announce new reporting capabilities for users to generate, schedule, and email Kibana dashboards as PDF reports, and a new graph API and UI, which allow you to explore, visualize, and analyze your existing data in new ways. With X-Pack, we hope that our users gain time to value by having features that seamlessly work with the Elastic Stack, as well as meet today's IT, security, and regulatory requirements. We hope you can give the and a whirl! ","locales":"ja-jp,ko-kr","title":"Heya, Elastic Stack and X-Pack"}
{"index":{}}
{"author":"Andrew Kroh","category":"Engineering","publish_date":"2016-02-16T00:00:00.000Z","url":"/blog/detecting_dns_tunnels_with_packetbeat_and_watcher","seo_title":"","content":" This post was updated to reflect changes in Packetbeat 5.x and Elasticsearch 5.x. Full-bleed source: https://commons.wikimedia.org/wiki/File:IDF_Uncovers_Terror_Tunnels_in_Gaza_(14684360244).jpg Thumbnail source: https://flic.kr/p/3QVF7S <h1>Detecting DNS Tunnels with Packetbeat and Watcher</h1> Data observed from monitoring DNS traffic on a network can be used as an indicator of compromise (IOC). This blog post will discuss how Elasticsearch and Watcher can be used with Packetbeat to alert when possible malware activity is detected. is our open source packet analyzer. It monitors the traffic on your network and indexes the DNS requests and responses into Elasticsearch where aggregations can be used to help make sense of the data. (formerly ) is part of and it provides alerting and notifications based on changes in your data. There are many use cases for alerting on data collected by Packetbeat such as alerting when the response times for web requests are above a threshold or when there is spike in HTTP errors returned by your web servers. The alerting described in this article has applications in network security. We are going to look at one specific use case -- detecting data exfiltration over DNS tunnels. Detecting DNS Tunnels Tunnels can be established over the DNS protocol to covertly move data or provide a command and control channel for malware. Often this technique is used to bypass the protections of corporate firewalls and proxy servers. Tunneling works by encoding data in DNS requests and responses. The client issues a query for a hostname and that query is eventually forwarded to the authoritative name server associated with the domain. There are a lot of different techniques that can be employed for detecting such traffic. We are going to look at using the number of unique hostnames for a domain as an IOC. DNS tunneling utilities must use a new hostname for each request which leads to a much higher number of hostnames present for the malicious domains in comparison to legitimate domains. Packetbeat Setup The first step is to install Packetbeat and configure it to collect DNS traffic. For this setup, the server running Packetbeat is connected to a port mirror so that Packetbeat can observe all the traffic between the local network and the Internet. The Packetbeat documentation has a great that explains installation and setup procedure, so I will just show the configuration used. # /etc/packetbeat/packetbeat.yml packetbeat.interfaces.device: en0 packetbeat.protocols.dns: ports: [53] include_authorities: true include_additionals: true output.elasticsearch: hosts: [\"localhost:9200\"] Watch your DNS Traffic The complete is stored in our repository along with all of the supporting files shown here. We are going to walk through the creation of this watch step-by-step. Watch Trigger The watch trigger specifies when the execution should start. This watch is scheduled to execute every 15 minutes. \"trigger\": { \"schedule\": { \"interval\": \"15m\" } }, Watch Input This first step in creating this watch is to design a set of aggregations to be used as the input to the watch. We want to find the cardinality of the hostnames associated with each second-level domain (e.g. ). We start with a query that has just two components, a time window and a whitelist. The time window and whitelist can be customized. Find more on this in the section. Next we use a terms aggregation to create buckets for each second-level domain. Then we apply a sub-aggregation to get the cardinality of the hostnames within that bucket. Finally we apply a bucket selector aggregation to select only the buckets having more than 200 unique hostnames. The watch will generate an alert when the number of unique hostnames breaks this threshold. GET packetbeat-*/dns/_search { \"query\": { \"bool\": { \"filter\": { \"range\": { \"@timestamp\": { \"from\": \"now-4h\" } } }, \"must_not\": { \"terms\": { \"dns.question.etld_plus_one\": [ \"aka","locales":"","title":"Detecting DNS Tunnels with Packetbeat and Watcher"}
{"index":{}}
{"author":"Megan Wieling","category":"","publish_date":"2016-02-15T00:00:00.000Z","url":"/blog/witwies-elasticon-2016","seo_title":"Where in the World is Elastic? - Elastic{ON}16","content":" Welcome to Find out which Elastic events and meetups are happening near you this week.  Upcoming EventsFebruary 17-19: Upcoming Meetups February 15: February 15: February 16: February 16: February 17: February 18: February 18: February 15: February 17: That's it for this week. Stay tuned next week for more Elastic happenings. - The Elastic Team P.S.  if you're interested in hosting a meetup or are giving a talk about Beats, Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic? - Elastic{ON}16"}
{"index":{}}
{"author":"Michael McCandless","category":"Engineering","publish_date":"2016-02-15T00:00:00.000Z","url":"/blog/lucene-points-6.0","seo_title":"","content":" Coming in Lucene's next major release (6.0) is a new feature called , using the  geo-spatial data structure to offer fast single- and multi-dimensional numeric range and geo-spatial point-in-shape filtering. As of this writing, Elasticsearch has not yet exposed points, but I expect that will change soon. This feature replaces the  numeric fields and numeric range query since it has better overall performance and is more general, allowing up to 8 dimensions (versus 1) and up to 16 bytes (versus the  today) per dimension. The k-d tree variant we implemented is the  which is specifically designed for efficient IO, such that most of the data structure resides in on-disk blocks, with a small in-heap binary tree index structure to locate the blocks at search time. This means you will finally be able to use Lucene to efficiently index and range-filter anything that can be encoded as fixed-length, ordered  , such as  , , , etc., along with 2D and 3D (and higher!) geo-spatial indices, and times-series values. k-d trees Block k-d trees are a simple yet powerful data structure. At index time, they are built by recursively partitioning the full space of N-dimensional points to be indexed into smaller and smaller rectangular cells, splitting equally along the widest ranging dimension at each step of the recursion. However, unlike , a block k-d tree stops recursing once there are fewer than a pre-specified (1024 in our case, by default) number of points in the cell. At that point, all points within that cell are written into one  on disk and the starting file-pointer for that block is saved into an in-heap binary tree structure. In the 1D case, this is simply a full sort of all values, divided into adjacent leaf blocks. There are k-d tree variants that can support removing values, and rebalancing, but Lucene does not need these operations because of its write-once per-segment design. At search time, the same recursion takes place, testing at each level whether the requested query shape intersects the left or right sub-tree of each dimensional split, and recursing if so. In the 1D case, the query shape is simply a numeric range whereas in the 2D and 3D cases, it is a geo-spatial shape (circle, ring, rectangle, polygon, cube, etc.). Here is a video showing how the leaf blocks are visited to find all 2D (latitude/longitude) points inside the London, UK polygon based on the  data:  #bkdvideo { width: 854px:  height: 480px:  } Once the recursion ends at a leaf block, if the cell overlaps the shape's boundary (blue cells) then each full-precision point in that block is tested against the shape. This check (\"does the query shape contain this point?\") may be somewhat costly since it is computed per-hit against a possibly complex shape, however it is only done for those leaf cells overlapping the boundary of the shape. If instead the leaf block is fully contained inside the query shape (the pink cells), the documents with values in that cell are efficiently collected without having to test each point. K-d trees are fast because they naturally adapt to each data set's particular distribution, using small leaf blocks where the indexed values are dense: notice how central London, where there is a higher density of points, is assigned smaller leaf cells. K-d trees also naturally find the right tradeoff of how deeply to recurse, by splitting up the dimensional space, versus at what point simply scanning the full-precision values in one cell is appropriate.  This is in contrast to legacy numeric fields which always index the same precision levels for every value (4 terms for a long value, by default) regardless of how the points are distributed.  Lucene's implementation Each indexed value, for example a single  added to your document, is translated into a fixed-length  for a specific number of dimensions. There are classes for each native type (, , etc.) that handle con","locales":"","title":"Multi-dimensional points, coming in Apache Lucene 6.0"}
{"index":{}}
{"author":"Suyog Rao","category":"Releases","publish_date":"2016-02-13T00:00:00.000Z","url":"/blog/logstash-2-2-1-released","seo_title":"","content":" [This post has been updated to include 2.1.3 release] We are happy to announce that Logstash version 2.2.1 and 2.1.3 has been released today! Jump to the page for the binaries, where you can also find the full list of for 2.2.1 and 2.1.3 . Bug Fixes These are bug fix releases, some of which we highlight below: Reinstating CLI option In , we re-architected the pipeline to have both filters and outputs execute in the same thread/worker unit, and introduced an option () to configure them. Inadvertently, we removed an existing CLI option which has been used to control the number of threads used for filter stage. In 2.2.1, we’ve reinstated this flag, and made it an alias to configure pipeline workers. This option () has also been marked deprecated, and will be removed in the next major version. Please note that the short option will continue to work in both 2.2.0 and future versions. Our apologies for any inconvenience this has caused. Flushing issue in new pipeline Fixed a bug where filters that periodically flush buffered events would not work in v2.2.0. This bug affects filters like multiline, metrics, aggregate filter () Others Please Logstash 2.2.1 and let us know what you think on Twitter (@elastic) or on our . You can report any problems on the GitHub issues page. ","locales":"","title":"Logstash 2.2.1 Released"}
{"index":{}}
{"author":"Court Ewing","category":"Releases","publish_date":"2016-02-12T00:00:00.000Z","url":"/blog/kibana-4-4-1-and-4-3-2-and-4-1-5","seo_title":"","content":" Today we’re releasing patch versions of Kibana 4.4, 4.3, and 4.1 that bump the bundled version of node to include recent security fixes in node.js. We recommend that all users upgrade their Kibana installs immediately. The 4.4.1 release also includes a bug fix in Kibana itself that was deemed “way too annoying not to fix”. ​ 4.4.1 Changes 4.3.2 Changes 4.1.5 Changes Where to download You can download Kibana at our page. If you encounter any bugs with either of these versions, please file them on the github page. If you have any questions or concerns, do no hesitate to reach out on or our discussion . ","locales":"","title":"Kibana 4.4.1, 4.3.2, and 4.1.5 released"}
{"index":{}}
{"author":"Jason Dickson","category":"Culture","publish_date":"2016-02-10T00:00:00.000Z","url":"/blog/what-its-like-to-attend-elastic-training","seo_title":"What It's Like to Attend Elastic Training","content":" The best thing that I heard all day was, \"This class isn't going to be too technical.\" Unlike many of you reading this post, I don't have a programming background. I'm a writer-editor guy and I'm new to both Elastic (the company) and the Elastic Stack — you know, Elasticsearch, Logstash, Kibana, and Beats. The gentleman who made the statement above was , software engineer, a core Logstash developer, and one of two official Elastic instructors for the day-long course I was taking, . After introducing himself, Kurt also provided a quick overview of what we would learn in the sunny conference room, which was full of IT, operations, and engineering pros. And there I was, an Elastic employee seeking a better understanding of our open source offerings, feeling like a fish out of water among dozens of very smart people. For me, hearing that the course contained sections like \"Intro to the Elastic Stack\" and \"Understanding Event Data\" was a great relief. Kurt's co-instructor was , also an Elastic software engineer. A former biologist/ecologist, he fell in love with creating visualizations while using JavaScript at Beats Electronics and is now a Kibana data visualization engineer. A Data StoryFirst up was a cool data story using a data set about UK-based cars. I counted at least a dozen different charts and graphs created with Kibana, as Kurt talked us through interesting conclusions including a spike in car models due to the geographic distribution of the fans of legendary rally driver and taxi cabs in London with over 1 million miles on their odometers. Next, Kurt talked through the Elastic Stack overview, and although I've been part of Elastic's team since last year, I gained more understanding of the big picture. Following this, we all dove into learning about event data. Did you know that the timestamp of some logs reflect the number of seconds since the year 1970? I didn't. We also listened as Kurt explained time-based data, indexing in Logstash, and aggregations in Elasticsearch. The best takeaway for me during this was Kurt's analogy about needles in a haystack: He said that with Elasticsearch and Kibana, we can turn the simple idea of searching for a single needle upside down. Finding out the average length of every needle in the haystack is now possible, as is finding out how many needles came from this field and how many from the field a hundred yards away. The power of Kibana, and the Elastic Stack as a whole, is that ingesting, searching, and visualizing the information about the needles is easier and far less time-consuming than with other offerings. In the whiteboard diagram above, Kurt gave us an example of how you can break down aggregations into categories for use in Kibana — generic sales data, broken down by days of the week (buckets), then by total sales dollars and average sales dollars (metrics). This helped me tremendously during the afternoon session, which featured a lot of hands-on lab activities. After a little over an hour of lecture and question-answer time, Kurt said, \"OK. Non-technical people may want to close your eyes now.\" I kept mine open. I felt ready, after the extremely helpful intro presentations. Hosted ElasticsearchAfter a tasty lunch, each of the attendees received a training cluster on . Since it comes with a free instance of Kibana, this made it super easy for us to learn and explore Kibana 4 by actually utilizing the stack. For me, just logging in successfully was a \"Wooooo!\" moment. I even helped a couple of classmates who got lost. (It helped me to look at the presentation slides before the class — they are provided to attendees in advance.) Highlights of the Kibana CourseBy the time we began learning how to use , I was doing great. Inverting filters is awesome:  on a given data set, there were lots of geographic locations that you could sift through, so I inverted a search for the U.S. city of San Antonio","locales":"","title":"What It's Like to Attend Elastic Training"}
{"index":{}}
{"author":"Martijn Laarman","category":"Engineering","publish_date":"2016-02-09T00:00:00.000Z","url":"/blog/ga-release-of-nest-2-0-our-dot-net-client-for-elasticsearch","seo_title":"","content":" This marks the first GA release of our 2.0 client with well over a 1000 commits since 1.7.1 (the currently last released NEST version in the 1.x range). Back to the drawing boardWe took some time to go back to the drawing board. NEST was originally started in 2010 and there are many choices that have accumulated in the code base that don't make sense anymore. So we stepped back to properly formalize how we see the lifetime of a call and worked off of that. Armed with the following diagram, we completely rewrote NEST's internals:  The old based code is now replaced with with a much saner approach to exceptions and errors in addition to exposing enough information as an audit trail so you don't ever have to guess what went down during a call. Our internals now also reflect this: This pipeline now handles all of the failover/sniffing/pinging logic and directly reflects the flow diagram. We also simplified down just 2 methods. This means the outer edges are clean ( and ) and implementing your own should be really really simple. All of these (and also and ) can be injected on the constructor of the client. Test FrameworkAnother huge endeavour is the rework of our test framework. NEST 1.x was always well tested but used 5 different test projects and 5 years worth of changing our minds as how best to write tests and assertions, thus becoming a big hodgepodge of , , , combined with several different ways to compare json with object graphs and vice-versa. Trying to write a new test quickly became because there was no clear cut way for how best to write said test. So the first thing we did as part of our 2.0 branch was to completely delete all of our tests. This gave us carte blanche during our rewrite. As of 2.0, we have one test project, , with all tests written in such a way that they can be run in unit test mode and integration test mode. . All of the API endpoint tests test all four request variations - two DSL's (using fluent and object initializer syntax) with both synchronous and asynchronous variations of each. We also test all of the moving parts of the Elasticsearch DSL (Aggregations, Sorting, IndexSettings, etc) in the same way. In addition to the more formal unit and integration tests, we also implemented a thing we dubbed to allow us to write tests in a more story telling form, with multi-line comments serving as the narrative for our asciidoc documentation while using the to pick out the interesting bits of code. This gives us the benefit of always compiling our documentation in addition to having only one place where we document, test and assert how a piece of code is supposed to work. Another huge component of our testing framework is the that allows us to write tests for any situation and how we expect the client to behave. For example: /** we set up a 10 node cluster with a global request time out of 20 seconds. * Each call on a node takes 10 seconds. So we can only try this call on 2 nodes * before the max request time out kills the client call. */ var audit = new Auditor(() => Framework.Cluster .Nodes(10) .ClientCalls(r => r.FailAlways().Takes(TimeSpan.FromSeconds(10))) .ClientCalls(r => r.OnPort(9209).SucceedAlways()) .StaticConnectionPool() .Settings(s => s.DisablePing().RequestTimeout(TimeSpan.FromSeconds(20))) ):  audit = await audit.TraceCalls( new ClientCall { { BadResponse, 9200 }, //10 seconds { BadResponse, 9201 }, //20 seconds { MaxTimeoutReached } }, /** * On the second client call we specify a request timeout override to 80 seconds * We should now see more nodes being tried. */ new ClientCall(r => r.RequestTimeout(TimeSpan.FromSeconds(80))) { { BadResponse, 9203 }, //10 seconds { BadResponse, 9204 }, //20 seconds { BadResponse, 9205 }, //30 seconds { BadResponse, 9206 }, //40 seconds { BadResponse, 9207 }, //50 seconds { BadResponse, 9208 }, //60 seconds { HealthyResponse, 9209 }, } ):  This showcases the tests combined with an","locales":"","title":"GA Release of NEST 2.0, our .NET client for Elasticsearch"}
{"index":{}}
{"author":"Suyog Rao","category":"The Logstash Lines","publish_date":"2016-02-09T00:00:00.000Z","url":"/blog/logstash-lines-2016-02-09","seo_title":"","content":" Welcome back to The Logstash Lines! In these weekly posts, we'll share the latest happenings in the world of Logstash and its ecosystem.This week, we are super excited to report that dynamic config reloading feature has been merged to master. With this feature, any config changes made to the file will be picked up dynamically, and the internal pipeline restarted to apply these changes. This means Logstash as a process does not need to be restarted to update configuration. To enable this, run LS with:bin/logstash -f config_file --auto-reload --reload-interval 2 is how often LS watches the config file for changes, defaulting to 3 secondsThis is also getting back ported to 2.3 release, so look out for that!2.2.0 and 2.1.2 ReleaseAs part of release bonanza, LS 2.2.0 and 2.1.2 was released with new pipeline architecture being the highlight among other features/bugs. See for details. A user reported a with the new pipeline which affects a subset of LS filters (metrics, multiline, etc). These filters use flushing logic to periodically emit events which are in-flight. This has been fixed and we're targeting a 2.2.1 release this week.LS MetricsThe aim of this project is to expose internal metrics of Logstash using an API. Follow for more details.Others:Many exciting things are in store for Logstash and we'll talk about that in detail at our user conference - . Our entire engineering team will be at this conference, so please come by and say hi! We'd love to talk about roadmap, issues and upcoming features. See you in San Francisco! ","locales":"","title":"The Logstash Lines: 2.2 Release, Dynamic Config Reloading"}
{"index":{}}
{"author":"Dimitrios Liappis","category":"Engineering","publish_date":"2016-02-08T00:00:00.000Z","url":"/blog/create-an-ami-from-your-own-vm-image","seo_title":"Create an AMI from your own VM image","content":" Creating your own AMI from scratchContinuing the trend of AWS-related articles, this time I am looking at how to generate an Amazon Machine Image (AMI) from an ISO source. Why?There are a large number of publicly available AMIs. For example the ones owned by Amazon can be searched per region using aws --region eu-central-1 ec2 describe-images --owner amazon Those AMIs can be used as a basis for customized AMIs, generated for example using the excellent builder. Some times though there is a need to create an AMI from an existing virtualization source e.g. a file from VirtualBox. It would be great if we could create our customized OS image either through an interactive installation or in an automated way via Packer and import it as an AMI in EC2. It turns out that this is possible using . Prerequisites and limitations Manual stepsWith the prerequisites satisfied the process is: Automating the processThe above steps can be tedious and since I needed to import vagrant boxes, I created a tool to automate this: The required parameters are: The s3bucket and the (temporary) key used for uploading the VM. is optional but if you omit it, expect a certain naming convention like For example ./oel7.1-x86_64-virtualbox.box is a valid name. Displays progress statistics. Very useful if the script is not run from another program. By default it will create copies of the temporary AMI that AWS import-image creates in three regions -- us-east-1, us-west-2, eu-central-1. It's easy to add or remove destination regions in ExampleFor an existing oracle linux vagrant box: $ ./amiimporter.py --s3bucket mybucket --vboxfile ./oel7.1-x86_64-virtualbox.box --verbose INFO:root:Uploading ./tmpdir/packer-virtualbox-iso-1453910880-disk1.vmdk to s3 99% INFO:root:Running: aws --region eu-west-1 ec2 import-image --cli-input-json {\"Description\": \"temp-hvm-oel-7-20160129134521\", \"DiskContainers\": [{\"UserBucket\": {\"S3Bucket\": \"mybucket\", \"S3Key\": \"temp-hvm-oel-7-20160129134521\"}, \"Description\": \"temp-hvm-oel-7-20160129134521\"}]} INFO:root:AWS is now importing vdmk to AMI. 98% INFO:root:Done, amiid is ami-7c7bcc0f INFO:root:Successfully created temporary AMI ami-7c7bcc0f Created ami-8d322ae1 in region eu-central-1 Created ami-8d16f1ed in region us-west-2 Created ami-89e4c9e3 in region us-east-1 INFO:root:Deregistering temporary AMI ami-7c7bcc0f INFO:root:Deleting updateded s3 file s3://mybucket/temp-hvm-oel-7-20160129134521 WarningsIf you use a Vagrant box as a source for your AMIs make sure that the vagrant user does not have the default password and/or insecure key as otherwise your deployed instances will be easily hacked. Also, depending on the distribution, may be unable to resize your root partition automatically. This may come handy. ConclusionDespite the fact that AWS public AMI store is very rich in images, there are always corner cases where you need to create something from scratch. I hope the process illustrated above is not too daunting and perhaps the included script will make it even easier! ","locales":"","title":"Create an AMI from your own VM image"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2016-02-08T00:00:00.000Z","url":"/blog/weekly-beats-1-1-0-released","seo_title":"","content":" Welcome to Weekly Beats! With this series, we're keeping you up to date with all that's new in , from the details of work in progress pull requests to releases and learning resources. Early last week we released Beats 1.1.0 together with the first version of Winlogbeat. You can read more details about it on the . Other notable things going on: Add support for Redis and Kafka outputsCurrently you can configure Elasticsearch, Logstash and Redis as outputs. Redis became deprecated after we added support for Logstash as you can simply use the to dump your data to Redis. What happened was that a lot of people deployed it like this: Beats -> Logstash -> Redis -> Logstash -> Elasticsearch We received many requests from the community to add support for more outputs, besides Elasticsearch and Logstash to avoid having an additional Logstash instance to translate the data to the desired queuing system. After internal discussions, we decided to follow the community feedback, so we will un-deprecate the Redis output and add support for the Kafka output. So the deployment scenario becomes: Beats -> Redis/Kafka -> Logstash -> Elasticsearch More details can be found in the . Merge together the scripts for generating docs and templateEach Beat had two scripts to generate the Elasticsearch template and the documentation based on the fields.yml. The fields.yml file contains details about all exported fields and it is located in each Beat repository. The scripts were sharing a lot of common code among all the Beats, so we decided to them together into two generic scripts that are able to generate the template and the documentation no matter what the Beat is. The scripts are now part of libbeat and they can also be used by the community Beats. ","locales":"","title":"Brewing in Beats: Kafka output"}
{"index":{}}
{"author":"Michael McCandless","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2016-02-08T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-query-profiler-geopoint-fields","seo_title":"This Week in Elasticsearch and Apache Lucene - Query Profiler and Geopoint Fields","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top NewsElasticsearch 2.2.0 released with a query profiler and supercharged geopoint fields. Already available on Found— elastic (@elastic) Elasticsearch Core Apache Lucene Watch This Space, where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - Query Profiler and Geopoint Fields"}
{"index":{}}
{"author":"Megan Wieling","category":"","publish_date":"2016-02-08T00:00:00.000Z","url":"/blog/witwies-drupalgov-canberra-2016","seo_title":"Where in the World is Elastic? - DrupalGov Canberra 2016","content":" Welcome to Find out which Elastic events and meetups are happening near you this week.  Upcoming Events February 8: Upcoming Meetups February 8: February 8: February 9: February 9: February 10: February 13: February 14: That's it for this week. Stay tuned next week for more Elastic happenings. - The Elastic Team P.S.  if you're interested in hosting a meetup or are giving a talk about Beats, Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic? - DrupalGov Canberra 2016"}
{"index":{}}
{"author":"Kristina Frost","category":"Culture","publish_date":"2016-02-08T00:00:00.000Z","url":"/blog/calling-all-lady-avengers","seo_title":"","content":" Community.Community is a word that means a lot to all of us here at Elastic, be it in regards to the community of engineers who use and even contribute to our Stack, or our community of over 300 employees scattered across the globe. I think often about the communities I consider myself a member of, and how they might keep getting stronger, keep getting better. Better is a tricky word and it can look like a lot of things: it can look like more fun, for instance, with my friends:  it can look like more efficiency, for my team:  but for the context of this blog post, it looks like more diversity for my company and for our industry. I’m not sure when, exactly, it became apparent to me that membership in certain communities afforded the people within them more privilege and power than those without. This is not knowledge we’re born with, after all. Kids have unparalleled imaginations, and those imaginations usually don’t come with glass ceilings. The ceilings come later. At some point, I assented to the fact intellectually: I knew, for example, that certain schools had better reputations than others:  I knew that my own race and socio-economic class afforded me luxuries that not all of my peers received. Maybe because I was so privileged in that respect, the gender divide didn’t feel like a real limitation for me for a long time. Emotional assent to that fact came along much, much later: when I’d entered the workforce, and the stereotypes so many of us are familiar with started to appear in a real way, like, for example, that in nearly all of my performance reviews I’ve been applauded for passion, efficiency, and leadership whilst simultaneously being hit with that word we seem to use only for women: “bossy.” I know I’m not alone. I know the science, now:  all of the sociological work that tells us that the hurdles put in front of women in the workplace are very real, and that the hill we climb is very steep.I came to Elastic from a big company, and part of the reason why I was so excited to come here was the prospect of building things for the first time, from the ground up. Not because I think I can do so flawlessly, but because whatever mistakes I make, and whatever mistakes my team makes, they’ll be new mistakes, our mistakes, mistakes that we own:  not mistakes we’re inheriting that were set in place five or ten years ago, or processes still tied to “” One of these things, and I’m excited to introduce you all to it, is our “Lady Avengers,” a name we’ve basically made up because of the company love of superheroes. A recent gathering of the Mountain View Chapter of the Lady Avengers Who are the Lady Avengers? We’re  every female employee of Elastic, and we meet monthly to talk about what it’s like to be a woman who works in the technology space, and what it’s like to be a woman who works for Elastic, and how we can make both of those things better. We haven’t been at this for very long, so we’re a scrappy bunch of superheroes, and we’re just getting started when it comes to deciding what kinds of things we want to do and build for the very first time. It’s my pleasure on behalf of all of us to invite those of you who are attending Elastic{ON} to join us for our Lady Avengers Breakfast, on Thursday morning at 8:45 AM.What is the Lady Avengers Breakfast? Think of it as one part community kick-off, one part guided meet-and-greet, and one part … well, breakfast. It’s hosted by us, and we’ll be breaking up into themed groups, to host roundtables on things like working with emerging technologies, culture and diversity crafting, professional development, or even just as a place to share experiences and network. Women from all parts of Elastic will be there to participate:  from every department from Engineering to Sales, from all around the world, including many of our recruiters and our new VP of Human Resources (one of our newest Lady Avengers!). So if you’re attendin","locales":"","title":"Calling All Lady Avengers"}
{"index":{}}
{"author":"Martijn van Groningen","category":"Engineering","publish_date":"2016-02-03T00:00:00.000Z","url":"/blog/securing-fields-and-documents-with-shield","seo_title":"","content":" Securing data with Shield was already possible at the index level by defining privileges for indices and aliases via Shield's . Since Shield 2.0 data can be secured at an even lower level, up to the field and document levels inside an index, let's set up field level security for an imaginary ticketing platform. Assume the following: { \"subject\" : \"Missing emails\", \"message\" : \"Last week when...\", \"severity\" : \"low\", \"time_spent_in_minutes\" : 5, \"escalated\" : false, \"private_notes\" : [\"This is likely caused by a bug\"] } There are two kinds of users using this data, the customers creating tickets and the support engineers interacting via the ticket system with the user and eventually resolving the ticket. Both have access to the same tickets, but customers shouldn't have to see all properties of a ticket. The ‘time_spent_in_minutes', ‘escalated' and ‘private_notes' are properties of the ticket data that are private to the only support engineers and these fields can be made hidden and inaccessible by enabling field level security for the ‘customer' role: customer: indices: 'ticket_index': privileges: read fields: - subject - message - severity As can been seen in the example ‘roles.yaml' file is that enabling field level security is as easy as defining a list of fields that are accessible for a role. Configuring document level security is similar, only then a query needs to be defined that includes documents that are accessible. More details about how to configure field and document level security can be found in the  documentation. The need for lower level access controlThere are many other use cases can benefit from controlling access to the data on the field and document level. For example when data is shared across many organizations or departments within an organization, but not all parties involved are allowed to see all properties of the data. Before Shield 2.0 data would have to be duplicated. This would mean that each department in an organization would have its own index and then in Shield each department role would only allow access to their own index. With field and document level security duplicating data is no longer needed. All departments will share the same index and each department role will have a list of allowed fields and optionally a query that dictate the visible fields or documents. When field and document level security is enabled it is applied for all Elasticsearch read APIs, in a secure manner, so how could this implemented? Filter response approachA naive approach would be to filter responses. Each api that returns data would need to check if keys or values inside the response are allowed to be returned. This might work out for filtering fields or documents that aren't visible in the search and get APIs, but doesn't prevent someone from running a query or an aggregation of a field that isn't visible. For example the total hit key in the search response would then still indicate that there is more than is visible. So in order for field and document access control to work correctly, the request needs to be filtered too, which means that queries and aggregations on not allowed fields need to be removed. By removing disallowed queries and aggregations this means the request needs to rewritten before execution and this is harder than it looks, especially if a search request contains many compound queries and complex aggregations. Also with what query should a disallowed query be replaced with? And if this modified request is ran through the explain or profile api how would that look? Also how would field or document level access control be implemented in other read APIs? There are many APIs in Elasticsearch and not all APIs are structured in the same way. On the ES side this would require to do access control checking in many different places. Data security leaks should be avoided at all cost and with this approach there is a high chance that th","locales":"","title":"Securing Fields and Documents with Shield"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2016-02-02T00:00:00.000Z","url":"/blog/elasticsearch-2-2-0-and-2-1-2-and-1-7-5-released","seo_title":"Elasticsearch 2.2.0, 2.1.2, and 1.7.5 released","content":" Today we are pleased to announce the release of based on , and bug fix releases of , and . All of these new releases are already available on , our Elasticsearch-as-a-service platform. This week’s release bonanza also includes new versions of , , , and .Elasticsearch 2.2.0 contains two awesome new features: a and greatly improved . It ships with and contains an important fix for a bug which caused slow shard recovery in Elasticsearch 2.1.0, along with .All users are encouraged to upgrade.Latest stable release:Bug fixes in 2.1:Bug fixes in 1.7.5: ","locales":"de-de,fr-fr,ja-jp,ko-kr","title":"Elasticsearch 2.2.0, 2.1.2, and 1.7.5 released"}
{"index":{}}
{"author":"Court Ewing","category":"Releases","publish_date":"2016-02-02T00:00:00.000Z","url":"/blog/kibana-4-4-0","seo_title":"","content":" Kibana 4.4.0, with support for Elasticsearch 2.2.0 has landed. There are oh so many changes and we totally understand if you can't contain yourself and are just too impatient and just absolutely need to download it right now. If you’re already sick of all these words, grab the right now. But hey, you might want to try some breathing exercises, slow down a bit. Now then, take a deep breath, and walk with me through the magical forest of new features. So many colors You want to select specific colors for your visualizations? Done. Click the legend for the value you want to change and BEHOLD the color palette! Not only that, but you can now filter legend values in or out with the click of a button. Shared URL shorteningDon't you love massive URLs? No? Huh, ok, well that explains this new feature. Kibana now has a built in URL shortener. Just click the shorten button and tah-dah, easily shareable tiny URLs RPMs, DEBs, oh my! Get ready to simplify those deployment scripts, Kibana 4.4.0 is available as an RPM and Debian (and Ubuntu!) packages. Check out the  for repo info! Shield your eyes!But that’s not all! If you act now (or any time from this point onward), you can also start using the brand new , which is an infinitely better way to handle user logins. All that and more! Bug fixes Plugin stuffWell aren't you ambitious? We added some new plugin functionality. Feel free to hack around and ping us in #kibana on irc.freenode.net if you have questions. Go get it!That's it, that's all. You don't have to go home, but you can't stay here. Well, you can, but there's nothing else to read. And come to think of it, you should probably only go home if its not the middle of the work day for you. We're totally not taking the blame for you getting in trouble. Ok, so instead of going home, how about you upgrade to Elasticsearch 2.2.0 and ? If you make anything cool, share it with us . ","locales":"","title":"Kibana 4.4.0 is eye meltingly colorful"}
{"index":{}}
{"author":"Monica Sarbu","category":"Releases","publish_date":"2016-02-02T00:00:00.000Z","url":"/blog/beats-1-1-0-and-winlogbeat-released","seo_title":"","content":" Today, we announce new versions of the entire Elastic Stack, including a tighter  and an updated version of ES-Hadoop. Detailed blogs for product releases are available in the of the blog. And yes, the blog has categories -- you know, for searchability.Here are the highlights from the . For more details check out the . Winlogbeat: A new beat for Windows Event logsYou like Filebeat, but you want to also send Windows Event logs to Elasticseach? There’s a Beat for that! It’s called Winlogbeat, and you can download it from . Winlogbeat supports both the new and old styles of the Windows event log APIs, meaning that you can use it on any Windows version starting with XP. Just like Filebeat, Winlogbeat has a registry file that tracks which events were acknowledged by Elasticsearch or Logstash, so you don’t lose events in case of restarts, network partitions, or Elasticsearch/Logstash unavailability. Winlogbeat comes with a sample Kibana dashboard that you can use as a starting point for your customized dashboard. You can easily load the sample dashboards by using the load command described in the . Multiline supportA commonly requested feature for Filebeat is to be able to merge related log lines into a single event. Think of the way most applications dump their exceptions into logs. Wouldn’t it be nice to have a single event per exception? This was already possible by using the Logstash multiline codec, but for many users, it’s more convenient to configure multiline in the same file where they configure the file paths. The following example shows how to configure Filebeat to handle a multiline message where the first line of the message begins with a bracket ( ). multiline: pattern: ^\\[ negate: true match: after Filebeat takes all the lines that do not start with [ and combines them with the previous line that does. For example, you could use this configuration to join the following lines of a multiline message into a single event: [beat-logstash-some-name-832-2015.11.28] IndexNotFoundException[no such index] org.elasticsearch.cluster.metadata.IndexNameExpressionResolver$WildcardExpressionResolver.resolve(IndexNameExpressionResolver.java:566) org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteIndices(IndexNameExpressionResolver.java:133) org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteIndices(IndexNameExpressionResolver.java:77) org.elasticsearch.action.admin.indices.delete.TransportDeleteIndexAction.checkBlock(TransportDeleteIndexAction.java:75 For more details about multiline, please check the . Filtering improvementsIt’s now possible to very efficiently filter lines out in Filebeat by using regular expressions. So if you were, for example, using Logstash to drop all debug messages, you can save a lot of bandwidth and CPU power by doing the filtering at the source. You can configure Filebeat to drop all the log lines that match a certain regular expression or to include only the log lines that match a specific regular expression. To export any lines that start with or : include_lines: [“^ERR”, “^WARN”] To drop any debug lines that start with : exclude_lines: [“^DBG”] It’s also possible to ignore files based on the file names. Filebeat handles this efficiently by simply not opening the matching files. To ignore all the files that have a extension: exclude_files: [“.gz$”] Beats dashboards for windows usersWindows users are now able to load the default Kibana dashboards by using the script. It has similar options as the load.sh script for Unix systems, so you can use -url for passing the Elasticsearch URL or -user to authenticate with Elasticsearch by username and password. Send us feedback!A big Thank You to everyone who contributed code, reported issues, created a new Beat or just tried the Beats. Start your experience with Beats 1.1.0 and let us know what you think on , , or open an issue on . ","locales":"","title":"Beats 1.1.0 & Winlogbeat released"}
{"index":{}}
{"author":"Andrew Cholakian","category":"Releases","publish_date":"2016-02-02T00:00:00.000Z","url":"/blog/logstash-2-2-0-and-2-1-2-released","seo_title":"Logstash 2.2.0 and 2.1.2 released","content":" The release bonanza continues… Today, we announce new versions of the entire Elastic Stack, including a tighter of Shield with Kibana and an updated version of ES-Hadoop. Detailed blogs for product releases are available in the category of the blog. And yes, the blog has categories – you know, for searchability. Logstash 2.2.0 and 2.1.2 is available for today as part of the release bonanza! 2.2.0 release includes changes to the internal architecture of the pipeline, but is still backward compatible with old configuration. Read below for highlights, and check the for more details. Logstash 2.1.2 is mainly a bug fix release, and its changelog can be found . Next Generation Pipeline The single largest change in Logstash 2.2 release is the addition of the “Next Generation Pipeline” (also known as the “NG pipeline”). We’ve fully re-architected the pipeline to provide more performance and pave the way for future persistence efforts. Where the previous pipeline processed one event at a time, the new pipeline works in micro-batches, processing groups of events at a time. Additionally, the filter and output stages now run as separate execution phases in the same thread, vs. running in separate thread pools. Check out the diagram below for a richer view of these changes. As illustrated above, the new pipeline first has each worker accumulate a batch of events before processing the filter + output stages. This batch has a maximum size of and will wait at most milliseconds from the last event taken from the queue to proceed with filter/output processing. The new pipeline no longer has distinct threads for filtering and outputting as the old pipeline did, but rather runs the filter and output stage in sequence in each worker thread. You will most likely find that you need more worker threads to achieve the same throughput as the old pipeline–perhaps even more than the number of real CPU cores–but that these threads run much more efficiently. The reason you’ll need more worker threads is that the output stage of execution is often idle waiting on IO while talking to, say, Elasticsearch or another remote service. We’ve seen performance increases of up to vs. the original pipeline on real-world workloads. This comes from dramatically lowering overhead per-thread, and letting the OS scheduler smartly allocate resources. For an in-depth guide to tuning the NG pipeline, check out the revamped . Smarter Defaults, and Better Output management. We’ve made managing the performance of outputs much simpler in Logstash 2.2 with automatic output worker scaling. With the NG pipeline an output worker works in a fundamentally different way than previous Logstash releases. Before, each output worker would get its own thread. This led to increased cross-thread communication and context switching and was less efficient, in addition to complicating future persistence work. In the NG pipeline the number of output workers determines how many instances to fill an output worker ‘pool’ with. Output instances are pulled from this pool by pipeline worker threads as needed. In previous versions of Logstash the default number of output workers was 1. In Logstash 2.2 this number is by default the number of pipeline workers, which now default to be equal to the number of cores on the system. So, if you have a four core system expect to see four pipeline workers with four output ‘workers’ available.This is actually a conservative number for the NG pipeline, as these threads are often idle when in I/O wait. Be sure to read the for the full story on tuning values here. The internals of the output stage of logstash have changed quite a bit as well, in ways that have significant implications for both users and plugin authors. Output plugins are now encouraged to implement as the primary interface for receiving events, and are discouraged from managing their own buffers. When run with Lo","locales":"","title":"Logstash 2.2.0 and 2.1.2 released"}
{"index":{}}
{"author":"Costin Leau","category":"Releases","publish_date":"2016-02-02T00:00:00.000Z","url":"/blog/es-hadoop-2-2-0-and-2-1-3-released","seo_title":"Elasticsearch for Apache Hadoop 2.2.0 and 2.1.3 released","content":" The release bonanza continues… Today, we announce new versions of the entire Elastic Stack, including a tighter of Shield with Kibana and an updated version of ES-Hadoop. Detailed blogs for product releases are available in the category of the blog. And yes, the blog has categories – you know, for searchability. I am pleased to announce that ES-Hadoop is joining the release bonanza through the GA release of Elasticsearch for Apache Hadoop (ES-Hadoop) and the bug fix release of ES-Hadoop . As always, the artifacts are available at the and or . Highlights in ES-Hadoop 2.2 Bug-fixes aside, ES-Hadoop 2.2 introduces a series of new features: GA release compatible with ES 2.x ES-Hadoop 2.2 is officially compatible with Elasticsearch 2.x while backwards compatibility with Elasticsearch 1.X (though we really upgrading). ES-Hadoop automatically detects the target Elasticsearch version and act accordingly without any user intervention. Overhauled geo support Similar to Elasticsearch, support has been overhauled in ES-Hadoop 2.2 - not only and types are properly detected, but also their schema is inferred (despite being over a dozen data formats across both types). Network improvements ES-Hadoop 2.2 introduces support for Elasticsearch environments where access is done only through one central point. This extends the number of topologies that ES-Hadoop works with, along side client-node only and direct connection. The latter scenario has also been optimized by specifically routing traffic only to data nodes and filtering out master nodes. The configuration options have been improved to allow configuration of the JVM HTTPS proxy along with resolving of hostnames to IPs (useful when using Elasticsearch with network publishing enabled). Better runtime diagnostics To prevent user error and misconfigurations, ES-Hadoop 2.2 introduced classpath checks to make sure only one version is used at a given time:  this alleviates scenarios where different versions of the project are deployed leading to an unsupported scenario. Further more, incorrect usage of libraries (such as saving a without the Spark SQL support) are also reported. Apache Spark 1.5 and 1.6 support ES-Hadoop 2.2 tracked the releases of all its libraries, in particular those of Apache Spark, in both cases leveraging the new features added such as or the simplified “es” , both available in Spark 1.5 or eliminating (Spark 1.6). All while still with the previous versions of Spark. Such features provide not just richer constructs for the user but also improve performance by to Elasticsearch more and more of Spark SQL. Extended configuration options The support for multi-dimensional fields (arrays) has been enhanced as one can now specify upfront the dimensions for a given field (whether nested or not), quite useful in strictly typed environments (like Spark SQL) especially when the data does not conform exactly to its declaration. Additionally, options to include or exclude certain fields as long as the number of documents being read were added. YARN enhancements A batch of updates were done to the YARN module by upgrading to Elasticsearch 2.2.x and introducing the option for the JVM system properties to be passed directly to the children container. Repository HDFS is moving soon The HDFS snapshot and restore plugin (repository HDFS) has been and is undergoing a overhaul in terms of security. Shout out to for his support in making this happen. It has been quite an effort considering Hadoop is with the Java Security Manager, simply asking a plethora of permissions with many of them way too dangerous (such as execute on all permissions during a basic startup). The is for the plugin to be officially part of Elasticsearch proper as an official plugin in an upcoming release. Until that happens, it is still available as part of the ES-Hadoop project. More about it, in a future blog pos","locales":"","title":"Elasticsearch for Apache Hadoop 2.2.0 and 2.1.3 released"}
{"index":{}}
{"author":"Michael McCandless","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2016-02-01T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-cluster-cloning-hosted-elasticsearch","seo_title":"This Week in Elasticsearch and Apache Lucene - Cluster Cloning in Hosted Elasticsearch","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top NewsJust how easy is it clone your cluster in our hosted service? (Hint: very) — elastic (@elastic) Elasticsearch Core Apache Lucene Watch This Space, where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - Cluster Cloning in Hosted Elasticsearch"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2016-02-01T00:00:00.000Z","url":"/blog/weekly-beats-redisbeat-community","seo_title":"","content":" Welcome to Weekly Beats! With this series, we're keeping you up to date with all that's new in , from the details of work in progress pull requests to releases and learning resources. Last week we continue working on the big features that will be available in the next major release: add in Packetbeat and in libbeat. Beside these features, we are improving continuously the documentation based on our users feedback. The highlights this week are: New community Beat: Redisbeat is used for Redis monitoring. It is a lightweight agent that periodically reads the status from the Redis INFO command. It exports general information about the Redis server, about the clients, memory statistics, persistence statistics, redis general statistics, replication statistics, CPU statistics, Redis commands statistics, cluster statistics and keyspace statistics. Enhancements in PacketbeatWe are thrilled to receive two big pull requests this week from the community to and in Packetbeat. Big Thank you to the contributors and we will do our best to merge them as soon as possible. Enhancements in TopbeatTopbeat adds more information to the running processes. It now exports the username that started the process. This information might be useful in case the same command is started by different users and username seems to be the only way to differentiate between them in the Kibana dashboards. This enhancement is available on Unix and Windows systems and the implementation can be found . ","locales":"","title":"Brewing in Beats: Redisbeat from community"}
{"index":{}}
{"author":"Alvin Chen","category":"Engineering","publish_date":"2016-02-01T00:00:00.000Z","url":"/blog/logstash-moving-away-from-node-protocol-and-multiline-filter","seo_title":"Logstash Moving Away from Node Protocol and Multiline Filter","content":" The Logstash project has been constantly evolving and seeking out novel ways to become more robust and feature rich. At this juncture, it’s time to leave certain legacy things behind to make life easier both for Logstash users and developers. The topics to be outlined in detail here are the node protocol for Elasticsearch output and the multiline filter. Below is a brief overview of the Elastic recommended migrations for these respective components. Discouraging the Node Protocol The node protocol for the Elasticsearch output plugin has been a formidable pain for both Logstash users and developers. It’s tough to debug and difficult to maintain. In Logstash 2.0, the as it’s very fast and possesses a much better operational experience. Additionally, a is coming soon which will circumvent any use cases where the node protocol was previously used as a way to monitor Logstash node health. Lastly, the node protocol isn’t supported for communication with protected Elasticsearch clusters. Therefore, usage of the node protocol is now discouraged, and it’s strongly recommended for any current users of this protocol to migrate to the HTTP protocol in the next few months prior to the next major Logstash release. The HTTP protocol with Logstash is also the easiest way to ingest data into your clusters on - the best hosted Elasticsearch solution. For any questions or concerns, please feel free to discuss in this . Deprecating the Multiline Filter Historically, there’s been two different ways to process multiline events in Logstash. In the last couple months, the has been strengthened on various fronts, inclusive of an enhancement which reconciled the stream identity bug. In order to mitigate confusion between the usage of the codec and filter plugins, the single-threaded multiline filter will effectively be deprecated at the Logstash 2.2 GA. As multiline events are best processed earlier in the pipeline, it’s strongly recommended to migrate to the more scalable multiline codec in the next few months prior to the next major Logstash release. Additionally, if you’re using the awesome, lightweight platform for shipping your data, this can also be done on the edge with . For further details, please review this . ​Many exciting things are happening in Logstash land and we’ll be talking about them in detail at our user conference in San Francisco, CA. Please join us to learn more about the Elastic stack and network with many other enthusiastic Elastic users! ","locales":"","title":"Logstash Moving Away from Node Protocol and Multiline Filter"}
{"index":{}}
{"author":"Megan Wieling","category":"","publish_date":"2016-02-01T00:00:00.000Z","url":"/blog/witwies-oop-konferenz-2016-linux-conf-au","seo_title":"Where in the World is Elastic? - OOP Konferenz 2016 & linux.conf.au","content":" Welcome to Find out which Elastic events and meetups are happening near you this week.  Upcoming Events February 1-5: February 1-5: Upcoming Meetups February 2: February 3: February 3: February 4: February 5: February 1: February 2: February 2: February 4: February 2: That's it for this week. Stay tuned next week for more Elastic happenings. - The Elastic Team P.S.  if you're interested in hosting a meetup or are giving a talk about Beats, Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic? - OOP Konferenz 2016 & linux.conf.au"}
{"index":{}}
{"author":"Konrad Beiske","category":"Engineering","publish_date":"2016-01-29T00:00:00.000Z","url":"/blog/cluster-cloning-in-the-cloud","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud.One of the advantages of using a SaaS offering is flexibility for on-demand scaling. With Found this means you can create and delete clusters as you like and only pay for the hour APIs they’re running. Building on the Snapshot and restore API of Elasticsearch, Found makes it easy to duplicate a cluster. In this blog post we will see how this works and how it may be used in a few different scenarios. Finally we will have a brief look at how it is implemented which will also serve to show why the clone operation has absolutely no performance impact on the source cluster. Use casesA common denominator in all of these use cases is that they benefit from the isolation that a separate cluster provides, whether that is isolation of hardware resources or data resources. Ad hoc analyticsFor most logging and metrics use cases it is cost prohibitive to have all the data in memory, even if that is what provides the best performance for aggregations. Cloning the relevant data to an ad hoc analytics cluster that can be discarded after use can be a very cost effective way to experiment with your data, and at the same time without risk of hurting performance in production. Test upgradesThe safest way to check that both your indexes and your application is ready for the next Elasticsearch version is to copy the indexes to a new cluster and test the entire upgrade path by first upgrading Elasticsearch and then making sure that your application works. Enable your developersRealistic test data is crucial for uncovering unexpected errors early in the development cycle. What can be more realistic than actual data from the production cluster? Giving the developer team access to experiment with real production data is not only great for breaking down silos, but may also provide new insights about the domain. A safe and isolated playground is the essential benefit for this use case. Test Mapping changesMapping changes almost always require reindexing. Unless your data volume is trivial it also takes some time. Tweaking the parameters to achieve best performance usually takes a little trial and error. While this use case could also be handled by running the scan and scroll query directly against the source cluster, it is worth noting that a long lived scroll has the side effect of blocking merges even if the scan query is very light weight. Integration testingTest your application against a real live Elasticsearch instance with actual data. If you automate this, you could also aggregate performance metrics from the tests and use those metrics to detect if a change in your application has introduced a performance degradation. How to clone a clusterThe steps required: Prepare the target clusterIn order for the restore to be successful the target cluster must meet the following conditions: Both clusters have to be in the same region in order to have access to each others backups. In most cases any Elasticsearch version equal to or higher than the version of the source cluster will be compatible. The exception is if upgrading more than one major version at a time as Elasticsearch then might not have a Lucene version capable of reading the indexes. Same thing happens when an index originally created on an Elasticsearch version older than the current version have segments written in an old Lucene format, but this is usually not a problem as the the format is upgraded automatically during merges. The target cluster is not required to have all the plugins and dictionaries of the source cluster, just the ones required to open the cloned indexes. In practice this means any dictionary or synonym file referenced by a custom analyser in any of the indexes, aka user bundles on the cluster config page. Slightly less common, but just as hard a requirement is if any of the indexes use a custom field type provided by a ","locales":"","title":"Cluster cloning in the cloud"}
{"index":{}}
{"author":"Andrew Cholakian","category":"Engineering","publish_date":"2016-01-28T00:00:00.000Z","url":"/blog/using-nmap-logstash-to-gain-insight-into-your-network","seo_title":"","content":" In this post we’ll look at a brand new logstash codec plugin: . This plugin lets you directly import scan results into Elasticsearch where you can then visualize them with Kibana. Nmap is somewhat hard to describe because its a sort of swiss army knife of network tools. It crams many different features into a single small executable. I’ve put together a small list of things you can do with Nmap below, though it is by no means complete! Using Logstash, Elasticsearch, and Kibana you can create neat dashboards, like the one I have for my home LAN below: Monitoring Host Availability with Nmap Let’s start by just poking around with some Nmap basics. Let’s say we simply want to check if a host is up or not with an ICMP ping. We can do this by simply running . You should see the output below: You may be wondering why we need to run our ping command with . The reason is that ICMP ping packets be sent as root, otherwise the method will be used. This is on most platforms. While the output here is nice and human readable, it is not something Logstash can parse. To get machine readable XML output you’ll need to use the option, which if is used for the filename redirects to stdout. Let’s try running . You should see the same output, but with more verbosity and in XML format. Now that we’ve got our bearings let’s setup a Logstash server to receive this data. To understand this setup let’s quickly recap what a Logstash codec is. Logstash codecs simply provide a way to specify how raw data should be decoded, regardless of source. This means that we can use the Nmap codec to read Nmap XML from a variety of inputs. We could read it off a message queue or via syslog for instance, before passing the data on to the Nmap codec. A very flexible solution for a lot of people is to use the . This input sets up a webserver inside the logstash process which listens for requests and turns the request body into a Logstash event–in our case using the nmap codec. For now we’ll use a the output on to let us see the parsed nmap data. Try out the config below: Then, in your Logstash folder, run . Once you have the Nmap codec installed you can start logstash with . Logstash is now ready to watch for Nmap XML on port 8000. You can send a simple ping by running the following in your shell. We’ll use cURL to transport the Nmap XML to Logstash. Note that we’re using with Nmap to send XML to stdout, and to set stdin as the request body for our request. We’re also setting a custom header: .The Logstash HTTP input will make this header available to us as part of our events. After sending this request you should see a bunch of output in your terminal from logstash. There should be two Logstash events in your terminal now, one with a of , the other (note that this ‘type’ field is distinct from the Logstash convention of ‘@type’). You should also see a bunch of HTTP metadata from the HTTP input, including our custom header. Nmap Codec Event Types OK! So, now we know how to get some basic info out of nmap. Let’s take a deeper look at the data coming out of the Nmap codec, which does some restructuring and denormalization of the Nmap XML. Since we did a ping scan these were the only types of event created. However, for richer Nmap scans more types are created, the types are listed below: A Small Network Monitor Using the Elasticsearch output and Kibana we can setup a more fully featured example. This is something I run on my own home network to check a few different things. Here we’ll use some of Nmap’s more powerful features, the ability to target an entire subnet at once. My home network runs on the subnet 192.168.1.0/24, for instance. We can turn all pretty much all the useful options with the flag, which will per the giving us the command below. . In the line above we’ve used a different descriptive HTTP header in this example. We could use this in our Logst","locales":"","title":"Using Nmap + Logstash to Gain Insight Into Your Network"}
{"index":{}}
{"author":"Tudor Golubenco","category":"Brewing in Beats","publish_date":"2016-01-27T00:00:00.000Z","url":"/blog/weekly-beats-generator","seo_title":"","content":" Welcome to Weekly Beats! With this series, we're keeping you up to date with all that's new in , from the details of work in progress pull requests to releases and learning resources. Quite a few things going on while we’re working on the next major and preparing for Elastic{ON}. This week also brings us a new community Beat. Beat generator: template for creating a new Beat We started , a new project that makes it a lot easier to start a new Beat. One command and you get all the boiler code required, the Makefile to build it, a correct Go dependency setup and a way to keep up to date with the libbeat changes (: ). We hope to not only make it easier to start new Beats, but also to have a more unified dev process around the community Beats.   If you plan to create a new Beat, we recommend using it already. New community Beat: Elasticbeat It was bound to happen, a community Beat to monitor Elasticsearch: . We’ll be following this one for sure. Improved system testing Also on the theme of making it easier for the community Beats creators, the system tests were refactored to avoid duplication and to be importable from new Beats. In addition, they were improved to automatically fail on panics in the logs or on non-zero exit code from the Beat. Special thanks go to community contributor , who proposed several improvements and helped with implementing them. Filebeat: introduced close_older setting The ignore_older setting in Filebeat used to do , which used to cause issues because sometimes there was no single value convenient for both. This splits them and also changes the defaults. ignore_older is now disabled by default and close_older is set to one hour. Topbeat: added support for capturing the full command line Topbeat used to only collect the process name, now it captures the full , which was an often requested feature request. This also works on Windows! Performance improvements in the publisher pipeline We’ve talked in the past about the performance issues we’re having on the “at-least-once” communication between Filebeat  and Logstash. It’s a fairly difficult issue because the Beat needs to balance between sending large batches fast to increase throughput and being able to reduce the output when Logstash is busy. We refactored the code and while waiting for the current one to be ACKed. This helps with the overall throughput by enabling load balancing between multiple output threads, at the cost of memory usage. With the right settings and enough memory and CPU power, we’ve seen Filebeat pushing around 45K events/s, compared to around 18K before this change. Freebsd and Solaris are now part of our CI Jenkins now runs the tests on these two new platforms, which is the first step in supporting them. All tests from Filebeat and Packetbeat are passing already. Topbeat would require more work. Work in progress on generic filtering in libbeat Monica is making on the generic filtering feature in libbeat, from which all Beats are expected to benefit. For the moment it is possible to filter fields from a generic event, which already covers a lot of the feature requests that we received. Work in progress on Packetbeat flows Steffen on a very promising Packetbeat feature, ability to extract information about TCP/UDP/TLS flows for which we don’t understand the upper layers. This should open a new set of possible use cases for Packetbeat. ","locales":"","title":"Brewing in Beats: Beat generator"}
{"index":{}}
{"author":"Georg Kostner","category":"User Stories","publish_date":"2016-01-28T00:00:00.000Z","url":"/blog/elastic-stack-at-wurthphoenix-neteye","seo_title":"WÜRTHPHOENIX NetEye: Our Elastic Stack Story","content":" This article was published on on January 27. Market Requirements – Why Log Management?It all began with the new decree (the \"Garante per la protezione dei dati personali\") issued by the Italian data protection authorities in 2008. This regulation ( | ) stipulates that all companies must log all administrators' system access data and keep them archived for at least six months. This approach is intended to facilitate and standardize the monitoring of system administrators' activities and, above all, protect sensitive company data. In other words: Security Auditing. Carrying out a detailed analysis of the requirements stipulated by the data protection authorities allowed us to identify the following four categories: In 2010, we were therefore faced with the challenge of providing our customers with an appropriate solution for all these categories within our NetEye IT Systems Management solution. Existing Options – What was already available in the world of open source?As our NetEye monitoring tool is based on a number of open source modules, it seemed a natural first step for us to take an in-depth look at the existing options in the world of open source. We wanted to find out about the tools that were already available for gathering log data. Our research revealed that Snare and Epilog were suitable for collecting logs. This combination presented some limitations, however: After analyzing these weaknesses, we came to the conclusion that we were not prepared to put up with such shortcomings. We therefore decided to develop our own agent. The SAFED (ecurity uditing orwardr aemon) agent we created was based on Snare and Epilog and was made available to the community as a new option for collecting logs. []. We decided to use rsyslog to capture events in Linux. In the first version, we settled on Solr from Apache for the indexing of logs. We also developed our own interface to search logs via Solr. All this meant that we were extremely well equipped to face the demands of 2010. Added Value – We want more!Although our solution enabled compliance with the prescribed directives, it did not present any particularly large advantages for IT management. We were therefore very keen to develop our solution further so that it would provide our customers with additional benefits in terms of IT service management. Customer feedback allowed us to get to know the more sophisticated requirements in the world of log management and security information and event management (SIEM). Before we knew it, our task was no longer just about gathering and archiving logs. Instead, we were faced with a new list of demands: The combination of the SAFED agent, rsyslog and Solr was no longer sufficient. We thus began searching once again for suitable tools to adapt NetEye to the new market requirements. We came across the Elastic Stack in January 2014. Our developers spent a good amount of time evaluating it and detected an opportunity to expand NetEye into a fully-fledged log management solution with the help of the Elastic Stack. We elected to integrate the Elastic Stack into NetEye. The main reasons for this decision were: [To be exact, in the interim we used Grok as a parser, which was technically quite complex. When Grok was integrated into Logstash, it became easier to use. This was a further argument for the integration of the Elastic Stack.] Our web search interface was replaced with Kibana. Solr gave way to Elasticsearch. From that point on, we used Logstash as a log parser. In addition, Elasticsearch allowed us to carry out aggregation and indexed searches.  The only thing we still lacked for the essential SIEM functions was an event handler, an element that reacts proactively to event inputs and, depending on the type of incident involved, triggers a specific action. We developed the NetEye Event Handler to gather Syslog events, e-mails, SNMP traps,","locales":"de-de,fr-fr","title":"WÜRTHPHOENIX NetEye: Our Elastic Stack Story"}
{"index":{}}
{"author":"Michael McCandless","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2016-01-25T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2016-01-25","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top NewsBuilding an image search engine with deep learning and : — elastic (@elastic) Elasticsearch Core Apache Lucene Watch This Space, where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2016-01-25"}
{"index":{}}
{"author":"Megan Wieling","category":"","publish_date":"2016-01-25T00:00:00.000Z","url":"/blog/witwies-osc2016-enterprise","seo_title":"Where in the World is Elastic? - OSC2016.Enterprise","content":" Welcome to Find out which Elastic events and meetups are happening near you this week. Upcoming EventsJanuary 29: Upcoming MeetupsJanuary 25: January 26: January 27: January 27: January 25: January 25: January 27: January 28: January 31: January 28: That's it for this week. Stay tuned next week for more Elastic happenings.- The Elastic TeamP.S.  if you're interested in hosting a meetup or are giving a talk about Beats, Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic? - OSC2016.Enterprise"}
{"index":{}}
{"author":"Michelle Carroll","category":"Culture","publish_date":"2016-01-21T00:00:00.000Z","url":"/blog/share-your-community-spirit-with-elastic-snap","seo_title":"Share your community spirit with #ElasticSnap","content":" Here's something you may not know about Elastic: During new employee training, we have a fun tradition of sharing pics of ourselves with the theme of an . It's inspired by our CTO's affinity for black t-shirts, and the fact that all the newbies have just received their first wearable Elastic swag. All around the world, folks sport their Elastic gear in cool locations and share photos over chat. Our company is pretty distributed, and it's awesome to get a visual of all the different people (and places) flying the Elastic flag. With coming up fast, we're expanding this tradition to help visualize the Elastic community. After all, the event is the single largest face-to-face gathering of Elasticsearch users and knowledge. We want to see (and share) your best Elastic in the wild photos. How to participateWe're collecting high-res photos of the Elastic community using Elasticsearch, working on Elastic Stack projects, or just having fun, all while our t-shirts, logo stickers, or other be-clustered items are on display. There are two ways to join in on the fun: As always, only submit photos that you are comfortable (and have permission) to share publicly. Creativity is greatly appreciated – and welcomed :). Five participants will be randomly chosen to receive an Elastic cluster t-shirt, so get snapping! ","locales":"","title":"Share your community spirit with #ElasticSnap"}
{"index":{}}
{"author":"Nik Everett","category":"Engineering","publish_date":"2016-01-21T00:00:00.000Z","url":"/blog/elasticsearch-queries-or-term-queries-are-really-fast","seo_title":"Elasticsearch Queries, or Term Queries are Really Fast!","content":" I remember a couple years ago I was at a convention talking to folks about Elasticsearch or Lucene and one of them said something like \"Everyone knows that if you really want to search fast you need to just use term queries.\" He was certainly right about term queries being fast but I really wanted to run some numbers and see just how right he was. And that felt like a good time to write a blog post to help with the \"everyone knows\" part. Term queries are really fast Term queries are pretty fast! On my development desktop with a single spinning disk I get 8,000 searches containing a single term query a second. Once you start adding multiple terms to the query it gets faster if they are ed together and slower if they are ed together. This is because OR queries find more hits and scoring and counting those hits takes more time. We can control for this using . If is set then each shard stops searching as soon as it has hit that many results. If the document that would have had the best score wasn't hit because the shard terminated early then it's just not returned. If you are sorting by something other than score this is still a problem because the documents aren't encountered in a useful order. It also breaks the total hit count because each shard stops counting after hits. On the other hand it really improves the performance of unselective queries so it's worth thinking about even though it'll only be appropriate for somewhat niche use cases. Values of less than 10,000 make s faster than s and values greater than 10,000 make s faster than s. In my dataset. With my two term queries. This is because s accept every candidate document so they have to find fewer candidates. Testing methodology Now a note about testing methodology: this isn't super scientific. I'm using a rather old dump of English Wikipedia's search index for its content namespaces (available , the file looks like ). How to load your own copy would be a great topic for another .... Anyway! I'm using for generating the actual load which uses a Lua script to generate the queries. The terms I'm using for the queries come from . I'm running both wrk and Elasticsearch on the same machine. I'm using a single shard and all the default arguments for Elasticsearch 2.1.1 including its 1GB heap. So I have about 30GB of free memory for Linux to use for the page cache. Every query I run has so I don't spend time loading the , highlighting, or any of that stuff that a real search has to do. After loading the index I d it which is totally cheating. A real production system can't optimize an index that is always changing and Wikipedia is always changing. But the index is 124GB and my development desktop's SSD only has 101GB of space total so I'm having to use my spinning drive, a 1TB WD Blue spinning at 7200RPM. If I don't optimize I very quickly saturate the poor disk and my tests just show that I need to buy a better disk. Which doesn't make for a great blog post. I want to talk about the CPU overhead of these queries and optimize does a good job of making that possible. But take everything here with a grain or two of salt. Benchmark for yourself. My goal is to put you in the right ballpark. I use for all of my queries because it's convenient. I don't believe you should use for queries sent by users because it's too powerful and too brittle but it super convenient for my lua query generator to just generate a string query rather than worry about generating JSON. Various query types All being said, have a look at the graph above which compares some different types of queries. A couple of things: first and foremost I've shifted this graph to log scale. Sorry! Without doing that the fast queries just overwhelm the slow queries and you can't see anything. The horizontal axis is how frequently the term comes from a list of \"common\" words. All queries contain just two ","locales":"","title":"Elasticsearch Queries, or Term Queries are Really Fast!"}
{"index":{}}
{"author":"Sun-Tsung Kim","category":"User Stories","publish_date":"2016-01-21T00:00:00.000Z","url":"/blog/elastic-stack-for-root-cause-analysis-at-mapp","seo_title":"Elastic stack for Root Cause Analysis","content":" The vitality of the Elastic technology stackDifferent modules of our software produce their own logs, in different formats. Previously, we did all those dirty grep, sed, awk moves to extract certain patterns out of the raw logs and then do analysis on this. It was absolutely difficult, and it was impossible to see any trends based on different parameters. When we started using the power of the Elastic Stack, the whole log analysis became faster, more proactive, meaningful, and accurate. With Elasticsearch we can now do full text search in near real time. Using Logstash and its powerful plugins, we can deal with different types of logs from different software modules and extract valuable fields out of logs. Kibana gives us the power of creating and using dashboards as well as extracting the analysis results based on the logs. Also, passing the information from team to team became easy by sharing dashboards. Plugins such as Kopf, Curator and Marvel help us to manage Elasticsearch and its indices.   The Elastic technology stack from head to toeWhen we say that we use the Elastic Stack, this means that we are covering the full logfile analysis story, content and technology. Let's start with the application that produces the logs. We know it and the people behind it because of our Root Cause Analysis (RCA) role. So we can give recommendations on the content of the logfile, and also on the structure. Next we make this visible in the Elastic stack, either by feeding it into an existing cluster or by creating a new stack. In the latter case, it's us who set up Elasticsearch, Logstash and Kibana. Then we go back to the users to ask how this works for them. Working with the Kibana dashboards, they will start to think differently and come up with new use cases. We'll keep developing the Elastic Stack, configure Logstash, do the sizing, optimize and upgrade all components. The Elastic Stack users are Third Level Support, DevOps, Developers and Product Management. And us. This is very important, there's no better way to understand your user than being one yourself. A powerful combination: Elastic Stack and RCAIn this post, I'll argue that this is an amazingly powerful combination: RCA, the Elastic Stack and a full team. For one, as RCA, we've always been bridging teams and departments. We talk to everyone who is affected by incidents, and to everyone who knows about their causes and helps to prevent them - in short, to everyone. Second, we have a vital interest in the Elastic Stack, using it a lot, while at the same time the Elastic Stack has a great \"Power to the User\" built in philosophy that makes it possible to answer questions from many perspectives. Third - let me take one step back. Jez Humbel wrote in  that \"Bad behavior arises when you abstract people away from the consequences of their actions.\" In this spirit, we don't tell people \"Don't worry, we take care of this\", but aim at bringing people back in touch with the consequences of their actions. The Elastic stack is a great tool for this. Fourth, the Elastic Stack is flexible, so we are, too. Every component, Elasticsearch, Logstash and Kibana, adapts very well to many situations. It processes all kinds of input and lets you combine this with other data sources. For example, we extract information on human readable names from a database, build a dictionary, then use the Logstash translate plugin to add this information to our events.Attack of the BuzzwordsNow, what happens when you have a team with the mission to identify and analyse incidents and the tool and the knowledge to spread this across the company? I'd like to show how this relates to Agile, Microservices, DevOps and Lean - sorry about this avalanche of buzzwords, but I promise that I have a reason to use them. The Elastic Stack is a natural microservice. It is small, it serves one purpose, and it is deployable independ","locales":"","title":"Elastic stack for Root Cause Analysis at Mapp"}
{"index":{}}
{"author":"Emmanuel Benazera","category":"User Stories","publish_date":"2016-01-20T00:00:00.000Z","url":"/blog/categorizing-images-with-deep-learning-into-elasticsearch","seo_title":"Categorizing images with deep learning into Elasticsearch","content":" is a young open source deep-learning server and designed to help in bridging the gap toward machine learning as a commodity. It originates from a series of applications built for a handful of large corporations and small startups. It has support for , one of the most appreciated libraries for deep learning, and it easily connects to a range of sources and sinks. This enables deep learning to fit into existing stacks and applications with reduced effort. Machine learning is the next expected commodity on the developer's stack. Many software packages, most of them open source, are slowly but surely empowering the developers with these new technologies for automation. As the developer of , an open source deep-learning server, I've been building a range of smart applications for a variety of large corporations and small startups. In most of these production settings, the existing stack relies on a series of data backends, among which Elasticsearch is prominent. For this reason Deepdetect builds a direct connection to all backends through a little trick: the server supports output templates so that data can be molded to fit any sink backend. We recently applied this trick to in just a few steps. The result is that you send images to Deepdetect, images get tagged (a hedgehog, a plane, etc.), then the tags and the image URL get indexed into Elasticsearch directly without any glue code. This is it. You can now search images with text, even when no caption was available. Beyond cool, this is also scalable as prediction works over batches of images, and multiple prediction servers can be set to work in parallel. We tell you below how to reproduce this very simple setting for your own applications. Machine Learning and Deep LearningBut first, if you are not familiar with the topic, machine learning has become a ubiquitous technology that is powering a growing number of high automation software offerings, sometimes referred to as smart or intelligent applications. These range from and to , , , audio and video recognition (and even !). In a nutshell, machine learning automates classification tasks in two steps: the training step builds a model out of data for a targeted task, i.e. for image classification, while the prediction step leverages that model to predict — for instance, the category of more images. There are some cool demonstrations out there:  do you believe we can accurately ? Machine learning dates back to the 1960s, though it is the last decade's surge in data, cheap computational resources, and crucial scientific achievements that truly unlocked its potential in production.(1) The most advanced applications these days leverage the field of , a set of models in the form of complex neural networks with the ability to capture high-level abstractions in data. These models are of course computed by dedicated algorithms that for the most part apply a small set of linear and nonlinear operators repeatedly over slices of the training data. Because these operations need to be repeated millions of times over matrices that can reach up to billions of entries, they are best parallelized on special processors, originally dedicated to graphical computations such as video games, and known as Graphic Processing Units (GPUs). Deep learning continues to reveal spectacular properties, such as the ability to recognize images or classify text without much engineering. Raw images or text are fed to the algorithm along with the desired output, and the resulting model can be used to predict the output on more data. This prediction has very high accuracy, ! This is a big step forward compared to previous machine learning systems in which engineers had to half-blindly help the algorithms tricks and transforms to the data. Machine Learning as a CommodityFor these reasons, it is believed that . This compares to the way","locales":"","title":"Categorizing images with deep learning into Elasticsearch"}
{"index":{}}
{"author":"Dale McDiarmid","category":"Releases","publish_date":"2016-01-20T00:00:00.000Z","url":"/blog/deploying-elasticsearch-with-ansible","seo_title":"Deploying Elasticsearch with Ansible","content":" With the recent release of Elasticsearch 2.0.0 a completely new Ansible role has been released as well. This post will show you how to install and configure multiple Elasticsearch nodes using this new role. As Consulting Engineers at Elastic we regularly deploy large Elasticsearch clusters across multiple machines. These clusters vary in topology and configuration and are often deployed on heterogeneous hardware in complex architectures. Repeating the same task of software deployment and configuration, on potentially hundreds of machines per customer, is a great opportunity for automation. In the interests of both efficiency and accuracy — as well as our sanity! — we regularly review and seek new tools to simplify this process. By minimizing the time spent on actual software deployment, we maximize the time for more interesting problems that our customers regularly pose! We continue to support a Puppet module which was recently updated to support Elasticsearch 2.x. Whilst an excellent configuration and orchestration management tool, Puppet can present a steep learning curve and significant investment for some organizations. For deploying new self-contained clusters — with no history of Puppet within the organization — Ansible's push architecture can represent a simpler alternative. Ansible is a Python-based automation tool that wraps SSH, allowing complex idempotent commands to be sequenced. It can configure systems, deploy software, and orchestrate more advanced IT tasks such as continuous deployments or zero-downtime rolling updates. With no requirement for an agent, or external dependencies other than SSH access and Python 2 on the target host, it represents an ideal solution for deploying an Elasticsearch cluster on a set of newly provisioned servers. Whilst Puppet remains a popular configuration management tool for Linux systems, Ansible has obtained equivalent interest in the last few years. In response to both our own requirements and a growing community interest, we are now officially releasing an Ansible role to simplify the deployment of Elasticsearch. The role's structure directly maps to its key capabilities: installation of prerequisites (e.g. Java), installation of Elasticsearch based on the target platform, Elasticsearch configuration, management of plugins and finally service management. The assignment of the role to a host results in the installation of an Elasticsearch instance. For hosts where multiple instances are required, simply assign the role multiple times — as illustrated in the below example. The configuration of Elasticsearch is supported through a map, which is in turn serialized to yaml, thus requiring no changes when new parameters are added. To help with illustrating the capabilities of the role we've assembled a quick example using an Ubuntu-based docker image. The user will require: The following assumes Ansible has been installed in /opt/ansible on your desktop. Where not specified, the user should assume all actions should be performed from this machine. sudo mkdir -p /opt/ansible/playbooks/ sudo chown -R <User> /opt/ansible/playbooks cd /opt/ansible/playbooks && git clone https://github.com/elastic/ansible-elasticsearch-example.git cd /opt/ansible/playbooks/ansible-elasticsearch-example/git clone https://github.com/elastic/ansible-elasticsearch.git roles/elasticsearch The file playbook.yml declares a very simple playbook with one play. This play applies the Elasticsearch role twice to a server belonging to the “nodes” group. The first application of the role will install a dedicated master node, whilst the second installs a data node. Users can easily modify this file to contain either more plays, more roles or a combination of both. dockerpull gingerwizard/ansible-test docker run -d -P --name ansible-test -h ansible-test gingerwizard/ansible-test docker ps -a docker inspect <container_id> | grep \"IPAddress\" [n","locales":"","title":"Deploying Elasticsearch with Ansible"}
{"index":{}}
{"author":"Tudor Golubenco","category":"Brewing in Beats","publish_date":"2016-01-19T00:00:00.000Z","url":"/blog/weekly-beats-more-community-beats","seo_title":"","content":" Welcome to Weekly Beats! With this series, we're keeping you up to date with all that's new in , from the details of work in progress pull requests to releases and learning resources. Unifiedbeat - Beat for IDS/IPS event logs reads records from binary files generated by network intrusion detection software like Snort or Suricata, and indexes the records in Elasticsearch. Its author wrote a in which he explains why Elasticsearch is a great option for storing these logs. We agree and think this is a great use of the Beats framework. Factbeat - Beat for Puppet Facter info is a new community Beat, created by from Elastic. It runs Facter periodically and sends the results to Elasticsearch. Having all the facts about your servers available in Elasticsearch makes it easy to query and visualize your infrastructure in new and interesting ways. Expand env variables in configuration files A commonly requested feature was to be able to use environment variables for configuring the Beats. Andrew came up with an elegant that allows you to use environment variables in the configuration files, while allowing for default values. This works automatically for all Beats. Winlogbeat new field names In time for the first release of Winlogbeat, we’ve the fields exported by Winlogbeat to be more uniform with the ones exported by the other Beats. Filebeat refactoring We’ve made more to the Filebeat code, making it possible to cleanly shutdown Filebeat in all the corner cases. PowerShell script for loading the dashboards Thanks to a , our next release will include a that makes it easy to load our sample dashboards on Windows. Testing environment We now have a dockerized for the manual QA phase. This makes it very easy to test against specific versions of Elasticsearch, Logstash, and Kibana. ","locales":"","title":"Brewing in Beats: More community Beats"}
{"index":{}}
{"author":"Ted O'Meara","category":"User Stories","publish_date":"2016-01-19T00:00:00.000Z","url":"/blog/allovue-knowledge-scaling-versatility-over-all-else","seo_title":"Knowledge Scaling at Allovue: Versatility over all else","content":" For school districts, every financial decision they make - be it as hot button as labor contracts and books provided to students or as benign as which light bulbs to purchase - has ramifications on the ability of every teacher to do their job of providing the best learning environment for their students. But too often these decisions are made without input from those teachers and students who are most affected by cuts or expenditures. The team at has developed a solution that provides easy access to all account books and tables for school districts, so even those not directly involved in the decision process can easily track when funds are going and more importantly where funds are lacking. In turn, financial decisions can be made knowing the whole picture of real effects on children and teachers as opposed to just simply a business bottom line. Allovue chose Elasticsearch to provide a replacement for their Postgres backend search functionality to their industry-leading tool as it easily allows for aggregation across multiple unique datasets and structures, all while reducing response times from 2 seconds to under 20ms.  How can a school run out of paper before the holiday break? Jess Gartner was faced with this question, among many others tossed onto the heap of questions and tasks that the third-year teacher had perpetually running in the cusp of her conscious thought. These questions kept flowing in and out of thoughts mixed with the lesson planning, scheduling, and other activities that are core to a teacher’s day-to-day tasks. It turns out that her situation happens often. Teachers all across the United States have to shell out money from their own pockets to keep their classrooms going. Buying pens and pencils. Buying classroom decorations. Buying supplemental materials for a special lesson.Now as CEO of , Jess is able to see how big of an impact financial decision-making in a school district can mean for individual teachers and their classrooms. The more she explored this problem, the clearer it became that most school districts lack systems and processes to build financial literacy outside of their accounting and budget offices. They also cannot supply financial data with the same quality, timeliness, and supports as academic data. Between central office accountants and school district superintendents, chief academic officers, principals, and board members, a lot of valuable information is lost. As a result, the conversation about effective resource allocation and the conversation about effective education practice are siloed. Scaling for school districtsAmong the challenges with education finance are the ways in which financial transactions can be categorized. Because school districts are funded with local, state, and federal dollars, and all of these sources have different reporting requirements and restrictions on how those dollars can be spent, school districts have a very robust . A is a collection of metadata that describes the source and purpose of each dollar spent in a school district. All businesses have a chart of accounts, but the detailed reporting and compliance needs of schools lead to varied and complex charts of accounts. A well-organized chart of accounts is made up of several , each of which can be thought of as an independent dimension to describe an expenditure measure. Each segment looks very much like a tree. There are parent nodes, and then there are children, grandchildren, and so on. They mostly use numeric codes that follow certain rules. For instance, you may have a chart segment that has a parent node of with a code of . has a child: , which in turn has a child node: . Each school district has their own chart of accounts, which can have any number of segments, which themselves can have any number of elements and hierarchical depths.When you create a budget, you specify how much mon","locales":"","title":"Knowledge Scaling: Versatility over all else"}
{"index":{}}
{"author":"Michael McCandless","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2016-01-18T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2016-01-18","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top NewsLooking to upgrade your deployment from 1.x to 2.x? We’ve got the video for you & it’s OnDemand now! — elastic (@elastic) Elasticsearch Core Apache Lucene Watch This Space, where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2016-01-18"}
{"index":{}}
{"author":"Megan Wieling","category":"","publish_date":"2016-01-18T00:00:00.000Z","url":"/blog/witwies-snowcamp-scale14x","seo_title":"Where in the World is Elastic? - SnowCamp & SCaLE 14x","content":" Welcome to Find out which Elastic events and meetups are happening near you this week. Upcoming EventsJanuary 20 - 21:  January 21 - 24: Upcoming MeetupsJanuary 18: January 19: January 20: January 20: January 21: January 21: January 23: January 20: That's it for this week. Stay tuned next week for more Elastic happenings.- The Elastic TeamP.S.  if you're interested in hosting a meetup or are giving a talk about Beats, Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic? - SnowCamp & SCaLE 14x"}
{"index":{}}
{"author":"Tudor Golubenco","category":"Brewing in Beats","publish_date":"2016-01-12T00:00:00.000Z","url":"/blog/weekly-beats-run-nagios-checks","seo_title":"","content":" Welcome to ! With this series, we're keeping you up to date with all that's new in , from the details of work in progress pull requests to releases and learning resources. New community Beat: NagioscheckbeatThere’s a new community Beat, written by Elastic’s , and it’s really interesting. runs Nagios checks and sends the results to Elasticsearch. I don’t think it was ever easier than this to scale and get analytics insights from all those Nagios scripts.  Huge potential here, if you ask me. Thank you, Jay :-) New community Beat: hsbeat (HotSpot VM metrics) is another very interesting and promising community Beat. It collects performance data from the Java HotSpot VM by reading and decoding the binary performance logs created by the VM. Filebeat to Logstash throughput improvementsSteffen continued to investigate the about low throughput performance when sending synchronously to Logstash, and for a significant (9x) increase in the Filebeat -> LS throughput. These new libbeat defaults are at odds with the maximum memory usage of Packetbeat due to the multiplication of buffers (buffer size * bulk size), so there is another that solves this issue by making the publisher know if the message contains one event or multiple (possibly thousands). This also improves the configurability of these buffers from the configuration file. With these changes, Filebeat is at around 60% of the throughput achieved by the Logstash Forwarder. The difference seems to come from the JSON serialization, which we need for creating new features easier, and from making the protocol safer for back pressure situations. Steffen has ideas for closing and even reversing this gap, but they require larger code changes. Metricbeat (name not final) design startedNicolas put together a , which stirred up very pragmatic conversations about what we want to achieve with Metricbeat, how it relates to other Beats (especially the existing community beats), and even how the implementation will look like. If you want to join the discussion, you can do so in this , which also contains our current vision for Metricbeat. Unified release processTo improve the way all Elastic projects are being built, tested, and released, a common interface needs to be implemented by all of them. We started on this interface, which included moving the code into the main beats repository. Topbeat exports total CPU times instead of user CPU timesTopbeat used to export user times for each process, but the Linux top command shows total times, which are arguably more important. So we topbeat to export total times instead. ","locales":"","title":"Brewing in Beats: new community Beat for Nagios checks"}
{"index":{}}
{"author":"Michael McCandless","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2016-01-11T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2016-01-11","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top NewsHere’s your cookbook on how to deploy 2.0.0 with . — elastic (@elastic) Elasticsearch Core Apache Lucene Watch This Space, where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2016-01-11"}
{"index":{}}
{"author":"Megan Wieling","category":"","publish_date":"2016-01-11T00:00:00.000Z","url":"/blog/witwies-aws-newyork-boston-meetup","seo_title":"Where in the World is Elastic? - AWS New York and Boston Meetup","content":" Welcome to Happy new year! We are back again with Elastic meetups happening near you this week!Upcoming MeetupsJanuary 12: January 12: January 12: January 13: January 14: January 14: January 12: January 13: January 14: January 14: January 12: That's it for this week. Stay tuned next week for more Elastic happenings.- The Elastic TeamP.S.  if you're interested in hosting a meetup or are giving a talk about Beats, Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic? - AWS New York and Boston Meetup"}
{"index":{}}
{"author":"Jay Greenberg","category":"Engineering","publish_date":"2016-01-18T00:00:00.000Z","url":"/blog/a-case-for-self-monitoring-systems","seo_title":"A case for self monitoring systems","content":" TheoryNetwork infrastructure has evolved in recent years.  Systems reside in a combination of the corporate office, datacentre co-locations, and the cloud.  Trends in automation and configuration management have simplified massive scaling. A single security domain interconnected by private lines is no longer a luxury that engineers can expect.   Ad-hoc measures must be implemented such as virtual private networks and least privilege firewall permissions, on a case-by-case basis, requiring extensive architecture and maintenance.   System logging and monitoring are essential components in stable and intelligent business operations - but too often, security is complicated - or worse, compromised.   A smart engineer will usually choose the simplest approach, as it is often the most stable and secure. In most of today’s networks, logging and monitoring are mutually exclusive, one pushing back to a central location, and the other polling from a central location.   The polling piece of this dual-architecture has several problems: Theoretically, it is now possible to simplify the traditional architecture, streamlining logging, monitoring, performance statistics and business reporting into a single platform - Elasticsearch. Proof of ConceptBeatsIn order for a solution to be viable, it must ultimately address any need that arises, on any platform.  The platform is a lightweight, open source data shipper, and runs on most .  The community has already begun to monitor their favourite services. A wealth of Nagios Checks already exist (currently over 5000 in the ), and can be integrated with Beats via .   Here is an example of how we would configure nagioscheckbeat to check the redis service every 10 seconds: period: \"10s\" name: \"redis\" cmd: \"/usr/lib/nagios/plugins/check_redis.pl\" args: \"-H 127.0.0.1 -R -m -T -f -A\" ElasticsearchBeats must have an output, such as Logstash or an Elasticsearch cluster.   Self-hosted, or , the organization’s cluster must be secure and accessible by all end systems.   KibanaElasticsearch and serve nicely for reporting performance metrics: Apache Workers fit perfectly into a percentage visualization: Kibana’s new plugin allows for more detailed time series analysis.  For example, we could overlay arbitrary metrics in a single visualization, for custom correlations during an RCA. Alerting on Crossed Thresholds or Unresponsive Services does two things.  It publishes performance metrics — for making pretty graphs — but also reports the status of each check against warning & critical thresholds defined in the configuration.  Those checks are published separately from the metrics, so we can watch the results directly.   Elastic’s plugin can be configured to alert with little effort, and even supports notifications. This Watcher configuration will send an email when any has reported a CRITICAL status in the last 30 minutes: PUT _watcher/watch/critical_watch { \"trigger\" : { \"schedule\" : { \"interval\" : \"1m\" } }, \"input\" : { \"search\" : { \"request\" : { \"indices\" : [ \"nagioscheckbeat*\" ], \"body\" : { \"query\":{ \"filtered\":{ \"query\" : { \"bool\" : { \"must\" : [ { \"term\" : {\"_type\": \"nagioscheck\"} }, { \"range\" : {\"@timestamp\" : {\"gte\" : \"now-30m\"}} }, { \"term\" : {\"status\" : \"CRITICAL\" } } ] } } } } } } } }, \"condition\" : { \"compare\" : { \"ctx.payload.hits.total\" : { \"gt\" : 0 }} }, \"actions\" : { \"send_email\" : { \"throttle_period\": \"30m\", \"email\" : { \"to\" : \"me@elastic.co\", \"subject\" : \"Alert from Watcher - Service(s) Critical\", \"body\" : \"One or more services reported a CRITICAL state within the last 30 minutes. See the attached file for details. You will not receive another notification for 30 minutes.\", \"attach_data\" : true } } } } Notice the directive, which ensures that alerts are only sent periodically until the condition is resolved.    Alerting on Lost HeartbeatsNo monitoring system would be complete without knowing if a host is down","locales":"","title":"A case for self-monitoring systems"}
{"index":{}}
{"author":"Suyog Rao","category":"The Logstash Lines","publish_date":"2016-01-12T00:00:00.000Z","url":"/blog/logstash-lines-2016-01-12","seo_title":"","content":" Happy New Years to our users and welcome back to The Logstash Lines! In these weekly posts, we’ll share the latest happenings in the world of Logstash and its ecosystem. Prepping for the 2.2 release: This past week was spent testing internal release candidates for our next feature release - 2.2. We found and fixed the following issues: Manageability Plugins: Beats Input: Refactored beats input to primarily fix under high data volume. Replaced the in-house blocking size queue implementation with Java’s Synchronous Queue. Reorganized code to make testing easier. File Input: Added new settings - and to mirror existing functionality in Filebeat. This will help close file descriptors for files which are not being actively written to. Previously, users had to restart LS to release these resources. ES Output: Reviewed and merged scripted update support for HTTP protocol. Fixed a regression where http errors would not sleep for , sending the CPU into a spin. Kafka output: Work continues on rewriting Kafka input to use the new 0.9 version of consumer API. Also adding SSL support for both producer and consumer based on the new APIs. JDBC input: Added functionality to save query run state by using any column number, not just time-based columns . Until next time! ","locales":"","title":"The Logstash Lines: 2.2 Release prep, JDBC Input enhancements!"}
{"index":{}}
{"author":"Costin Leau","category":"Releases","publish_date":"2016-01-08T00:00:00.000Z","url":"/blog/es-hadoop-2-2-rc1-released","seo_title":"Elasticsearch for Apache Hadoop 2.2 RC1 is out","content":" Celebrating the start of 2016, Elasticsearch for Apache Hadoop (ES-Hadoop) 2.2 rc1 has been . Packing a significant number of bug fixes and enhancements, this release candidate is the last step towards a full general availability release for the current development branch. As always, the artifacts are available at the or . ling updatesES-Hadoop 2.2 RC1 introduced support for the just-released Spark 1.6, in particular skipping that otherwise would be processed again in Spark (despite being already handled by the connector). For large result sets, this results in an important optimization. The push-down translation has been improved, in particular when dealing with filters by providing better matching when dealing with raw terms vs. values (such as dates or timestamps). Speaking of Spark SQL, the schema declaration has been to handle multi-valued/array fields in a simple and elegant fashion (whether the fields are nested or not). In addition, the connector configuration is now sanitized and passed throughout a Spark job:  this addressed a subtle bug caused by command line-only properties being discarded during a job stage and causing abnormal behavior. YARN EnhancementsA batch of updates were done to the YARN module by upgrading to Elasticsearch 2.1.x and allowing JVM system properties to be passed directly in the children container. Repository HDFS The repository HDFS plugin has seen a lot of activity. While currently for Elasticsearch 2.0 and 2.1, it requires the JVM security manager to be disabled (as Hadoop is significantly greedier than Elasticsearch itself in terms of permissions), starting from Elasticsearch 2.2, due to the security improvements the plugin can customize its own code base grants. Please note that the migration of the plugin to Elasticsearch core has and is currently scheduled for Elasticsearch 2.3. More about that in a future blog post! Network improvementsThe wan/cloud has seen a lot of uptake which exposed the connector to more varied network topologies and configuration. This led to a number of fixes in the way ES-Hadoop handles Elasticsearch clusters with hostnames and IPs (typically with network publishing enabled) and the translation between the two. Overall, the connector picks up more information about its environments, reducing the amount of extra configuration on the user's behalf. FeedbackPlease let us know what you think about RC1! We love to hear from you on , or the . ( works too). Looking forward to 2016! ","locales":"","title":"Elasticsearch for Apache Hadoop 2.2 RC1 is out"}
{"index":{}}
{"author":"Nick Knize","category":"Engineering","publish_date":"2016-01-07T00:00:00.000Z","url":"/blog/supercharging-geopoint","seo_title":"Supercharging geo_point fields in Elasticsearch 2.2","content":" “I want to location enable my application but where do I start? Elasticsearch has had geo support for some time but how does it work? What are all of those for? What kind of performance can I expect?” The explosive growth of geospatial and spatiotemporal data for so many diverse emphasizes the need for efficient geospatial data structures and analysis tools. With many Geospatial specialty applications and libraries already available, few offer the scalability and flexibility of combined full text search and aggregations with location based information. Enter Elasticsearch. We hope this post answers some of those nagging questions regarding how geospatial field types work and how you can begin location enabling your Elasticsearch-driven applications. So what's new? Elasticsearch has supported for some time:  and a short while ago we provided a demo and overview of the available in . While the existing benefit from some of the low level improvements in 2.0 (specifically ), the 2.2 release supercharges fields by improving the underlying data structure and query approach. Indexed by default Prior to 2.2, fields used no default indexing structure (aside from a “lat,lon” ). This is because, until Lucene 5.3, a core Geo data structure based on the internal (that did not require a third-party library associated with a ) did not exist. In order to circumvent this limitation, so the could be used, the required specifying at least one of: : , : . At the implementation level the option stored latitude and longitude data as two separate fields, while the option encoded all fields as an . Since both structures are based on the , and core types, respectively, supporting 2D geo data indexing required no additional indexing code. With the release of Lucene 5.3, a new specialized GeoPointField type is now available. This field type is built on the internal inverted index structure and all of the amazing work accomplished by the Lucene Community to make this structure as efficient and performant as possible. While an inverted index is not a typical structure used for geospatial data, it can be remarkably fast when applied correctly:  and since the structure reuses all of the same codec logic implemented by Lucene, all of the dangerous corruption issues related to introducing new file types for new data structures are less of a concern. In order to work with the inverted index, the GeoPointField type uses a quad-tree raster graphics based approach by encoding latitude and longitude values as a single 64 bit integer and using variable length prefix codes for the terms in the term dictionary. The following graphic illustrates this technique for geo_point data.   Further encoding were then applied to minimize the size of the terms dictionary, thus minimizing the overall size of the index. With the performance improvements to there is no need to store every prefix term up to the full resolution. Each point can be approximated using the prefix as a precision step of the top 32 most significant bits. Points along the search boundary can then be further scrutinized using full precision obtained from . This two-phase approach strikes a balance between index size and processing time. Rasterized queries The new approach brings improved efficiency for all available in Elasticsearch. Prior to 2.2, field queries were accomplished using a two-phase approach ( in the 2.0 release). The first phase used either a prefix stemmer for indexed points, or lat/lon for indexed points to query based on the bounding box of the search area. The full precision results were then checked against the search criteria (e.g., , , ). While the improvements to the two-phase iterator improved query efficiency (especially for ), complex distance and polygon queries still required a check of every point within the bounding box. The 2.2 indexing improvements minimize these checks by applying the be","locales":"","title":"Supercharging geo_point fields in Elasticsearch 2.2"}
{"index":{}}
{"author":"Scott Fingerhut","category":"Culture","publish_date":"2016-01-07T00:00:00.000Z","url":"/blog/elasticon-chairs-for-your-butts","seo_title":"Elasticsearch User Conference - our butts trying the chairs for ElasticOn comfort","content":" So, after doing our inaugural Elastic{ON} conference last year, then a 12-city global tour (raising over $150,000 for charity) we learned a lot. Two big things were: Behold, the collection of candidate chairs we had sent to the Elastic Mountain View offices for us to put to the test. We didn't even have the cheap seats sent to our office. All of these cost considerably more than typical default conference chairs. Unfortunately, our budget couldn't support the killer reclining/massaging Lazy Boys we would have all loved. We didn't just look at these and sit in them for a minute, we spent an entire day in them. Here's the status: We've (well some of us) eliminated these chairs (which were the same but different colors) really quickly. We sat in them and although they nicely cradled our bums, when we started rocking in them we quickly came to the conclusion that there was some risk of the legs (of the chair) just snapping off. Then we noticed that the clear chairs gave a perfect view of well, our butts. Upon sitting in the chair, someone in back of me (not to be named) said \"I'm getting crackitis. Are you going to repair some plumbing Scott?\" That was enough for me. Even so, Jason, our other tester persevered and logged this: Jason's report — \"Captain's Log 0830\" Then I came back from lunch to this: At first this one felt like a nice cupped hand under us. It provided some lower back support and looked sort of cool, but when we leaned back, it just hurt. #3 is done, not much more to say. These are all very similar chairs in structure but have different fabric, so we are doing more rigorous testing. We want to know how slippery they are, how they deal with extra moisture and overall, how comfy they are. . Nice and cool to sit in but a little slippery. If we put our butt near the front end we might just fall right off. We need to figure out the absorbent power of this chair. . These have got the best ratings from our folks. I'm torn a little between the two of these. Like the white fake leather chair, they feel good on the entire back. At this point, the color seems to be more of the debate in the office versus the feel. I have a great comeback for those that prefer based on color. That is \"hey, you don't get to make a call unless you log at least 5 hours in this chair.\" Sadly, many people don't take me up on it. But, for those that do, here's their commentary: \"You know I think the blue is better looking, but if I close my eyes, they both seem almost identical. When I feel the chairs, the blue fabric is a bit softer and the darker color might hide spill and stains more.\" \"Scott, why are you making me sit in this chair? I hate you.\" \"Here's the deal, if you sit in any chair long enough it's going to hurt… Well, except a massage chair, where someone is rubbing your back with medium pressure, and there is soft, tranquil music in the background... Wait, what were we talking about?\" \"Scott, do you have to say 'butt' so much? Tuchus, caboose and toosh are perfectly good alternatives!\" Final conclusion — It's going to be either 4, 5, or 6. We'll make it a mystery. But, we realized that it's important to take breaks and stretch every so often. So, we'll be trying to suggest small stretch breaks and we encourage you to just stand up once in awhile (apologize to the person behind you) and strike your best yoga pose. We hope to see you at in San Francisco! ","locales":"","title":"Putting our Butts to Work for Your Elastic{ON} Comfort"}
{"index":{}}
{"author":"Mark Walker","category":"Culture","publish_date":"2016-01-07T00:00:00.000Z","url":"/blog/elastic-donation-helps-upgrade-abilitynet-expert-resources","seo_title":"Elastic donation helps upgrade AbilityNet expert resources","content":" StoryEvery year helps hundreds of thousands of disabled people use digital technology to achieve their goals at work, at home and in education. Although thousands of people use our face-to-face services and call our free telephone helpline, our web-based resources are the number one way for people to access to our expert knowledge. That’s why the support of Elastic was so important to us.The website offers free access to our expert resources and knowledge. That could be downloading factsheets, reading our blogs, attending our free webinars or using - our guide to every accessibility feature built in to every mainstream desktop computer, laptop, tablet and smartphone. As a charity we need to continually invest in these expert resources and recent funding from Elastic will help us reach even more people. Elastic is a fast-growing technology provider of search, logging, and analytics software, used by some of the biggest businesses in the world to power a huge range of services. From live trading data used by banks to the Guardian newspaper’s live content, their tools are designed to take data from any source and search, analyse and visualise it in real time.A key part of its success is that Elastic works with a huge community of developers who actively grow the open source tools at the core of the Elastic services. Nurturing this community has been a vital part of its success and the Elastic team recently hit the road for a whistle-stop global tour to connect with users and share knowledge.Each conference of the  featured a charity partner and AbilityNet was chosen as the beneficiary for the London event.TestimonyHead of Marketing Mark Walker attended the event on behalf of AbilityNet: ““\"” ","locales":"","title":"Elastic donation helps upgrade AbilityNet expert resources"}
{"index":{}}
{"author":"Cecilie Myhre","category":"News","publish_date":"2016-01-06T00:00:00.000Z","url":"/blog/hosted-elasticsearch-monitoring-available-on-found","seo_title":"Hosted Elasticsearch: Monitoring Now Available on Found","content":" Today marks a milestone: we have achieved the goal we set when Found joined forces with Elastic back in March last year by making the entire Elastic stack available to our hosted Elasticsearch customers. First, we integrated Kibana 4 in July, Shield for security in September, Watcher for alerting in October, and today, we're excited to announce that the Elastic monitoring plugin, Marvel, is now available on Found, our hosted Elasticsearch product. This is a major engineering milestone for us as it means that all of our Elastic plugins are now available on Found. is a monitoring plugin that lets you view and analyze the health and performance of your Elasticsearch clusters in real time as well as historically. The is built on top of Kibana 4, which features a complete UI redesign. To start using Marvel on Found, all you need to do is to enable Marvel and then select a cluster to which you want to ship your Marvel metrics data. We recommend creating a dedicated Elasticsearch cluster for this so it doesn't interfere with any production clusters you are running. Next, you simply launch Kibana on your Marvel cluster and select the Marvel app on the Kibana menu bar to access the key metrics of your clusters, indexes, and nodes. No additional configuration is needed. That's it — just Marvel-ous. Want to try it out? Sign up for a ! This milestone signifies an end, but also a beginning. As a highly engineering-driven company, there's always going to be a ‘but wait, there's more', so let's have a peek at what's on the horizon. But first let's take a quick look at how we got to where we are today. Developer's Best Friend in the CloudA few years ago, we were a tiny startup in Norway with some wonderful customers who were all fanatic Elasticsearch users looking for a better way to host and manage their Elasticsearch clusters than simply doing it themselves on AWS. We stood up to the challenge, and back in 2012 we launched the public beta of our hosted Elasticsearch service. We hit challenges early on and ‘failed fast', which gave us a wealth of experience enabling us to execute well in a fast moving cloud space. Our mission was to be the developer's best friend, by hiding complexity while remaining flexible, and last but not least, by delivering a rock solid, reliable hosted Elasticsearch service. It was still early days in the hosted Elasticsearch domain when we went GA in 2013, but the product showed significant traction in the following months. By the time Elastic acquired Found short of one year ago, we had hundreds of paying customers. Since then, the ride has been simply amazing. A Dream Come True: Found + ElasticWe were secretly hoping to one day become a part of ‘the core Elasticsearch team', and we had good reasons to believe that Elastic was planning to enter the hosted Elasticsearch market. As it turned out, we were right and Elastic had such faith in what we built that they scrapped the plan of building their own service and instead ‘popped the question'. In March 2015, . Usually, acquisitions tend to be rather bumpy, but based on strikingly similar culture, ethos, and a vision for the future, it was simply a great fit for us and our customers. One result was that many more and also larger companies signed up for Found as they entrusted in the fact that the company who created Elasticsearch was now supporting the Found hosted Elasticsearch service. Today, we are humbled that we are hosting three times as many clusters as before the acquisition and honored to have awesome customers such as HotelTonight, Docker and Instacart enjoying the benefits of Elasticsearch in the cloud. Story complete with MarvelWith a steadily growing cloud team in 2015 and access to the people who built the Elastic products, we have finally packaged all Elastic products into our hosted Elasticsearch solution. As Elastic continues","locales":"","title":"Hosted Elasticsearch: Monitoring Now Available on Found"}
{"index":{}}
{"author":"Ryan Schneider","category":"Culture","publish_date":"2016-01-06T00:00:00.000Z","url":"/blog/ama-booth-elasticon-tour-2015-recap","seo_title":"","content":" At Elastic, we love the community that uses the Elastic Stack to build amazing products and help solve problems. We recently just wrapped up the world Elastic{ON} Tour where we held 12 one-day events sharing roadmaps, customer use cases, demos, and much more. One of the mutual favorite parts of the Elastic{ON} Tour was the AMA Booth (Ask Me Anything). We read and listen to every piece of feedback and know that asking our engineers, consultants, and architects about details of the various parts of the Elastic Stack is what our users loved the most. We love working the AMA as well and it helps us share new features, explain how things are implemented, share architectures, and offer guidance to make things faster or debug pesky problems. Coming this February 17 - 19 in San Francisco at , the AMA will be a station of amazement full of Elastic engineers, tech leads, founders, and the creators of open source projects Logstash, Kibana, and Beats. Due to this popularity, without further ado, here is a list of our favorite AMA Booth moments and questions: Popular Questions AMA Booth Moments These reasons ultimately are why we feel the AMA Booth is a critical part of our events and will continue to be. For (San Francisco, February 17-19) we are going bigger and better with the AMA. We'll have the largest AMA Booth ever, with every single technical resource in our company at the conference. The AMA will be open almost the entire time of the conference too. So bring your thirst for knowledge, your hardest questions and we look forwarding to Answering Anything…. well almost anything. ","locales":"","title":"AMA (Ask Me Anything) World Recap, Bigger Than Ever For Elastic{ON}16"}
{"index":{}}
{"author":"Pius Fung","category":"Engineering","publish_date":"2016-01-05T00:00:00.000Z","url":"/blog/practicing_the_art_of_zendesk_with_the_elastic_stack","seo_title":"Practicing The Art of Zen(desk) With The Elastic Stack","content":" is a popular lightweight helpdesk solution. In this blog post, I will demonstrate how to use the Elastic Stack to provide discovery and visualization capabilities on data that is extracted from Zendesk via a custom Logstash plugin. Logstash: Gather What Matters (Not Just For Logs)Logstash is often used for log collection. However, it is highly extensible and has an intuitive plugin framework developers can use to build their own plugins to fetch data from other sources. To give a perspective on what it will take to build a custom Logstash plugin: I am a software support engineer (not a seasoned developer) with some experience writing internal tools and integrations. With no prior Ruby knowledge, I was able to follow the and write a custom Logstash Zendesk plugin that fetches objects from Zendesk using the official . The majority of the time (~60%) was actually spent on learning the basics of Ruby and interaction with the Zendesk REST API and Zendesk Ruby API Client. The rest was writing code, testing and tweaking the implementation. The custom/community mentioned above supports fetching various Zendesk objects like organization, user, ticket, comment and topic. The following is an example input block (refer to the Github repository linked above for additional details). You can control the types of objects fetched, fetch mode (full or incremental based on last updated time) and whether to append comments to the ticket in chronological order, etc. input { zendesk { domain => \"company.zendesk.com\" user => \"user@company.com\" # password => \"your_password\" api_token => \"your_api_token\" organizations => true users => true tickets => true tickets_last_updated_n_days_ago => 1 comments => true append_comments_to_tickets => true topics => true sleep_between_runs => 60 } } And the logging output of the plugin at --verbose level: Kibana: Open Up Your MindWith the Zendesk data ingested into Elasticsearch via Logstash, you can use the popular tool to search, discover and visualize the data. For example, here’s a date histogram aggregated by the severity levels of tickets over time. Imagine the other possibilities such as returning the top N customers mentioning keywords like ldap, hadoop, etc.. in their comments. Watcher: Help Others Be MindfulWith the commercial plugin (), you have endless opportunities to create watches not just to help streamline support operations (eg. tracking high severity and aging tickets, etc..). With the power of text search and aggregations, you can also monitor and alert on product quality trends. For example, the following watch’s identifies tickets with comments mentioning out of memory conditions in the past 30 days and aggregates the result by week showing the top 10 tickets sorted by version and date. This will produce a payload that can be to your desire and then sent to an output action. To whom do you send this to? Imagine setting up an output action for the payload to alert the product management team using their or / room so they can determine if there are areas in the product that can be improved (eg. circuit breakers). Shield: Block Out DisturbancesYou don’t want your customer data to be wide open. Use the commercial plugin () to require and access control, and set up to further secure your data. Classified customers? No problem, Shield provides more granular role based access control via . Found: Find Peace Of MindDeploying the Elastic Stack on is quick and simple. It literally takes just minutes to spin up a Found cluster provisioned with the latest and greatest Elastic releases. Since it is important to prevent unauthorized access to the data set, you can choose which includes the commercial plugins mentioned above (Shield and Watcher). Once the Found cluster has started, you can simply configure the in your Logstash pipeline to reference the Found cluster’s host(s), Shield use","locales":"","title":"Practicing The Art of Zen(desk) With The Elastic Stack"}
{"index":{}}
{"author":"Joe Fleming","category":"Engineering","publish_date":"2016-01-05T00:00:00.000Z","url":"/blog/kibana-development-tooling","seo_title":"Development Tooling Behind Kibana","content":" Here on the Kibana team, we're in a unique position:  we consume the products that other teams are building as part of what we are building. At a minimum, we have to stand up an Elasticsearch instance and we have to index some data. Short of that, we can't run Kibana, and we can't do our jobs.  This may seem simple enough, but it starts to get complicated pretty quickly when we need to test against many different versions of Elasticsearch. We also often need to test different plugin configurations, different datasets, and even run development versions of Elasticsearch. With all these varying requirements, things can get complicated and time consuming quickly. In order to make our lives easier, it's in our best interest to automate as much of that process as possible. Naturally, we've built some tools to help with that. Automating DataLet's work backwards and start with indexing data first. To help us with that task, we use a tool called . As the project states, it pushes fake HTTP traffic logs into Elasticsearch. There's a little bit of edge-case data that it generates, but it's basically the kind of logs you expect from any Apache or nginx server, with a bell shaped traffic pattern and everything. It's a quick and dirty - and very convenient - way for us to get data indexed so that we can start creating visualizations. It's really handy for smoke-testing things, particularly when we are spinning up new clusters, which we do a lot. It's also a great way to get people new to the team up and running, before they find something real that they want to index. Automating ElasticsearchMakelogs is great, but its scope is tiny, and its utility outside of Kibana development is limited. Perhaps more interesting is the tool we use to automate Elasticsearch, a tool named . Short for Elasticsearch Version Manager, esvm was born out of our need to maintain not just multiple versions of Elasticsearch, but also multiple clusters, each with its own unique configuration and data. It will download the specified version of Elasticsearch, start it up with the default configuration, use your computer's hostname for the cluster name to prevent external auto-joining, and even wrap its output to help make it a little easier to read. If you want to run the latest version of Elasticsearch, no arguments are required, just run . If you want a different version, just pass it as an argument, like . If, instead of a version, you need to run a build from a specific branch, something like will do just that. All of that is handy, but esvm gets really interesting, and really powerful, when you use it with a JSON config file.  Automating Cluster ConfigurationThe esvm config takes all the settings from the standard elasticsearch.yml and passes them as runtime settings. Common settings like enabling CORS, turning on mlock, and turning off multicast make great default settings. Any clusters you define will inherit the default settings, but also allow you to override them as needed. Plus, you can define the version or branch to use, and any plugins to install. For example, if you wanted to run a 3 node cluster built from the latest commit on the master branch, this is all it takes: \"clusters\": { \"latest\": { \"branch\": \"master\", \"nodes\": 3 }, } Then run and you're up and running. From there, adding plugins is easy, just add a \"plugins\" section to the cluster configuration and a list of the plugins you'd like to install. \"plugins\": [ \"license\", \"shield\" ] Relaunch the cluster and now you've got Shield installed with an evaluation license ready to go. And if you'd like to pre-define some users and their roles, just add this to the configuration: \"shield\": { \"users\": [ { \"username\": \"kibana\", \"password\": \"notsecure\", \"roles\": [\"kibana4_server\"] }, { \"username\": \"user\", \"password\": \"notsecure\", \"roles\": [\"kibana4\"] } ] } And because you can define as many cluster configurations as you'd like, a single JSON file ","locales":"","title":"Development Tooling Behind Kibana"}
{"index":{}}
{"author":"Miguel Bosin","category":"Engineering","publish_date":"2016-01-04T00:00:00.000Z","url":"/blog/key-point-to-be-aware-of-when-upgrading-from-elasticsearch-1-to-2","seo_title":"Elasticsearch 2.x upgrade","content":" Overview As part of any software deployment life cycle, one is often faced with the need to upgrade to the latest release of a product:  both to keep up with new features and bug fixes as well as ensure supportability. Elasticsearch is no different and with the release of version 2.0 a number of major changes have been included, which can result in breaking an existing 1.x installation upon upgrade. This is referred to as in the documentation. Please note that a FULL cluster restart will be needed. on upgrading from Elasticsearch 1.x to 2.x. So how do I upgrade to Elasticsearch 2.x smoothly? With a bit of testing in a non-production environment, there should be very little reason for any upgrade to fail or create inconsistencies in your data. This does, however, require some planning which could be summarised as follows:                  My upgrade is broken, please help! Should the upgrade gremlins appear and attempt to wreak havoc in your life, regardless of having carefully followed the previous set of steps, there are some additional avenues you can pursue: Looking for ways to minimize your downtime or reduce your maintenance window for a full cluster restart? If the risks seem too great or the upgrade mountain too high to climb, there are some options you can also consider: Although this approach will require more physical (or virtual) resources initially (although the likes of a test environment could be repurposed for this function temporarily at little additional capacity cost), it will be for a very short period of time. Once the new cluster is able to match the data of the old, along with all required functionality for your front-end application, the full switch-over may be an easier alternative. Known Issues Below are a list of issues that have been seen which may be of value when considering your upgrade plan, along with any troubleshooting you need to perform with failed or partial upgrades: ","locales":"de-de,fr-fr,ja-jp,ko-kr","title":"Key points to be aware of when upgrading from Elasticsearch 1.x to 2.x"}
{"index":{}}
{"author":"Erik Redding","category":"Engineering","publish_date":"2015-12-30T00:00:00.000Z","url":"/blog/log-in-upgrade-be-happy","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud.There are a few different types of users on Found, the official Elasticsearch as a Service by Elastic. We have customers that want the latest and greatest and are first to jump on the latest releases of Elasticsearch. On the other hand, we also have customers that want to keep things where they are because they’re satisfied with the cluster’s feature set. Either way, we’re glad you’re on board but we want to make sure you’re getting the most from your cluster. Upgrades are Easy on FoundOne of the best things about using Found is the ability for customers to be able to upgrade Elasticsearch versions and cluster sizing with just a couple of clicks. Another great feature of Found is the same-day availability of the latest Elasticsearch releases. We bill by the hour, so spinning up a test cluster is affordable. As an Ops guy, the features on Found make me smile because I can make sure my Elasticsearch clusters are running with the latest bug fixes, security patches and performance gains in Elasticsearch. At Elastic we use Found in a myriad of ways, from site search to BI and analytics. The fast upgrade paths in combination with high-availabilty configurations let me rest easy while we dog-food Found clusters to support the running of our business. When I’m looking to upgrade a cluster to a new release, I’ll typically build out a quick checklist of tasks. Here’s my abbreviated minor-version upgrade checklist: If everything goes well, we’ll carry out the upgrade. Major release versions typically require much more testing, but again, smoke-testing a major release upgrade can be just as easy depending on your configuration. The  has some more information around renaming indexes on restore and other tips. The LatestWhy upgrade versions, you ask? There are some key features for different Elasticsearch versions that we want our Found customers to be made aware of and embrace. 1.X ClustersIn general, clusters running anything older than 1.7.4 are encouraged to move to 1.7.4. Breaking changes are outlined in our :  most customers will find that they can upgrade easily to 1.7.4. If you’re wondering what’s new for the 1.7 releases, there have been a series of blog posts around the release announcements for , , , and . These posts outline high-level notable features for each release. 2.0.X and 2.1.X ClustersIf you’re running an Elasticsearch 2.0.X or 2.1.0 cluster without replicas, please upgrade to 2.0.2 or 2.1.1 today because there is a very important fix around . The default cluster configuration on Found is to have a single replica shard, but many people adjust settings for their use case. Kibana ReleasesWe manage all the details behind the Kibana releases so you don’t have to worry about keeping those up to date. If there’s a security-related patch like we saw with the , we will upgrade the Kibana instance. Finding Out What’s NewCustomers interested in keeping up with the latest releases and improvements in Elasticsearch can follow the  for the latest release announcements and more. The  are a great way to keep up with the community and include . In Conclusion… ","locales":"","title":"Log In, Upgrade, Be Happy"}
{"index":{}}
{"author":"Blake Niemyjski","category":"User Stories","publish_date":"2015-12-22T00:00:00.000Z","url":"/blog/being-exceptionless-with-elasticsearch","seo_title":"Exceptionless with Elasticsearch","content":" is an open source technology company that deals in real-time event reporting and logging, focusing specifically on error reporting. Our goal is to help the world's developers improve their applications and user experience by being… well... exceptionless. To be Without ExceptionExceptionless provides real-time error reporting for JavaScript, Node, , Web API, WebForms, WPF, Console, and MVC apps. Everything is pulled into a dashboard that organizes each event into a simple yet robust and fully-featured view that gives you quick access to actionable data that will help your app become exceptionless! We’ve learned quite a bit since the first version of Exceptionless, and we’d love to share our story on why we ended up migrating to Elasticsearch with great success. In the first version of Exceptionless, we were using MongoDB for our primary document data store. We chose it for it’s ease of use, ability to scale out instead of up, and the cross platform story. It worked pretty well until we started to grow, then we began having issues scaling several features, such as real-time dashboards with automatic time zone shifting, backups, restores, and storage. Real-time DashboardsWhen we first built our real-time based dashboards with MongoDB, we had to do a ton of research on timezones, which is a monumental task in and of itself. We found that we would need to pre-aggregate data into 15 minute buckets of time (to account for different time zone offsets) into separate collections, in addition to our event collections. This would allow us to quickly return statistics data for any 15 minute time frame, but it also added the expense of additional storage and processing time needed to shift the buckets of stats into the desired time zone with each request. We were really excited to learn that, by moving to Elasticsearch, we would not only solve this difficult problem using the , but that it would also allow us to: Fig. 2: Exceptionless Kibana 3 Dashboard Backup and RestoringOne of the huge pain points prior to moving to Elasticsearch was backing up MongoDB. There isn’t a lot of documentation for doing so in production, which leads users to bring down the cluster to do a quick backup via MongoDump, zip the backup, and upload it to some destination. The other option is to pay monthly fees and run an extra node for the MMS service, which does oplog backup and restore. One of the most underrated features of using Elasticsearch is backups and restores! It has been the most pleasant experience to configure and use. We setup a on one of our nodes to run for cleanup, and to run a backup to azure blob storage via the . The nicest thing about this is you can quickly choose exactly what index to restore to a development machine if you wish to debug production data, while keeping your data secure and backed up in the cloud. Disk SpaceAfter migrating our data we found that it used about one-fifth the disk space as MongoDB (please note that wasn’t released until sometime after we migrated). This allows us to cut our storage costs across the board. Scaling OutThere is nothing worse than waking up to find that a VM reboot of one of the nodes or adding a new node caused your MongoDB cluster configuration to become corrupted. This is a very scary situation we once had to deal with and resolve by restoring the cluster configuration. We always found that dealing with the MongoDB cluster configuration is something you don’t touch if it’s working. We love how easy it is to an Elasticsearch cluster. It’s a dream come true! No matter how you want it to run, it’s just a simple configuration setting or REST call away! Best of all, you can share your configuration with others to get feedback! ConclusionWe are very happy with the move to Elasticsearch and continue to receive additional value from it with every release. We really like that it’s easy to setup and is open source ","locales":"","title":"Being Exceptionless with Elasticsearch"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2015-12-21T00:00:00.000Z","url":"/blog/weekly-beats-filebeat-filtering-inputs","seo_title":"","content":" Welcome to ! With this series, we're keeping you up to date with all that's new in , from the details of work in progress pull requests to releases and learning resources. Last week we released Beats 1.0.1.  , and send us your feedback on . Jstatbeat We are happy to see another Beat created by the community: created by.  It used for monitoring the JVM garbage collector monitoring by reading jstat results and indexing them in Elasticsearch. There is a blog post about the new Beat (in Japanese). Fixing ignore outgoing transactions in Packetbeat An regarding configuration option was reported last week. The problem appeared as the configuration option was working only when the topology map was created. The allows usage of ignore_outgoing configuration option without the need to have the toplogy map enabled. Additionally, setting the “direction” to outgoing is fixed in the same.   Reduce bulk size The bulk size value is changed from 10k to 200 with the. The high value of 10k caused memory issues when Logstash was not available or slow to process data. This is because the 10k gets multiplied with the worker queue size (1000). Exclude files in Filebeat The adds support for the “exclude_files” configuration option to the Filebeat prospector. You can specify a list of regular expressions to match the files that will be ignored. This is useful if you want to include all the files in a directory (/vat/log/apps), but exclude those that have been rotated and zipped (.gz). filebeat: prospectors: - paths: [ \"/var/log/apps/*\"] exclude_files: \"\\\\.gz$\" Non-string fields values in Filebeat This adds support for non-string values of the custom Logstash fields in the  configuration file. Each field can now be a scalar value, an array, a dictionary, or any nested combination of these. Add basic instrumentation to the Publisher extends the -httpptof interface to export a few basic metrics from the libbeat publishers over. These are very useful for us to troubleshoot performance issues and to detect when there are drops in Packetbeat and Topbeat. Working on adding multiline support in Filebeat support in Filebeat is an important feature request by our community. We started working on it and you can follow its status in this. Winlogbeat in the next releaseThe first version of Winlogbeat is almost ready and it will be included in the next minor release. It has the ability to read event logs from Windows Event Log API and Event Logging API. You can follow the Winlogbeat status under the. ","locales":"","title":"Brewing in Beats: Filebeat filtering inputs"}
{"index":{}}
{"author":"Efstathios Xagoraris","category":"User Stories","publish_date":"2015-12-21T00:00:00.000Z","url":"/blog/powering-itvs-devops-engine-with-the-elastic-stack","seo_title":"Powering ITV’s DevOps engine with the Elastic stack","content":" The ChallengeAt ITV we have several legacy products and several teams working on them. At the same time the teams are trying to deliver new and exciting products. Those systems emit logs to standard locations and designing a logging system was mostly a task for the product itself. There were different teams acting in a SILO mode, delivering a solution that worked only for them. We wanted a solution that works for all our products and different teams can easily inherit. This is the solution we came up with at ITV: Our objective was simple: One of the problems we tried to solve was around logging. Logging is one of the things that is constantly changing along with the product life. Traditionally we used to store logs in the file system and create reporting applications along with custom dashboards & interfaces for both development and management teams to search the logs. The problem with that approach is that it’s not repeatable, it’s not common and it is difficult to develop additional dashboards. We don’t want to SSH into servers to search application logs in 2015. We want to surface information to a higher level interface and give access to users through dashboards to search logs. The Elastic Stack to the RescueWe started using the Elastic technology stack 1.5 years ago and recently we made the decision to put the full Elastic stack deep in our Common Platform specification as the standard logging mechanism for all ITV products hosted by the platform. We tried and tested every component. It had to play nicely with our configuration management and our cloud environment before we adopted it. Elasticsearch’s ability to index & search million of log entries daily was impressive. The ability to create dashboards and share them using Kibana was unbelievably easy compared to the past. Finally, Logstash convinced us handling multiple sources with so many filter capabilities. All of that together lead to the fact that the Elastic stack won a thumbs up by our operations team. Using the Elastic Stack in our Brand New ITV HUBITV has replaced both ITV Player and ITV.COM with a new destination for catch-up content and online services, the . With over 11 million registered users and 726 million long form requests (video requests for actual programs and not short clips) last year we have a web platform that generates lots of logs. We use the Elastic stack to create useful dashboards for Ops & Devs to display operation metrics. Our Elasticsearch instances live on AWS and already handle millions of logs every day. Logstash ability to handle logs from multiple sources allow us to handle AWS logs such as Cloudtrail & ELB logs in the same Elastic stack. Elasticsearch’s curator keeps things tidy and everything is managed through our configuration management in a scalable solution.Below we have an example for some useful dashboard’s created by our developers to help them debug production issues: . What's Next?ITV Hub is only one of the many products within ITV that uses the Elastic stack. We already started building other products based on Kibana 4. Also we want to try the new Elasticsearch 2.0 and the time series data store as a potential replacement of our existing data store. It looks very promising and some of the early demos are exciting! Our core logging system is powered by the Elastic stack so all our product teams directly benefit from having a logging solution covered right at the beginning of the life of a product. System & Application dashboards really tear down the wall between Dev & Ops and the logs stream very smooth with the Elastic stack :) ","locales":"fr-fr","title":"Powering ITV’s DevOps Engine with the Elastic stack"}
{"index":{}}
{"author":"Kristina Frost","category":"Culture","publish_date":"2015-12-21T00:00:00.000Z","url":"/blog/the-giving-culture-of-elastic","seo_title":"The Giving Culture of Elastic","content":" I'm an only child from a big family.Growing up, my family lived pretty far away from the rest of my relatives, so every holiday season it was off \"to Grandmother's house we go\" -- not in a sleigh through a picturesque winter landscape, but in a car, driving anywhere from six to fourteen brutal hours across the American midwest. Both of my parents come from pretty big families, and so Christmas and Thanksgiving have always been the time of year when I'm thrust out of my comfortable, quiet family unit of three and into a circle of countless relatives. I joke with friends of mine that only children are tyrants. There are pros and cons to this: con - if you are an only child you don't share dishwasher duty with your siblings a few nights a week. You do it every night of the week. But on the flip side, when there's a child's birthday in the house, every present is for you. I guess what I'm getting at is that these early Christmases were the first time that I ever really remember watching other people open presents, the first place where I learned about giving and receiving in the context of a community. The first place where the principle of \"it is better to give than to receive\" ever made itself evident in my life.Of course, now I'm a grown-up, and so I don't just receive presents, I give them. In fact, I have a lot of fun every year trying to find the funniest gift, or the coolest one, or the one that people will really remember for more than just a month. I think these kinds of holidays are a way to celebrate each other perhaps as much as whatever is that our particular family tradition suggests we reflect on at this and other times of the year. And our family and friends are very often the easiest people to celebrate. What I've been reflecting on this holiday season, amidst a world in which tragic events surround us and frighten us, amidst divisive rhetoric that tends to suggest people who aren't from our tribe, whatever that tribe looks like, aren't worth our time or our energy or even our charity is: how much harder it is to celebrate the people we don't know.And I've been reflecting on what the world looks like for lots of children who don't get a share in the kinds of privileges that I grew up with. About a world where none of the presents are ever for them. Where every year the holiday season comes and goes, and it's never their turn at the table.Which is why, this month, I'm glad to be here at Elastic. Don't get me wrong: I've been glad to be here, because we build a product that an enormous community of folks out there in the world are using to try to solve problems for all kinds of companies, companies that build apps that are used by all kinds of people. I've been glad because I work for a , and because I finally work somewhere with a crack analytic team that might be able to solve one of life's biggest mysteries and tell me which White Elephant gift is really the most bang for my buck.More than all that, though, I'm also extremely proud: we've capped off a series of tour stops around the world where we raised nearly $150,000 for 9 local charities each with a concentration on boosting STEM (science, technology, engineering & math) programs for underserved youth. I lead a Women of Elastic brunch here in our Mountain View office (another blog post for another day, that), and we ladies challenged ourselves to double down on our own philanthropy efforts this holiday season. \"I just don't believe any child should ever go hungry,\" said one of our most tenured Territory Managers. But she doesn't just say it, she lives it, and she's the one who set up a donation drive in the office for a local food bank which wound up providing 3,240 meals. Canned food drives are great, don't get me wrong, but at wholesale rates, most food banks can really stretch the dollars of any donation, and it was fantastic to see just how far we were able to go. Our men of the office, not to go unmatched, led","locales":"","title":"The Giving Culture of Elastic"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2015-12-17T00:00:00.000Z","url":"/blog/elasticsearch-2-1-1-and-2-0-2-and-1-7-4-released","seo_title":"Elasticsearch 2.1.0 and 2.0.1 released","content":" Today we are pleased to announce bug fix releases of based on , , and Elasticsearch 2.1.1 contains a number of important bug fixes. Where possible, these have been backported to Elasticsearch 2.0.2. All users of Elasticsearch 2 are advised to upgrade to Elasticsearch 2.1.1 if possible, or at least to Elasticsearch 2.0.2. Bug fixes in 2.0: Bug fixes in 1.7.4: Elasticsearch 1.7.4 contains a few improvements to delayed shard allocation that will make recovery after node restarts more efficient. The most important changes in 2.1.1 and 2.0.2 are as follows: Translog corruption when disk fullThe transaction log could be corrupted when writing to disk fails for some reason such as a full disk. When Elasticsearch detects the corruption it will fail the shard and recover to a new node. However, if replication is disabled — a configuration that we advise against — then this bug could lead to data loss. ( , backported to 2.0.2) We have also removed a spurious log message indicating that a temporary transaction log file could not be deleted (, backported to 2.0.2) and another reporting failures to write to a transaction log which has already been closed (, backported to 2.0.2). Preventing conflicting mappingsFields with the same name in different types in the same index must have the same mapping — conflicting mappings are not allowed. Unfortunately, when using dynamic mappings, only the first attempt to add a conflicting field would be rejected. Subsequent attempts could succeed. This was a serious bug as the conflicting mappings prevent shards from the index from being relocated to a different node or from being recovered after a restart. (, backported to 2.0.2). This change also fixes an issue that prevented users of field datatypes provided by plugins from upgrading, e.g. , , . NullPointerException during indexingA number of users of 2.1.0 reported NullPointerExceptions while indexing, which turned out to be caused by a failure to clear an empty during refresh. This has been fixed in and in . Children aggregation missing documentsThe aggregation only evaluated segments that contained matching parent documents, which meant that it might miss child documents that existed in other segments and so report incorrect results. (, backported to 2.0.2) Tribe node configurationA bug was introduced in 2.1.0 that caused Tribe nodes to require their own configuration directory for each cluster, instead of accepting their settings from the main configuration. () Index and alias name conflictsA bug in 1.x allowed an index template to create an alias with the same name as an index (fixed in ). This is an illegal state that prevents users from upgrading to 2.x. Any index that has an alias with the same name must be deleted before upgrading. However, the exception thrown in 2.x was incomprehensible and has been replaced with a more meaningful message. ( , backported to 2.0.2) Performance regression after deleting an indexIn 2.1.0, creating an index, indexing some docs, deleting the index, then creating an index with the same name led to a performance regression on further indexing requests. () Watcher email alertsA classloader bug which sometimes prevented Watcher from sending emails has been fixed in both 2.1.1 and 2.0.2.ConclusionPlease , try it out, and let us know what you think on Twitter () or in our . You can report any problems on the . ","locales":"","title":"Elasticsearch 2.1.1, 2.0.2, and 1.7.4 released"}
{"index":{}}
{"author":"Tudor Golubenco","category":"Releases","publish_date":"2015-12-17T00:00:00.000Z","url":"/blog/beats-1-0-1-released","seo_title":"","content":" Today we are releasing a patch release for the Beats, fixing some of the issues reported since the 1.0.0 release. Changed the default bulk size for the communication with LogstashThe previous default was quite high (10000), which worked good in normal operations but could cause a significant increase in memory usage when Logstash was temporary unavailable. We have reduced the default to 200, which worked good in the real world installations that we tried it on. The setting can be changed from the option from the logstash setting in the configuration of any of the Beats. Improved the force_close_files configuration settingThe settings exists in Filebeat to deal with the way Windows still shows deleted files in the file system until they are closed by all the processes that have them open. We tweaked its behavior a little to better detect when files are rotated on Windows. See this for the discussion. Fixes and performance improvements for the Redis and MongoDB parsersSeveral performance issues and crashes in the Redis and MongoDB protocol implementation in Packetbeat have been solved. Please see the release notes for the links to the relevant tickets and pull requests. ","locales":"","title":"Beats 1.0.1 released"}
{"index":{}}
{"author":"Court Ewing","category":"Releases","publish_date":"2015-12-17T00:00:00.000Z","url":"/blog/kibana-4-3-1-and-4-2-2-and-4-1-4","seo_title":"","content":" Today we’re releasing stability and security updates to Kibana 4.3, 4.2, and 4.1. We strongly recommend that all users upgrade their Kibana installs immediately. Due to the security fixes, Found customers are being updated automatically. ​ 4.3.1 Changes 4.2.2 Changes 4.1.4 Changes Where to download You can download Kibana at our page. If you encounter any bugs with either of these versions, please file them on the github page. If you have any questions or concerns, do no hesitate to reach out on or our discussion . ","locales":"","title":"Kibana 4.3.1, 4.2.2, and 4.1.4 released"}
{"index":{}}
{"author":"Martin Smith","category":"User Stories","publish_date":"2015-12-17T00:00:00.000Z","url":"/blog/deploying-elasticsearch-200-with-chef","seo_title":"ES Chef Cookbook","content":" Chef is a configuration tool written in Ruby and Erlang. It uses a pure-Ruby, domain-specific language (DSL) for writing system configuration “recipes”. Chef is used to streamline the task of configuring and maintaining a company’s servers, and can integrate with cloud-based platforms such as Rackspace, Internap, Amazon EC2, Google Cloud Platform, OpenStack, SoftLayer, and Microsoft Azure to automatically provision and configure new machines. As mentioned above, the can help you move from a demo environment to a full-fledged production ready cluster. With the newest 2.0.0 release, the cookbook can help you automate and maintain the following: Features of the Chef cookbook for Elasticsearch: Pre-requisites Before we begin, there a few things you should have already for this tutorial. Namely With those out of the way, let’s start by showing how to build a wrapper cookbook that can install, configure, and start the Elasticsearch software on a new server. Build your wrapper cookbook Now we’re ready to build our new cookbook. We’re using the here, which means we’re making a new cookbook that we run directly on our new ES server:  it will be the only cookbook we run on our ES server, and it will for the new server as well. We will be calling our wrapper cookbook elasticsearch-chef-blog, and we’ll use ChefDK’s chef command to create it: root@chef-workstation01:~# chef generate cookbook elasticsearch-chef-blog-demo Compiling Cookbooks... Recipe: code_generator::cookbook ... root@chef-workstation01:~# cd elasticsearch-chef-blog-demo/ root@chef-workstation01:~/elasticsearch-chef-blog-demo# ls -la total 48 drwxr-xr-x 6 root root 4096 Nov 22 20:39 . drwx------ 8 root root 4096 Nov 22 20:39 .. -rw-r--r-- 1 root root 47 Nov 22 20:39 Berksfile -rw-r--r-- 1 root root 1029 Nov 22 20:39 chefignore drwxr-xr-x 7 root root 4096 Nov 22 20:39 .git -rw-r--r-- 1 root root 126 Nov 22 20:39 .gitignore -rw-r--r-- 1 root root 355 Nov 22 20:39 .kitchen.yml -rw-r--r-- 1 root root 267 Nov 22 20:39 metadata.rb -rw-r--r-- 1 root root 77 Nov 22 20:39 README.md drwxr-xr-x 2 root root 4096 Nov 22 20:39 recipes drwxr-xr-x 3 root root 4096 Nov 22 20:39 spec drwxr-xr-x 3 root root 4096 Nov 22 20:39 test Next, we’re going to add the Elasticsearch cookbook as a dependency in metadata.rb in our new cookbook. We’ll add the line at the bottom of metadata.rb, so that our full file now looks like so: name 'elasticsearch-chef-blog-demo' maintainer 'The Authors' maintainer_email 'you@example.com' license 'all_rights' description 'Installs/Configures elasticsearch-chef-blog-demo' long_description 'Installs/Configures elasticsearch-chef-blog-demo' version '0.1.0' depends 'elasticsearch', '>= 2.0.0' elasticsearch Write your recipe Next, we need to write the simple recipe that utilizes Chef resources to install a simple Elasticsearch node. We will edit the file recipes/default.rb, and we’ll place the following snippets there: A note about resource names Many of the resources provided in this cookbook need to share configuration values. For example, the resource needs to know the path to the configuration file(s) generated by and the path to the actual ES binary installed by . And they both need to know the appropriate system user and group defined by . Search order: In order to make this easy, all resources in this cookbook use the following search order to locate resources that apply to the same overall Elasticsearch setup:  Create a user Whether we decide to install from a .tar.gz archive or using the provided OS packages, we use the elasticsearch_user resource here to be sure that a user and group are created for Elasticsearch on our system. The later resources will use this resource to determine what the appropriate user should be for Elasticsearch. By default, we name this user elasticsearch. elasticsearch_user 'elasticsearch' Alternately, if you’re installing by package, and would rather le","locales":"fr-fr","title":"Deploying Elasticsearch 2.0.0 with Chef"}
{"index":{}}
{"author":"Eric Fitz","category":"User Stories","publish_date":"2015-12-17T00:00:00.000Z","url":"/blog/aee-found-solution-with-elasticsearch-as-a-service","seo_title":"Why AEE switched from Postgres to Elasticsearch as a Service","content":" The ChallengeMuch of energy -- especially utilities -- is regulated at the state level. That means 50 different legislatures and executive branches to monitor and engage with. But the most challenging policymaking entities to keep track of are the Public Utilities Commissions (PUC). PUCs influence $100 billion of investment annually through market rules, gas and electricity rates, and access to the grid. They have tremendous power to either foster or thwart innovation. To survive and prosper, advanced energy companies must be able to access and track what is happening across all 50 states. Similarly, a host of nonprofit organizations, government agencies, and the media, charged with reforming, regulating, and reporting on this critical piece of our national infrastructure requires similar information, and to be useful, it needs to be accurate and up to date. More pages of content than Wikipedia To help address this challenge, we have aggregated and indexed over 45M pages of regulatory documents from across the country, a collection larger than the English version of Wikipedia. In the past two years alone, PUCs and participants in their proceedings have generated over 10 million pages of text. While typically stored in 50 different PUC websites, many of which are difficult to navigate, all of these filings are available on our PowerSuite platform, which makes this energy policy data searchable and actionable for the first time. Our users range from private companies with memberships or paid subscriptions (e.g. Opower, GE, and EnerNOC) to a large number of users with free accounts including government employees (e.g. DOE, EPA, and NREL), journalists (e.g. GreentechMedia, InsideEPA, and E&E news), and higher education (e.g. Harvard, MIT and Berkeley).   Full text search on Postgres? When we launched our core platform in the summer of 2014, our data was being served from a PostgreSQL database. In our initial testing (with a more limited dataset) we had determined that its full text search would meet the majority of our initial search needs with the maintenance benefits of a single ACID data store. However, as our user base grew and we continued to aggregate more data, we started to hit some challenges. Our nightly index maintenance tasks were creating tremendous index bloat leading to longer and longer processing times and reduced search performance, and memory requirements to performatively serve highly normalized data were becoming cost prohibitive. At the same time, our users were asking to perform ever more complex queries with exact phrases, boolean logic, and on-the-fly categorizations to support drill-down searching. Solution “Found”: Elasticsearch as a Service We had considered Elasticsearch as an alternative in the past, but did not have the resources to manage additional servers. With our users in need of a more complete set of search features and our Postgres cluster at the practical limit for vertical scaling, we launched a transition to Elasticsearch.   As a small development team inside a nonprofit, we still could not afford the DevOps engineering time required to quickly set up and then manage this novel type of server cluster. We discovered , among other Elasticsearch as a Service firms, and selected them after witnessing the announcement of their acquisition by Elastic at the first annual Elastic{ON} conference. With Found, we could stand up and configure a large production cluster fully compliant with Elasticsearch best practices in mere hours, adjusting its size and configuration on the fly as we observed our data loading and search utilization in real time. Our setup We began our cluster with a generous 64GB of memory and 512GB of disk space. After observing the disk size of our initial data load and the memory requirements of normal user activity, we scal","locales":"","title":"Making Advanced Energy Policy Documents Searchable and Actionable for the First Time"}
{"index":{}}
{"author":"Zachary Tong","category":"Engineering","publish_date":"2015-12-16T00:00:00.000Z","url":"/blog/implementing-a-statistical-anomaly-detector-part-3","seo_title":"Implementing a Statistical Anomaly Detector in Elasticsearch - Part 3","content":" Welcome to the third and final installment of this series on building a statistical anomaly detector in Elasticsearch.  As a quick recap, let’s look at what we’ve built so far: Today, we’ll take what we built in Part 1 and 2 and automate it completely using , Elastic’s real-time alerting and notification plugin for Elasticsearch. With Watcher’s ability to use mustache templating and groovy scripting, it is a remarkably powerful alerting engine.  We can encode the entire Atlas system in just two watches.  The first watch will generate all of the surprise data (just like Part 1) while the second watch will create the threshold and check for anomalies (like Timelion in Part 2). Let’s get started! Data Collection WatchThe first watch’s job is to collect the top 90th surprise values for each metric on an hourly basis, emulating the data collection process we built in Part 1.  This means we can leverage most of the hard work from that section (e.g. the pipeline aggregation).   First, here is the entire watch (then we’ll break it down piece-by-piece): PUT _watcher/watch/atlas { \"trigger\":{ \"schedule\":{ \"hourly\" : { \"minute\" : 0 } } }, \"input\":{ \"search\":{ \"request\":{ \"indices\":\"data\", \"types\": \"data\", \"body\":{ \"query\":{ \"filtered\":{ \"filter\":{ \"range\":{ \"hour\":{ \"gte\":\"now-24h\" } } } } }, \"size\":0, \"aggs\":{ \"metrics\":{ \"terms\":{ \"field\":\"metric\" }, \"aggs\":{ \"queries\":{ \"terms\":{ \"field\":\"query\" }, \"aggs\":{ \"series\":{ \"date_histogram\":{ \"field\":\"hour\", \"interval\":\"hour\" }, \"aggs\":{ \"avg\":{ \"avg\":{ \"field\":\"value\" } }, \"movavg\":{ \"moving_avg\":{ \"buckets_path\":\"avg\", \"window\":24, \"model\":\"simple\" } }, \"surprise\":{ \"bucket_script\":{ \"buckets_path\":{ \"avg\":\"avg\", \"movavg\":\"movavg\" }, \"script\":\"(avg - movavg).abs()\" } } } }, \"largest_surprise\":{ \"max_bucket\":{ \"buckets_path\":\"series.surprise\" } } } }, \"ninetieth_surprise\":{ \"percentiles_bucket\":{ \"buckets_path\":\"queries>largest_surprise\", \"percents\":[ 90.0 ] } } } } } } }, \"extract\":[ \"aggregations.metrics.buckets.ninetieth_surprise\", \"aggregations.metrics.buckets.key\" ] } }, \"actions\":{ \"index_payload\":{ \"transform\":{ \"script\": { \"file\": \"hourly\" } }, \"index\" : { \"index\" : \"atlas\", \"doc_type\" : \"data\" } } } } It’s long, but don’t panic!  A lot of it is repeated code from Part 1.  Let’s start looking at the individual components: PUT _watcher/watch/atlas { \"trigger\":{ \"schedule\":{ \"hourly\" : { \"minute\" : 0 } } }, The first thing in our request is the HTTP command.  Watches are stored inside your cluster, so we execute a PUT command to the endpoint and add a new watch called “atlas”.  Next, we schedule the watch to run with a “trigger”.  Triggers allow watches to run on schedules, much like a cronjob.  We are going to use an , which fires every hour on the hour. After our trigger, we define the \"input\" to the watch: \"input\":{ \"search\":{ \"request\":{ \"indices\":\"data\", \"types\": \"data\", \"body\":{...}, \"extract\":[ \"aggregations.metrics.buckets.ninetieth_surprise\", \"aggregations.metrics.buckets.key\" ] } }, Inputs provide the data that a watch uses to make decisions.  There are a variety of inputs available,  but we’ll use a input.  This input executes an arbitrary Elasticsearch query and allows a watch to use the response for later processing.  The “request” parameter defines the details about the request: the indices/types to query and the request body (which is the pipeline aggregation we built in Part 1).  Combined with the trigger, our watch will execute the large pipeline agg against the raw data every hour. The “extract” parameter lets us extract details that we are interested in, to simplify further processing in the watch.  It is conceptually very similar to , merely a filtering mechanism to reduce response verbosity.  Here we are using it to extract the five top-90th percentile sur","locales":"","title":"Implementing a Statistical Anomaly Detector in Elasticsearch - Part 3"}
{"index":{}}
{"author":"Michael McCandless","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2015-12-14T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2015-12-14","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top NewsShould you put your data into a new index or into a new type of an existing index? — elastic (@elastic) Elasticsearch Core Apache Lucene Watch This Space, where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2015-12-14"}
{"index":{}}
{"author":"Dimitrios Liappis","category":"Engineering","publish_date":"2015-12-14T00:00:00.000Z","url":"/blog/autoresize-ebs-root-volume-on-aws-amis","seo_title":"Autoresize EBS root volume for AWS AMIs","content":" I can’t increase the size of my EBS root volume when I launch a new Debian instance!Setting a larger than default (8GiB) root EBS device, when launching Debian AMIs, does not result in a similarly sized root filesystem. In this article we are investigating why this doesn’t work out of the box, and how to create new AMIs that will be resized correctly after launch. Example:You want to launch a new Debian instance with the root device on Amazon EBS. So you select an appropriate root EBS AMI, for example:  In the “Add Storage” step, change the default size from 8 GiB to something larger:  33 GiB in this example: After the instance has come up, ssh using your keypair and your root filesystem is still showing 8GiB: admin@ip-172-31-2-65:~$ df -h Filesystem Size Used Avail Use% Mounted on /dev/xvda1 7.8G 641M 6.8G 9% / Where did my space go?Digging a bit deeper with we can see: admin@ip-172-31-2-65:~$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT xvda 202:0 0 33G 0 disk └─xvda1 202:1 0 8G 0 part / The Amazon EBS disk () presented to the OS has the right size: 33G However the partition where the OS resides () is stuck at 8G. Linux AMIs come preinstalled with a customized version of the package. This includes Python modules that can and , CLI tools like and . On standard Debian AMIs, they aren’t there. CentOS AMIs/EPEL in comparison have the rpm package available providing the tool. cloud-init tools will also detect if the root file system (/) size differs from the partition size and utilize the appropriate filesystem expansion tool (e.g. for ext4) to match the partition size. Resizing an online partition on a linux AMINaturally we would like our AMIs to resize the root partition by themselves, to use all available space. Since is not available in Debian AMIs, we can accomplish this with and an init script that runs on the first boot (when the instance gets launched). Making work non-interactivelyOne challenge here is that most tools will complain about resizing the partition containing a mounted [root] file system. The version of parted shipped with Debian jessie allows us to and -- albeit not so well documented -- non interactively as well. Normally, a command like this would suffice, but because the partition is in use, prompts for confirmation which would interrupt the flow in a script: root@ip-172-31-2-65:~# /sbin/parted /dev/xvda resizepart 1 100% Warning: Partition /dev/xvda1 is being used. Are you sure you want to continue? /sbin/parted: invalid token: 100% Yes/No? Luckily has an  that allows us force grow the partition without prompts: root@ip-172-31-2-65:~# /sbin/parted ---pretend-input-tty /dev/xvda resizepart 1 yes 100% Warning: Partition /dev/xvda1 is being used. Are you sure you want to continue? Information: You may need to update /etc/fstab. The output may not be very encouraging, but it worked: root@ip-172-31-2-65:~# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT xvda 202:0 0 33G 0 disk └─xvda1 202:1 0 33G 0 part / The only thing remaining is to resize the filesystem. This will be done automatically by the tools after a reboot or by running Putting it all togetherIf you are building your own custom AMI, it makes sense to add an init script that does the above steps immediately after a new instance gets launched and ensures it won’t rerun itself again. An important requirement is to have parted installed:  you can either do that inside the script or ensure gets preinstalled as part of the scripts building your custom AMI. For Debian AMIs running the will create an init script linked to that will take care of all the resizing. As the last step it will deactivate itself and report activities under Are other AMIs affected by this problem?Yes! CentOS6 AMIs are affected. Instead of though -- which is an old version that can’t resize online partitions -- make sure is installed from the package  This pa","locales":"","title":"Autoresize EBS root volume on Linux-based Amazon AMIs"}
{"index":{}}
{"author":"Megan Wieling","category":"","publish_date":"2015-12-14T00:00:00.000Z","url":"/blog/witwies-elasticontour-tokyo","seo_title":"Where in the World is Elastic? - Elastic{ON}Tour Tokyo","content":" Welcome to Find out which Elastic events and meetups are happening near you this week. Upcoming EventsDecember 16: Upcoming MeetupsDecember 14: December 15: December 15: December 16: December 15: December 16: December 14: December 19: December 20: That's it for this week. Stay tuned next week for more Elastic happenings.- The Elastic Team P.S. if you're interested in hosting a meetup or are giving a talk about Beats, Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic? - Elastic{ON}Tour Tokyo"}
{"index":{}}
{"author":"Lucie Daeye","category":"Culture","publish_date":"2015-12-15T00:00:00.000Z","url":"/blog/django-girls-review-of-elasticon-tour-munich","seo_title":"Django Girls review of Elastic{ON} Tour Munich","content":" is a non-profit organization that empowers and helps women to organize free, one-day programming workshops by providing tools, resources and support. It was born in Berlin in July 2014 and started by two Olas:  and . Today, Django Girls is a volunteer run organization with hundreds of people contributing to bring more women into the Python & Django communities. This summer, Django Girls celebrated its : nearly one hundred event have happened since its inception. Growing that quickly has been amazing but also a bit scary: to make it more sustainable, the support team decided to recruit someone to help them. I was lucky enough to be selected and I started working for Django Girls in September.  One of the perks of being the Django Girls Awesomeness Ambassador is receiving cool emails. Some of these emails include those from future organizers really excited about their workshops, as well as from attendees and coaches who just want to say thank you. When we received an email from expressing interest in supporting the Django Girls mission, I was only starting the job and I have to say, I was as excited as our organizers: sponsorship for Django Girls and going to Munich to run a booth, count me in! I started to prepare a booth, think about what to say to people and what swag to bring with me. A few days before going to Munich, we received another email from Elastic saying they managed to raise almost 15,000 € for us (see slide below from the event presentation. I went to Munich still not believing that all this money was for us! “Of course it is!”, said Livia, my contact from Elastic and she asked me what we planned to do with it. I'm especially excited, because thanks to the Elastic sponsorship my future as a Awesomeness Ambassador is secured: currently, most of the money Django Girls Foundation receives covers my role in the organization. My job is to make the life of the easier so they can work on other projects: a , cool , etc. My main job is to stay on top of the Django Girls inbox and make sure that anyone who asks for help will be answered swiftly with the necessary support, especially making sure everyone is happy and that planned events are happening as scheduled. I also help maintain our website and resources: our and its translations, , . On top of securing the existence of the Awesomeness Ambassador position, we also plan to use this money on two cool initiatives. The first one is sending swag boxes to organizers full of stickers, buttons and tattoos - it will be our “Django Girls Organizer Starter Kit”. If you haven’t heard about us already, you have to look at of our events: we want a positive learning atmosphere and all these little details contribute to it. The second initiative is working on the inaugural Django Girls Summit! Yes, the Summit! We are extremely excited about this plan. We want to organize a two day unconference where organizers could meet and share their experiences about  Django Girls workshops: what was hard, how they handled problems, how they find sponsors and so on and so on. We imagine it as a place where people who are making Django Girls what it is now could meet in person and learn from each other. We are really excited about this and can’t wait to start planning it! Thank you again Elastic for the visibility you’ve gave to our organization, for the booth at Munich and for this awesome sponsorship. You have been with us from almost the very beginning, supporting us on many different levels: as mentors, supporters and sponsors. Thank you for being awesome! ","locales":"","title":"Django Girls review of Elastic{ON} Tour Munich"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2015-12-14T00:00:00.000Z","url":"/blog/weekly-beats-webinar-performance-optimizations","seo_title":"","content":" Learn more about the Beats from our Webinar Last week we demoed Topbeat, Filebeat and Packetbeat in front of a big audience. Check the summary of the Webinar and watch the one hour demo to see the Beats in action and the hands on training presented directly by the Beats development team. Unified ChangelogWe combined all CHANGELOG.md files from all the Beats repositories into a single CHANGELOG.asciidoc in this . For each release the following sections are available: Breaking changes, Bugfixes, Added and Deprecated and contain details about the changes  in each Beat including libbeat. Group cpu usage per core informationThanks to a , cpu usage information is exported for each available core under the main root. Currently it is available only on Unix systems. Depending on the number of cores on your server, the number of exported fields can be quite high. To have a cleaner and organized output, we decided to group all the cpu usage per core information into the cpus group and is the result. Additionally the is added to make all this specific information optional. TCP layer drop connection state on gapA appeared when there was a gap in the TCP stream that caused parser errors, crashes and current packet data lost. To the issue the connection is dropped and re-initalized when a gap occurs. Winlogbeat The Winlogbeat is currently able to read the event logs from the Event Logging API that is provided in Windows XP, Windows 2003 and works also on all newer versions, but some events cannot be read through this API. The next step would be to read the event logs from Windows Event Log API that is available on the newer Windows systems: Windows Vista, Windows Server 2008 and give you additional information. You can watch the evolution of the new Beat under this . Change default configuration file pathIn the previous version, the default directory of the configuration file was set differently depending on the OS, so each binary package had a different default path to the configuration file. The simplifies it by setting the default directory of the configuration file to the directory where the binary is placed. Benchmarking  PacketbeatPacketbeat is parsing each message received on the network and allocates memory for each set of data to extract. This week we run some small experiments to reduce the number of memory allocations in the and . New Blog Post All the Beats are now leaving together in the same. Read about how we merged all the GitHub repositories into one. ","locales":"","title":"Brewing in Beats: Webinar and Performance Optimizations"}
{"index":{}}
{"author":"Alvin Chen","category":"Engineering","publish_date":"2015-12-14T00:00:00.000Z","url":"/blog/introducing-community-maintainers-for-logstash-plugins","seo_title":"Introducing Community Maintainers for Logstash Plugins","content":" Through the years, the Logstash project has evolved into a kind of swiss army knife for data ingestion, collecting and unifying a variety of data from a myriad of data sources into Elasticsearch and other destinations. Beyond classic logging use cases, you can now unlock deeper insights from , monitor or any , and easily derive value out of network packet data, system metrics, and more with the  framework. It just keeps on growing... ","locales":"","title":"Introducing Community Maintainers for Logstash Plugins"}
{"index":{}}
{"author":"Thierry Delprat","category":"User Stories","publish_date":"2015-12-14T00:00:00.000Z","url":"/blog/nuxeo-search-and-lucene-oh-my","seo_title":"","content":" Content Repository & SearchAt , our job is to provide a content repository, a stack of associated services and the needed assembly tools so that people can build their own content-centric business application in a clean, maintainable and scalable way.When it comes to scalability, the first challenge is usually not so much the storage volume but the ability to quickly execute search queries.Indeed, most of the screens of applications heavily rely on queries: searching for documents under a given hierarchy, in a given state or associated to a task... As a result, pretty much all screens of the application are issuing one or several queries.In addition, platform users can configure both the data structures and the screens so the used queries can vary a lot and can become very complex. Our History with LuceneWe've been building a content repository for more than 10 years, so we know about this search challenge. Since everything we do is open source, we have a long history with Apache Lucene.Strangely enough, we started using Lucene at a time where our repository was running on Python/Zope. We used to build an XML-RPC search service. This hybrid solution was a pain to setup (compile Lucene to native code via GCJ), but when it was finally up and running the performance was just amazing.When we moved the whole platform to Java, Lucene was logically one of the building blocs we re-used. At this time, our repository backend was and since the built-in Lucene index was not enough to fulfill our requirements for complex queries, we integrated that was providing a transactional layer on top of Lucene (and had an interesting ). This Lucene integration was not so successful for us since we ended up with a lot of missing sync and transaction deadlocks issues at the index level.Along with Jackrabbit limitations, we decide to re-write completely the repository implementation, 100% SQL based and then 100% ACID.The result is a very reliable storage where everything relies on the SQL database. However, there are some limitations in terms of queries: So in 2013, we started implementing Elasticsearch connector for the Nuxeo Platform with the goal to get rid of these problems: spend less time on complex SQL tuning and focus on Elasticsearch index mapping. Elasticsearch Integration OverviewHybrid StorageThe idea was to build an additional index on top of the repository: One of the main advantages of this approach is that queries are written once using NXQL and then, depending on the configuration, it will be executed by the repository or the Elasticsearch index. On a per query basis, transactional behavior or search speed can be favored.Later, when we introduced the MongoDB backend for the repository, this demarcation became synchronous vs asynchronous, but still, Elasticsearch remains the solution for blazing fast search.How do we know that? Well, we did actual benchmarks! PerformancesAs soon as we had a first version of the  connector, we started doing performances benchmarks.One of the first tests we did was comparing the performances the same NXQL query between a Nuxeo Repository internal SQL index and the Elasticsearch index.The results are very significants: Happy with these first results, we finished the integration and did some more benchmarks.One of the key performance aspects is re-indexing speed: We tested re-indexing speed and measured a throughput of about 3,500 documents/s. This is fast enough so that a full re-indexing would not be a problem. But, we actually saw that the bottleneck for the re-indexing was not Elasticsearch, but the repository SQL backend. So, we ran again the same test with the MongoDB backend and measured a re-indexing throughput of 10,000 documents/s.We also tested the Elasticsearch capacity to handle scale out. For that, we injected a lot of queries on a Nuxeo Repository via REST API. The limit reached was about 3,000 queries/s. Then adding a second Elasticsearch node gave us about 6,000 que","locales":"","title":"Nuxeo, Search and Lucene, Oh My!"}
{"index":{}}
{"author":"Daniel Palay","category":"User Stories","publish_date":"2015-12-11T00:00:00.000Z","url":"/blog/godaddy-and-the-quest-to-find-out-why-cant-i-have-ninja-com","seo_title":"Why Can't I Have Ninja.Com","content":" When setting up your website, what’s in a name, really? Is it long? Short? Personally meaningful? Or is it just a random collection of words, words, words (or characters - lookin’ at you there, xkcd)? And after you figure out the name, then comes the even harder part...do you want to be a .com or a .co (hey, take it from us, those m’s can be expensive), a .org, .net. or a .banana?If you are like most people, your first step to domain name ownership could be a quick visit to GoDaddy - after all it’s the web’s largest URL registry. And wouldn’t you know, their search function, recommendation engine, and even some other fun stuff are all powered by Elasticsearch. Wait a tick, how do I know all of this?  Yes, they did publish an about it, but really it’s because we had the pleasure of sitting down with Chris Ambler, Principal Software Development Engineer at GoDaddy, at Elastic{ON}15 to get the , face to face. He walked us through the star-crossed journey of a URL-seeker. It can an arduous journey from concept (some people try keywords while others are just clicking around for divine inspiration) to dead ends (WHAT?!? WHY CAN’T I HAVE NINJA.COM) to resolution (hey look, that recommendation is perfect, why didn’t I think of that?). But as you’ll see, the final act of getting that perfect URL is well worth the complete works that come before it.And in the name of brevity - it is the soul of wit after all - if you get nothing else from this video or blog, understand these two points: It's not that you can't have ninja.com, it's just that you actually wanted something better and GoDaddy already knew it.  ","locales":"","title":"GoDaddy & the Quest to Find Out Why Can't I Have Ninja.Com?!"}
{"index":{}}
{"author":"Tudor Golubenco","category":"Engineering","publish_date":"2015-12-11T00:00:00.000Z","url":"/blog/the-beats-are-moving-together-into-single-git-repository","seo_title":"","content":" A time comes in the life of many development teams when they consider switching from multiple source code repositories to one (or the other way around). For the team behind the Beats open source projects, this time was a few weeks ago, and it resulted in us merging , , , and into a single git repository last week. The Why We were all feeling for some time that having to constantly keep our eight or so repositories in sync was time consuming, but I personally thought about it more as a necessary annoyance that we can live with, rather than something we must fix in the short term. So when someone casually brought up the idea during an internal chat, I didn’t think the cost of switching would be worth it. For one thing, with the Beats being open source, the way we organize the repositories is not just our internal business, but it is part of the way we communicate with the wider developer community around the projects. I was worried that a single repo would make the individual Beats feel less independent and might discourage folks from creating their own Beat under their personal Github account. Nevertheless, we started to gather in a document the pros and cons and we payed more attention to all the tasks that we have to do because of the multiple repos. Small things like bumping the version number in the docs before releasing, creating new Github labels, closing and merging the changelog files,  publishing the Github release, or just checking the open issues across repositories take a significant amount of time when you have to do them four times. Yes, most of this stuff can be automated, and indeed we had various scripts and tools that helped us a lot. But even then, there was a lot of time wasted, and creating these tools also didn’t come for free. We do everything via pull requests and we review every single one, no matter how trivial. We also don’t merge them until the continuous integration systems give the green light. So, in the last few days before a release all of the tiny changes added up and were taking forever. Then there were the tasks that are not so easy to automate, like backporting features or bug fixes between release branches. If the feature or bug affected more than one repository, the tedious and error-prone task of solving rebase conflicts had to be done more than once. Another important aspect for us was how friendly we were to the occasional external contributor. If someone wanted to fix a bug in Packetbeat but the code was actually in libbeat, they would first have to find the code in a different repository. Then after fixing it, figure out how to update the libbeat code into Packetbeat (godep), test it, and open pull requests in both libbeat and Packetbeat. A libbeat change can potentially break the tests in another Beat as well, and we could not expect an occasional contributor to clone three more repositories and run the tests before submitting the PR. After debating these points, we all agreed that a single repository would help us move faster and with fewer errors. The How We thought that switching would be easy, because except for Packetbeat, all the other repositories are only one to six months old. But even for our young projects, there were quite a few things to consider. Timing The more in-progress work you have at the time of the migration, the bigger the disruption is, so we chose to do the migration in the week after a major release, when most new projects were still in the design phase. Git history We felt keeping the Git history was important, both to credit our external contributors and for us to be able to track down changes. Luckily the command makes this extremely easy. We imported the code from each Beat into a subdirectory in the final repository. One special requirement we had here was to have both the master and our latest release branch 1.0.0 available, so we actually did the subtree import twice, o","locales":"","title":"The Beats are Moving Together into a Single Git Repository"}
{"index":{}}
{"author":"Shay Banon","category":"News","publish_date":"2015-12-10T00:00:00.000Z","url":"/blog/microsoft-azure-marketplace-elasticsearch-kibana-and-more-now-available","seo_title":"","content":" Over the past few years, we’ve had a wonderful relationship with Microsoft.Earlier this year at our user conference —  — we had the privilege of having Pablo Castro, an early .NET, Azure, and SQL Server engineer, present the many different use cases of Elastic’s technology at Microsoft during his keynote. It included powering search for MSN.com, one of the Internet’s biggest web portals:  using Elasticsearch within the Dynamics CRM product line for enhancing the user experience:  and as the search framework within Azure.With a shared goal to make it as easy as possible to deploy our technologies, today, I’m really excited to announce that we’ve worked with Microsoft to make the Elastic stack available on the . As more and more developers build and deploy apps on Azure, using an (ARM) Solution template created by Elastic, developers can easily set up an Elasticsearch cluster directly, thus simplifying their deployment. In addition, we’ve made it possible to deploy all of our open source products, such as, Kibana, Logstash and Beats. And for those who want support and our commercial plugins Shield (security & authentication), Watcher (alerting), and Marvel (monitoring), an Elastic subscription is now available for any Azure-based deployments. Most importantly, this ensures that developers using Azure have a full and native Elasticsearch experience.In addition to creating the ARM template and the Marketplace listing with Microsoft, I’m also thrilled that our teams have been able to work closely on some open source innovation. One example is enabling Elasticsearch to run more efficiently on like Azure File Storage that will benefit both Elastic and Microsoft users with lower costs.Thank you Microsoft, and to our users, please let us know your experience.  ","locales":"","title":"Microsoft Azure Marketplace: Elasticsearch, Kibana, and More Now Available"}
{"index":{}}
{"author":"Adrien Grand","category":"Engineering","publish_date":"2015-12-09T00:00:00.000Z","url":"/blog/index-vs-type","seo_title":"","content":" Who has never wondered whether new data should be put into a new type of an existing index, or into a new index? This is a recurring question for new users, that can’t be answered without understanding how both are implemented. In the past we tried to make elasticsearch easier to understand by building an analogy with relational databases: indices would be like a database, and types like a table in a database. This was a mistake: the way data is stored is so different that any comparisons can hardly make sense, and this ultimately led to an overuse of types in cases where they were more harmful than helpful. What is an index?An index is stored in a set of shards, which are themselves Lucene indices. This already gives you a glimpse of the limits of using a new index all the time: Lucene indices have a small yet fixed overhead in terms of disk space, memory usage and file descriptors used. For that reason, a single large index is more efficient than several small indices: the fixed cost of the Lucene index is better amortized across many documents. Another important factor is how you plan to search your data. While each shard is searched independently, Elasticsearch eventually needs to merge results from all the searched shards. For instance if you search across 10 indices that have 5 shards each, the node that coordinates the execution of a search request will need to merge 5x10=50 shard results. Here again you need to be careful: if there are too many shard results to merge and/or if you ran an heavy request that produces large shard responses (which can easily happen with aggregations), the task of merging all these shard results can become very resource-intensive, both in terms of CPU and memory. Again this would advocate for having fewer indices. What is a type?This is where types help: types are a convenient way to store several types of data in the same index, in order to keep the total number of indices low for the reasons exposed above. In terms of implementation it works by adding a “_type” field to every document that is automatically used for filtering when searching on a specific type. One nice property of types is that searching across several types of the same index comes with no overhead compared to searching a single type: it does not change how many shard results need to be merged. However this comes with limitations as well: This means types can be helpful, but only if all types from a given index have mappings that are similar. Otherwise, the fact that fields also consume resources in documents where they don’t exist could make things worse than if the data had been stored in separate indices. Which one should I use?This is a tough question, and the answer will depend on your hardware, data and use-case. First it is important to realize that types are useful because they can help reduce the number of Lucene indices that Elasticsearch needs to manage. But there is another way that you can reduce this number: creating indices that have fewer shards. For instance, instead of folding 5 types into the same index, you could create 5 indices with 1 primary shard each. I will try to summarize the questions you should ask yourself to make a decision: In conclusion, you may be surprised that there are not as many use cases for types as you expected. And this is right: there are actually few use cases for having several types in the same index for the reasons that we mentioned above. Don’t hesitate to allocate different indices for data that would have different mappings, but still keep in mind that you should keep a reasonable number of shards in your cluster, which can be achieved by reducing the number of shards for indices that don’t require a high write throughput and/or will store low numbers of documents. ","locales":"","title":"Index vs. Type"}
{"index":{}}
{"author":"Sebastian Herzberg","category":"User Stories","publish_date":"2015-12-09T00:00:00.000Z","url":"/blog/how-the-elastic-stack-keeps-our-taxis-rolling","seo_title":"How the Elastic Stack Keeps our Taxis Rolling","content":" The Logging-Cluster We started out in 2013 with a pretty simple standard configuration of the Elastic technology stack. It had two Elasticsearch nodes and one node for Logstash and Kibana. This worked well back in the days before our exponential growth started in 2014. This setup was running until mid 2015. The cluster was abandoned during our growth phase and thus it was lacking performance. By mid 2015 we had about 2 TB of data stored in Elasticsearch. Also, in 2013 we decided to move away from a monolithic backend structure towards microservices. By now we have split our backend into around 50 microservices. We needed to gather logs from all of these services in one place. By July 2015 we decided to set up a new cluster that would support our needs for the upcoming years. The requirements were: Hardware setup and installation procedure With these requirements we went to AWS and checked for a reasonable instance size. Three instance sizes were in line with our requirements (prices may be outdated): The m1.xlarge seemed to be gold here. They are cheap and they have 4 HDDs which could be combined to a RAID0. Ten m1.xlarge instances would form a cluster with 150 GB RAM and 16,8 TB of disk. With this much disk we could easily increase the amount of time that we keep the logs. To configure ten nodes and install an equal configuration of Elasticsearch, we used Ansible. The playbook combined roles for: Logcluster architecture  Logcluster usage examples Conclusion It is our goal to provide the best taxi experience in Europe and expand our service to do more and more cities over the next years. With around 50 microservices we need an information hub to get an overview of the overall system status. For us the Elastic-Technology-Stack provides this overview. But it also gives developers the chance to dig deep into bugs and closely follow the impact of changes. ","locales":"de-de,fr-fr,ko-kr","title":"How the Elastic Stack Keeps our Taxis Rolling"}
{"index":{}}
{"author":"Jay Chin","category":"User Stories","publish_date":"2015-12-09T00:00:00.000Z","url":"/blog/elastic-fantastic-excelians-review-of-elasticon-tour-london","seo_title":"Elastic Fantastic: Excelian's review of Elastic{ON} Tour London","content":" In this particular project we were tasked with creating a for one of the large investment banks. The project used Elasticsearch, Logstash, and Kibana to store and provide analytics on grid performance, capacity, logs, and so forth. It was a very successful project for Excelian and had fantastic feedback from the client. I believe one of the reasons for the success of this project was the close relationships that developed between Excelian and some of the Elastic engineers. They were able to optimise the setup but were also on hand to troubleshoot any issues that arose. When I conveyed the feedback to Elastic they were delighted and that is when they invited me to present at . Elastic{ON}Tour was a fantastic and genuinely inspiring event. From a personal perspective, I managed to meet up with various users within the Elasticsearch community and discovered many interesting use cases for Elasticsearch. I already knew that many large organisations such as Netflix, Facebook, Microsoft, and Cisco are using Elasticsearch for building out their search capabilities within the organisation. However, it was also really interesting to learn about some of the more unusual use cases. I wasn’t previously aware that NASA uses Elasticsearch to store and analyse data sent back from the Mars Curiosity Rover, or how the Victoria and Albert Museum plans to use Elasticsearch to analyse visitor data to determine how they could optimise the placement of their artifacts within the museum. The event was very well structured. The morning of the conference was focused on the Elastic roadmap and introducing the new features of Elasticsearch 2.0, Kibana 4.2, Beats, Watcher, and Found. A nice little insight into the creation of Elasticsearch was revealed when Shay Banon (the creator of ElasticSearch) began his keynote speech by explaining how he created Elasticsearch in order to create a tool for his wife to search for cooking recipes! The later part of the day was more focused on real life user stories and how they use Elasticsearch within their organisation. Graham Tackley from The Guardian gave us a walkthrough of the tools they have built with the Elasticsearch stack to do real-time media analytics that can process up to 40 million documents a day. There were also two presentations from Goldman Sachs on how they utilise Elasticsearch to build a Goldman Sachs search engine and also a firm-wide task list. My particularwas focused on how we decided on the use of Elasticsearch, some of the reporting requirements from the user and details on the architecture we built out. There were only a handful of people in the audience who were from the financial services industry but I did manage to get some very interesting questions during the talk and even during the drinks reception. It was really good to see how the technology was able to be transferred across multiple industries, and how we can all learn from each other. After the presentations, it was time for the more informal part of the conference and everyone adjourned for beer and food. It was during this time that I got the chance to talk to Shay Banon and discussed some interesting trends within financial services which are perfect candidates for Elasticsearch, e.g. fraud detection, market sentiment analysis, and suchlike. I also had an opportunity to talk to Elastic's co-founder Uri Boness on the challenges of deploying Elasticsearch in the financial services industry and some upcoming features in the roadmap that would help with this, i.e. data centre replication. Elastic{ON} Tour London 2015 was an enlightening experience for me. I discovered so many novel use cases for Elasticsearch and I had a chance to talk to the people who built those solutions. I was also very excited to be able to share my experience in building out an Enterprise Grid reporting solutio","locales":"","title":"Elastic Fantastic: Excelian's review of Elastic{ON} Tour London"}
{"index":{}}
{"author":"Tudor Golubenco","category":"Brewing in Beats","publish_date":"2015-12-07T00:00:00.000Z","url":"/blog/weekly-beats-merged-git-repositories","seo_title":"","content":" The past week we did a reorg of our Github repositories and worked on the design for a few key features we’re planning for the next future releases. Repository merging We merged libbeat and the four officially supported beats into a single repository: . We’ll have a blog post detailing the reasons, but we essentially found a single repo to result in a lot less work for us but also be more efficient in terms of communication with the rest of the community. Filebeat filtering One of the most often requested features for Filebeat is to be able to do basic line filtering before sending to Logstash. Think of it as doing a “egrep -v” to filter out, for example, debug messages before sending the logs out. If you were previously dropping those lines in Logstash, there are now two handy options in Filebeat that can save you network traffic and CPU load: include_lines and exclude_lines. All the details are in the and the . Design for generic filtering In addition to the simple filtering described above, we started thinking also about a more generic way to filter events or fields, from which all Beats will benefit. The discussion is happening in this and you are invited! Design for multiline Another Frequently Requested Feature for Filebeat is about being able to merge related lines (like the ones from an exception, for example) into a single event. This can be done in Logstash already with the , but doing this closer to where the files are read might make it more reliable for some people and easier to configure and maintain for others. This is also in the design phase, see this . Once again, your input here would be valuable. Design for Packetbeat’s HTTP body handling Another interesting on which design is discussed is about making Packetbeat be able to understand the application specific HTTP payload. For example, if we know that the body is JSON created by an Elasticsearch client or server, we could extract some more interesting information, like the index name, the search query, the number of results, etc. The same can be done, and with relatively little effort, for any protocol that works over http, like CouchDB, Docker’s protocol, XML-RPC, JSON-RPC, etc. Apachebeat We are very happy to see another Beat created by the community: created by . Similar to the existing , Apachebeat can be used to insert the key Apache metrics into Elasticsearch. Connect to Elasticsearch over proxies It’s now for all Beats to also use an HTTP proxy when connecting to Elasticsearch. Packetbeat performance improvements Steffen has been profiling and improving the performance of the and parsers in Packetbeat. These are still in progress and being discussed. Winlogbeat Winlogbeat is slowly getting closer to being releasable, and now it also has his own . ","locales":"","title":"Brewing in Beats: The Git repository merge"}
{"index":{}}
{"author":"Megan Wieling","category":"","publish_date":"2015-12-07T00:00:00.000Z","url":"/blog/witwies-elasticontour-sydney-melbourne","seo_title":"Where in the World is Elastic? - Elastic{ON}Tour Los Angeles and Seattle","content":" Welcome to Find out which Elastic events and meetups are happening near you this week. Upcoming EventsDecember 8: December 10: Upcoming MeetupsDecember 7: December 7: December 8: December 10: December 10: December 10: December 11: That's it for this week. Stay tuned next week for more Elastic happenings.- The Elastic Team P.S. if you're interested in hosting a meetup or are giving a talk about Beats, Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic? - Elastic{ON}Tour Sydney and Melbourne"}
{"index":{}}
{"author":"Suyog Rao","category":"Releases","publish_date":"2015-12-08T00:00:00.000Z","url":"/blog/logstash-2-1-1-and-1-5-6-released","seo_title":"","content":" We are happy to announce that Logstash versions 2.1.1 and 1.5.6 has been released today! Jump to the page for the binaries, where you can also find the full list of changes that made these releases. Bug Fixes This is mainly a bug fix release, some of which we highlight below: Windows Memory Leak Issue This release bundles a new version of JRuby - , which fixes an important memory leak issue reported on Windows when using the file input (). Check our on how we debugged this issue and collaborated with the JRuby team to resolve it. This fix has also been backported to version 1.5.6. File Input Elasticsearch Output (2.1.1): Others (2.1.1): Please Logstash 2.1.1 and 1.5.6 and let us know what you think on Twitter () or on our . You can report any problems on the GitHub issues page. ","locales":"","title":"Logstash 2.1.1 and 1.5.6 released"}
{"index":{}}
{"author":"Michael McCandless","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2015-12-07T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2015-12-07","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top NewsWhy multiple clusters can be easier to manage than one giant cluster: — elastic (@elastic) Elasticsearch Core Apache Lucene Watch This Space, where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2015-12-07"}
{"index":{}}
{"author":"Pius Fung","category":"","publish_date":"2015-11-30T00:00:00.000Z","url":"/blog/clustering_across_multiple_data_centers","seo_title":"Clustering Across Multiple Data Centers","content":" We are frequently asked whether it is advisable to distribute an Elasticsearch cluster across multiple data centers (DCs). The short answer is \"no\" (for now), but there are some alternate options available described below. This blog post is intended to help you understand why this is the case, and what other options are available to you.But Why?The architecture and design decisions that we make in Elasticsearch are based on certain assumptions, including the assumption that nodes are located on a local network. This is the use case that we optimize and extensively test for, because this is the environment that the vast majority of our users operate in.Network disruptions are much more common across WAN links, between geographical distributed DCs. Even if there is a dedicated link between DCs.  Elasticsearch is built to be resilient to networking disconnects, but that resiliency is intended to handle the exception, not the norm.Running a single Elasticsearch cluster that spans multiple DCs is not a scenario we test for and there are a number of additional reasons why it is not a recommended or supported practice we will go into below. (Note:  On AWS, running a cluster across availability zones within a single region is supported as Amazon provides consistent high bandwidth and low latency.)Expect the UnexpectedLatency is a problem in distributed systems.   High latency slows indexing because the indexing request is for indexing, and all cluster-wide communications (eg. cluster state updates) in Elasticsearch.If connectivity between nodes in a cluster is momentarily lost, it’s likely that remote shards will be out of date and any single update processed while in disconnected state will invalidate all content held on isolated replicas.This means that Elasticsearch requires the copying of these out of date shards to sync up replicas from their primaries to ensure consistency of data and search responses.Sending full shards for multiple indices may overwhelm a WAN based connection or cause considerable slowdown, leaving your cluster in a degraded state for an extended period of time.Assuming the correct setting of , in the event of a network disconnect between two or more DCs, only the DC with the elected master node will remain active. This can cause many issues for applications in the different DCs which may be attempting to index new data, as the nodes not part of the active cluster will reject any attempted writes.This also provides a challenge with cluster sizing. When the link between the two DCs is broken, the active half of the cluster will need to bear the full load of indexing and queries for all requests.When the link is restored, these nodes will also be pushing data and documents across the network while still handling the full indexing and request load. This necessitates larger or more powerful clusters to ensure enough CPU and IOPS to maintain acceptable performance during such events.What Are The Options ?Here are 3 common scenarios on how this may look to give you some ideas.Here you would have your application code write to a replicated queuing system (e.g. Kafka, Redis, RabbitMQ) and have a process (e.g. ) in each DC reading from the relevant queue and indexing documents into the local Elasticsearch cluster.This way if network connectivity is lost between the DCs, when it is restored, the indexing will continue where it left off. can be used to backup indices at regular intervals (eg. to S3) and restore to the passive DC for disaster recovery. Tools such as make automation of the Snapshot process very simple. Restoration can either be done as soon as the Snapshot has completed or at scheduled times such as hourly or even just once daily.The Snapshot and Restore only copies the segments files that do not already exist in the snapshot repository, so except for the initial snapshot, your backups are incremental which reduces the amount of space nee","locales":"","title":"Clustering Across Multiple Data Centers"}
{"index":{}}
{"author":"Fabian Hueske","category":"User Stories","publish_date":"2015-12-07T00:00:00.000Z","url":"/blog/building-real-time-dashboard-applications-with-apache-flink-elasticsearch-and-kibana","seo_title":"Building real-time dashboard applications with Apache Flink, Elasticsearch, and Kibana","content":" Gaining actionable insights from continuously produced data in real-time is a common requirement for many businesses today. A wide-spread use case for real-time data processing is dashboarding. A typical architecture to support such a use case is based on a data stream processor, a data store with low latency read/write access, and a visualization framework. In this blog post, we demonstrate how to build a real-time dashboard solution for stream data analytics using Apache Flink, Elasticsearch, and Kibana. The following figure depicts our system architecture. In our architecture, Apache Flink executes stream analysis jobs that ingest a data stream, apply transformations to analyze, transform, and model the data in motion, and write their results to an Elasticsearch index. Kibana connects to the index and queries it for data to visualize. All components of our architecture are open source systems under the Apache License 2.0. We show how to implement a Flink DataStream program that analyzes a stream of taxi ride events and writes its results to Elasticsearch and give instructions on how to connect and configure Kibana to visualize the analyzed data in real-time.Why use Apache Flink for stream processing?Before we dive into the details of implementing our demo application, we discuss some of the features that make Apache Flink an outstanding stream processor. Apache Flink 0.10, which was recently released, comes with a competitive set of stream processing features, some of which are unique in the open source domain. The most important ones are: [if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves></w:TrackMoves> <w:TrackFormatting></w:TrackFormatting> <w:PunctuationKerning></w:PunctuationKerning> <w:ValidateAgainstSchemas></w:ValidateAgainstSchemas> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF></w:DoNotPromoteQF> <w:LidThemeOther>EN-GB</w:LidThemeOther> <w:LidThemeAsian>JA</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables></w:BreakWrappedTables> <w:SnapToGridInCell></w:SnapToGridInCell> <w:WrapTextWithPunct></w:WrapTextWithPunct> <w:UseAsianBreakRules></w:UseAsianBreakRules> <w:DontGrowAutofit></w:DontGrowAutofit> <w:SplitPgBreakAndParaMark></w:SplitPgBreakAndParaMark> <w:EnableOpenTypeKerning></w:EnableOpenTypeKerning> <w:DontFlipMirrorIndents></w:DontFlipMirrorIndents> <w:OverrideTableStyleHps></w:OverrideTableStyleHps> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\"></m:mathFont> <m:brkBin m:val=\"before\"></m:brkBin> <m:brkBinSub m:val=\"&#45: -\"></m:brkBinSub> <m:smallFrac m:val=\"off\"></m:smallFrac> <m:dispDef></m:dispDef> <m:lMargin m:val=\"0\"></m:lMargin> <m:rMargin m:val=\"0\"></m:rMargin> <m:defJc m:val=\"centerGroup\"></m:defJc> <m:wrapIndent m:val=\"1440\"></m:wrapIndent> <m:intLim m:val=\"subSup\"></m:intLim> <m:naryLim m:val=\"undOvr\"></m:naryLim> </m:mathPr></w:WordDocument> </xml><![endif][if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"276\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" N","locales":"","title":"Building real-time dashboard applications with Apache Flink, Elasticsearch, and Kibana"}
{"index":{}}
{"author":"Zachary Tong","category":"Engineering","publish_date":"2015-12-02T00:00:00.000Z","url":"/blog/implementing-a-statistical-anomaly-detector-part-2","seo_title":"Implementing a statistical anomaly detector in Elasticsearch - Part 2","content":" Last week, we which distills thousands of data-points into a handful of representative metrics.  This forms the basis of Atlas, and does all the heavy lifting required to implement the anomaly detector. This week, we'll finish the implementation and generate some fun graphs. The aggregation we built is designed to be run over a specific window of time: given a date range, it will emit a 90th percentile surprise value for each metric. To fully implement Atlas, we need to plot those 90th percentile values themselves over time. This functionality is not currently possible using just Pipeline aggs (although a \\\"sliding histogram\\\" functionality has been proposed which would fill the gap).   Instead, we are going to move the responsibility to , which is well suited for this type of post-processing (Timelion is a new {Re}search project to put fluent time-series manipulation inside Kibana:  you ). If you revisit the simulator code, you'll see that after the data has been generated.  We slide our Pipeline agg across the data in one-hour increments (with a window size of 24 hours).  We also use to minimize the response output: we don't actually care about the 60,000 buckets… we just want the \\\"ninetieth_surprise\\\" from each metric.  Filtering the response cuts down network transfer considerably.  The values are then indexed back into Elasticsearch so we can chart them later. We pre-processed these values ahead of time in the simulator to simplify the demonstration, but in a real system you'd likely have a Watcher or cronjob executing the query every hour and saving the results. Plotting 90th percentile surprise With the heavy lifting done last week, we can turn to TimeLion to finish the implementation. The first order of business is to pull down the 90th values for a particular metric.  We can do that with the following TimeLion syntax: .es('metric:0', metric='avg:value').label(\\\"#0 90th surprise\\\") Which will generate a graph that looks something like this: Well that looks fun!  There is definitely  happening.  Let's walk through what this chart means, since it is fundamental to how Atlas works: Effectively, if we see a spike we can conclude the underlying data has changed enough to shift our normal variance, likely due to a disruption.  This is the heart of Atlas: don't watch your data because there is just too much.  Instead, watch the variance of the 90th percentile of deviations from the mean. If you compare the above graph to the actual data for metric #0, you’ll see the stark difference: Building the Atlas Dashboard Of course, the trick is to now automatically identify those spikes and graph/alert on them.  Let's start building that logic.  Atlas alerts when the 90th percentile surprise is 3 standard deviations above the moving average. If you decompose that problem, you'll see several necessary components: First, we construct the rolling three standard deviations.  We do this with a custom function (see footnote for source, it is essentially identical to a function), then multiply it by three to get the third sigma: .es('metric:0', metric='avg:value') .movingstd(6) .multiply(3) Next we write a snippet to calculate a rolling average of the data itself: .es('metric:0', metric='avg:value') .movingaverage(6) And finally, we combine those two snippets by adding them together to create the \\\"threshold\\\".  This will create a line that is three standard deviations above the moving average of the data: .es('metric:0', metric='avg:value') .movingaverage(6) .sum( .es('metric:0', metric='avg:value') .movingstd(6) .multiply(3) ) Now that we have a \\\"threshold\\\", we can plot this with the original data and see how they compare: Hmm, ok.  It's not really clear right now if the threshold is working or not.  The chart is difficult to read:  as soon as the surprise value spikes\\xC2","locales":"","title":"Implementing a statistical anomaly detector in Elasticsearch - Part 2"}
{"index":{}}
{"author":"Michael McCandless","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2015-11-30T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2015-11-30","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top NewsFrom the Found vault: Understanding the Memory Pressure Indicator — elastic (@elastic) Elasticsearch Core Apache Lucene Watch This Space, where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - 2015-11-30"}
{"index":{}}
{"author":"Dara Gies","category":"Engineering","publish_date":"2015-12-01T00:00:00.000Z","url":"/blog/scaling_elasticsearch_across_data_centers_with_kafka","seo_title":"Scaling Elasticsearch Across Data Centers With Kafka","content":" Overview Organizations often produce and consume data in multiple regions, sometimes within a country and sometimes globally.  Regions often have local data centers to meet local security, privacy or performance requirements. Data created in one region may need to be accessed in other regions. Requirements that may play a role in determining an appropriate architecture include high availability, fault tolerance, disaster recovery, ingestion latency and search and access latency. Limited network bandwidth and high latency between data centers can be key considerations for determining a multi-region and multi-data center architecture. Queueing data is a central theme in distributed architectures for reasons including guaranteed message delivery, low network bandwidth and message throughput variation. The purpose of this blog is to show and discuss proposed solutions to a use case where distributed departments produce data locally that must be replicated across data centers and be made accessible for search and access. Use Case Assume a creative agency with offices in New York and London where media assets, such as images, videos, HTML documents, CSS documents, etc are created in the respective offices and maintained in separate Digital Asset Management (DAM) systems. Media assets are shared between New York and London and are discoverable through a search user interface. ","locales":"","title":"Scaling Elasticsearch Across Data Centers With Kafka"}
{"index":{}}
{"author":"Megan Wieling","category":"","publish_date":"2015-11-30T00:00:00.000Z","url":"/blog/witwies-elasticontour-losangeles-seattle","seo_title":"Where in the World is Elastic? - Elastic{ON}Tour Los Angeles and Seattle","content":" Welcome to Find out which Elastic events and meetups are happening near you this week. Upcoming EventsDecember 1: December 3: December 1: December 3 - 4: December 1 - 2: Upcoming MeetupsNovember 30: December 1: December 1: December 1: December 1: December 2: December 1: November 30: That's it for this week. Stay tuned next week for more Elastic happenings.- The Elastic Team P.S. if you're interested in hosting a meetup or are giving a talk about Beats, Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic? - Elastic{ON}Tour Los Angeles and Seattle"}
{"index":{}}
{"author":"Loggy D. Wood","category":"Culture","publish_date":"2015-12-01T00:00:00.000Z","url":"/blog/movember-wrap-up","seo_title":"","content":" Ahoy! It’s Loggy D. Wood here with a Movember recap! Stay tuned for other causes that Elastic employees are supporting with our,campaigns. ","locales":"","title":"Movember: It's a wrap"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2015-11-30T00:00:00.000Z","url":"/blog/weekly-beats-1-0-0-released","seo_title":"","content":" The past week was memorable for the Beats team as we finally released 1.0.0.   Release bonanza! Beats 1.0, updates to , , — & Found has the latest. Learn more: — elastic (@elastic) Change proc type in TopbeatBecause we were using as a type, but also in the field names (e.g. ), this made the requests ambiguous and the result was wrong when using Elasticsearch 1.x for some of the widgets in our sample dashboards. To fix it, we have the to in Topbeat. This change breaks backward compatibility.  Community BeatsWe are excited to see two new community Beats created last week: Give these two Beats a try and let us and the authors know what you think! Elasticsearch Ping over httpsFix the Ping function by setting the credentials into the HEAD request sent by the Beat to Elasticsearch. The fix is . Set default ignore_older to 24h in FilebeatThere were some issues with Filebeat failing to reopen files after it closed them, so we decided to of the ignore_older option to 24h. This is the value that was also used by the Logstash Forwarder and it is a safer default. Simplify the main functionFor each Beat the main function looks similar, so we decided to simplify it by moving all the common parts into the run function in libbeat and call it from each Beat. With this change the main function of each Beat resumes to one line of code: , and . ","locales":"","title":"Brewing in Beats: 1.0.0 released"}
{"index":{}}
{"author":"Antonio Bonuccelli","category":"","publish_date":"2015-11-30T00:00:00.000Z","url":"/blog/postmortem-memory-leak-hunting-windows","seo_title":"","content":" At Elastic we stand next to our users to help them designing their deployments, architecting their solutions but also troubleshooting problems that can be encountered in day to day operations. Nasty bugs come out now and then and sometime identifying their root cause can be a painful and frustrating exercise, going through debug logs, heap/thread dumps, OS metrics and failed reproduction attempts. In this blog post, we will go through the story of our investigations to identify a bug which has been impacting a large amount of users and their logstash deployments and tell how we have joined forces between support and development teams in Elastic in order to solve it. This all started with several reports of abnormal behaviour in terms of memory consumption have been reported by our users to our support team but also through GitHub issues, see and . In this case, an analysis of the JVM heap usage over time using JConsole showed no significant anomalies within heap memory consumption, showing the classic healthy “sawtooth”, where garbage collection periodically frees memory: Interestingly though, the process total memory footprint on Windows would keep growing and growing over time, well beyond the configured JVM limits (1GB). So where to start from to understand where is this memory being used? Reproducing the problem When there are no steps to follow, in order to recreate a problem, the only approach that usually gives results is just to recreate an environment as close as possible, in terms of OS and application configuration and versions, as the one where the problem has been showing its symptoms: every little detail can make the difference. Using the same configuration and log files from one of our customers we have started monitoring a test environment where we have set up Logstash on a Windows 2012 instance and started observing the behavior as new files to be processed were automatically generated. As reported by the users impacted by this bug, the problem with the high memory consumption of the Logstash process would develop in time, only after several hours of usage:  this is usually a common pattern when dealing with memory leaks, however as seen in JConsole picture above, no signs of leak were observable within the Java heap. At this point, we’ve started looking around for alternative more comprehensive memory monitoring/troubleshooting tools as it became evident after hours of tests that what we were looking for was not something that JConsole could show us. VMMap is part of Microsoft Sysinternals and is a process virtual and physical memory analysis utility. It shows a breakdown of a process’s committed virtual memory types as well as the amount of physical memory (working set) assigned by the operating system to those types. Using VMMap, a very granular breakdown of memory usage for the Logstash process was revealed to us, after we’ve attached it to the test Logstash instance:  this allowed us to go beyond the simple Heap/Off Heap distinction available in JConsole and keep our investigations going. What jumped out fairly quickly was the amount of memory reported as “Unusable”: Unusable Memory Window’s virtual memory manager has a which means that allocating a smaller chunk will result in “wasted” virtual memory. In itself this isn’t a problem, but such a big size of Unusable Memory indicates that there’s a high number of < 64KB allocations which aren’t being freed. Selecting “Unusable Memory” in VMMap confirmed this: All these 56KB chunks meant that the each resulted from a 8KB allocation, which we were able to confirm in “Private Data”. In VMMap this section refers to the memory used by the process’s heap, including both and other non JVM memory usage (e.g. through native allocations). Since JConsole and other JVM heap analysis tools showed low memory consumption, this indicated off-heap allocations. At this stage, after days of testing and som","locales":"","title":"Post-mortem: Memory Leak Hunting on Windows"}
{"index":{}}
{"author":"Alex Brasetvik","category":"Engineering","publish_date":"2015-11-30T00:00:00.000Z","url":"/blog/multiple-elasticsearch-clusters","seo_title":"Multiple Elasticsearch Clusters, or a Monster Cluster?","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud.Elasticsearch provides a pretty large toolbox for composing complex cluster topologies. You can make heterogenous clusters with beefy nodes hosting your hot indices, and have less expensive nodes host historical data, e.g. using node attributes and . While you can use these features to make the One Monster Cluster to rule them all, this post presents some arguments on why managing multiple separate clusters can actually be simpler than having a single large multi-purpose cluster, even though it means managing more nodes. Performance Reasoning and Limiting Blast RadiusElasticsearch is , and the different use cases can have wildly different performance characteristics and things to learn. While you can have a single cluster handling your app’s autocompletion, full text search, analytics, and logging, at some point the success or failure of one of these will cause grief. A bug or traffic surge can cause your logging activity to go through the roof, overwhelming your cluster and therefore bringing your search to a crawl, possibly causing your site to be unusably slow exactly when it shouldn’t be. Autocompletion workloads require having almost everything in memory and a lot of CPU to respond within the tight “instantaneous” time budget. So do e-commerce style searches with a lot of aggregations backing navigational aids. Plain “full text search” can be less demanding, as can write-only indexing of logs until someone suddenly runs a fancy aggregation over a huge timespan. Mixing all these different workloads together makes reasoning about performance hard. Is it OK to drop some logs while your site is seeing a sudden burst of traffic? How slow can autocompletion or navigational search be while still providing an acceptable user experience? How do we ensure high priority searches remain fast when unexpected things happen? Can we scale high priority workloads separately? How would a failure cascade? Analysing how both load and failures cascade through your system are highly related – a slow system can be indistinguishable from one that is dead. Therefore, separating different priority workloads into completely separate clusters (and processes) makes it much simpler to contain the “blast radius” of a failure. If your autocompletion cluster fails, it shouldn’t bring down the rest of your search and analytics with it. Handling multi-tenancy is hard, and better left to the operating system, which has full control of the memory- and CPU-limits and -priorities of processes. Upgrading by CloningWhile most upgrades of Elasticsearch can be done inline through rolling restarts, major version upgrades require a full cluster restart. If you want to do the upgrade without any downtime, you will need to clone the cluster and route traffic to the new cluster when it is ready. If you do not index continuously, this should be a rather simple process. If you do need to do this while also handling index requests, things get a bit more tricky, and your indexing/syncing process needs to be able to index to multiple clusters. Our post on may have some interesting pointers here. Since Elasticsearch cannot be downgraded once upgraded, it’s important to test carefully when upgrading. Upgrading through cloning is arguably the safest way to upgrade: no changes are done to the cluster you know works. Rolling back means just continuing to use the existing cluster, while resolving any issues that were discovered with the new version. A Cluster in Every ContinentIf you have users all over the world, having the ability to index to multiple clusters enable globally distributed replicas of your indices. This will let you do latency-based routing, where you route users' requests to the cluster closest to them. Extrem","locales":"","title":"Multiple Elasticsearch Clusters or a Monster Cluster?"}
{"index":{}}
{"author":"Megan Wieling","category":"","publish_date":"2015-11-27T00:00:00.000Z","url":"/blog/elastic-goes-back-to-school","seo_title":"Elastic Goes Back To School","content":" Just when you think you’ve graduated from university, there’s always something that pulls you right back in. We all remember the fun of exams, the deadlines, the never ending books, the long days, and of course, the partying. And then there’s that amazing moment when you finally graduate and say goodbye to university. But who would’ve thought that going back to school could actually be quite interesting? We’re happy to say that we got invited to give a guest lecture at the VU University in Amsterdam to teach computer science and engineering students the basics of Elasticsearch. , one of our core software engineers, was the lucky one to give this presentation to around 100 students. Surprisingly enough only two students have heard about Elasticsearch and Lucene before but guess what, that was about to change... ","locales":"","title":"Elastic Goes Back To School"}
{"index":{}}
{"author":"Suyog Rao","category":"Releases","publish_date":"2015-11-24T00:00:00.000Z","url":"/blog/logstash-2-1-0-released","seo_title":"","content":" We are happy to announce that Logstash 2.1.0 has been released today! This release is packed with enhancements, some of which we highlight below. Jump to the page for the binaries, where you can also find all the that made this release.Installing Plugins OfflineLogstash has a thriving where our community is constantly adding the ability to connect to different inputs, outputs, and processing data. The latest count of plugins you ask? 210 plugins. Previously, with the 1.5.0 release, we made it really easy to install and update available plugins by hosting them on public . Users who deployed Logstash in an environment which did not have internet connectivity - air-gapped datacenter or firewalled access - wanted to interact with plugins. In this release, we provide multiple solutions to address this situation. Details below:As mentioned before, Logstash uses the public as a repository for its plugins. If you are in a situation where you don't have access to this server, or maybe you are developing plugins for internal use, you can mirror RubyGems.org and use this as the installation source. Several open source projects enable you to run your own plugin server, for example - Geminabox, Artifactory, etc. Once you have this setup, you can simply point the Gemfile shipped with Logstash deployment to point to this source. Detailed steps .We've added to the plugin script to prepare plugin packages (with dependencies) so they can be installed in an offline box. The solution requires a staging machine running Logstash that has access to a public or private Rubygems server. This staging machine downloads and packages the files used for offline installation. Once you have this package, you can transfer this to your air-gapped machine or host it on a shared network drive. This becomes the installation source for plugins. With this you can simply do In addition to the above solution, we've decided to periodically publish - every point release - a full-resolved Logstash binary that includes all known plugins and dependencies. You can download this uber package .Shutdown ImprovementsBuilding on shutdown enhancements which were in Logstash 2.0.0, we've added the ability to start Logstash with a CLI option () that allows it to predictably shutdown when the operator initiates it. This is particularly useful in situations where the processing pipeline is blocked, trying to communicate to external sources. Users were frustrated with not being able to shutdown Logstash in these situation. Please be aware that shutting down Logstash could still lead to loss of any in-flight messages.To help with the shutdown situation, logic has been added to detect a potential stall while processing events. The stalled detection uses both the count of inflight events in internal queues and an analysis on busy worker threads. A staleness in both count and running threads triggers the warning mechanism and forces an unsafe shutdown. A report is published when this happens:^CSIGINT received. Shutting down the pipeline. {:level=>:warn} Received shutdown signal, but pipeline is still waiting for in-flight events to be processed. Sending another ^C will force quit Logstash, but this may cause data loss. {:level=>:warn} {:level=>:warn, \"INFLIGHT_EVENT_COUNT\"=>{\"input_to_filter\"=>20, \"total\"=>20}, \"STALLING_THREADS\"=>{[\"LogStash::Filters::Ruby\", {\"code\"=>\"sleep 10000\"}]=>[{\"thread_id\"=>15, \"name\"=>\"|filterworker.0\", \"current_call\"=>\"(ruby filter code):1:in `sleep'\"}]}} The shutdown process appears to be stalled due to busy or blocked plugins. Check the logs for more information. {:level=>:error} {:level=>:warn, \"INFLIGHT_EVENT_COUNT\"=>{\"input_to_filter\"=>20, \"total\"=>20}, \"STALLING_THREADS\"=>{[\"LogStash::Filters::Ruby\", {\"code\"=>\"sleep 10000\"}]=>[{\"thread_id\"=>15, \"name\"=>\"|filterworker.0\", \"current_call\"=>\"(ruby filter code):1:in `sleep'\"}]}} {:level=>:warn, \"INFLIGHT_EVENT_COUNT\"=>{\"input_to_filter\"=>20, \"total\"=>20}, \"STALLING_THREADS\"=>{[\"LogStash::Filters::Ruby\", {\"code","locales":"","title":"Logstash 2.1.0 released"}
{"index":{}}
{"author":"Loggy D. Wood","category":"Culture","publish_date":"2015-11-25T00:00:00.000Z","url":"/blog/movember-data-dive-part-2","seo_title":"","content":" [if gte mso 9]><xml> <o:OfficeDocumentSettings> <o:AllowPNG></o:AllowPNG> </o:OfficeDocumentSettings> </xml><![endif][if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves></w:TrackMoves> <w:TrackFormatting></w:TrackFormatting> <w:PunctuationKerning></w:PunctuationKerning> <w:ValidateAgainstSchemas></w:ValidateAgainstSchemas> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF></w:DoNotPromoteQF> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>JA</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables></w:BreakWrappedTables> <w:SnapToGridInCell></w:SnapToGridInCell> <w:WrapTextWithPunct></w:WrapTextWithPunct> <w:UseAsianBreakRules></w:UseAsianBreakRules> <w:DontGrowAutofit></w:DontGrowAutofit> <w:SplitPgBreakAndParaMark></w:SplitPgBreakAndParaMark> <w:EnableOpenTypeKerning></w:EnableOpenTypeKerning> <w:DontFlipMirrorIndents></w:DontFlipMirrorIndents> <w:OverrideTableStyleHps></w:OverrideTableStyleHps> <w:UseFELayout></w:UseFELayout> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\"></m:mathFont> <m:brkBin m:val=\"before\"></m:brkBin> <m:brkBinSub m:val=\"&#45: -\"></m:brkBinSub> <m:smallFrac m:val=\"off\"></m:smallFrac> <m:dispDef></m:dispDef> <m:lMargin m:val=\"0\"></m:lMargin> <m:rMargin m:val=\"0\"></m:rMargin> <m:defJc m:val=\"centerGroup\"></m:defJc> <m:wrapIndent m:val=\"1440\"></m:wrapIndent> <m:intLim m:val=\"subSup\"></m:intLim> <m:naryLim m:val=\"undOvr\"></m:naryLim> </m:mathPr></w:WordDocument> </xml><![endif][if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"276\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=","locales":"","title":"Movember Data Dive: Eating & Drinking Patterns in the US"}
{"index":{}}
{"author":"Shay Banon","category":"News","publish_date":"2015-11-24T00:00:00.000Z","url":"/blog/release-bonanza-beats-1-0-elasticsearch-shield-watcher-marvel-logstash-2-1-and-kibana-4-3-are-now-available","seo_title":"","content":" The returns! Today, we're thrilled to announce new versions of the entire Elastic Stack, and the first generally available version of Beats! ","locales":"","title":"Release Bonanza! Beats 1.0, Elasticsearch, Shield, Watcher, Marvel, Logstash 2.1 and Kibana 4.3 are Now Available!"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2015-11-24T00:00:00.000Z","url":"/blog/elasticsearch-2-1-0-and-2-0-1-released","seo_title":"Elasticsearch 2.1.0 and 2.0.1 released","content":" Today we are pleased to announce the release of based on and a bug fix release of .Latest stable release:Bug fixes in 2.0:If you can't wait to get your hands on the new, shiny goodness…we understand. Get on with downloading the latest and greatest, or spin up a cluster on — the only hosted Elasticsearch service built and supported by the people building the software.Elasticsearch 2.1.0 adds some great new features and has a number of important enhancements: ","locales":"","title":"Elasticsearch 2.1.0 and 2.0.1 released"}
{"index":{}}
{"author":"Tudor Golubenco","category":"Brewing in Beats","publish_date":"2015-11-23T00:00:00.000Z","url":"/blog/weekly-beats-last-step-before-ga","seo_title":"","content":" Welcome to ! With this series, we're keeping you up to date with all that's new in , from the details of work in progress pull requests to releases and learning resources.In the past week we released 1.0.0-rc2 and we are now ready for our first generally available release!Two more important bugs were reported, tracked down, cornered, and fixed. was about lines above 1023 character potentially getting truncated due to a buffer that was .The could cause Filebeat to get stuck in case of quickly rotating files and we’ve got reports for it , so we are quite happy that we could before the GA.We found an with the error handling code of the response from Elasticsearch. The code assumed the “error” key to be of type string, which is no longer true in Elasticsearch 2.0. The came just in time for RC2.The beats use the “beat.hostname” for the hostname where the Beat is running, but other Logstash plugins typically use the “host” field for the same thing. This caused annoyance to users, so we listened and found the compromise of having the Logstash Beats plugin automatically .The Winlogbeat (ICYMI, the Beat for Windows events logs) got and i to mirror the automated tests for our other officially supported Beats.Our own Robin Clarke played with cross-compiling Topbeat for ARM and tried it on his RaspberryPi. It just worked :-). Not quite an official supported platform for the Beats yet, but it’s good to know that’s well within reach.We’re doing the last bits of work in preparation for the GA release. Stay tuned! ","locales":"","title":"Brewing in Beats: Last step before the GA"}
{"index":{}}
{"author":"Monica Sarbu","category":"Releases","publish_date":"2015-11-24T00:00:00.000Z","url":"/blog/beats-1-0-0","seo_title":"Beats 1.0.0","content":" A big Thank You to everyone who contributed code, reported issues, or just tried the Beats during the beta stage. Start your experience with Beats 1.0.0 and let us know what you think on , , or open an issue on . ","locales":"de-de,fr-fr,ja-jp,ko-kr","title":"The Beats 1.0.0"}
{"index":{}}
{"author":"Michael McCandless","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2015-11-23T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2015-11-23","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top NewsMy blog about using as a time series database: — Felix Barnsteiner (@felix_b) Elasticsearch Core Apache Lucene Watch This Space, where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene"}
{"index":{}}
{"author":"Zachary Tong","category":"Engineering","publish_date":"2015-11-23T00:00:00.000Z","url":"/blog/implementing-a-statistical-anomaly-detector-part-1","seo_title":"","content":" This graph shows the min/max/avg of 45 million data points (75,000 individual time series over 600 hours). There are eight large-scale, simulated disruptions in this graph...can you spot them? No? It’s ok, I couldn’t either. When you aggregate all your data together into a single graph, the variability of your data tends to smooth out all but the most obvious changes. In contrast, here are a random selection from the 75,000 series that make up the first graph: These individual charts make it obvious where disruptions might be occurring. But now we have a new problem: there are 75,000 such charts to keep an eye on! And since every chart has its own dynamic range and noise profile, how do you set a threshold for all of them? This conundrum is faced by many companies. As data collection increases, we need new ways to automatically sift through the data and find anomalies. This is exactly what eBay has done with their new . The authors of the algorithm realized that any individual series may look anomalous simply due to chance so simple thresholds won’t work, while at the same time aggregating all the data together smooths the data out too much. Instead, Atlas aggregates together correlated changes in the variance of the underlying data. This means it is robust to noise, but remains sensitive enough to alert when there is a real, correlated shift in the underlying distribution due to a disruption. After reading the paper, I wanted to see if it could be implemented in Elasticsearch. Over the next three parts of this article, we’ll be exploring how Atlas works and how to build it with a combination of Pipeline aggregations, TimeLion graphs and Watcher alerts. To give you a sneak peak, our final chart will look like this: Which accurately places an “anomaly bar” on top of each simulated disruption that was generated for this subset of data. Let’s get to it! Simulating a production cluster The first order of business is to generate some data. Since we aren't eBay, we'll have to settle for simulated data. I wrote a which generates our dataset. At a high-level, the simulation is very simple: To make the simulation realistic, each has a unique gaussian distribution which it uses to generate values. For example, the tuple may pull values from a normal distribution centered on and a width of 2 std, while the tuple may pull values from a distribution centered on 70 and 1 std. This setup is important for several reasons. First, the same query and metric combination will have a different distribution on each node, introducing a wide amount of natural variability. Second, because these distributions are normal Gaussian, they are not limited to a hard range of values but could theoretically throw outliers (just statistically less likely). Finally, even though there is a large amount of variability between the tuples...the individual tuples are very consistent: they always pull from their own distribution. The final piece in the simulator is simulating . Each tuple also has a \"disrupted\" distribution. At random points in the timeline, the simulator will toggle one of three disruptions: The disruptions randomly last between 2 and 24 hours each, and may overlap. The goal of these disruptions are to model a wide variety of errors, some which may be subtle (handful of queries which show abnormal results) while others are more obvious (an entire node showing abnormal results). Importantly, these disruptions pull from the same dynamic range as the \"good\" distributions: they aren't an order of magnitude larger, or have drastically wider spread (which would both be fairly easy to spot). Instead, the distributions are very similar...just different from the steady-state \"good\" distributions, making them rather subtle to spot. Find the disruptions! Ok, enough words. Let's look at some graphs of our data. We saw this graph earlier:  it is the min/max/avg of all data ","locales":"","title":"Implementing a Statistical Anomaly Detector in Elasticsearch - Part 1"}
{"index":{}}
{"author":"Megan Wieling","category":"","publish_date":"2015-11-22T00:00:00.000Z","url":"/blog/witwies-elastic-meetup-london-paris","seo_title":"Where in the World is Elastic? - Elastic Meetup in London and Paris","content":" Welcome to Find out which Elastic events and meetups are happening near you this week. Upcoming EventsEuropeNovember 26:  AsiaNovember 28:  Upcoming MeetupsNovember 23: November 24: November 24: November 25: November 25: November 25: November 26: November 26: November 25: That's it for this week. Stay tuned next week for more Elastic happenings.- The Elastic Team P.S. if you're interested in hosting a meetup or are giving a talk about Beats, Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic? - Elastic Meetup in London and Paris"}
{"index":{}}
{"author":"Paul Echeverri","category":"News","publish_date":"2015-11-19T00:00:00.000Z","url":"/blog/the-fine-manual","seo_title":"","content":" More than any other company where I’ve worked, Elastic understands the value of clear, useful documentation. A year and a half ago, when I started work as Elastic’s first technical writer, I walked into a place where there was an established culture of engineers that considered documentation a vital part of the software, as attested by the , which was already in place before I started. Building on that culture as we grow the technical publications staff is one of the things that makes this job so exciting. The Process The core process is similar to how it’s done anywhere else: Product Management sets priorities, and Technical Publications talks to the engineers about the current projects to gather requirements and initial insight regarding the new features or improvements. Documentation tasks are tracked with issues on GitHub, just as any other request to improve the software’s usability or functionality. This unified process keeps documentation and engineering effort aligned. When documentation is ready to review, Engineering reviews the material for technical accuracy, we make the necessary corrections, and when there’s a consensus that the material is ready, the source is merged into the repo. But what do I mean by with respect to technical documentation? The Technical Details Documentation at Elastic is written in , a markup language similar to MarkDown, but with a broader feature set. A script invokes the tools that transform the AsciiDoc, first to DocBook XML, then to HTML. The script was created and is maintained by , and you can take a look at it yourself at the docs repository. That repository also contains a that governs which repositories are polled for source AsciiDoc, which books to build, and how version selectors are laid out. To build all the that you see on our site, a cron job makes sure this happens every half hour: This process lets us be very responsive to documentation issues. Fixes typically go live to the Web less than an hour after we merge them into the relevant repository. What this Means to You: Contributing to Documentation Elastic is committed to open source, and one of the signal joys of open source is finding a problem and fixing it. Just as with any other request on a project, you can always open an issue with the documentation on any of our public projects. We’ll assign the issue and get it taken care of. To contribute more directly, open a pull request with your proposed changes. To make the adoption of your changes smoother, here are some best practices: Understand the Documentation Structure The overview of our process earlier in this post is a good high-level description of how things work, but for a good documentation pull request, it helps to understand how documentation is structured at a more granular level. For example, we make use of AsciiDoc declarations, which are defined with pairs of characters, as in this example: Call a defined variable by using the variable’s name in braces: Typically, documentation for a given Elastic software release starts with the file in the directory for that software’s GitHub repository, and is where any attributes used for that documentation are defined. The file has statements that incorporate the text of all the other files in the directory. Files in that directory that use attributes will give you errors about undefined attributes if you don’t build them from . If your contribution relies on new attributes, be sure to specify them in , or they won’t work outside of the one file you did specify them in. Minimize Use of Tables A lot of tables are just used to present two-columnar data, like lists of term definitions or parameter values. For that kind of data, the or work well. Test Build Your Contribution Go to the docs repo on GitHub, download the build script, and make sure your contribution builds cleanly. If you want to go the extra mile, post the rendered HTML online somewhere like so reviewers can look at ","locales":"","title":"The Fine Manual: Documentation Processes at Elastic"}
{"index":{}}
{"author":"Rashid Khan","category":"Releases","publish_date":"2015-11-17T00:00:00.000Z","url":"/blog/kibana-4-2-1-and-4-1-3","seo_title":"","content":" Today we're releasing stability and security updates to Kibana 4.1 and 4.2. The 4.2.x series requires Elasticsearch 2.0+, while the 4.1.x series supports Elasticsearch 1.4.4 - 1.7. Grab 4.2.1 or 4.1.3 right here:  4.2.1 Fixes and enhancements 4.1.3 Fixes and enhancements ","locales":"","title":"Kibana 4.2.1 and 4.1.3 released"}
{"index":{}}
{"author":"Arvind Prabhakar","category":"User Stories","publish_date":"2015-11-18T00:00:00.000Z","url":"/blog/elasticsearch-plus-streamsets-reliable-data-ingestion","seo_title":"","content":" Elasticsearch provides a powerful platform for real-time analytics at scale. Reliable and high-quality data ingestion is a critical component of any analytics pipeline. The recently launched Data Collector provides an enhanced data ingestion process to ensure that data streaming into Elasticsearch is pristine, and remains so on a continuous basis.The StreamSets Data Collector provides an ingestion infrastructure that helps you build continuously curated ingestion pipelines, and improves upon legacy ETL and hand-coded solutions in three important ways: StreamSets customers ingest data from a variety of sources, including 3rd-party data feeds, internal systems, and physical sensors. The more they sanitize their data while in motion, the greater the confidence in their analysis using Elasticsearch and Kibana. The less downtime and data loss in their data pipelines, the more complete and accurate their analysis is. Data Drift Breaks and Corrodes PipelinesA chief culprit in the battle for data quality and pipeline reliability is data drift, which is the accumulation of numerous unanticipated changes that occur in data streams. Data drift can break ingest pipelines or corrupt data.In the brittle world of schema-centric ETL, opaque pipelines can break due to the smallest upstream alteration - such as a re-ordering or renaming of fields or minor changes in data type formatting.  Each time there is an upstream change it risks stopping the ingest process in its tracks, or worse - causing silent data corruption that goes unnoticed for months. When ETL pipelines break, doing forensics and diagnosing their logic is difficult if not downright impossible for large distributed pipelines.  The entire pipeline is a black box requiring substantial effort to debug and get back online.Data drift also creates an insidious second consequence. It acts as a silent killer of data quality. When semantic changes are missed or ignored - as is common - data quality erodes over time with the accumulation of dropped records, null values and changing meaning of values.The operational impact of data drift and the resulting corrosion is that the results of real-time analysis become unreliable or, due to frequent outages, the analysis becomes impossible to perform. The business impact is that bad data leads to bad decisions and entire areas of analytic potential are left on the shelf. StreamSets Makes Continuous Big Data Ingest SimpleStreamSets was designed from the ground up to allow customers to easily build reliable pipelines in the face of data drift. You can use a drag-and drop interface to connect sources to Elasticsearch and also connect pre-built data preparation functions such as field parsing or PII masking.  You can route data based on pre-conditions such as creating an error queue for unexpected values.As an example, the screenshot below shows a pipeline for analyzing payment transactions using Elasticsearch as a destination (1).  The pipeline includes data routing for credit card vs. cash transactions (2), masking of credit card data (3) and routing of suspect location data into a Kafka message queue (4).  All of this was set up, tested, activated, and monitored without writing any code. In addition to its user interface, StreamSets provides a wide array of APIs for extensibility and customization, making it not a special purpose tool but rather a core piece of modern data infrastructure.  Early uses for it have been wide and varied, including: Elasticsearch and StreamSets are naturally complementary and critical components in a real-time analytics architecture.  Customers can use StreamSets to connect a wide variety of sources to both Elasticsearch and Found to provide real-time search capabilities using a constantly curated data flow. Both are built for real-time in-memory performance. Both scale out gracefully through distributed architectures. And both are open sou","locales":"","title":"Elasticsearch plus StreamSets for reliable data ingestion"}
{"index":{}}
{"author":"Monica Sarbu","category":"Releases","publish_date":"2015-11-17T00:00:00.000Z","url":"/blog/beats-1-0-0-rc2-released","seo_title":"Beats 1.0.0-RC2 released","content":" We are pleased to announce the release of Beats 1.0.0-rc2. This is the last planned release candidate before 1.0.0-GA. The more beta testers we have, the sooner we can release Beats 1.0.0 GA, so please , , or , try it out, and let us know what you think on Twitter () or on our . You can report any problems on the , , or  GitHub issues page. ","locales":"","title":"Beats 1.0.0-rc2 released"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2015-11-16T00:00:00.000Z","url":"/blog/last-changes-before-rc2","seo_title":"Weekly Beats: Last changes before RC2","content":" With this series, we're keeping you up to date with all that's new in , from the details of work in progress pull requests to releases and learning resources. During the past week we continued fixing the issues that were reported from the community and we took our last opportunity to simplify and clarify some field names and configuration options before the 1.0.0 GA release. All these changes, some breaking compatibility since RC1, were based on the feedback from the community. Winlogbeat for shipping Windows Event Logs Andrew published to Github this week. It ships Windows Event Logs to Elasticsearch directly or via Logstash. It installs as a Windows service on all versions since Windows XP. Note that the is for now a proof of concept project and is not yet released. See this for the current status. Export beat.name instead of shipper With this each Beat is now exporting instead of , which is more clear, and also adds which is often needed/useful. Before, we used the shipper field to both indicate the Beat name and the OS hostname depending on configuration, so separating the two is meant to simplify the configuration and usage. Fix memory leak in Topbeat A memory leak in Topbeat caused problems on Windows. This is fixing it. It also instructs the go tooling to use the built in race detector when running our tests. This should catch a few bugs in the future. Rename -test with -configtest The option, that is passed to the command line of any Beat, is option to make it clear that this is for testing the configuration file and not for the Beat itself. Remove enabled as a configuration option for outputs This simplifies the configuration file options around the enabled/disabled outputs. The enabled/disabled configuration options for outputs and TLS options are removed and instead you can just comment out or uncomment in the configuration file to disable or enable a certain output or TLS. Remove line from the Filebeat exported fields Filebeat anymore. The line number was not correctly set in case of Filebeat restarts, and making it correct would have affected the performance. We always recommend using the offset field instead as it gives you the correct value in all cases. Update all Beats repositories automatically Currently each Beat has its own repository in GitHub and they share a few common files defined in the libbeat repository. The defines a script to automatically copy the common files of all Beats in libbeat to each Beat by executing make update in the Beat repository. Check for more details on what files are copied from libbeat. Add system tests to Topbeat are added to Topbeat, using the model used by Packetbeat and Filebeat. They are executed by Jenkins and Travis on Linux and OS X and by on Windows. ","locales":"","title":"Brewing in Beats: Last changes before RC2"}
{"index":{}}
{"author":"Shay Banon","category":"News","publish_date":"2015-11-17T00:00:00.000Z","url":"/blog/elasticon-16-biggest-elasticsearch-gathering-ever-is-coming-soon","seo_title":"Elastic{ON}16 : The Biggest Elasticsearch Gathering Ever is Coming Soon!","content":" On the heels of our and our , we're now less than 90 days from our second annual user conference, , from February 17-19, 2016. Last year's event was a huge success, with more than 1,100 amazing attendees and talks from NASA's Mars Rover team, Goldman Sachs, Netflix, and Microsoft. What I enjoyed most was that it was a conference about learning, and that amongst each other, our users, customers, and employees got to share ideas about how they use Elasticsearch, Logstash, and Kibana. With our new products like Beats, Found (Elasticsearch as a Service), and our security, alerting, and monitoring extensions (Shield, Watcher, and Marvel), I'm certain that conversations at will extend beyond our wildest imagination. So this year, we're not only making even bigger and expecting more than 2,500 attendees, but we're also using this as an opportunity to make it even better! Some things I'm most excited about for Elastic{ON}. I hope you can join us! And I hear from our team that we have more than 500 people registered in our first week of open registration and that will be ending soon. If you want to , please do so ASAP as we'd love to hear your topic. See everyone in February, and here is a taste of last year: ","locales":"","title":"Elastic{ON}<sup>16</sup>: The Biggest Elasticsearch Gathering Ever is Coming Soon!"}
{"index":{}}
{"author":"Megan Wieling","category":"","publish_date":"2015-11-16T00:00:00.000Z","url":"/blog/witwies-elasticontour-newyork-chicago","seo_title":"Where in the World is Elastic? - Elastic{ON}Tour London & Paris","content":" Welcome to Find out which Elastic events and meetups are happening near you this week. Upcoming EventsNovember 17: November 16-18: November 17: November 18: November 19: Upcoming MeetupsNovember 18: November 19: November 19: November 16: November 17: November 18: November 18: November 22: That's it for this week. Stay tuned next week for more Elastic happenings.- The Elastic Team P.S. if you're interested in hosting a meetup or are giving a talk about Beats, Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic? - Elastic{ON}Tour New York and Chicago"}
{"index":{}}
{"author":"Suyog Rao","category":"The Logstash Lines","publish_date":"2015-11-17T00:00:00.000Z","url":"/blog/logstash-lines-2015-11-17","seo_title":"","content":" Welcome back to The Logstash Lines! In these weekly posts, we’ll share the latest happenings in the world of Logstash and its ecosystem. 2.0 Feedback Thanks to our users for downloading and trying Logstash 2.0 – we’ve received great feedback so far! In particular, a few users ran into issues while upgrading both their Elasticsearch and Logstash versions to 2.0. These were caused by breaking changes introduced in 2.0, which is a major version upgrade. You can find more information about upgrading Logstash and Elasticsearch to 2.0 We started an to rewrite the class in pure Java. Event is the main object which encapsulates data and provides an API for the plugins to perform processing on the event content. Having Event implemented in pure Java will improve performance, make possible faster serialization by avoiding costly type conversion between JRuby and Java which in turn will help with an efficient persistence implementation. These changes are internal and will not involve any breaking changes to users. As you can see, there’s been a ton of activity in the past week. We are gearing up for another feature release, so stay tuned! ","locales":"","title":"The Logstash Lines: 2.0 Feedback"}
{"index":{}}
{"author":"Megan Wieling","category":"","publish_date":"2015-11-16T00:00:00.000Z","url":"/blog/witwies-elasticontour-wrap-up","seo_title":"Where in the World is Elastic? - Elastic{ON}Tour Wrap Up","content":" Welcome to There are no Elastic events or meetups taking place this week. However, with last Wednesday in Tokyo marking the official end of our tour, we thought we'd share some of our favorite moments. Enjoy!Did we not come to your city? Trying to join in on the fun? Check out our upcoming Elasticsearch User Conference, We just announced a bunch of and there will be 2 pre-conference you're going to want to check out. Elastic{ON} TourAs always, stay tuned next week for more Elastic happenings.- The Elastic Team P.S. if you're interested in hosting a meetup or are giving a talk about Beats, Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic? - Elastic{ON} Tour Wrap Up"}
{"index":{}}
{"author":"Rashid Khan","category":"News","publish_date":"2015-11-12T00:00:00.000Z","url":"/blog/timelion-timeline","seo_title":"","content":" The install commands towards the end of this post have been updated to include Kibana 5. So I’m putting together a demo of Kibana, and thinking: . At the same time, I’d been mulling a new expression syntax for Kibana: . I had a trans-atlantic flight the next day, and I had a plan: A tiny, in-flight, one-person, hack-a-thon to build an experiment that scratched both the time series and expressions itches. By the time I landed in Houston, I had a syntax, a grid of charts and a few simple functions. A few more hours of hacking and I was in Phoenix answering questions I’d never been able to ask before: Timelion makes all that possible, and more. Timelion is an Elastic {Re}search project into time series, but its more than just an experiment: Its completely usable. By you. Right now. Elastic {Re}search? {Re}search is Elastic’s lab for cutting edge technology. {Re}search is our playground, the sandbox in which we build our wildest sand castles. {Re}search projects are launched by Elastic engineers that want to grab an idea, run with it, and share it. The projects launched under the {Re}search tag highlight our next-big-things, in packages you can use (and abuse) today. Some are standalone, but many are plugins and extensions for our existing products. We invite you to install, experiment, and communicate your experiences with our {Re}search: Thats why they exist, to gather feedback on our ideas. This is TimelionTimelion, pronounced brings together totally independent data sources into a single interface, driven by a simple, one-line expression language combining data retrieval, time series combination and transformation, plus visualization. Every Timelion expression starts with a data source function. For example (or for short). That’s about as simple as it gets right? Count everything in Elasticsearch over time. . As you can see, functions always start with a '.' (dot). What if I want to answer that first question above: . Well, I could plot the two parts together on the same chart, by separating them with a comma. But there's a better way ... We can do more than individual functions, we can chain functions! What I want is to divide total hits by unique users. What we're doing here is saying: Get everything, then divide every point in that series, by every point in this cardinality-of-user-field series I'm passing to We can do that, we can do more: , using the exact same syntax. For example, the Worldbank’s Data API. Series even can be grouped together into lists with parenthesis and a function applied to the grouping. All data sources can receive an offset argument, eg offset=-1M to compare last month as if it was happening now. Timeline can even fit series with dissimilar intervals to a reference, then enabling you to divide you by-the-minute Elasticsearch series with say, yearly Worldbank data. That means we can mix and match these sources, even within the same expression.Thus we can ask crazy questions like “What percentage of the US Gross Domestic Product is my company personally responsible for year-to-date?” Also, if that number is big, how about sharing with your old buddy Rashid? Just kidding. Or am I? Funk-shun Al There’s 25 different functions, from simple arithmetic like addition and division to moving averages, cumulative sums and derivatives. That said, Timelion functions and data sources are totally pluggable and super easy to write. We’d love your help rounding out the offering, so get hacking! Go get itI won't give it all away, there’s more to it than just this post. Timelion will launch a tutorial to step you through configuration and some simple starter functions, the rest is up to you to discover. Installing it is easy, run this, bounce the Kibana server. and refresh your browser: . Or you can try it on Found, the best hosted Elasticsearch in the history of the universe, for free: . If you're on 5.x the correct command is: Now use it. Abuse it.Once you'v","locales":"de-de,fr-fr,ja-jp,ko-kr","title":"Timelion: The time series composer for Kibana"}
{"index":{}}
{"author":"Loggy D. Wood","category":"Culture","publish_date":"2015-11-12T00:00:00.000Z","url":"/blog/movember-data-dive-part-1","seo_title":"","content":" Howdy! As you all know from last week, I have decided to this November and raise money for the Movember Foundation. The is a global charity that aims to increase awareness and funds for men’s health issues, with a focus on 4 key areas: prostate cancer, testicular cancer, poor mental health, and physical inactivity., which is the 3rd leading risk factor for global mortality, is the newest focus area for the Movember Foundation. I have decided to jump in on this initiative by doing what I do best: analyzing data with my partners-in-crime, Elasticsearch and Kibana. And in order to help raise awareness around physical inactivity, I decided to look into the from the Centers for Disease Control & Prevention (CDC).Every year, the CDC conducts approximately 500,000 telephone surveys to collect data on a variety of personal health-related topics, such as nutrition, drinking habits, physical activity and health history. With this data set, I investigated macro patterns of physical activity and exercise in the United States. Next week, we will dive into relations between personal behavior and health risks such as diabetes, hypertension, and cardiovascular conditions.The big picture!The 2013 survey included ~491,000 respondents, with 40% men and 60% women. Given the Movember focus on , we will only be looking at the male respondent data in the rest of the analysis here.It is heartening to see that almost 75% of the male respondents had at least some exercise in the last 30 days. On the flip side, only about 52% met their recommended aerobic guidelines and an even lower 30% met their recommended muscle strength guidelines. So, there’s definitely some room for improvement there.Conducting large scale surveys is tricky because you want to make sure that survey design and execution do not pollute or bias the results in any way. For example, in this survey you see a slight impact of interview month on the results. The graph below shows that people interviewed in summer months showed slightly better aerobic fitness levels. People tend to be more active during summer, and what we are observing here is likely the effect of in the survey.Walk on America!Walking is by far the most popular physical activity, with close to 60% of the male respondents listing it as their primary physical activity. Running, weightlifting, and gardening figure in the 2nd, 3rd and 4th spot. Given my woody roots, I was psyched to see Gardening and Yard Work in the top 10.Interestingly, if you look at how much time people spend on their primary physical activity per week, the picture changes a bit as expected. Popular fitness activities like walking, running and weightlifting figure pretty low on the duration metric. Outdoors activities like farming, fishing, skiing, and hunting rise to the top of the chart.Yoga and Pilates are great, but ...Relating the exercise type to the recommended goals, you can see that activities like yoga, weightlifting, and pilates are great for muscle strengthening, but don’t help much with meeting recommended aerobic guidelines. Activities like boxing, rock climbing, wrestling, calisthenics, and aerobics provide a more balanced cardio + strength workout.Buck up, Texas!The BRFSS survey measures personal behavior and health risk patterns at the state level, and this information can be then used by local authorities to design and deploy targeted health initiatives and disease prevention programs.  Let’s continue exploring the big picture from a geospatial angle. Montana residents takes the top spot as the most active state, with an average 650 minutes of total physical activity per week. Texas is at the bottom of the rung with total weekly activity of 368 minutes per respondent.Exercise and DemographicsMoving on to demographic factors, it’s interesting, but not surprising, to see how the type of exercise varies with age. You can see that as people age, they gravitate towards activities with lo","locales":"","title":"Movember Data Dive: Physical Activity Behavior in the US"}
{"index":{}}
{"author":"Adrien Gallou","category":"","publish_date":"2015-11-11T00:00:00.000Z","url":"/blog/amelioration-de-la-pertinence-d-elasticsearch-chez-decitre","seo_title":"Amélioration de la pertinence d'Elasticsearch chez Decitre","content":"   Introduction & Contexte Ces hypothèses seront à définir en fonction du résultat des études précédentes. Travail sur le nombre de résultats ","locales":"fr-fr","title":"Amélioration de la pertinence d'Elasticsearch chez Decitre"}
{"index":{}}
{"author":"Monica Sarbu","category":"Brewing in Beats","publish_date":"2015-11-09T00:00:00.000Z","url":"/blog/weekly-beats-shipped-rc1","seo_title":"","content":" Welcome to ! With this series, we're keeping you up to date with all that's new in , from the details of work in progress pull requests to releases and learning resources. Release 1.0.0-RC1We released 1.0.0-RC1 this past week after closing a total of 100 pull requests in Github.  See the for more details. 100 pull requests later, we bring you Beats 1.0.0-rc1: — elastic (@elastic) When using Logstash as output in your Beat, you need to make sure you have the latest Logstash Beats Input plugin installed by . Note that the current version of Logstash is not coming with the latest Logstash Beats Input plugin, so you need to after installing . We are thrilled to see that so many people are trying out our software and help us improve it by posting issues on the or Github. of issues were fixed during this week. The most interesting ones are: New Community BeatThis week another community Beat was created, called , that is used for uWSGI monitoring. It is a lightweight agent that reads stats from uWSGI server periodically. uWSGI server must expose its stats via . Give it a try and let us know what you think about it. Thank you for creating your second Beat! Configurable Topbeat outputThanks to another community contributor, you can now what type of data Topbeat is outputting. You can select if you want system wide statistics, per process statistics, file system statistics or all of them. Export actual memory usage in Topbeat Add timestamp to Beat log events ","locales":"","title":"Brewing in Beats: Shipped RC1"}
{"index":{}}
{"author":"Aaron Mildenstein","category":"Engineering","publish_date":"2015-11-09T00:00:00.000Z","url":"/blog/introducing-the-de_dot-filter","seo_title":"","content":" Hello, fellow Logstashers! We’ve been pleased with the community response to our recent 2.0 release.  As this was a major release, there were some breaking changes, especially in conjunction with the Elasticsearch 2.0 release.  One of these changes was that .  A few Logstash plugins had to be changed to not use dotted fields to be compatible, like the metrics and elapsed filters.  Unfortunately, many users have no control over the sources of their fields.  This has resulted in a poor user experience where dotted fields existed.  To address this issue, the  filter has been created.  You can find the plugin documentation for it . You can install the de_dot filter using the plugin command: bin/plugin install logstash-filter-de_dotThe de_dot filter will replace dots with underscores by default.  It’s as simple as that!  If I had a dotted field called , the de_dot filter would change the field name to .  You can also choose the separator with the  configuration option.  A separator does not have to be a single character, either.  You can make a separator anything you want, except a dot.  Please remember to change your queries and filters in Kibana to match whatever you change in the de_dot filter! We also added an option to translate dots into nested fields.  If you set  in your configuration, Logstash will ignore the  and attempt to convert dotted fields into nested fields.  For example, if I had a dotted field called , it would become .  The flexibility is yours to choose. We’ve provided a  configuration option to allow you to specify which fields need to be processed.  If no fields are provided, Logstash will check  of your fields for dots.  The de-dot process can be a performance hit to your pipeline, so pre-specifying fields will prevent Logstash from having to check each for dots.    The idea behind specifying  manually is to prevent Logstash from having to check every field, which can result in improved performance.  If you are in a situation where you do not know all possible field names, it may be helpful to tag events which  have dotted fields, and put the de_dot filter within a conditional that checks for that tag. The  option also allows you to de_dot nested fields.  The current release of the de_dot filter will only check the top level of an event for dotted fields.  If you have a dotted field in a nested object, the de_dot filter cannot find it without help.  In this case you must use the  directive to specify a nested field.  This can be done by using the field reference syntax:  will find the nested dotted fields  and .  You can use either the  or  options to act on these fields. We hope that the de_dot filter will help ease your transition to Logstash 2.0 and Elasticsearch 2.0.  If you need help configuring the de_dot filter, please visit our discussion forum at .  If you have suggestions or find an issue, please submit a ticket at . Happy Logstashing! ","locales":"","title":"Introducing the de_dot filter"}
{"index":{}}
{"author":"Megan Wieling","category":"","publish_date":"2015-11-09T00:00:00.000Z","url":"/blog/witwies-elasticontour-munich","seo_title":"Where in the World is Elastic? - Elastic{ON}Tour London & Paris","content":" Welcome to Find out which Elastic events and meetups are happening near you this week. Upcoming EventsNovember 10: November 9-10: November 12: November 14: Upcoming MeetupsNovember 9: November 10: November 11: November 12: November 12: November 10: November 11: November 11: November 11: That's it for this week. Stay tuned next week for more Elastic happenings.- The Elastic Team P.S. if you're interested in hosting a meetup or are giving a talk about Beats, Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic? - Elastic{ON}Tour Munich"}
{"index":{}}
{"author":"Sam Bobb","category":"User Stories","publish_date":"2015-11-11T00:00:00.000Z","url":"/blog/elasticsearch-on-octopart","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud. Ever worked hard on a project but when you show it off everyone says, “wait, what’s different?” In this post, we’re going to attempt to show off one of those projects. Four weeks ago, we replaced our SolrCloud cluster and custom document store with a shiny new Elasticsearch cluster. While this is a major change to the backend that powers Octopart, on the surface everything looks the same History and our decision to changeAt Octopart we have a search index, which provides ordered results to user queries, a document store, which returns content for the results and detail pages, and a relational database that provides persistent data storage. ThriftDB is an application built at Octopart to keep the Solr search index, Thrift document store, and relational database synchronized while supporting schema changes and multiple document types. Longtime Octopart and HackerNews users may remember ThriftDB.com and hnsearch.com — projects built on Octopart’s ThriftDB technology stack. When Octopart was founded in 2007, Elasticsearch did not exist yet. Solr was the most full featured open-source search engine. ThriftDB leveraged Thrift definitions to automate the creation of Solr schemas while providing many useful features not available at the time like document storage, dynamic fields, and schema introspection. ThriftDB worked well for Octopart over the years and enabled quick iteration on schema design and search implementation. In 2014, we discontinued hnsearch.com and the public ThriftDB service to focus completely on part search. However, Octopart continued to use ThriftDB internally. As internal libraries tend to do, ThriftDB accumulated cruft and hacks over the years. New developers had to understand many layers before making changes. It lacked tooling and was difficult to debug. As Octopart grew, we had performance problems as well: our search index lagged hours behind the document store. When we decided it was time to replace ThriftDB, Elasticsearch was a natural fit. Many of the custom features in ThriftDB are available out-of-the box in Elasticsearch. As a popular open-source project, Elasticsearch has a great community and powerful debugging and monitoring tools. We wanted to go with a hosted Elasticsearch solution, and we chose Elastic’s own service, Found. It was simple to set up, and we were able to focus our energy on our application instead of configuring and managing the cluster. Found made it easy to incrementally scale our cluster as we transitioned more of our application to Elasticsearch. The Found blog and forum were tremendous resources for understanding the resiliency and performance implications of different cluster configurations. Migration and testingWith careful planning and the support of the entire company, we were able to accomplish this change in three months with a team of two software engineers. First, we modified the code that populates our search index with denormalized part records to write to two places: the legacy system and our new Elasticsearch cluster. While the legacy system continued to power the website, we were free to build, test, and tear down the new system many, many times. Next we wrote a “shim” to replace our existing ThriftDB API with a version that talked to Elasticsearch and continued supporting all of the options exposed via our public API like faceting, ad-hoc filter queries, drilldown and more. Starting with basic part pages we worked progressively to simple searches and then tackled faceting and filtering. Along the way, we refined our Elasticsearch mapping. Finally, we ran searches on both the legacy and new system, compared the results, and fixed many bugs until we were happy with feature parity, result quality, and speed. Four weeks ago the new system went live. ResultAfter our recent acquisit","locales":"","title":"Octopart on Elasticsearch: All New, Basically the Same"}
{"index":{}}
{"author":"Monica Sarbu","category":"Releases","publish_date":"2015-11-05T00:00:00.000Z","url":"/blog/beats-1-0-0-rc1-released","seo_title":"Beats 1.0.0-rc1 released","content":" We are pleased to announce the release of Beats 1.0.0-rc1. Lots of fixes and improvements Thank you everyone for testing the 1.0.0-beta4 release and for the help making the Beats better! We received a lot positive feedback along with a long list of issues, many of them Windows-related. There have been going into this release! For more details on changes that went into each Beat, have a look at release notes for , , and . Breaking changes Up to the beta4 release, Beats were using the “timestamp” field for storing the date and time information. With the addition of Filebeat and the Logstash integration, this started to cause confusion because Logstash uses “@timestamp” for date and time, and automatically adds it to the record. Having two timestamps with almost the same information ended up being redundant. So, based on your feedback, we decided to rename “timestamp” to “@timestamp” in all Beats before the GA release, resulting in only one consistently-named timestamp field.  Unfortunately, this change changes the structure of the Beats events, so the data generated with older Beats is not compatible with the new Kibana dashboards distributed with each Beat. However, the existing dashboards will continue to work with the old data. When upgrading to 1.0.0-rc1, you need to reload the template and the dashboards to visualize the new data in Kibana.  Follow the getting started guide on loading the , the , the , and also for loading the sample . If you have configured the Beat to send data via Logstash, you need to to the latest version in order to work with the “@timestamp” change. Apt and yum repositories It’s now easier to install Packetbeat, Filebeat, and Topbeat, using the . All packages are now signed and uploaded to where all the other Elastic products are also available. Happy testing The more beta testers we have, the sooner we can release Beats 1.0.0 GA, so please , , or , try it out, and let us know what you think on Twitter () or on our . You can report any problems on the , , or GitHub issues page. ","locales":"","title":"Beats 1.0.0-rc1 released"}
{"index":{}}
{"author":"Loggy D. Wood","category":"Culture","publish_date":"2015-11-02T00:00:00.000Z","url":"/blog/loggy-first-movember","seo_title":"My first movember","content":" It’s November: quick, what’s the first thing you think of? ","locales":"","title":"My First Movember"}
{"index":{}}
{"author":"Suyog Rao","category":"The Logstash Lines","publish_date":"2015-11-02T00:00:00.000Z","url":"/blog/logstash-lines-2015-11-02","seo_title":"","content":" Welcome back to The Logstash Lines! In these weekly posts, we’ll share the latest happenings in the world of Logstash and its ecosystem. Its been a busy week here in Logstash land! We released 2.0.0 and 1.5.5 for week! has Elasticsearch 2.0 compatibility, shutdown semantics, and some nice performance boosts across the board. is a bug fix release, and the last planned one for 1.5 series. It also included Beats input, so users don’t necessarily have to upgrade to 2.0 to use Beats. Resiliency: Persistent Queues Yep, we’ve talked about this feature , but had to put it on the back burner to clear some tech debt, and to lay out some foundation. Post 2.0, we’ve renewed our efforts around this feature. First step is to rewrite in pure Java so we don’t pay the serialization cost across jruby-java boundary while persisting data to disk. We made refactoring existing code around this class to create a pluggable Event gem - logstash-core-event. This will allow us to have a drop-in replacement for the existing ruby Event class. Centralized Config management Discussed functional requirements for and brainstormed few design approaches. Plugins Oh, hey, one more important update – its Movember and your favorite ! Go on, rock the ‘stache! Until next time! ","locales":"","title":"The Logstash Lines: Release Bonanza Edition!"}
{"index":{}}
{"author":"Megan Wieling","category":"","publish_date":"2015-11-02T00:00:00.000Z","url":"/blog/witwies-elasticontour-london-paris","seo_title":"Where in the World is Elastic? - Elastic{ON}Tour London & Paris","content":" Welcome to Find out which Elastic events and meetups are happening near you this week. Upcoming EventsNovember 3: November 5: November 6-7: Upcoming MeetupsNovember 5: November 5: November 5:  November 7: Please note that not all the meetups posted here are sponsored or organized by Elastic, but we are all about supporting the community and bringing together those who are talking about Elastic. That's it for this week. Stay tuned next week for more Elastic happenings.- The Elastic Team P.S. if you're interested in hosting a meetup or are giving a talk about Beats, Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic? - Elastic{ON}Tour London & Paris"}
{"index":{}}
{"author":"Felix Barnsteiner","category":"User Stories","publish_date":"2015-11-04T00:00:00.000Z","url":"/blog/elasticsearch-as-a-time-series-data-store","seo_title":"","content":" As the project manager of , an open source performance monitoring tool, I've recently been looking for a database to replace the cool-but-aging Graphite Time Series DataBase (TSDB) as the backend. TSDBs are specialised packages for storing (performance) metric data, like the response time of your app or the CPU utilisation of a server. Ultimately, we were looking for a datastore that is easy to install, scalable, supports a variety of functions, and has great support for visualizing the metrics.We've previously worked with Elasticsearch, so we know it is easy to install, scaleable, offers many aggregations, and has a great visualisation tool in Kibana. But we didn't know if Elasticsearch was suited for time series data. We were asking the question. In fact, (you know, those folks who shoot protons in circles) did a between Elasticsearch, InfluxDB and OpenTSDB and declared Elasticsearch the winner.The Decision ProcessElasticsearch is a fantastic tool for storing, searching, and analyzing structured and unstructured data — including free text, system logs, database records, and more. With the right tweaking, you also get a great platform to store your time series metrics from tools like collectd or statsd.It also scales very easily as you add more metrics. Elasticsearch has built in redundancy thanks to shard replicas and allows simple backups with Snapshot & Restore, which makes management of your cluster and your data a lot easier.Elasticsearch is also highly API driven, and with integration tools like Logstash, it's simple to build data processing pipelines that can handle very large amounts of data with great efficiency. Once you add Kibana to the mix you have a platform that allows you to ingest and analyse multiple datasets and draw correlations from metric and other data side-by-side.Another benefit that isn't immediately obvious is instead of storing metrics that have been transformed via a calculation to provide an endpoint value that gets graphed, you are storing the raw value and then running the suite of powerful aggregations built into Elasticsearch over these values. This means that if you change your mind after a few months of monitoring a metric and you want to calculate or display the metric differently, it's as simple as changing the aggregation over the dataset, for both historic and current data. Or to put it another way: You have the ability to ask and answer questions that you didn't think about when the data was stored!With all that in mind, the obvious question we wanted to answer is: What's the best method for setting up Elasticsearch as a time series database?First Stop: MappingsThe most important part to start is your mapping. Defining your mapping ahead of time means that the analysis and storage of data in Elasticsearch is as optimal as possible.Here's an example of how we do the mappings at stagemonitor. You can find the original over in our :{ \"template\": \"stagemonitor-metrics-*\", \"settings\": { \"index\": { \"refresh_interval\": \"5s\" } }, \"mappings\": { \"_default_\": { \"dynamic_templates\": [ { \"strings\": { \"match\": \"*\", \"match_mapping_type\": \"string\", \"mapping\": { \"type\": \"string\", \"doc_values\": true, \"index\": \"not_analyzed\" } } } ], \"_all\": { \"enabled\": false }, \"_source\": { \"enabled\": false }, \"properties\": { \"@timestamp\": { \"type\": \"date\", \"doc_values\": true }, \"count\": { \"type\": \"integer\", \"doc_values\": true, \"index\": \"no\" }, \"m1_rate\": { \"type\": \"float\", \"doc_values\": true, \"index\": \"no\" }, \"m5_rate\": { \"type\": \"float\", \"doc_values\": true, \"index\": \"no\" }, \"m15_rate\": { \"type\": \"float\", \"doc_values\": true, \"index\": \"no\" }, \"max\": { \"type\": \"integer\", \"doc_values\": true, \"index\": \"no\" }, \"mean\": { \"type\": \"integer\", \"doc_values\": true, \"index\": \"no\" }, \"mean_rate\": { \"type\": \"float\", \"doc_values\": true, \"index\": \"no\" }, \"median\": { \"type\": \"float\", \"doc_values\": true, \"index\": \"no\" }, \"min\": { \"type\": \"float\", \"doc_values\": true,","locales":"de-de,fr-fr,ja-jp,ko-kr","title":"Elasticsearch as a Time Series Data Store"}
{"index":{}}
{"author":"Michael McCandless","category":"Engineering","publish_date":"2015-11-03T00:00:00.000Z","url":"/blog/elasticsearch-performance-indexing-2-0","seo_title":"","content":" A little over a year ago I  with Elasticsearch, but a lot has changed since then so here I explain the changes that affect indexing performance in Elasticsearch 2.0.0. Store throttling is now automaticPrior to 2.0.0, Elasticsearch throttled ongoing merges at fixed rate (20 MB/sec by default), but this was often far too low and would easily lead to merges falling behind and subsequent index throttling.  As of 2.0.0, Elasticsearch now : when merges are starting to fall behind, the allowed IO rate is increased, and decreased when merges are keeping up. This means a sudden but rare large merge on a low-throughput indexing use case should not saturate all available IO on the node, minimizing impact to ongoing searches and indexing. But for heavy indexing, the allowed merge IO will increase to match the ongoing indexing. This is good news!  It means you shouldn't need to touch any throttling settings: just use the defaults. Multiple Using multiple IO devices (by specifying multiple  paths) to hold the shards on your node is useful for increasing total storage space, and improving IO performance, if that's a bottleneck for your Elasticsearch usage. With 2.0, there is an important change in how Elasticsearch spreads the IO load across multiple paths: previously, each low-level index file was sent to the best (default: most empty) path, but now that switch is per-shard instead.  When a shard is allocated to the node, the node will pick which path will hold all files for that shard.  The improves resiliency to IO device failures: if one of your IO devices crashes, now you'll only lose the shards that were on it, whereas before 2.0 you would lose any shard that had at least one file on the affected device (typically this means nearly all shards on that node). Note that an OS-level RAID 0 device is also a poor choice as you'll lose all shards on that node when any device fails, since files are striped at the block level, so multiple  is the recommended approach. You should still have at least 1 replica for your indices so you can recover the lost shards from other nodes without any data loss. The Auto-ID optimization is removedPreviously, Elasticsearch optimized the auto-id case (when you didn't provide your own id for each indexed document) to use append-only Lucene APIs under-the-hood, but this proved problematic in error cases and so . We also  so that this optimization was no longer so important, and this allowed us to  entirely as they consumed non-trivial heap but didn't offer much ID lookup performance gain. Finally, as of 1.4.0, Elasticsearch  to generate its IDs, for better lookup performance over the previous UUIDs. All of this means you should feel free to use your own id field, with no performance penalty due to the now removed auto-id handling, but just remember that your choice of id field values . More indexing changes...Beyond these changes there have been a great many additional 2.0 indexing changes such as:  (instead of using CPU- and heap-costly field data at search time), moving the dangerous delete-by-query API from a core API to a safe , exposing control over , better and , reducing heap required when merging and a number of improvements in resiliency such as . If you are curious about the low-level Lucene actions while indexing, add  to your , but be warned: this generates impressively copious output! Our  shows that we are generally moving in the right direction with our defaults over time: the docs/sec and segment counts for the defaults (4 GB) heap has caught up to the  performance line. ","locales":"","title":"Performance Considerations for Elasticsearch 2.0 Indexing"}
{"index":{}}
{"author":"","category":"Brewing in Beats","publish_date":"2015-11-02T00:00:00.000Z","url":"/blog/weekly-beats-on-the-way-to-rc1","seo_title":"","content":" Welcome to ! With this series, we're keeping you up to date with all that's new in Beats, from the details of work in progress pull requests to releases and learning resources. Getting FeedbackThe Beta4 release brought us positive feedback from the community, but also more work for our team. There were many questions and issues opened that kept us busy for the week, but also motivated us to improve the Beats on the way to GA. Our current plan is to release an RC1 the middle of this week, and if things go well, release the GA a bit later. Apt and yum repositoriesIt’s now easier to install Packetbeat, Filebeat, and Topbeat by using the apt and yum commands. The beta4 release packages, as well as previous releases, are now signed and uploaded to packages.elasticsearch.org where all the other Elastic products are also available. The steps to install the Elastic public key and then the packages from the repositories are in this . Renamed the ‘timestamp’ field to ‘@timestamp’Everything started with an reported against Filebeat where the timestamp information is available twice in Elasticsearch when Logstash output is enabled. Filebeat sets the `timestamp` to the time when the event is read, and Logstash adds an additional `@timestamp` field set to the time when the event was received or to the value parsed from the log line. Having two timestamp fields, usually with similar but not identical values, can be quite confusing. So we renamed `timestamp` to `@timestamp` in all Beats. Filebeat exports the `@timestamp` field, and if the Logstash date-filter is used, the field is overwritten with the real timestamp of when the event was produced. This keeps the field consistently named across Beats and also consistent with the file-input plugin from Logstash. On the downside, the meaning of the `@timestamp` field is different for each Beat. For Packetbeat, it means the time of the request, for Topbeat it is the time when system metrics were read, and for Filebeat it is the time when the log line was read. Filebeat Windows fixesWindows is a challenging platform for log readers due to its strict file locking rules. With the help of the community, we discovered and fixed an important this week. Because Filebeat was requesting GENERIC_WRITE access to the files, they were being marked as read-only for new open operations.  Our automatic tests failed to detect this issue because the tests are written in Python, and Python simply ignores this flag.   fixed it. Add stdout as file outputWe added stdout as file output as the result of this on discuss. This output is useful for testing and potentially for chaining the Beats. Getting to an awesome documentationThis week we worked closely with , our great technical writer, on improving the Beats documentation. You can already see the improvements online for the , , , and . Closing other Beta4 IssuesA big Thank You to everyone who tried the Beta4 release and posted issues on or . We fixed this week. ","locales":"","title":"Brewing in Beats: On the way to RC1"}
{"index":{}}
{"author":"Michael Basnight","category":"Engineering","publish_date":"2015-10-30T00:00:00.000Z","url":"/blog/tear-away-your-acls-upgrade-your-found-cluster-to-shield","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud. One of the benefits of being a customer of Found, our official hosted Elasticsearch service, is that you get to use Elastic’s enterprise plugins, including . Found uses Shield on every cluster to ensure that your data is secure. It also has user authentication, so you can set up users and roles, and with Shield 2.0 (available in Elasticsearch 2.0 on Found), you can even limit the fields and documents that users can access! As our awesome Elastic engineers come out with features, Found customers benefit from them! #winning. A brief history of Found ACLBefore Found implemented Shield, we had our own way of doing Access Control Lists (ACL). Some of our long time customers very likely implemented those ACLs in production. The legacy ACLs, while they worked very well, they had their downsides. It was a piece of infrastructure Found had to implement and maintain. It was also easy to make mistakes because of the syntax of the ACL.  As you can imagine, we jumped at the chance to make our infrastructure less complicated by migrating all our clusters to use Shield. Now we have a world class solution providing ACL and node-to-node security for every cluster on Found! Now doesn’t that just give you warm fuzzies? ","locales":"","title":"Tear away your ACLs: Upgrade Your Found Cluster To Shield"}
{"index":{}}
{"author":"Adrien Grand","category":"","publish_date":"2015-10-29T00:00:00.000Z","url":"/blog/modeling-data-for-fast-aggregations","seo_title":"","content":" When wanting to improve search performance, one common reaction is often to start looking at what settings can be tuned in order to make elasticsearch perform better. Unfortunately, even when successful, this approach rarely improves performance by more than a few percent. On the other hand changing the way that documents are modeled can have a dramatic impact on performance. For instance imagine that you want to find all documents that contain a given 3-letters substring (eg. “ump” would find documents containing “jumps”). The naive approach would be to to index documents with the default configuration and then search using a query. Unfortunately, the wildcard query is one of the slowest queries that is exposed in Elasticsearch. On the other hand, you could change your analysis chain in order to index every substring using the tokenizer (so that “jumps” would generate three tokens “jum”, “ump” and \"mps\") and the wildcard query could be replaced with a simple term query, which performs much faster. The general advice is that you should model your data in order to make search operations as lightweight as possible. A corollary is that you cannot make good data modeling decisions without knowing the queries that your users run. And what we just said about queries can also be applied to aggregations: we will look at an example in order to see how this could work. Imagine that you allow your users to search real-estate listings documents which might look like this: { “location”: [-0.37, 49.2], “price_euros”: 188000, “surface_m2”: 132, “has_photo”: true, “has_swimming_pool”: false, “has_balcony”: true } And in order to give your users insights about the listings, you might use an aggregation that would look like this: { “aggs”: { “location_grid”: { “geohash_grid”: { “field”: “location”, “precision”: 3 } } }, “price_ranges”: { “range”: { “field”: “price_euros”, “ranges”: [ {“to”: 20000}, {“from”:20000, “to”: 50000}, {“from”:50000, “to”: 100000}, {“from”:100000, “to”: 200000}, {“from”:200000} ] } }, “surface_histo”: { “histogram”: { “field”: “surface_m2”, “interval”: 20 } }, “has_photo_terms”: { “terms”: { “field”: “has_photo” } }, “has_swimming_pool_terms”: { “terms”: { “field”: “has_swimming_pool” } }, “has_balcony_terms”: { “terms”: { “field”: “has_balcony” } } } Pre-compute data for aggregationsIf most requests would use the same ranges for the aggregation and the same interval for the aggregation, then it is possible to optimize this aggregation by directly indexing the ranges that a document belongs to and replacing these range and histogram aggregations with a terms aggregation. Here is what the document and aggregations would look like now: { “location”: [-0.37, 49.2] “price_euros”: 188000, “price_euros_range”: “100000-200000”, “surface_m2”: 132, “surface_m2_range”: “120-140”, “has_photo”: true, “has_swimming_pool”: false, “has_balcony”: true } { “aggs”: { “location_grid”: { “geohash_grid”: { “field”: “location”, “precision”: 3 } } }, “price_ranges”: { “terms”: { “field”: “price_euros_range” } }, “surface_histo”: { “terms”: { “field”: “surface_m2_range” } }, “has_photo_terms”: { “terms”: { “field”: “has_photo” } }, “has_swimming_pool_terms”: { “terms”: { “field”: “has_swimming_pool” } }, “has_balcony_terms”: { “terms”: { “field”: “has_balcony” } } } Merge aggregations togetherThis aggregation will likely perform a bit faster, but we can further improve it: all these terms aggregations are executed against a field that can only have a finite set of values and we could easily merge them all into a single terms aggregation: { “location”: [-0.37, 49.2] “price_euros”: 188000, “su","locales":"","title":"Modeling data for fast aggregations"}
{"index":{}}
{"author":"Pere Urbon-Bayes","category":"Engineering","publish_date":"2015-10-29T00:00:00.000Z","url":"/blog/logstash-functionality-through-testing","seo_title":"","content":" At Logstash, testing is fundamental to our engineering process. In this blog, we want to share this process with our amazing community. We’ll share how we do testing, including process and tools we use, and for Logstash plugin developers out there, we'll briefly describe how to write tests. ","locales":"","title":"Verifying Logstash Functionality Through Testing"}
{"index":{}}
{"author":"Suyog Rao","category":"Releases","publish_date":"2015-10-29T00:00:00.000Z","url":"/blog/logstash-1-5-5-released","seo_title":"","content":" After Wednesday’s , we still aren’t done – one more today! We are happy to announce the of Logstash version 1.5.5 today. This is mainly a bug fix release and the last release planned for the 1.5 series. Enhancements Filebeat Support In case you missed it, we a beta version of – the next-generation Logstash Forwarder. Filebeat is an agent to ship file-based logs to Logstash for further processing. Logstash version 1.5.5 works out of the box with Filebeat version 1.0.0-beta4 using the plugin. Performance improvements Other fixes Feedback Please Logstash 1.5.5 and let us know what you think on Twitter (@elastic) or on our . You can report any problems on the GitHub issues page. ","locales":"","title":"Logstash 1.5.5 released"}
{"index":{}}
{"author":"Shay Banon","category":"News","publish_date":"2015-10-28T00:00:00.000Z","url":"/blog/release-we-have","seo_title":"","content":" It is a big day here at Elastic, internally referred to as our \"release bonanza\" day. Most of our major products have fresh releases, and today they are aligned to make sure users will have the best experience when using them together. We have:  A lot of work has gone in Shield, our security product for Elasticsearch, and Watcher, our alerting and management product. Security wise we have implemented one of our most requested features, field and document level security, in a way that “goes all the way down” to Lucene. We have also allowed for users to implement their own pluggable security handling. Watcher now allows to disable watches, and send notifications to Slack and HipChat (we do love our bot ops). Official support for Elasticsearch 2.0, clean shutdown semantics, performance improvements across the board, and support. Phew, I am out of breath. What the team has done is impressive, humbling, and exciting! This is a great example of how working together, as a company at Elastic, and all of our users and contributors, results in our main mission: shipping products our users love, enjoy, succeed, and innovate on. Thank you.“A Lion, in Africa?” - We are not done, I will end with this teaser, coming (really really) soon : ) ","locales":"de-de,fr-fr,ja-jp,ko-kr","title":"Release, we have"}
{"index":{}}
{"author":"Uri Boness","category":"Releases","publish_date":"2015-10-28T00:00:00.000Z","url":"/blog/shield-watcher-and-marvel-2-0-ga-released","seo_title":"","content":" Today we're thrilled to announce the release of Shield, Watcher and Marvel 2.0. This is the first release of these extensions that is compatible with . Beyond Elasticsearch 2.0 compatibility, Shield and Watcher 2.0 introduce new and exciting features, expanding the security and alerting capabilities respectively. Shield Watcher MarvelWe’re super excited to introduce Marvel 2.0. With a complete UI redesign, built on top of Kibana 4, we have taken all that we’ve learned from Marvel 1.x and built an easier to use, streamlined monitoring UI. In the same spirit of Shield and Watcher, this first Marvel release lays the foundation for future growth and focuses on the key metrics required to efficiently monitor Elasticsearch 2.0. As part of the redesign, we reduced the interface to 6 pages: Cluster List An increasing number of our users and customers are running multiple clusters, and Marvel now makes it easy to monitor them all from a centralized monitoring cluster. Just configure each cluster to send data to the monitoring cluster, and Marvel does the rest. Cluster Overview The cluster overview shows the key performance metrics for a single cluster, allowing you to quickly identify spikes or valleys. The page also shows any active shard recoveries or relocations. Indices List The indices list shows all indexes in the cluster, along with a host of properties. The table updates live and supports interactive filtering and sorting. Ever wonder what your biggest index is? Wonder no more. Index Detail The index detail page shows the key performance metrics of your index, as well as providing a clear picture of where the shards are allocated. Nodes List The nodes list shows the set of nodes in the cluster, along with key performance metrics. The table updates live and supports interactive filtering. Easily identify nodes with high CPU usage or low disk space. Node Detail The node detail page captures the key performance metrics of an individual node, as well as the set of shards on the node. As part of being built on top of Kibana 4, there are some operational changes as well. Marvel now installs in two parts - the marvel-agent, and the Marvel user interface. Marvel AgentThe marvel-agent installs as a plugin into your elasticsearch cluster. It captures the key performance information and stores it locally or pushes it to a separate monitoring cluster. Marvel User InterfaceThe Marvel UI installs into Kibana as a plugin. This uses the new Plugin infrastructure in Kibana 4.2 to provide a separate Marvel App inside the Kibana interface, which is reachable with a new app-switcher control: The 2.0 release is a big one for our products, and we’re eager to hear what you think. Reach out to us in the forums: or by email at . ","locales":"de-de,fr-fr,ja-jp,ko-kr","title":"Shield, Watcher, and Marvel 2.0.0 GA Released"}
{"index":{}}
{"author":"Clinton Gormley","category":"Engineering","publish_date":"2015-10-28T00:00:00.000Z","url":"/blog/elasticsearch-2-0-0-released","seo_title":"Elasticsearch 2.0.0 GA released","content":" With 2,799 pull requests by 477 committers added since the release of Elasticsearch 1.0.0, we are proud to announce the release of , based on . As if that were not enough, we are also releasing version 2.0.0 of the , an all new streamlined which is now free to use in production, and a new open source . You can and read about the important here. The full changes list can be found here: Change logs for the commercial plugins can be found here: ","locales":"de-de,fr-fr,ja-jp,ko-kr","title":"Elasticsearch 2.0.0 GA released"}
{"index":{}}
{"author":"Boaz Leskes","category":"Engineering","publish_date":"2015-10-28T00:00:00.000Z","url":"/blog/sense-2-0-0-beta1","seo_title":"The Story of Sense - Announcing Sense 2.0.0-beta1","content":" ","locales":"de-de,fr-fr,ja-jp,ko-kr","title":"The Story of Sense - Announcing Sense 2.0.0-beta1"}
{"index":{}}
{"author":"Tudor Golubenco","category":"Brewing in Beats","publish_date":"2015-10-26T00:00:00.000Z","url":"/blog/weekly-beats-first-filebeat-release","seo_title":"","content":" Beats 1.0.0-beta4 release brings you Filebeat: the lightweight log shipper successor to Logstash-Forwarder: — elastic (@elastic) ICYMI, we on Thursday.  The clear highlight of the release is the first version of FIlebeat, the lightweight log shipper successor of the Logstash Forwarder. Thank you everyone that helped testing the internal candidate, we caught at least one pretty bad issue and we got a ton of input on how we can improve the docs. Most of the work this week went into preparing and testing the release. Other noteworthy things: Filebeat can on a per-glob basis to expect a particular encoding. This wasn’t a problem for the Logstash Forwarder because its serialization protocol was encoding agnostic and the user could configure a codec in Logstash. It was a problem for Filebeat because (1) it uses JSON for communicating with Logstash and (2) it can send the data directly to Elasticsearch. With Windows using UCS-2 by default, it was important for us to have a solution for the beta4. Even if this means an extra configuration parameter for Filebeat, we expect that configuring the encoding close to where the files are is better for users, so it is likely a plus overall. With the beta4 release, which makes it possible for all Beats to send data to Logstash without any intermediaries, we the Redis output. See this , where the rationale behind this decision is further explained. The Lumberjack code -- used by both the new Beats input plugin and the Lumberjack input plugins in Logstash -- used to live in its own library, even though fairly different code paths were taken depending on the plugin in use. This caused some dependency conflicts at install time. To fix this, and also to gives the beats-input plugin more freedom in changes to the protocol, the protocol code in the plugin repo.  Thanks to this change, installing the beats input is now as easy as `./bin/plugin install logstash-input-beats`.  With the next Logstash release, this step won’t be required at all as the plugin will be bundled in the default distribution. We continued our push for improving the docs, with everyone in the team contributing. We now have a professional editor helping with this, so we expect the docs quality to be rapidly improved. Related to the documentation effort, we have updated the sample configuration files to contain a lot more comments describing the available options. We found and fixed where the index names that we create (e.g. [packebeat-]YYYY.MM.DD) was not using the UTC timezone. We now have for the Filebeat -> Logstash communication also for the TLS enabled case, including server certificate validation. Yay for one less thing to manually check and worry about. The listing on the S3 bucket serving the is now easier to browse thanks to a new JS listing snippet. More importantly, the last built artifacts as their version and show up first on each page, so it is now easier to download the most recent build. We have an ongoing effort to execute the packaging tests daily on a bare metal machine (has to be bare metal because it makes use of Virtualbox machines), so having predictable download URLs was needed. ","locales":"","title":"Brewing in Beats: First Filebeat release"}
{"index":{}}
{"author":"Costin Leau","category":"Releases","publish_date":"2015-10-29T00:00:00.000Z","url":"/blog/elasticsearch-for-apache-hadoop-2-2-0-beta1","seo_title":"","content":" Continuing the from yesterday (or depending on your timezone, this morning), I am happy to announce the releases of Elasticsearch for Apache Hadoop (aka ES-Hadoop) and Both releases contain a number of important bug-fixes but also new features such as: Optimized data nodes routing To minimize memory pressure on master nodes, the default routing has changed to read and write data only through data nodes excluding masters all together. Consolidated and crowded clusters should see increased throughput and better stability especially in long running jobs. Support for Spark 1.5 Spark 1.5 is officially supported while maintaining compatibility with all the previous versions up to 1.x Enhanced push down operations in Spark Speaking of Spark, the push-down operations have been improved through the null-safe equality comparator (in Spark 1.5) and better generated clause. On top of that, ES-Hadoop features: Elasticsearch 2.0 GA support Yup, take the latest and greatest Elasticsearch release and run your Hadoop and Spark jobs against. As before, compatibility with Elasticsearch 1.x is preserved. Not only beta1 is compatible at the REST level but also the plugin has been rewritten to take advantage of the new plugin architecture in Elasticsearch 2.0. Support for restricted/WAN installs Running Elasticsearch in the cloud or behind a restricted firewall? Is access allowed only through a predefined number of gateways (that might or might not be part of Elasticsearch)? With beta1 this scenario is supported out of the box through a simple configuration option. Do note that for performance reasons it is desired to have full access to the Elasticsearch data nodes. Option to restrict the number of documents being read For cases where only a restricted number of matches need to be returned for a given query, beta1 introduces a new configuration option to limit the results. Feedback ES-Hadoop 2.1.x users are recommended to upgrade to 2.1.2 while those wanting to upgrade to Elasticsearch 2.0, to use ES-Hadoop 2.2.0-beta1. Let us know what you think on Twitter (@elastic) or on our forum. You can report any problems on the GitHub issues page. Better yet, if you would like to chat live about Elasticsearch and Hadoop/Spark, yours truly will be attending the (London, Paris, New York and Chicago). Looking forward to it! ","locales":"","title":"Elasticsearch for Apache Hadoop 2.2 beta1 and 2.1.2 released"}
{"index":{}}
{"author":"Jay Greenberg","category":"","publish_date":"2015-10-29T00:00:00.000Z","url":"/blog/f5-high-speed-logging-with-elastic-stack","seo_title":"","content":" A load balancer often serves as a gateway into your production network. In this position, it has a unique perspective of seeing all traffic, while also being aware of the mapping between web services and servers. A great deal of high level information can be learned from the load balancer with minimal effort, and it is independent of web server, application, and platform. This flow of information is highly consistent, and serves well for baselines. Naturally, we can log from this point (as an alternative to processing web server logs) to gain visibility in web traffic patterns and application behavior. In this blog, we will take a look at how to pair the Elastic Stack with an F5 , which is great at quickly balancing load across massive amounts of traffic. This is not unlike Elasticsearch & Kibana that gives us ‘wire speed’ access to our data for analysis, allowing us to answer important questions in near real time. : : :   From there you can drill down on these answers by time period, or other HTTP metric , and even compare them side-by-side with other services. This information is of course useful - and the F5 and Elastic architectures mix particularly well making it quick and easy to get to these answers. The F5 distributes logging traffic across a pool of Logstash Servers, conveniently including information about the Virtual Service. HSL Pools for LogstashF5’s High Speed Logging (HSL) mechanism is designed to pump out as much data as can be readily consumed, with the least amount of overhead, to a pool of syslog listeners. As it happens, Elastic Stack is designed to consume data in high volume. The HSL template packs the information into a parsable string, perfect for Logstash to interpret. Consult F5’s documentation on Request Logging for the official (and more detailed) procedure, but in brief, one must follow these steps:  GROK & The HSL TemplateOut of the box, we have accomplished a difficult task easily - we have our logs flowing into Logstash. Next, we need to tell logstash how to parse the messages before indexing them into Elasticsearch. Just like the Logstash cluster is analogous to the HSL Pool, the GROK pattern is analogous to the HSL Request Logging Template. The two can be thought of as a CODEC, converting HTTP request details to a parsable string, that can be subsequently parsed into JSON for output into Elasticsearch. Here is an example of a request logging template which will give us information about the load balancer’s Virtual Server, Pool Name, Server IP, Response Time, in addition to other common attributes. $CLIENT_IP $DATE_NCSA $VIRTUAL_IP $VIRTUAL_NAME $VIRTUAL_POOL_NAME $SERVER_IP $SERVER_PORT \"$HTTP_PATH\" \"$HTTP_REQUEST\" $HTTP_STATCODE $RESPONSE_SIZE $RESPONSE_MSECS \"$Referer\" \"${User-agent}\" Now on the other side, in our Logstash cluster, we decode and process the log entry generated by the F5. Logstash has 3 primary phases: Input, Filter, and Output, known as the Logstash has a built-in syslog input. But because you are decoding the data manually, you should just use raw TCP and/or UDP input plugins. The core of the filter phase is GROK - an unbiased parsing language - perfect for decomposing the log entry exactly as it was put together. Here is the GROK pattern used to decode the above template. \"%{IP:clientip} \\[%{HTTPDATE:timestamp}\\] %{IP:virtual_ip} %{DATA:virtual_name} %{DATA:virtual_pool_name} %{DATA:server} %{NUMBER:server_port} \\\"%{DATA:path}\\\" \\\"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\\\" %{NUMBER:response:int} %{NUMBER:bytes:int} %{NUMBER:response_ms:int} %{QS:referrer} %{QS:agent}\" After parsing, the filter phase can continue in any way you would like to enrich your data, such as IP address GeoLocation. DNS resolution, or even drop to Ruby for custom manipulation. Finally, the newly created event object is indexed into Elasticsearch via the Logstash Elasticsearch output plugin. Setting up Health Chec","locales":"","title":"F5 High Speed Logging with Elastic Stack"}
{"index":{}}
{"author":"Rashid Khan","category":"Engineering","publish_date":"2015-10-28T00:00:00.000Z","url":"/blog/kibana-4-2-0","seo_title":"","content":" Elasticsearch 2.0 + Kibana 4.2 = 💚 We're happy to announce the very first release of Kibana with support for Elasticsearch 2.0. What does that mean for you? Speed. Stability. Sweet new features. If you're itching to get moving, , now. Otherwise, read on for the fun stuff you can expect in Kibana 4.2 Afraid of the dark?Never! While we always recommend using a light background while composing charts and dashboards, sometimes you want them on a big screen, in a dark room, and nobody wants a bright white screen staring them down. To mitigate that effect, we've introduced a dark mode. Now you can comfortably enjoy Kibana dashboards in the NOC, the observatory, my soul, or really anywhere dark. Custom map providersKibana's included map provider is a great, but we heard you wanted more options. If you fancy yourself a deft map handler, try out Kibana 4.2’s support for WMS background maps. WMS is super powerful and there's a wealth of free services providing them, including the US Geological Survey: What's the scenario?If something is wrong, we want you to know right away, so we've created a server status page that will give you an overview of how Kibana is operating at that very moment, and if there are any components that need your attention. Of course, if you just need the comfort of knowing everything is a-okay, it's always available right away from the Status tab under settings. Faster in every wayBrowser refreshes happen faster than ever thanks to the new code bundling system in Kibana 4.2 that optimizes the code it serves to just what you need at that moment. Also, remember memory? Kibana remembers. Kibana 4.2 includes a big memory cleanup allowing long long long running dashboards while maintaining a small small small memory footprint. And oh so much more...There are tiny tweaks around every corner and we’ve laid the groundwork for some really exciting stuff in the coming weeks, so stay tuned to , our , and the and don’t miss a moment of the action. ","locales":"de-de,fr-fr,ja-jp,ko-kr","title":"Kibana 4.2.0 released"}
{"index":{}}
{"author":"Suyog Rao","category":"Releases","publish_date":"2015-10-28T00:00:00.000Z","url":"/blog/logstash-2-0-0-released","seo_title":"","content":" We are happy to announce that Logstash 2.0.0 has been released today! This introduces breaking changes in functionality for some configurations, so please read our for details, or the new documentation. You can read our previous posts for changes in other 2.0.0 pre-releases: Here’s a recap of headlining changes in 2.0: Compatibility with Elasticsearch 2.0 Elasticsearch 2.0 is generally as of today which contains many new features and enhancements. Logstash 2.0 is compatible out of the box with this release. Previous releases of Logstash defaulted to using the Java client to communicate with Elasticsearch:  2.0 defaults to the HTTP client. This provides a seamless experience for users to get their data, enrich it, and store and analyze with Elasticsearch. HTTP has functional parity with the other protocols (node and transport), is only marginally slower when connected to a single client, yet far easier to administer and work with. When using the HTTP protocol, one may upgrade Elasticsearch versions without having to upgrade Logstash in lock-step. For more detailed information (including performance numbers) on the switch to HTTP as default, please check the . We’ll continue to support the other (, ) protocols, but the plugin to use them requires manual installation: Compatibility Matrix The below table lists Logstash’s compatibility with Elasticsearch versions. Compatibility with Shield 2.0 This release is compatible with Elastic Shield 2.0 release. For HTTP protocol, no additional plugins are required. Please follow the documentation described . For the protocol, a separate plugin has to be installed for integrating with Shield 2.0 and above: Performance improvements This release also includes performance improvements in many areas so you can process your data faster with Logstash. Below are a few mentions: Filebeat Support In case you missed it, we a beta version of – the next-generation Logstash Forwarder. Filebeat is an agent to ship file-based logs to Logstash for further processing. 2.0.0 works out of the box with Filebeat version 1.0.0-beta4 using the plugin. Shutdown Handling In previous Logstash releases, when a shutdown was initiated, an Exception mechanism was used to signal the plugins to start shutting down. This process was problematic because plugins frequently use third-party code. When Logstash did not know how to handle the exceptions, unpredictable behavior often resulted. We fixed this by adding API calls (for example, ) for each plugin to communicate a shutdown event and let the plugin gracefully stop itself. This meant updating over 200+ plugins in the Logstash ecosystem to adhere to the new APIs! Although shutting down Logstash is not completely fixed yet – stalled outputs can still delay the shutdown – we have all the breaking API changes in 2.0 and can start iterating on fixes in point releases. Plugin developers: If you have developed plugins for Logstash 1.5, please consult the breaking changes document for a list of new APIs related to shutdown. Also, the repo provides sample code for using this new shutdown mechanism. Documentation The updated documentation for 2.0 and all plugins are available . Please consult this reference for any configuration changes. Updating to 2.0 Before updating to 2.0, please consult the in our reference docs. Feedback Many thanks to our users and contributors for making 2.0 a successful release. We appreciate all the testing of pre-releases and numerous patches contributed to this release. Please follow our to stay tuned on future enhancements, releases, etc. So, go on, give 2.0 a and let us what you think! ","locales":"de-de,fr-fr,ja-jp,ko-kr","title":"Logstash 2.0.0 released"}
{"index":{}}
{"author":"Michael McCandless","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2015-10-26T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2015-10-26","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top News 2.0 is coming...join this Wed for a demo, feature Qs, and more! — elastic (@elastic) Elasticsearch CoreChanges in 2.0: Apache Lucene Watch This Space, where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - October 26th 2015"}
{"index":{}}
{"author":"Suyog Rao","category":"Releases","publish_date":"2015-10-22T00:00:00.000Z","url":"/blog/logstash-2-0-0-rc1-released","seo_title":"","content":" We are pleased to announce the release of Logstash 2.0.0-rc1. This is the last planned release candidate for 2.0.0. This is a release candidate. Please do not deploy release candidates to your production environment. Changes We’ve had one change go into this release since : Prior implementations of the metrics filter plugin used dotted field names. Elasticsearch does not allow field names to have dots, beginning with version 2.0, so a change was made to use sub-fields instead of dots in this plugin. Please note that these changes make version 3.0.0 of the metrics filter plugin incompatible with previous releases. You can Logstash 2.0.0-rc1 and check the changes list . As always, we value your feedback, so please try RC1 and let us know what you think! ","locales":"","title":"Logstash 2.0.0-rc1 released"}
{"index":{}}
{"author":"Tudor Golubenco","category":"Brewing in Beats","publish_date":"2015-10-20T00:00:00.000Z","url":"/blog/this-week-in-beats-first-filebeat-release","seo_title":"","content":" Welcome to ! With this series, we're keeping you up to date with all that's new in , from the details of work in progress pull requests to releases and learning resources.Here is what we worked on over the past week. Preparing for the first Filebeat releaseWe’re very close to releasing Filebeat (the Logstash-Forwarder successor) and we are excited to share it with the rest of the world. ","locales":"","title":"Brewing in Beats: Preparing the first Filebeat release"}
{"index":{}}
{"author":"Pablo Saiz","category":"User Stories","publish_date":"2015-10-21T00:00:00.000Z","url":"/blog/grid-monitoring-at-cern-with-elastic","seo_title":"","content":" Mission & BackgroundThe mission of the WLCG project is to provide global computing resources to store, distribute and analyze the ~30 petabytes of data annually generated by the Large Hadron Collider (LHC) at CERN on the Franco-Swiss border. The accomplishment of this mission currently requires more than 300 petabytes of disk and 200 petabytes of tape. The WLCG Grid Monitoring team at CERN is in charge of providing tools and services that allow the monitoring and understanding of the complex WLCG infrastructure. Without this understanding, an efficient use of the system would be impossible. The team has developed multiple applications to retrieve, display, and analyze the monitoring data. The different applications are tuned to the expected audience: high-level views for management, detailed views for service administrators and individual users and illustrative views for the general public. Most of these applications are based on web servers on top of relational databases. Recently, the increased volume and complexity of monitoring data required the move to modern, state-of-the-art technologies. The next section describes three monitoring tasks which we use as pilot applications for our Elasticsearch evaluation. They cover various areas of the computing activities on the WLCG infrastructure: data access and distribution, data processing, and services health and performance. There are dedicated dashboards for each of these areas. The WLCG Data Collection ","locales":"de-de,fr-fr","title":"Grid Monitoring at CERN with Elastic"}
{"index":{}}
{"author":"David Pilato","category":"","publish_date":"2015-10-20T00:00:00.000Z","url":"/blog/to-shade-or-not-to-shade","seo_title":"To shade or not to shade","content":" Before version 2.0, Elasticsearch was provided as a JAR with some (but not all) common dependencies shaded and packaged within the same artifact. This helped Java users who embed Elasticsearch in their own applications to avoid version conflicts of modules like Guava, Joda, Jackson, etc. Of course, there was still a list of other unshaded dependencies like Lucene that could still cause conflicts. Unfortunately, shading is a complex and error prone process which for some people while for others. Shading makes it very difficult for developers and plugin authors to write and debug code properly because packages are renamed during the build. Finally, we used to test Elasticsearch unshaded then ship the shaded jar, and we don’t like to ship anything that we aren’t testing. We have decided to ship Elasticsearch without shading from 2.0 onwards. Dealing with version conflictsIf you want to use Elasticsearch in your Java application, you may have to deal with version conflicts with third party dependencies like Guava and Joda.  For instance, perhaps Elasticsearch uses Joda 2.8, while your code uses Joda 2.1.  You have two choices: The simplest solution is to upgrade. Newer module versions are likely to have fixed old bugs. The further behind you fall, the harder it will be to upgrade later. Of course, it is possible that you are using a third party dependency that in turn depends on an outdated version of a package, which prevents you from upgrading.   The second option is to relocate the troublesome dependencies and to shade them either with your own application or with Elasticsearch and any plugins needed by the Elasticsearch client. How to shade ElasticsearchTo help you achieve this, we have put together an example application which allows you to run: Your project’s probably looks something like this: The problem with the above is the Joda dependency: your project requires Joda 2.1, but Elasticsearch 2.0.0-beta2 requires Joda 2.8. Shading ElasticsearchTo resolve this situation, we create a new maven project that shades Elasticsearch and Shield.  The should look like this: Shade and relocate conflicting packagesNow shade and relocate all the packages which conflict with your own application by adding something like the below to the .  This example adds Joda 2.8: Running will create a shaded version of Elasticsearch, Shield, and Joda 2.8 which you can depend on in your application. Embed this jar within your applicationIn your project, you can now depend on: You can build and use the Elasticsearch TransportClient as before: To use your own version of Joda, just import . You can even access the shaded version of Joda by importing , although we don’t recommend doing so. Here’s an example showing how both versions can be accessed within the same JVM: It will print: Take a look at this which shows a full running example of the above. Future developmentsDealing with version conflicts is a perennial problem, but in the future we would like to make it easier than it is today. The simplest way to reduce conflicts between third party dependencies is to reduce the number of dependencies that Elasticsearch has.  We will never be able to remove all dependencies, but we’re making a start by in favour of native Java 8 code. We will also be investigating replacing (also in Java 8), and possibly even creating a thin Java client for Elasticsearch with minimal dependencies. Of course, these changes are not small.  Whatever we end up deciding is the right path to follow in the long term, it will take time for us to reach the destination.  In the meantime, we wanted to provide a relatively simple solution for today’s problems. ","locales":"","title":"To shade or not to shade"}
{"index":{}}
{"author":"Chris Hart","category":"","publish_date":"2015-10-22T00:00:00.000Z","url":"/blog/make-an-elasticsearch-powered-rest-api-for-any-data-with-ramses","seo_title":"","content":" In this short guide, I'm going to show you how to download a dataset and create a REST API. This new API uses Elasticsearch to power the endpoints, so you can build a product around your data without having to directly expose Elasticsearch in production. This allows for proper authentication, authorization, custom logic, other databases, and even auto-generated client libraries. is a bit like for open source. It provides the convenience of a \"backend as a service\", except that you run the server yourself and have full access to the internals. I recently came across the Gender Inequality Index, an interesting dataset published by the UN Development Programme. This dataset is a twist on the classic Human Development Index. The HDI ranks countries based on their levels of lifespan, education and income. The GII, on the other hand, ranks countries based on how they stack up in terms of gender (in)equality. The metrics in the GII are a combination of women's reproductive health, social empowerment, and labour force participation. Unfortunately, this dataset is missing non-binary gender identities, so our exploration will be a bit limited until that information is added. You can from the United Nations Development Programme. This dataset enables us to dig into some really interesting questions. Let's make a REST API out of the GII and then query it in fun ways using the Elasticsearch query DSL via the endpoint URL. You can jump ahead to see . Set up the projectBefore we dive in, make sure these pieces are in place: $ mkdir gii_project && cd gii_project $ virtualenv venv $ source venv/bin/activate (venv)$ pip install ramses==0.5.0 (venv)$ pcreate -s ramses_starter gii_api When prompted by , choose PostgreSQL (option 1) as your database and open the new project in a text editor to look around. Then, start the server to make sure it works. It should look something like this: (venv)$ cd gii_api (venv)$ pserve local.ini ... Starting server in PID 40098. serving on http://0.0.0.0:6543 Model and post the dataThere are two main files in the boilerplate project right now: , and . is a file, which is a DSL for describing REST APIs in YAML. It configures your endpoints. is the schema that describes the fake boilerplate \"Item\" model. We are going to replace these files with real ones based on the GII data. First, download the data to the root of the project (), and rename the old to . $ wget https://raw.githubusercontent.com/chrstphrhrt/ramses-elasticsearch/master/gii_api/gii_data.json ... 2015-08-31 15:58:34 (198 KB/s) - 'gii_data.json' saved [75659] $ mv items.json gii_schema.json : you can also get the data directly from the . I cleaned it up a little for this guide. Now edit the file to describe the fields we see in the raw data. Look in for the field names and types. Here's the first record for example: { \"labour_force_participation_rate_aged_15_and_above_male_2012\" : \"69.5\", \"hdi_rank\" : \"1\", \"gender_inequality_index_value_2013\" : \"0.068\", \"gender_inequality_index_rank_2013\" : \"9\", \"population_with_some_secondary_ed_aged_25_and_up_fem_2005_2012\" : \"97.4\", \"country\" : \"Norway\", \"_2010_2015_adolescent_birth_rate_births_per_1_000_women_aged_15_19\" : \"7.8\", \"labour_force_participation_rate_aged_15_and_above_female_2012\" : \"61.5\", \"_2013_share_of_seats_in_parliament_held_by_women\" : \"39.6\", \"population_with_some_secondary_ed_aged_25_and_up_male_2005_2012\" : \"96.7\", \"_2010_maternal_mortality_ratio_deaths_per_100_000_live_births\" : \"7\" } Now look in : { \"type\": \"object\", \"title\": \"Item schema\", \"$schema\": \"http://json-schema.org/draft-04/schema\", \"required\": [\"id\", \"name\"], \"properties\": { \"id\": { \"type\": [\"integer\", \"null\"], \"_db_settings\": { \"type\": \"id_field\", \"required\": true, \"primary_key\": true } }, \"name\": { \"type\": \"string\", \"_db_settings\": { \"type\": \"string\", \"required\": true } }, \"description\": { \"type\": [\"string\", \"null\"], \"_db_settings\": { \"type\": \"text\" } } } } W","locales":"","title":"Make an Elasticsearch-powered REST API for any data with Ramses"}
{"index":{}}
{"author":"Tudor Golubenco","category":"","publish_date":"2015-10-22T00:00:00.000Z","url":"/blog/beats-beta4-filebeat-lightweight-log-forwarding","seo_title":"","content":" We just released Beats version 1.0.0-beta4. It contains improvements for and but also something new: the first version of Filebeat, our new open source log forwarder that ships to Logstash or Elasticsearch. The goal of Filebeat is to tail logs and to ship them off servers to a central location for further processing. It is very lightweight in terms of consumed resources and has no dependencies or plugins to manage. Based on the Logstash Forwarder code baseWhile Filebeat is a new project, significant parts of its code base are based on the project, which has been used in production by many companies for years. Because Logstash Forwarder and Logstash don’t share any code and they are written in different programming languages (Go and Ruby), Logstash Forwarder was unfortunately neglected for too long by the dev community around Logstash and tended to lag behind in terms of improvements and bug fixes. A few community forks have been created, for example the and this by Etsy. By making Logstash Forwarder a project, which has a dedicated team of Go developers inside Elastic, we make sure the Elastic stack has a well maintained log forwarder. Over the past couple of months, we took the Logstash Forwarder code, split it into parts, replaced the more rusty bits, added tons of unit, integration and system tests and then put it back together in a Beat. Filebeat now shares a lot of code with our other Beats via , making the projects benefit from each other’s improvements. The logs are the queueOne of the core features of the Logstash Forwarder, which we have inherited in Filebeat is that after sending the log lines to Logstash it waits for a confirmation that the message was received. Only after this ack is received the persistent registry where it remembers how far it advanced in each file. This means that if there is a network partition between Filebeat and Logstash and the connection is temporarily lost, Filebeat simply waits for the connection to be re-established before advancing its pointer in the log files. Unless the connection cannot be established for a very long time and the logs are rotated, no log lines are lost. Similarly, if you restart Logstash, no log lines are lost. If you restart Filebeat, no log lines are lost. If there’s network congestion on the path, no log lines are lost. You get the idea. Note that there is also drawback to this “at-least-once” approach. Because we retransmit unacknowledged messages, log lines might be duplicated. For most people duplicate log messages are better than lost log messages, but we’ve been working on reducing this effect. I'm a lumberjack and I'm ok The protocol used between the Logstash Forwarder and Logstash already supports encryption, message batching and acknowledgements. The current version of Filebeat uses the same protocol but with a few improvements. Notably, for minimizing congestion situations that can result in duplicates, a slow start mechanism is used for the batch size, similar with the approach taken by TCP to avoid congestion. Partial ACKs were also introduced to help in this area. To allow for a simple migration path from the Logstash Forwarder, we have created a new input plugin for Logstash, called , and we made sure it can be used in parallel with the existing input plugin used by the Forwarder. Another change is making the certificate-based authentication optional so that it’s easier to use Filebeat while testing or developing. Yes, Filebeat works on WindowsI’m glad you asked. Just like Packetbeat and Topbeat, the other Beats projects that we have so far, Filebeat supports Windows. The Logstash Forwarder already worked on Windows but there were a few issues that severely crippled its functionality on that platform. We fixed those and now we run the same tests that we have for Linux and OS X on Windows as well, so you don’t have to worry about differences in behaviour when you have a mixe","locales":"","title":"Beats 1.0.0-beta4: lightweight log forwarding with Filebeat"}
{"index":{}}
{"author":"Suyog Rao","category":"","publish_date":"2015-10-20T00:00:00.000Z","url":"/blog/logstash-lines-2015-10-19","seo_title":"","content":" Welcome back to ! In these weekly posts, we'll share the latest happenings in the world of Logstash and its ecosystem. Road to Logstash 2.0This week, we shipped and versions for Logstash 2.0.0. These releases include few breaking changes, so its recommended you read the blogs and  for details. Many of our users were expecting a 1.6 release which was originally slated after version 1.5 -- since we had to introduce breaking changes, we decided to call our next release 2.0 (a major version). Our team is still focused to deliver on the themes mentioned in the -- resiliency, manageability and performance. We will target new enhancements like persistent queues as minor versioned releases (like 2.1, 2.2). We explain in detail in our previous .Beta2 also adds TLS 1.2 support (default), faster JSON serialization/deserialization, and better out of the box defaults. For example, the filter worker settings now default to half the number of cores available, allowing you to do more heavy lifting in your filters. Road to Filebeats We are working closely with the Beats team to help launch Filebeat, a successor for Logstash Forwarder. We created a new input called which will interact with the beats agent and receive messages over the Lumberjack protocol. ","locales":"","title":"Logstash Lines: Preparing for the 2.0 release"}
{"index":{}}
{"author":"Gaspard Du Jeu","category":"","publish_date":"2015-10-20T00:00:00.000Z","url":"/blog/docteur-souris-and-elastic-help-hospitalized-kids-in-france","seo_title":"Docteur Souris and Elastic Help Hospitalized Kids in France","content":" \"We are proud to support the cause fought by Association Docteur Souris that makes the lives of hospitalized children a little less painful.\" -  Docteur Souris will have a booth at the event to educate attendees about its activity, the various actions it is taking in more than 30 hospitals all around France with more than 1100 computers operating every day. The implementation, the follow-up, and the devices’ maintenance are free for the hospitals and free for the children and teenagers to use. Docteur Souris’ concept has been developed with a long-term solution of support, which will ensure the maintenance of the various measures and devices set up in health care facilities. With such a system, the computers can be long-distance controlled, easing their maintenance and reducing deadlines and costs. Therefore, thanks to the support of Elastic, more than 1200 children and teenagers will be able to connect themselves to the internet safely with a secured PaloAlto Network system. They will access Docteur Souris’ portal, rich in educational and entertaining content. “We feel honoured to partner with Elastic and thank the managing team to have chosen Docteur Souris for Elastic{ON} Tour Paris. Thanks to this partnership, Docteur Souris will continue to help more hospitalized children all over France to break their isolation.\"  If you need further information or wish to register for the Elastic{ON} Tour event in Paris, click . Currently Paris is sold out, but sign up on the waiting list and Elastic will contact you if a spot opens up. [if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves></w:TrackMoves> <w:TrackFormatting></w:TrackFormatting> <w:PunctuationKerning></w:PunctuationKerning> <w:ValidateAgainstSchemas></w:ValidateAgainstSchemas> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF></w:DoNotPromoteQF> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>JA</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables></w:BreakWrappedTables> <w:SnapToGridInCell></w:SnapToGridInCell> <w:WrapTextWithPunct></w:WrapTextWithPunct> <w:UseAsianBreakRules></w:UseAsianBreakRules> <w:DontGrowAutofit></w:DontGrowAutofit> <w:SplitPgBreakAndParaMark></w:SplitPgBreakAndParaMark> <w:EnableOpenTypeKerning></w:EnableOpenTypeKerning> <w:DontFlipMirrorIndents></w:DontFlipMirrorIndents> <w:OverrideTableStyleHps></w:OverrideTableStyleHps> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\"></m:mathFont> <m:brkBin m:val=\"before\"></m:brkBin> <m:brkBinSub m:val=\"&#45: -\"></m:brkBinSub> <m:smallFrac m:val=\"off\"></m:smallFrac> <m:dispDef></m:dispDef> <m:lMargin m:val=\"0\"></m:lMargin> <m:rMargin m:val=\"0\"></m:rMargin> <m:defJc m:val=\"centerGroup\"></m:defJc> <m:wrapIndent m:val=\"1440\"></m:wrapIndent> <m:intLim m:val=\"subSup\"></m:intLim> <m:naryLim m:val=\"undOvr\"></m:naryLim> </m:mathPr></w:WordDocument> </xml><![endif][if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"276\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name","locales":"","title":"Docteur Souris and Elastic Help Hospitalized Kids in France"}
{"index":{}}
{"author":"Michael McCandless","category":"","publish_date":"2015-10-19T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2015-10-19","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top NewsJust released new clients - 2.0.0 for upcoming Elasticsearch 2.0 and 1.8.0 for 1.x versions - — Honza Král (@HonzaKral) Elasticsearch CoreLast week we  which contained some good bug fixes for the Tribe node, synced flushing, and snapshot restore. Changes in 2.0: Changes in 2.x/3.x: In progress: Apache Lucene Watch This Space, where we'll share more news on the whole Elastic ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - October 19th 2015"}
{"index":{}}
{"author":"Suyog Rao","category":"Releases","publish_date":"2015-10-19T00:00:00.000Z","url":"/blog/logstash-2-0-0-beta3-released","seo_title":"","content":" Quick update to announce release for Logstash 2.0.0. Since we released last week, we found two important defects which are resolved in this release. This is a beta release and is intended for testing purposes only. There is no guarantee that Logstash 2.0.0-beta3 will be compatible with Logstash 2.0.0 GA. Bug Fixes Feedback Many thanks to users who tried beta2 and provided us feedback! As always, your input is very valuable. Head to our page and give it a try! If you find issues you can start a topic in our or create an . ","locales":"","title":"Logstash 2.0.0 beta3 released"}
{"index":{}}
{"author":"Haley Eshagh","category":"Culture","publish_date":"2015-10-15T00:00:00.000Z","url":"/blog/the-2015-elastic-blackout","seo_title":"","content":" There was a blackout yesterday, but you probably didn't notice. On October 14, 2015, darkness of the best kind consumed Elastic offices all around the world. What started out as a company onboarding activity for the latest batch of employees to join our Elastic cluster turned into a worldwide challenge: Take an awesome photo wearing Elastic gear around town. Colleagues represented Elastic during an afternoon MRI scan in Ohio, strolling by Big Ben in London, and saying \"hiya\" from Amsterdam, Singapore, New York, Austin, Seattle, Stockholm, Japan, France, Boston, Mountain View, and beyond. We thought we'd share some of the pictures with you. With over 80 people joining Elastic since August representing more than 15 countries, there were a lot of great photos to choose from.  And why black shirts? Do a quick image search of \"Shay Banon.\" (Yeah, there are a few outliers, but you get the idea.) Plus, they're standard issue at the company after our first . The lights are back on and there's more Elastic goodness in the works. Stay tuned. (And if you'd like a black shirt of your own, .) ","locales":"","title":"The 2015 Elastic Blackout"}
{"index":{}}
{"author":"Gaspard Du Jeu","category":"","publish_date":"2015-10-20T00:00:00.000Z","url":"/blog/docteur-souris-et-elastic-aident-les-enfants-hospitalises-en-france","seo_title":"Docteur Souris et Elastic aident les enfants hospitalisés en France","content":" A l’occasion du passage de Elastic{ON} Tour le jeudi 5 Novembre à Paris, la société Elastic, spécialisée dans la création de logiciels d’analyse de données, s’engage à reverser l’ensemble des frais d’inscriptions de son séminaire parisien à pour permettre aux enfants et adolescents hospitalisés  de se divertir, communiquer avec leurs proches et éviter toute rupture scolaire. L’Association Docteur Souris sera présente sur l’événement pour présenter les différentes actions mises en place dans plus de 30 hôpitaux en France avec plus de 1100 ordinateurs disponible chaque jour. La mise en œuvre, le suivi et la maintenance des dispositifs est gratuit pour les hôpitaux et l’usage gratuit pour les enfants. Pour en savoir plus sur le séminaire Elastic{ON} Tour à Paris et pour vous inscrire, cliquez . Nous sommes complets mais vous pourriez vous ajouter sur notre liste d'attente et nous vous laisserons savoir quand il y a des places disponible. [if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves></w:TrackMoves> <w:TrackFormatting></w:TrackFormatting> <w:PunctuationKerning></w:PunctuationKerning> <w:ValidateAgainstSchemas></w:ValidateAgainstSchemas> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF></w:DoNotPromoteQF> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>JA</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables></w:BreakWrappedTables> <w:SnapToGridInCell></w:SnapToGridInCell> <w:WrapTextWithPunct></w:WrapTextWithPunct> <w:UseAsianBreakRules></w:UseAsianBreakRules> <w:DontGrowAutofit></w:DontGrowAutofit> <w:SplitPgBreakAndParaMark></w:SplitPgBreakAndParaMark> <w:EnableOpenTypeKerning></w:EnableOpenTypeKerning> <w:DontFlipMirrorIndents></w:DontFlipMirrorIndents> <w:OverrideTableStyleHps></w:OverrideTableStyleHps> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\"></m:mathFont> <m:brkBin m:val=\"before\"></m:brkBin> <m:brkBinSub m:val=\"&#45: -\"></m:brkBinSub> <m:smallFrac m:val=\"off\"></m:smallFrac> <m:dispDef></m:dispDef> <m:lMargin m:val=\"0\"></m:lMargin> <m:rMargin m:val=\"0\"></m:rMargin> <m:defJc m:val=\"centerGroup\"></m:defJc> <m:wrapIndent m:val=\"1440\"></m:wrapIndent> <m:intLim m:val=\"subSup\"></m:intLim> <m:naryLim m:val=\"undOvr\"></m:naryLim> </m:mathPr></w:WordDocument> </xml><![endif][if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"276\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\"></w:L","locales":"","title":"Docteur Souris et Elastic aident les enfants hospitalisés en France"}
{"index":{}}
{"author":"Andrew Evans","category":"","publish_date":"2015-10-19T00:00:00.000Z","url":"/blog/hired-taps-elasticsearch-as-a-service-for-job-marketplace","seo_title":"Hired Taps into Elasticsearch as a Service for Job Marketplace","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud. ","locales":"","title":"Hired Taps into Elasticsearch as a Service for Job Marketplace"}
{"index":{}}
{"author":"Russ Savage","category":"Engineering","publish_date":"2015-10-14T00:00:00.000Z","url":"/blog/introducing-google-sheets-to-elasticsearch-add-on","seo_title":"Easily Visualize Spreadsheet Data with Elasticsearch & Kibana","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as . Also note that the add-on does not automatically work with versions 5.x and above. To benefit from 5.x using this add-on, you must index the data into a 2.4.4 cluster and then upgrade. Our latest Google Sheets add-on makes visualizing your spreadsheet data a snap. Today, we're rolling out a new that makes it easy to do just that. Using this add-on, along with , you can push data from a spreadsheet to your Elasticsearch cluster and get up and running with Kibana visualizations in just a few minutes. Setting Up a ClusterIf you already have a cluster you want to push data into, you can skip to the next section about pushing data. If not, we recommend you take a moment to before continuing. With just a few clicks, you will have a secure cluster up and running, including Kibana 4 for visualizations. Watch this short demonstration video on how to spin up a free hosted Elasticsearch cluster. After completing the sign up and configuration for Found, you should see a couple of endpoints displayed that look like this: http://<some-long-id>.us-east-1.aws.found.io:9200 https://<some-long-id>.us-east-1.aws.found.io:9243 We will use those later to send data to your cluster. Pushing Data to Your ClusterOur cluster is ready to go, so now it's time to open up your Google Sheet and get started. The video below walks you through the steps in detail. Helpful TipsFor those of you interested in contributing or understanding how this works, feel free to check out the code hosted on the . Here are a few helpful tips in case you get stuck: Visualizing it with KibanaAfter you have your data indexed into Elasticsearch, the easiest way to visualize it is with Kibana. If you are using Found, the URL for Kibana 4 can be found under the \"Configuration\" section of your cluster. You can browse the or check out this helpful . ","locales":"","title":"Easily Visualize Spreadsheet Data with Elasticsearch & Kibana"}
{"index":{}}
{"author":"Robin Clarke","category":"","publish_date":"2015-10-08T00:00:00.000Z","url":"/blog/logstash-configuration-tuning","seo_title":"Logstash configuration tuning","content":" Logstash is a powerful beast and when it’s firing on all cylinders to crunch data, it can use a lot of resources. The goal of this blog post is to provide a methodology to optimise your configuration and allow Logstash to get the most out of your hardware. ","locales":"","title":"Logstash configuration tuning"}
{"index":{}}
{"author":"Michael McCandless","category":"","publish_date":"2015-10-13T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2015-10-13","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top NewsElasticsearch 2.0.0-rc1 released! Not long until GA...— elastic (@elastic) Elasticsearch Core Apache LuceneWatch This Space, where we'll share more news on the whole ELK ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - October 13th 2015"}
{"index":{}}
{"author":"Shay Banon","category":"News","publish_date":"2015-10-07T00:00:00.000Z","url":"/blog/found-elasticsearch-as-a-service-with-alerts","seo_title":"","content":" I’m excited to announce that today, , our alerting and notifications plugin, is now available in , our hosted Elasticsearch product. This is super exciting for us as Watcher represents the second commercial product to be integrated into our hosted Elastic offering following this summer’s release of , our security plugin. In addition, for those of you in Asia, Found is in Sydney too, in addition to, being available in Tokyo and Singapore.Earlier this summer, I wrote a post on Found, our strategy, and how taking an open source product and turning it into a world-class service is more than just “hosting”. As the creators of the open source products -- Elasticsearch, Logstash, and Kibana -- we’ve worked hard to ensure Found leverages our 5 years of history of developing and supporting them to give you: Thanks to our teams working closely together, we can take the innovation we are doing in our open source and commercial products and easily extend it to Found, and provide massive benefits to our customers. For example: We’re also super happy that Found is enabling our customers to focus on running their core businesses, and that companies like Docker, HotelTonight, Instacart, and several Fortune 500 companies, use Found to run their mission critical applications.Of course our customers are asking us for more.Our philosophy, which ties heavily to our open source roots, is to give freedom to our users while using our products. Today, Found runs on Amazon AWS, but we are working on allowing our users to use Found as a standalone product that can be installed on premise and later be offered across multiple cloud providers. This is extremely exciting to me, and points to the adoption and popularity of what we have built. It is also something I hear users, who juggle multiple clusters, asking for all the time. ","locales":"de-de,fr-fr,ko-kr","title":"Found: Elasticsearch-as-a-Service, now with Alerts"}
{"index":{}}
{"author":"Megan Wieling","category":"","publish_date":"2015-10-19T00:00:00.000Z","url":"/blog/witwies-cfcamp","seo_title":"Where in the World is Elastic? - CFCamp","content":" Welcome to Find out what is happening near you this week in Elastic events and meetup land! Upcoming Events October 22 - 23: Upcoming Meetups October 22: October 22: October 23: October 20:October 21: October 22: October 22: October 22: October 22: October 22: October 21: October 25: That's it for this week. Stay tuned for Elastic happenings next week! - The Elastic Team P.S. if you're interested in hosting a meetup or are giving a talk about Beats, Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic? - CFCamp"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2015-10-15T00:00:00.000Z","url":"/blog/elasticsearch-1-7-3-released","seo_title":"Elasticsearch 1.7.3 released","content":" Today, we are happy to announce the bug fix release of , based on . This is the latest stable release. Users are advised to upgrade if they find themselves affected by any of the bugs which have been fixed.You can .Previous blog posts about the 1.7 series:This release contains a number of bug fixes including:Please , try it out, and let us know what you think on Twitter () or on our . You can report any problems on the . ","locales":"","title":"Elasticsearch 1.7.3 released"}
{"index":{}}
{"author":"","category":"Releases","publish_date":"2015-10-14T00:00:00.000Z","url":"/blog/logstash-2-0-0-beta2-released","seo_title":"","content":" We are pleased to announce the beta2 release for Logstash 2.0.0. Please note that there are some breaking changes in this release and we encourage you to read the entire blog. This is a beta release and is intended for testing purposes only. There is no guarantee that Logstash 2.0.0-beta2 will be compatible with Logstash 2.0.0 GA. Breaking Changes in 2.0.0.beta2 Releasing a major version of Logstash creates the opportunity to remove features and configuration options that have been deprecated in previous releases. The beta2 release changes the default value of the setting which is used to control the number of filter workers. This release also removes many plugin configuration options that are now obsolete. They are detailed below. Obsolete Configuration Options Until the introduction of conditionals, the only way to selectively apply filters and outputs to some events was to set the , and options. These configuration options have been deprecated for some time now and this release either removes them, or they are marked with the new tag. You will not be able to use them in your configuration file without a resulting error. Feedback when obsolete config options are used To make the removal of settings less aggressive to the user, an option to tag a plugin configuration setting as obsolete has been introduced. After a feature has been deprecated for some time, it will then be marked as obsolete. If an obsolete setting is used, a pre-configured message is presented to the user informing them of the obsolete status of the option, then Logstash terminates immediately. How does this look in real life? Imagine that a plugin has an option called and we wish to remove it. Here is a potential timeline of plugin releases: => plugin is shipped, includes the feature and option => minor version release, deprecates => major version, removes the code related to and marks it as => minor version, removes As you can see, the tag softens the user experience for a removed feature, making the user aware that the feature is gone and presenting the recommended alternative! A Better Shutdown Strategy This release improves shutdown handling in Logstash and its plugins. Up to and including 1.5.x, when a pipeline shutdown is initiated, either by SIGTERM or SIGINT, the following events occur inside Logstash: The way input plugins terminate in step 3 is problematic, since raising exceptions on the input plugin threads from the outside is unpredictable. Calling a means that any execution happening in that thread must deal with the exception or terminate. In the context of input plugins, the exception can happen during the execution of code from third party libraries that many input plugins use. Being unable to predict how the code behaves leads to undesirable outcomes. A plugin may exit normally, exit abruptly and lose buffered data, get stuck in an inconsistent state, or some other unknown behavior. The solution To make the input plugin shutdown more reliable, a strategy was proposed () to avoid and instead signal the plugin—in a thread-safe way—that a shutdown sequence has been started. Doing this delegates the responsibility of deciding how and when to shut down within each input plugin. This proposal was at the core level, such that now all input plugins have three methods for shutdown purposes: , and . The way to know a plugin is stopped is simply by waiting for the plugin run method to return. When the run method exits and the plugin thread exits, only then will the close method be called, and that only once. API Cleanup Making this change also created an opportunity to review the rest of the shutdown API between the plugins and the pipeline, and several methods in the Plugin Base class were found to be unnecessary and thus could be removed: , , , and . The method is renamed to and retains the responsibility of post-termination bookkeeping, as described above. Plugin Developers This change of shutdown strategy implies that all input plugins must ","locales":"","title":"Logstash 2.0.0 beta2 released"}
{"index":{}}
{"author":"Megan Wieling","category":"","publish_date":"2015-10-12T00:00:00.000Z","url":"/blog/witwies-bordeaux-developer-experience-euruko-2015","seo_title":"Where in the World is Elastic? - DevRelCon 2015","content":" Welcome to Curious to find out which Elastic events and meetups are happening near you this week? Check them out now!Upcoming Events October 16: October 17 - 18:  Upcoming MeetupsOctober 12: October 13: October 13: October 14: October 13: October 14: October 15: October 17: That's it for this week. Stay tuned for Elastic happenings next week! - The Elastic Team P.S. if you're interested in hosting a meetup or are giving a talk about Beats, Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic? - Bordeaux Developer eXperience, EuRuKo 2015"}
{"index":{}}
{"author":"Michael McCandless","category":"","publish_date":"2015-10-06T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2015-10-06","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top NewsStretching Horizons with . Read on to find out how they do it: . — elastic (@elastic) Elasticsearch Core Apache Lucene Watch This Space, where we'll share more news on the whole ELK ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - October 6th 2015"}
{"index":{}}
{"author":"Asawari Samant","category":"","publish_date":"2015-10-05T00:00:00.000Z","url":"/blog/hacking-education-with-the-elastic-stack","seo_title":"Hacking Education with the Elastic Stack","content":" Today, October 5th, is , and we wanted to celebrate it in our own geeky, Elastic way. We figured why not do something fun and interesting with a dataset related to education using Elasticsearch, Logstash, and Kibana to mark this special day and say thank you to teachers around the world. The DonorsChoose.org dataset immediately popped to mind. is an online charity founded in 2000 with the goal of directly connecting do-gooders that care about education with public schools and classrooms in need of resources. As it happens, the great folks at DonorsChoose.org also extend their “mission to connect” to their datasets. As a part of their initiative, they have provided about a decade’s worth of donations, resources, and projects data, as well as data APIs to data crunchers around the world for “hacking education” as they put it.So, we downloaded the datasets and decided to have fun with them. In this blog, we will share some of our insights. But, be sure to check out the code from our 'examples'  to join in on the fun and find your own insights. The Big PictureWe decided to start exploring the big picture, and the numbers are impressive. Between 2000 and 2014, DonorsChoose.org helped connect around donors to . This raised close to from that funded . Exponential Growth Absolute numbers are great, but we love dynamic trends too. We wondered how things had evolved since the early days of DonorsChoose.org. If you thought the above absolute numbers were impressive, the graph below showing the trend over time is equally stellar. You can see a steady growth in total monthly donations (in $) over time. Oh, and by the way, the Y-axis in the chart below is in log scale! Interestingly, we noticed that while the total monthly donation amount has steadily increased, the average donation value (indicated by the size of the dot above) has actually decreased – this is not surprising and can be expected as the initiative got wider adoption and more donors joined in. It is also perhaps influenced by the lack of a minimum donation policy, as DonorsChoose.org has adopted  a “no amount is too small” philosophy. And that brings us to our next insight: Every dollar matters!Every dollar matters!A quick look at the amount raised shows that even small donations (< $10) have a large impact over time, and have raised close to $1.7 million. The subsequent donation bracket from $10 - $100 netted out a whopping $60 million.This made us wonder how the typical donation amount varied by the donor’s location. So, we looked at the fraction of the donations in various dollar amount brackets by donor’s state. The typical distribution seems pretty consistent across most states. But, some states do stand out. For example, a greater fraction of donations from Oklahoma and Arkansas are in the > $500 bracket. By contrast, a larger percentage of donations received from Idaho and Alabama are in the sub $10 bracket.Looking at where the donations were coming from prompted our next question: Where are the donations going to and which areas received the most aid? Where are the donations going?A quick geomap immediately highlights that most of the donations are going towards schools in big metropolitan areas: New York, Los Angeles, San Francisco, Philadelphia, Washington, D.C., Chicago, Indianapolis, etc. There’s a strong correlation between number of projects  initiated in a city and the received aid.  But wait, there's more!As you can tell, we could go on and on with our insights. But, we would love for you to join in on the fun. Grab the code from our examples and mine your own insights. We have even included a to help you get started. The dashboards provide a peek into many other interesting facets of the data, like how donations have varied over time by different facets of the projects. We explored factors such as the project primary focus areas (Literature, Math, Science, et","locales":"","title":"Hacking Education with the Elastic Stack"}
{"index":{}}
{"author":"Bill Boebel","category":"","publish_date":"2015-10-14T00:00:00.000Z","url":"/blog/pingboard-employee-directory-app-powered-by-elasticsearch","seo_title":"","content":" When your company is small, information is readily accessible. As your company grows, though, people become disconnected and simple things become hard, such as remembering names and faces, knowing who does what, and simply finding someone’s phone number. was started when we noticed that many companies were building their own internal employee directories to solve this problem. When we dug in, we found that companies were essentially building the same thing because they couldn’t find an employee directory app that met their needs. These projects feel temporary from the start and typically fail because they lack an owner to push them forward and aren’t integrated with other employee systems, meaning the data is soon out of date.Pingboard was born to solve this problem. Every company needs place for employees to find information about the people they work with -- from photos, org chart and contact info, to skills, interests, and favorite coffee shop. People expect great searchOur first implementation of search in our web, iPhone, and Android apps was entirely client-side. It was fast and allowed you to search for coworkers by name or job title. Right away we heard from our customers that they wanted to search for additional employee attributes, including skill, team, and email address. Some companies found creative workarounds. One of our customers, Adecco, stored their business unit, division, region, and branch name in the nickname field so they would show up in search. People expect everything in the tools they use to be searchable instantly from a single search box. Great search used to be a differentiator, but now it’s essential. We knew we had to quickly move the search index to the server using a platform like Solr or Elasticsearch, in order to give our customers what they expect.Elasticsearch is a great fit for us because it's flexible, powerful, and fast. The Pingboard data model includes many different types of information, and we knew we needed a tool which would let us customize the way we search different fields. Finding a person by name requires a different indexing strategy than finding a group of people with the same skill, or looking up an email address. In addition, the data model can be extended by customers, and we wanted custom fields to be searchable as well.  let us index these fields based on their data type without having to know ahead of time what fields exist. Elasticsearch also makes it easy for us to adjust how we rank the search results by using . Designing a great search experienceWe talked with customers to determine the most important fields to make searchable and settled on: name, nickname, job title, skills, interests, team, email and phone number. We also threw in birthday month and employment start year to make it easy to answer questions like “Who’s birthday is coming up?” or “Who joined the company this year?”Our key design decisions were: Customers loved it, but still wanted moreUpgrading search was a big deal, so we announced it to all of our users via an in-app message. We got a lot of praise for the update, but we also started hearing things like: “Love it! When will this be available in your Android app?”“Can we also search for what project people are working on, which we store in a custom field?”“It would be cool if our employees could search the fun employee fields we added, including college and favorite drink.” Our work wasn’t done. We celebrated briefly and then got back to work. Searching everything, from anywhereIn Pingboard, you can add custom fields to store any type of employee information using text fields, number fields, lists, tags, links to other people, and more. Today, everything is searchable. In fact, companies can even choose which fields to include in their search index.Of course, searching the employee directory from the iPhone and Android employee directory app needs to be just as powerfu","locales":"","title":"Getting to know your coworkers made easier"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2015-10-07T00:00:00.000Z","url":"/blog/elasticsearch-2-0-0-rc1-released","seo_title":"Elasticsearch 2.0.0-rc1 released","content":" Today, we are happy to announce the release of , based on . Elasticsearch 2.0 is feature frozen. This is the last planned release candidate before 2.0.0 GA.: This is a release candidate and is intended for testing purposes only. Elasticsearch 2.0.0-rc1 is not compatible with 2.0.0-beta1 or 2.0.0-beta2, and there is no guarantee that it will be compatible with Elasticsearch 2.0.0 GA.You can .Change logs for the commercial plugins can be found here:The changes in this release candidate consist mostly of minor bug fixes and tidy ups, with the occasional enhancement like the and, in Shield, a caching interface which can be used by custom realms.Thank you to those of you who tested out 2.0.0-beta1 and 2.0.0-beta2 and reported problems. Given that this is the last planned preview release before 2.0.0 GA, we would appreciate as many testers as possible so that we can catch and fix issues before releasing the GA. ","locales":"","title":"Elasticsearch 2.0.0-rc1 released"}
{"index":{}}
{"author":"Megan Wieling","category":"","publish_date":"2015-10-05T00:00:00.000Z","url":"/blog/witwies-linuxcon-puppetconf-2015-aws-reinvent","seo_title":"Where in the World is Elastic? - DevRelCon 2015","content":" Welcome to Check out the Elastic events and meetups happening near you this week!Upcoming Events October 5 - 7: October 6: October 8: October 5 - 9: October 6 - 9: Upcoming MeetupsOctober 7: October 8: October 6: That's it for this week. Stay tuned for Elastic happenings next week! - The Elastic Team P.S. if you're interested in hosting a meetup or are giving a talk about Beats, Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic? - Linuxcon, PuppetConf 2015 and AWS re:Invent"}
{"index":{}}
{"author":"Emmanuel Gueidan","category":"","publish_date":"2015-10-05T00:00:00.000Z","url":"/blog/stretching-horizons-with-elasticsearch","seo_title":"Stretching horizons with Elasticsearch","content":" Emmanuel Gueidan is co-founder and CTO at, a Paris-based startup focused on enabling innovative operations and high performance applications through log management. Emmanuel has over 10 years experience building BI platforms. Dedicated to R&D and driven by innovation, he previously worked for and took over development of several cutting-edge pieces of the core product at Quartet FS, working on one of the first in-memory analytics real-time business intelligence tools. [if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves></w:TrackMoves> <w:TrackFormatting></w:TrackFormatting> <w:PunctuationKerning></w:PunctuationKerning> <w:ValidateAgainstSchemas></w:ValidateAgainstSchemas> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF></w:DoNotPromoteQF> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>JA</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables></w:BreakWrappedTables> <w:SnapToGridInCell></w:SnapToGridInCell> <w:WrapTextWithPunct></w:WrapTextWithPunct> <w:UseAsianBreakRules></w:UseAsianBreakRules> <w:DontGrowAutofit></w:DontGrowAutofit> <w:SplitPgBreakAndParaMark></w:SplitPgBreakAndParaMark> <w:EnableOpenTypeKerning></w:EnableOpenTypeKerning> <w:DontFlipMirrorIndents></w:DontFlipMirrorIndents> <w:OverrideTableStyleHps></w:OverrideTableStyleHps> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\"></m:mathFont> <m:brkBin m:val=\"before\"></m:brkBin> <m:brkBinSub m:val=\"&#45: -\"></m:brkBinSub> <m:smallFrac m:val=\"off\"></m:smallFrac> <m:dispDef></m:dispDef> <m:lMargin m:val=\"0\"></m:lMargin> <m:rMargin m:val=\"0\"></m:rMargin> <m:defJc m:val=\"centerGroup\"></m:defJc> <m:wrapIndent m:val=\"1440\"></m:wrapIndent> <m:intLim m:val=\"subSup\"></m:intLim> <m:naryLim m:val=\"undOvr\"></m:naryLim> </m:mathPr></w:WordDocument> </xml><![endif][if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"276\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\"></w:LsdException> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" ","locales":"","title":"Stretching Horizons with Elasticsearch"}
{"index":{}}
{"author":"Alexander Reelsen","category":"","publish_date":"2015-09-29T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2015-09-29","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top NewsNow OnDemand: @ Scale: Reference Architecture & Common Pitfalls” ft. lessons from & — elastic (@elastic) Elasticsearch Core Apache Lucene Watch This Space, where we'll share more news on the whole ELK ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - September 29 2015"}
{"index":{}}
{"author":"Nicolas Ruflin","category":"Engineering","publish_date":"2015-09-28T00:00:00.000Z","url":"/blog/code-coverage-for-your-golang-system-tests","seo_title":"","content":" To better understand which parts of our Golang applications are covered by test suites, we wanted to have a coverage report that aggregates data from all of our automated tests. However, we couldn't find any information on how to generate coverage reports from system tests of Golang applications. We found a solution using the Golang toolchain and this is a guide of how we did it. The screenshot below from Codecov.io shows an example test coverage report from our product. Having a code coverage report for unit tests is common practice and very well supported by the . The challenge with services which highly depend on the environment, like the operating system or other services, is that unit tests can only cover certain cases. Sometimes writing a simple test that runs the binary and compares the output with the expected output is much easier and faster. In addition, it reproduces what is happening when the binary is executed. Unfortunately in most cases these system tests do not count to the coverage report which also makes it hard to identify which parts of the code were actually run by executing the binary. Our Code Coverage Challenge At , we develop the products using Golang. Packetbeat is an open source project written in Golang that is designed to provide real‑time analytics for web, database, and other network protocols. During the implementation of Packetbeat, we faced the challenge that the most valuable tests were the system tests that executed the binary and checked the expected output. These tests allow us to generate files, feed it through Packetbeat and see if the outcome is as expected. Like this we are able to get files from users with problems and can fully reproduce the problem. In case we fixed a bug, we didn’t have any insights if the specific test covered the code changes except through introducing lots of log messages. Based on this we looked for a simpler solution that allowed us to identify which lines were executed by system tests. Our Test Coverage Reports During the implementation of the test code coverage for the Packetbeat project we faced different challenges which we managed to overcome. Now all our Beats automatically generate a coverage report for unit and system tests. These coverage reports are generated by our continuous integration system for each pull request and are published. An excerpt of one of these reports can be seen below. The report does not only show us which lines were executed and which not, but also the number of times each line was executed. This can be seen by the small number 34 in the screenshot, which means the line above was executed 34 times. The red lines were not executed by the unit or system tests. System Test Coverage Guide The goal of this blog post is to have a guide on how to implement a coverage report for system tests in your own Golang project. If you just want to see it working check out our and repositories. Inside the the command generates all the reports and html outputs. To try it out, one of the repositories and run the command inside the repository. The paragraphs below give a step-by-step description how you can implement a coverage report for system tests in your own Golang project. Step 1: Generating the Binary To run system tests, a compiled binary of the application is required. This binary is then executed in different environments with different configurations. Golang offers a unique way to generate a coverage binary instead of the default binary generated by . The binary for the code coverage which is generated writes a unique counter after every line of code and checks how many times this counter was called after the binary was executed. More technical details on how this works can be found on the . When is executed, this coverage binary is automatically generated again and disposed of afterwards. Golang also allows to generate this coverage binary with the following command: go test -c -covermode","locales":"","title":"Code Coverage for your Golang System Tests"}
{"index":{}}
{"author":"Megan Wieling","category":"","publish_date":"2015-09-28T00:00:00.000Z","url":"/blog/witwies-elasticontour-amsterdam","seo_title":"Where in the World is Elastic? - Elastic{ON}Tour Amsterdam","content":" Welcome to October is coming to an end! Check out which Elastic events and meetups are happening near you this week. Upcoming Events October 26 - 27: October 29: October 29:  October 26: Upcoming Meetups October 26: October 27: October 28: October 29:  October 27: October 28: October 26: Please note that not all the meetups posted here are sponsored or organized by Elastic, but we are all about supporting the community and bringing together those who are talking about Elastic. That's it for this week. Stay tuned next week for more Elastic happenings.- The Elastic Team P.S. if you're interested in hosting a meetup or are giving a talk about Beats, Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic? - Elastic{ON}Tour Amsterdam"}
{"index":{}}
{"author":"Megan Wieling","category":"","publish_date":"2015-09-28T00:00:00.000Z","url":"/blog/witwies-devrelcon-2015-strata-hadoop-world-ny","seo_title":"Where in the World is Elastic? - DevRelCon 2015","content":" Welcome to September is almost coming to an end and October is almost here. Check out the Elastic events and meetups happening near you this week.Upcoming Events September 30:  September 29 - October 1: Upcoming MeetupsSeptember 29: September 30: That's it for this week. Stay tuned for Elastic happenings next week! - The Elastic Team P.S. if you're interested in hosting a meetup or are giving a talk about Beats, Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic? - DevRelCon 2015 and Strata + Hadoop World NY"}
{"index":{}}
{"author":"Jurgen Altziebler","category":"Engineering","publish_date":"2015-09-28T00:00:00.000Z","url":"/blog/building-dashboards-using-data-from-the-federer-&-djokovic-tennis-rivalry","seo_title":"Building Dashboards Using Data from the Federer & Djokovic Tennis Rivalry","content":" As a double nerd who likes data viz and tennis, it wasn't long before I wondered what I would discover if I loaded tennis data into our popular open source products Elasticsearch, Logstash and Kibana. The Federer & Djokovic rivalry is one of the biggest in tennis, with a record breaking 42 matches and counting. As of September 2015, they are dead even at 21-21. Roger Federer won the first match in Monte Carlo in 2006 and he pretty much kept the upper hand until 2010. Djokovic kept improving and in 2011 he had his first blockbuster year, winning 3 out of 4 grand slams. Things were getting tighter and tighter between the two players. Djokovic won the last encounter in the final of the US Open 2015 in New York. Federer won 49.7% of all points played and lost. In tennis, a few points can mean the difference between winning the trophy or taking second place. It doesn’t matter if we visualize serious data for work or play.  Here are some quick tips for your dashboards: Now back to the tennis data! The Federer/Djokovic rivalry really heated up in the last four years, so we’ll take a look at that data. They played 19 times in the last four years, with Djokovic winning 12 out of 19 matches.  What hurts most for Federer fans is that Djokovic won 4 out of 5 grand slam encounters. No doubt that Federer, Djokovic and Nadal will go down as some of the most successful players in history, winning over 40 combined grand slams. This is a new dashboard from the same data set.  These are all of Federer’s matches from the last four years- 310 matches in total. Federer won 83% of all matches played. Although he is in his mid-thirties, his results are still impressive. And for extra kicks, this dashboard shows Roger Federer vs Rafael Nadal in the last four years with the Kibana dark theme. Enjoy your data and tell great stories! ","locales":"","title":"Building Dashboards Using Data from the Federer & Djokovic Tennis Rivalry"}
{"index":{}}
{"author":"Rashid Khan","category":"News","publish_date":"2015-09-24T00:00:00.000Z","url":"/blog/kibana_4.2_beta2","seo_title":"","content":" Kibana 4.2 is almost here. Any day now. Just around the corner. Wait for it…or don’t. Why wait? Who likes waiting? I don’t. Instant gratification awaits within. It can be yours, today. Now. Right now. , and to go with it, right now. You are young and life is long and there is time to kill today, so read on for the “deets” as the kids like to say. At least I think they say that, right?   Kibana now supports WMS map servers, and thus no longer needs internet access for tile map visualizations. That said, you’ll need to supply your own tiles if you’re going that route, but there’s also a plethora of totally open WMS servers out there.   No one more blinding NOC screens! The dashboard now has a switch to turn out the lights! Great for status dashboards in dark rooms. I mean, not literal dark rooms, not photo negative processing friendly, but you probably know that if you’re developing film in your closet. We’ve fixed lots of bugs with importing and exporting objects, scripted fields, and complex dashboard performance. And of course, there’s more to come, but don’t let that stop you from trying out Kibana 4.2 beta 2 today! Ok, there it is! Enjoy! As always, with issues, suggestions and contributions. Or, if you love IRC like we do, join us in #kibana on Freenode. You can also find us at ","locales":"","title":"Kibana 4.2 beta 2: The dark side of the moon"}
{"index":{}}
{"author":"Lasse Schou","category":"User Stories","publish_date":"2015-09-25T00:00:00.000Z","url":"/blog/avoiding-traps-enjoying-the-cheese-as-mouseflow-uses-elasticsearch","seo_title":"Avoiding Traps, Enjoying the Cheese as Mouseflow Uses Elasticsearch","content":" In 2015, the number of worldwide Internet users will surpass 3 billion and include nearly 40% of the world’s population (source: eMarketer). In analytics, the key to success is having data and knowing what to do with it. Our product, , is a website analytics tool that replays visitor behaviour (session replay) and generates heat maps showing where visitors click, move their mouse, scroll, interact, and are physically located. We collect more data from more clients than ever before and scalability is one of our top priorities.Our product is installed onto a website via a snippet of JavaScript which, collectively, captures billions of events each month. These events are stored and analysed to reconstruct a full session as it occurred, showing the who, what, where, and when of pertinent activity. This enables clients to make informed decisions about their site: understanding problems, building solutions, and deploying fixes.Tackling Scalability Issues as New Accounts Stream InIn the beginning, we used a sharded SQL-based database cluster for storage. This served quite well because our clients were mainly small and medium businesses. After a short time, bloggers wrote reviews and our clients started to refer others. This resulted in a stream of new accounts, some of them enterprise clients tracking millions of events, which stretched the limits of our platform. It became clear that, with growing message queues and data that wasn’t easy to sort or filter, it was time for a change.We searched for a distributed and redundant database system known for performance and found Elasticsearch. We installed it and started directing copies of inbound events to the cluster. After a few days, we wrote more complex queries and refined our views even further. It was clear that we were onto something: common queries like “find all the sessions from facebook.com, using an iPhone, who abandoned checkout” (which previously took a long time unless we had indices for that exact search) now took less than 1 second to execute. Our freetext search operations were also, naturally, vastly improved.Clearly Onto Something After Installing ElasticsearchAfter further use, Elasticsearch started to show its numerous benefits:First, Elasticsearch is distributed and redundant, meaning our platform is more reliable and data is safer. We have other data stores used for storage and backup but, since uptime is so important, having redundancy in the search layer is key.Second, Elasticsearch is based on Lucene which ensures that free text search operations are much faster. This benefits clients who search multiple unknowable terms and expect results in milliseconds.Third, Elasticsearch has built-in support for aggregations which means we can query data in a more consumable format (used in most of our reports).These benefits free up developer resources, giving us the flexibility to focus on our product instead of operations tasks. In the last two weeks, we launched The New Mouseflow (our fully revamped user interface with tons of new features) and onboarded over 12,000 private beta users. This would not have been possible without tackling our own scalability issues which, in large part, were solved by moving to Elasticsearch. If your project has similar requirements, we highly recommend taking Elasticsearch for a test-drive.Lasse Schou is the CEO of Mouseflow, a Denmark-based SaaS tool for performing web analytics and real-time user studies on websites. Lasse has been working with tech start-ups since 2002 and started Mouseflow in 2010 where he saw a need for visualizing online user behavior. Mouseflow is now serving over 45,000 customers in 160 countries and is using Elasticsearch to deliver big-data analysis in real-time.[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0","locales":"","title":"Avoiding Traps, Enjoying the Cheese as Mouseflow Uses Elasticsearch"}
{"index":{}}
{"author":"Shawn Anderson","category":"User Stories","publish_date":"2015-09-24T00:00:00.000Z","url":"/blog/road-to-a-better-user-search-experience","seo_title":"","content":" As one of the world's largest third party logistics (3PL) providers, (CHR) deals with a lot of data. We provide freight transportation and logistics, outsource solutions, produce sourcing, and information services to over 46,000 customers through a network of offices in North America, South America, Europe and Asia. To meet our customers' freight needs, we provide access to over 66,000 transportation providers worldwide. Accessing Shipment DataIn 2014, CHR handled approximately 14.3 million shipments. The transactional data supporting those shipments is represented by multiple terabytes of SQL data. In order to make the best decisions for our customers, our networks need access to this information as quickly as possible. As shipment volume increases and customer search requirements mature, we run into roadblocks with traditional SQL queries. For example: Elasticsearch has helped C.H. Robinson get past these roadblocks and opened new doors to providing an enjoyable user search experience.  Dipping Our Toes in ElasticsearchOur first challenge was getting our SQL data into Elasticsearch. Fortunately, we had recently completed a service bus initiative - it was the perfect fit for building a pipeline of data into Elasticsearch. As data changes come in from different sources, notifications are sent out to be picked up by load processes. These processes gather the relevant SQL data for searching and forward it on to Elasticsearch. This solved the day-to-day operations, giving us \"near real-time\" data for searching.The second problem to solve was bulk loading SQL data to backfill into Elasticsearch. We explored Rivers - both JDBC and RabbitMQ (the backbone of our service bus). But with the deprecation of Rivers and some of the problems they presented, we opted to build a custom bulk load process to ensure we could control the load on both sides of the pipeline. Initially, we used a distributed, multi-threaded approach, but quickly ran into . We increased the bulk threadpool queue size slightly (200), and throttled our bulk inserts to a more manageable level. This solved our initial data load problem and gave us a push button \"rebuild this index\" when needed.With all this data now funneling into Elasticsearch, we were free to create much richer user search interfaces, including:  The future of Elasticsearch @ C.H. RobinsonOne of the biggest challenges when moving to Elasticsearch is understanding user search expectations, and explaining what Elasticsearch relevancy means to your stakeholders. Users don't want to read , they just want to know why certain results are or are not being returned. This has been a challenge, because we have so many search use cases in different parts of the organization. However, initial reaction from our users on new search capabilities has been great, and we've already been able to iterate on feedback to improve our queries. Some improvements we’ve made include: It's clear we're only seeing the tip of the iceberg in terms of potential use cases and problems that Elasticsearch might help solve. Partnering with Elastic has given us a great starting point, assisting us in building out a search cluster that meets our initial use cases and data needs, with room for growth. If Elasticsearch were a video game, it would definitely follow - easy to learn, hard to master. As we look to the future, we hope to utilize many more features of Elasticsearch such as: ","locales":"","title":"The Road to a Better User Search Experience"}
{"index":{}}
{"author":"Asawari Samant","category":"Engineering","publish_date":"2015-09-23T00:00:00.000Z","url":"/blog/getting-started-with-elk","seo_title":"Getting Started with ELK","content":" Are you a new user looking for easy-to-use examples to get started with the ELK Stack (Elasticsearch, Logstash, and Kibana)? Or perhaps you're a not-so-new user looking for starter code to ingest standard logs (e.g., Nginx access logs, Twitter feed, etc.) and don't want to write a Logstash config file from scratch. We are thrilled to announce a new and improved in GitHub where we will share easy-to-use examples for getting started with Elasticsearch, Logstash and Kibana, i.e. . So, what's in an example?The objective of each example is simple:  To give you everything you need to go from raw data to an insightful (and pretty) Kibana dashboard in a few easy steps. While the exact contents of each example may vary slightly, in general, each will include sample data (or instructions to obtain the data), a Logstash config file for ingest, custom Elasticsearch mapping/template files, and Kibana files to load pre-built dashboards. And, of course, detailed instructions on how to download and run the example.  We have intentionally kept the structure and content simple and crisp to optimize the download-and-play experience. That said, if you have suggestions on how we can improve these further, we are eager to listen via , or . ContributingAt Elastic, we are all about sharing with — and learning from — our vibrant user community. Every day we are amazed by the new and fascinating ways in which our users are pushing the boundaries with our products. We would love to see that awesomeness shared in this example repo. If you have an example (however simple or complex) to share, simply in the GitHub repo with a short description of your example and we will work with you to package it and make it available to others to enjoy. The is laid out in the repo ReadMe. Don't have an example, but have an idea or suggestion for the demos team at Elastic? We are all ears! Once again, to let us know. While we may not be able to get to all of them, we promise to do our best. Give it a try!We hope you enjoy this new project, and we look forward to seeing your awesome contributions in the repo soon! ","locales":"","title":"Getting Started with ELK"}
{"index":{}}
{"author":"Shay Banon","category":"News","publish_date":"2015-09-21T00:00:00.000Z","url":"/blog/hope-to-see-you-on-tour-this-fall","seo_title":"","content":" It’s been almost 6 months since our first user conference -- -- and while we were able to bring together 1,000+ people in San Francisco, many of you have been waiting for us to come on tour and visit a city near you.As our community has grown to around the globe, I’ve always felt that events like meetups, user conferences, and city tours are important to engage our community, foster new friendships, learn and share new ideas, and for you to hear from us as to what we’re working on and how you can get involved.So starting this October and through December, the team at Elastic will be hitting the road across 12 cities in the U.S., EMEA and Asia. After a short break for the holidays, we’ll be hosting our in San Francisco at Pier 48 from February 17 - 19 2016, so please save this date in your calendar as we’ll be doubling this event in size and content. Based on lots of great feedback from the community, each tour stop will have a full-day of content, and will include products technical deep dive and roadmap covering Apache Lucene, Elasticsearch, Kibana, Beats, Logstash and Hadoop. Our customers and users will share their use cases ranging from search, logging, analytics, and more. By popular request, there will be an Ask Me Anything (AMA) booth open for the entire day for anyone to ask our developers any questions they want!Lastly, I’m personally excited that 100% of the registration proceeds (approx $75 USD per ticket) will be donated to local charities. Thanks to all of you, you’ve helped us build a wonderful community, and I want everyone to know that they in addition to helping us build better products, by attending our tour events, you’re helping others, such as: I can’t wait to see you either on tour or in San Francisco at Elastic{ON} 16. as we already have more than 1,600 people registered and are nearing capacity for many of our cities. As always, if you have a great story to tell, don’t forget to !  ","locales":"","title":"Hope to see you {ON} tour this Fall"}
{"index":{}}
{"author":"Jean-Michel Gaud","category":"User Stories","publish_date":"2015-09-23T00:00:00.000Z","url":"/blog/from-online-retailer-to-geographically-distributed-marketplace-with-Elastic","seo_title":"From Online Retailer to Geographically Distributed Marketplace – with Elastic","content":" In late 2011, joined the myriads of hopeful dot-com billionaires by launching an e-commerce start-up with the aim of revolutionising the floral industry. While our initial website contained a few advanced B2B features specifically developed for the florist industry, it remained a largely stock-standard e-commerce implementation. It’s hard to remember a world without e-commerce. Almost any product nowadays can be purchased over the internet and delivered to one’s home – flowers are no exception. For merchants, creating an online presence is now easier than ever. There are a plethora of providers who offer feature-rich out-of-the-box e-commerce solutions. Some of these solutions can be set up and made ready to transact in minutes. For the SME (subject matter expert) who requires a deeper level of customisation, there are free open-source frameworks that can be modified and scaled to enterprise levels with relative ease. The mass proliferation of e-commerce technology and its subsequent mass adoption have led to a situation, at least in our sector, where the e-commerce ‘space’ is heavily saturated. Consumers now have more providers to choose from than ever. On the other hand, differentiation between competing providers remains low. The South African online flower market is dominated by a single incumbent who, until fairly recently, remained largely unchallenged. Over the years, competitors have entered the market but none have managed to make any waves big enough to seriously challenge the incumbent’s hold. Towards a Marketplace In 2014, SA Florist secured a seed series funding investment of ZAR 3 million (roughly 350000 USD) from the South African franchise of Dragons’ Den. This allowed us to ramp up our operations. What followed was a complete overhaul of our business model, heavily relying on Elastic to enable this rapid change. We realised that if we wanted to become a serious player in the market, we couldn’t do things the same way as our competitors. We would have to differentiate ourselves and offer consumers a far better value proposition. After much research, and encouraged by the high-profile successes of companies such as Uber and Airbnb, we decided to pivot our business from an online retailer into a geographically distributed marketplace. Why Elastic Elastic was the obvious choice of technology to power our marketplace. Several factors led to this decision: In a .NET Environment As a company that uses a Microsoft .NET / ASP.NET / C# / MVC / SQL Server based web application technology stack, integrating and working with Elastic as a data source is a pleasure. Where are you sending to? As I mentioned earlier, our initial website contained a few advanced B2B features specifically developed for the florist industry. These B2B features allow florists (or other retailers who can’t ship their products nationally) to take orders in their shops for distant areas, and relay them to a florist nearby the recipient. We extended the data structure underlying the B2B system, by adding to our code, a layer of JSON serializable data objects. The data objects were given a geo_point property that corresponds to the physical location of the florists’ shops. We then built an index management utility to CRUD our objects. A bit of MVC’ing later and voila! – a geographically distributed flower marketplace. In order to filter by distances, Elastic needs geo coordinates, so we used Google’s APIs and an auto-complete lookup to verify and pinpoint customers' street addresses. Version 1.0 of the marketplace took us about a month to take live. It took the time of two developers to accomplish this. Features we Love \"We highly recommend Elastic. It has allowed us to transform our business in record time with minimal resour","locales":"","title":"From Online Retailer to Geographically Distributed Marketplace – with Elastic"}
{"index":{}}
{"author":"Michael McCandless","category":"","publish_date":"2015-09-22T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2015-09-22","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top NewsElasticsearch requires less disk than you may think:  even less in 2.0 with best_compression! — elastic (@elastic) Elasticsearch Core Apache Lucene Watch This Space, where we'll share more news on the whole ELK ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - September 22 2015"}
{"index":{}}
{"author":"Clinton Gormley","category":"News","publish_date":"2015-09-17T00:00:00.000Z","url":"/blog/elasticsearch-2-0-0-beta2-released","seo_title":"Elasticsearch 2.0.0-beta2 released","content":" Today, we are happy to announce the release of , based on . This is the last planned beta release before the 2.0.0 release candidate. : This is a beta release and is intended for testing purposes only. Elasticsearch 2.0.0-beta2 is not compatible with 2.0.0-beta1, and there is no guarantee that Elasticsearch 2.0.0-beta2 will be compatible with Elasticsearch 2.0.0 GA.  You can . Thank you to those of you who tested out 2.0.0-beta1 and reported problems. The changes in Elasticsearch core since 2.0.0-beta1 consist mostly of bug fixes and tidy ups. Our commercial plugins, however, deliver some important new features in this release, which you can read about in .  The highlights are as follows: ","locales":"","title":"Elasticsearch 2.0.0-beta2 released"}
{"index":{}}
{"author":"Megan Wieling","category":"","publish_date":"2015-09-21T00:00:00.000Z","url":"/blog/witwies-percona-jduchess","seo_title":"","content":" Welcome to Find out what September has in store for you and check out the Elastic events and meetups for this week.Upcoming Events September 21 - 23:  September 22:   Upcoming Meetups September 22: September 23: September 24: September 21: September 23: September 24: September 24: September 24:  That's it for this week. Stay tuned for Elastic happenings next week! - The Elastic Team P.S. if you're interested in hosting a meetup or are giving a talk about Beats, Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic? - Percona, JDuchess"}
{"index":{}}
{"author":"Uri Boness","category":"News","publish_date":"2015-09-17T00:00:00.000Z","url":"/blog/shield-and-watcher-2-0-0-beta2-released","seo_title":"Shield and Watcher 2.0.0-beta2 released","content":" Today, we are happy to announce the releases of . These are the last planned beta releases before the 2.0.0 release candidates. These are beta releases and are intended for testing purposes only. There is no guarantee that either Shield or Watcher 2.0.0-beta2 will be compatible with their respective 2.0.0 GA. Shield This release comes with some exciting new features in Shield. When it comes to securing your data in Elasticsearch, one of the more commonly requested features is the ability to define access control on documents and fields. With Shield 1.x, you could simulate document level security using a combination of filtered aliases and index level permissions per role. While this approach works for some cases, there are certainly cases where this becomes unmanageable (due to the number of aliases and roles one needs to define). Furthermore, it is not a bulletproof solution as it doesn’t work for some of the APIs (e.g. parent/child searches), and it doesn’t provide any form of field level security. We are delighted to announce that will be a first class feature in Shield 2.0 and is introduced in this beta2 release. What does being a “first class feature” mean? Well… it is no longer bound to index aliases. Instead, you can define what documents and fields are accessible by role as part of the role definition. For field level access control you can now specify a list of accessible fields per index. For document level access control you can specify a query per index. These two together effectively create a filtered view of the index prior to any search executed by users with the relevant roles. The following snippet shows what the configuration may look like as part of the role definition: With the above role definition, users with the us_customer_care role will only have read access to help desk issues for customers in the US, and this read access is limited to a 4 fields - , , , and . All other fields will be treated as non-existent for this user and attempting to search on those will yield no results. You can read more about Document and Field level security . Another feature we’ve added in this release is support for . With this support, it is now possible to enable a user to impersonate other users and by that execute requests on their behalf. User impersonation comes in handy when authentication is applied on the application level (outside of Elasticsearch/Shield), but the authorization should still be applied in Elasticsearch to ensure sensitive data is not compromised. Setting this “run_as” privilege is configured per role definition. As such, a best practice would be to create dedicated “run_as” roles for a set of impersonated users and assign this role to the relevant power users. The following snippet shows an example of such role: The example above uses wildcards to defines a new role that enables users to impersonate any other user with a principal with suffix. You can read more about user impersonation . Last but certainly not least, we’re excited to announce that we’ve opened up Shield for extensions. More specifically, you can now write a custom implementation of an authentication realm and plug it into Shield. If for any reason, you’re forced to use an authentication mechanism that is not supported out-of-the-box by Shield, you can now write your own plugin just for the authentication part and still use all the other goodness Shield has to offer, including the extensive role-based access control over indices, document and fields, secured communication using SSL and IP filtering and of course the tightly integrated audit logging. To read more about extending Shield realms, please read the . We’ve also created an example custom realm to serve as a reference. Watcher One of the more common requests for watcher was the ability to disable watches. Often users define watches to monitor different systems. But there comes a ","locales":"","title":"Shield and Watcher 2.0.0-beta2 released"}
{"index":{}}
{"author":"Peter Colclough","category":"User Stories","publish_date":"2015-09-17T00:00:00.000Z","url":"/blog/how-avaaz-uses-the-elk-stack-to-improve-their-production-systems","seo_title":"How Avaaz uses the ELK-stack to solve their logging issues","content":" - meaning \"voice\" in several European, Middle Eastern and Asian languages - launched in 2007 with a simple democratic mission: organize citizens of all nations to close the gap between the world we have and the world most people everywhere want. Avaaz is a global petitioning site with more than 40 million members which was hitting some particularly interesting issues. Central, at the time, was the fact they were outgrowing their database architecture, and were hitting database access issues. A victim of their own success.The Issue is to find out 'Why' As we all know, we have logs: we have slow logs, we have Nagios, but all of these don't give the exact information of what a server is doing at any point in time. Briefly, I had come across Lucene three years ago as a 'search engine'. It was being used with Solr by a previous retail client. It was spectacular. At the time I had investigated using mysql-proxy in order to read the actual query metrics directly off the servers, using a hand crafted Lua script attached to Proxy. While never used in anger this was more than helpful in ironing out some database issues ... but we didn't have a sane place to plot the results on a graph and analyze them.Looking back at other ClientsAnother client of mine called Madbits, a startup in image interpretation (a dark art like no other ), had issues with a MongoDB setup, and also retrieving information in anything close to a reasonable time for a web site. Part of the issue was down to MongoDB, and the way it had been set up (never, ever, just have a single unit... not designed for that). After scaling out Mongo we had stunning results, but we then had issues with 'user generated tags'. Enter Lucene. We immediately faced a challenge in trying to use it. MongoDB stores documents in Bson (a binary json format) but Lucene uses Java objects. We could do the conversion our selves, but instead discovered Elasticsearch. We could flip documents between MongoDB and Elasticsearch with very little, if any changes. Wow! That works. So in came Elasticsearch and later Kibana.However, we also needed to log the DB. So with some handy Logstash parsing, we were able to push Mongo logs into ES using Logstash. We then used Kibana to graph it which was also stunning. Direct insight into the workings of a fully sharded DB (across 3 data centres) in graph form... cool.Introducing Elasticsearch at AvaazAt Avaaz we had issues with MySQL. So we put in mysql-proxy, altered the Lua script to output Json to a file, used Logstash to push into ES, and we were there. Not only did mysql-proxy give us the ability to scale out the slaves, we got actual insight to every query going into each machine. For the record, this now also includes a couple of AWS RDS instances too. We ran up some Kibana-3 dashboards, and the results were astonishing.The only issue we had was the sheer volume of data. We were recording off the live systems, onto a development ES cluster. Initially we were getting in excess of 150m documents per 24 hours on a normal day. On a busy day that could double. Elasticsearch was fine, it consumed with no issues, it was just storage and expansion. So we tweaked the cluster setup, added some more disk (Raid 0,and a reboot - simple), then made the decision to add a _ttl of 1 day allowing Elasticsearch to manage the deletion, and built a series of aggregation scripts. This enabled us to see the last 24 hours, and have Elasticsearch automatically drop the documents that were too old. The aggregated information helped us see what was happening on a day by day basis, aggregated by the hour. On our new production cluster we are saving 3 days at a time, so we can see what happened at the weekend.What changed?What did this show us? A lot - and please bear in mind that the code was 6 years old, and had been written with ","locales":"","title":"How Avaaz uses the Elastic Stack to Improve their Production Systems"}
{"index":{}}
{"author":"Michael McCandless","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2015-09-15T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2015-09-15","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top NewsLooking for some weekend reading? Check out ’s primer on “Hot-Warm” architecture — elastic (@elastic) Elasticsearch Core Apache Lucene Watch This Space, where we'll share more news on the whole ELK ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - September 15 2015"}
{"index":{}}
{"author":"Andrew Cholakian","category":"Releases","publish_date":"2015-09-16T00:00:00.000Z","url":"/blog/logstash-2-0-0-beta1-released","seo_title":"","content":" If you’ve followed our announcements recently, we have been releasing beta versions for our Elastic products to work with the recent release of Elasticsearch 2.0 beta. Logstash is getting on the too! Read along to find out what’s in this release. This is a beta release and is intended for testing purposes only. There is no guarantee that Logstash 2.0.0-beta1 will be compatible with Logstash 2.0.0 GA. Why version 2.0? We’ll be starting a new chapter in Logstash history with the forthcoming release of Logstash 2.0! This release is primarily about two things: Versioning scheme To provide better out of the box experience with Elasticsearch, we realized we needed to make breaking changes, and our current ad-hoc versioning strategy would not work for users. We would like to adhere to a versioning and release strategy that can better inform you, our users, about any breaking changes to the Logstash configuration formats, plugin APIs and other functionality. Logstash releases follows a three-placed numbering scheme X.Y.Z. where X denotes a major release version which break compatibility with existing configuration or functionality. Y denotes releases which includes features which are backward compatible. Z denotes releases which includes bug fixes and patches. In compliance with this scheme, today, we are releasing a beta1 version of 2.0.0 which introduces some breaking changes you can read about below. If you’ve followed along with our please note that the overall path of our Roadmap remains the same, but only the numbers have changed. Our team is still focused to deliver on the themes mentioned in the Roadmap – resiliency, manageability and performance. We will target new enhancements like persistent queues as minor versioned releases (like 2.1, 2.2) since they’ll be backwards compatible. The 2.0.0-beta1 release, specifically, will have the following changes: Elasticsearch Output now Defaults to HTTP We’ve altered the Elasticsearch output to now default to HTTP. For those who want to use the ‘node’ and ‘transport’ protocols, support for those is now provided in a separate , that must be downloaded separately. We decided to not bundle this functionality with the core Logstash distribution because it adds a good 30MB of size to the download, and creates a weird situation for users wanting to use Elasticsearch 1.x node/transport. While we have no plans to deprecate support for the node and transport protocols we strongly discourage their use for the reasons below: We will still be supporting both plugins. We have, in fact, performed a major refactor on both plugins to remove dead code and make more efficient use of internal client objects. So, if you still prefer to use the native protocols, by all means install the new plugin. If you want to use these java plugins with Elasticsearch 1.x cluster be sure to install specific versions in the 1.x plugin range. The 2.x releases of will only work with Elasticsearch 2.0. Configuration Changes The Elasticsearch output has a few configuration changes to be compatible with the 2.0 beta1 release of Elasticsearch. Please make sure to read the updated documentation for configuring the and protocols. These configurations are not backward compatible, so you will have to update your existing config files. How We Benchmarked the new HTTP Elasticsearch Output As mentioned above, we recently benchmarked the different protocols and found HTTP was only about 3% slower (when using multiple output workers) given a realistic logstash config for parsing apache weblogs. This is a small price to pay for a considerable improvement in operational simplicity. Moving to HTTP also provides much better compatibility across ES version upgrades. You won’t have to upgrade the logstash Elasticsearch output every time you upgrade your Elasticsearch cluster if you use HTTP. We hit these issues ourselves in our benchmarking, finding that some versions of the new Elasticsearch betas ","locales":"","title":"Logstash 2.0.0 beta1 released"}
{"index":{}}
{"author":"Russ Savage","category":"Engineering","publish_date":"2015-09-15T00:00:00.000Z","url":"/blog/analyzing-salesforce-data-with-logstash-elasticsearch-kibana","seo_title":"Analyzing Salesforce Data with Logstash, Elasticsearch, and Kibana","content":" In the spirit of Dreamforce and being part of Elastic, I’m excited to introduce a new that makes it easy to pull vast amounts of data out of Salesforce and into Elasticsearch. This plugin allows you to query almost any object in Salesforce and pull it into the Logstash pipeline.  From there, you can output it to your analytics tool of choice (ours is Kibana) to discover insights that you may have missed otherwise. We use it internally to keep an eye on our our sales pipeline and customer health over time.  Working directly with sales and marketing operations, we outlined a number of challenges they had that might be solved with this solution. Those included: It’s very challenging to look back in time and see trends in the data. Many companies have configured Salesforce to save reporting snapshots, but if you're like me, you want to see the data behind the aggregate report. I want the ability to drill down to any level of detail, for any timeframe, and find any metric. We found that Salesforce snapshots just aren't flexible enough for that. Logstash already allows users to pull in vast amounts of data from a variety of sources, and with this plugin, we are adding Salesforce as a source. Once the data is in our Elasticsearch cluster, we use Kibana to quickly filter and display the data in ways we couldn't before using Salesforce’s built in reporting. Now, let’s focus on how you can set up the Logstash Input for Salesforce for your own organization. Querying Salesforce from LogstashAs I mentioned earlier, we’ve been using the internally to handle all of this data. We decided others could benefit from this plugin as well so we and made it available for anyone to install using the standard Logstash plugin framework. With a fresh install of the latest Logstash, you should be able to install this plugin with: bin/plugin install logstash-input-salesforce Based on your configuration, the plugin will query from Salesforce and create an event for each row of data returned. You can then filter and transform the data as needed before pushing it to wherever it needs to go. To configure this plugin, we will need to gather some credentials from Salesforce. This involves creating a new in Salesforce and generating a user token. Depending on your Salesforce permissions, you might need to ask your Administrator to do this for you. You should only need to do this once. Here is an example of the minimum information you need to enter. Of course, the scope can be configured as needed but just make sure to give enough access for the sObjects you want to query. Also, for the callback URL, you can just enter localhost since the plugin will be using password authentication. Once you have a new application set up, you will need the Consumer Key and Consumer Secret values, so keep track of those. While you’re in Salesforce, you should also go ahead and generate a token for the username you will be using for authentication. on how to generate or reset your token if you don’t already have one. Now we have everything we need for connecting to Salesforce from Logstash.  Let’s start with a simple Logstash config that will connect to your sandbox Salesforce instance (test.salesforce.com) and pull in all the Opportunity objects. input { salesforce { use_test_sandbox => true #you are testing in a sandbox right? client_id => 'CONSUMER_KEY_FROM_SALESFORCE' client_secret => 'CONSUMER_SECRET_FROM_SALESFORCE' username => 'you@example.com.sandbox' password => 's3crEt!' security_token => 'SECURITY_TOKEN_FROM_SALESFORCE' sfdc_object_name => 'Opportunity' } } output { stdout { codec => rubydebug } } Here, we are telling the plugin to use the test sandbox, connect with our credentials, and pull in all the Opportunity objects. By default, the input will pull in all the fields for that . If you would only l","locales":"","title":"Analyzing Salesforce Data with Logstash, Elasticsearch, and Kibana"}
{"index":{}}
{"author":"Oren Raboy","category":"User Stories","publish_date":"2015-09-15T00:00:00.000Z","url":"/blog/how-totango-uses-elasticsearch-for-customer-successs-platform","seo_title":"","content":" is a Customer Success platform used by subscription and recurring revenue businesses to reduce customer churn, drive product adoption, and maximize customer lifetime revenue.  Today companies have unprecedented data and visibility into their users’ behavior and the business results achieved by their customers. Totango monitors this data to generate insights on customer health and engagement. Using Totango, companies can pinpoint at-risk accounts that need attention:  spot opportunities to increase user engagement and boost revenue:  and then implement customer success best practices to scale up operations across a growing customer base.Totango is used by some of the fastest-growing technology companies, including public companies like Zendesk and Autodesk:  mid-stage companies like BigCommerce and Jobvite:  and innovative startups like Optimizely and Mixpanel. How Totango Drives Customer SuccessTotango is all about analyzing customer data and translating it into actions for improving Customer Success.With Totango, a Customer Success Manager can get instant feedback about which users require special attention, so they can better support them, increase adoption, and ultimately, grow  the lifetime value of a customer, all while reducing churn. For example, when using Totango a success manager gets notified when a user first starts using an advanced functionality, or if the number of active users in an account has declined to an all-time low. Totango can even send emails to users when certain conditions are met to fully automate the success process. Totango’s Architecture RequirementsTo accomplish this, we designed the Totango technology platform with a few key architecture requirements: Balancing these requirements requires a solid Data Architecture and use of the best NoSQL technologies the industry has to offer. We were fortunate to be one of the early adopters of Elasticsearch and have been using it for the past 4 years as a core technology in our stack. It has enabled us to address the needs above while supporting our growing customer base. Elasticsearch's flexibility has been key to us continuously innovating and delivering new functionality to the market.In this post, we will highlight how Elasticsearch has helped us build the world’s finest data platform for Customer Success. Elasticsearch in The Totango Platform ArchitectureWithin the Totango data architecture, Elasticsearch serves as the data-store infrastructure in our serving layer. When a new event (such as a user logging in, or a new business transaction) is collected and processed by Totango, its results are indexed in realtime into the Elasticsearch index, making it available for use by the Totango users.Additionally, each day Totango processes all collected information, applying statistical models and other data-science techniques. It then indexes a daily snapshot of insights within our historical archive, which also leverages Elasticsearch. Using this archive, we can present daily and weekly trends of metrics to see how things progress overtime.In aggregate, Totango maintains multiple Elasticsearch indices for every one of our clients, each aggregating many millions of documents representing the state of a user or account in a snapshot in time.All this happens transparently to our customers who receive a virtual, cloud-based solution. Elasticsearch, along with the rest of our infrastructure is abstracted and wrapped in the beautiful, action-oriented user-interface we developed at Totango.  The user receives alerts of important events, analyzes customer usage through segmentation and reports and even setup rules to send an email-campaign when users perform certain actions. This all happens without having to consider the complexities of collecting, normalizing, processing, and storing complex multidimensional data.  Benefits of Elasticsearch We’v","locales":"","title":"How Totango uses  Elasticsearch to build the world’s finest Customer Success Platform"}
{"index":{}}
{"author":"Peter Kim","category":"Engineering","publish_date":"2015-09-15T00:00:00.000Z","url":"/blog/elasticsearch-storage-the-true-story-2.0","seo_title":"","content":" Several months ago we wrote about the and today we’re here to refresh those tests with the new which includes several enhancements. For those of you looking for a quick answer, these tests showed an overall compression ratio of 0.732 (index size on disk / raw log file size) for those of you who are doing both search and visual analysis of your log data.But of course, the answer to “How much hardware will I need” is still “It depends!” Many factors need to be taken into account and both this post and our previous are meant to elaborate on these factors in detail so you can make informed choices for both the best performance and the right hardware purchases.Changes since last timeWhen we wrote our last blog post, we ran our experiments using Elasticsearch 1.4. Since then, there have been hundreds of new features, enhancements and bug fixes made to Elasticsearch. With respect to disk storage requirements, the key changes are: 1) the addition of a best_compression option for stored fields and 2) doc_values enabled by default. In addition, we found a bug in our original experiments that had a significant impact on index size. DEFLATE: no footballs have been tampered in these testsAs described in this  by Adrien Grand, one of our engineers and core Lucene committers, Lucene 5.0 added a new compression option using the DEFLATE algorithm (the same algorithm behind zip, gzip, and png). Elasticsearch 2.0 is the first version of Elasticsearch to rely on Lucene 5.x and thus is able to leverage the new compression option. This is exposed via the . In our tests, using best_compression reduces our index size between 15-25% depending on the configuration. This is substantial, especially when looking at larger clusters. Many large Elasticsearch clusters are 50-100 nodes and more, where cluster size is primarily driven by sheer data volume. When you can cut down the amount of hardware by 15-25%, that’s a pretty significant change for the better.So what’s the catch? The stored fields (value of the _source field) are what’s compressed, so there’s only a performance penalty due to decompression when the stored fields are returned in the query response. When the size=0 parameter is added to the request (as is recommended for pure aggregations queries and what Kibana does under the covers), there is no decompression penalty. There is a small performance penalty at index time:  in many cases, people will gladly give up the extra CPU required for compression in exchange for disk space but you’ll have to consider this in the context of your requirements. Alternatively, we expect many users will want to utilize a using the . In this scenario, time-based indexes on hot nodes can be configured to be created with the default LZ4 compression:  when they’re migrated to the warm nodes (configured with index.codec: best_compression in elasticsearch.yml), the indexes can be compressed by . It may be preferable to pay the CPU penalty of compression during this optimize process (which we often recommend executing when the cluster is known to be less utilized) than at the time of initial indexing.What's up doc_values? were introduced with Elasticsearch 1.0 as an alternative means of storing data that is better suited for analytics workloads by using a columnar representation and can be accessed off the JVM heap. Initially, using doc values was only recommended in specific scenarios but over time, we’ve seen our users wanting to run analytics on continuously growing volumes of data that simply could not be executed under the constraints of the JVM heap. Fortunately, we’ve made significant enhancements to eliminate most of the performance gap between using field_data and doc_values, to the point where we felt comfortable for fields of all types except analyzed string fields.There are two ways doc values can impact a cluster’s hardware requirements. First, enabling doc","locales":"","title":"Part 2.0: The true story behind Elasticsearch storage requirements"}
{"index":{}}
{"author":"Megan Wieling","category":"","publish_date":"2015-09-14T00:00:00.000Z","url":"/blog/witwies-kiratech-trax-tech-linkedIn","seo_title":"","content":" Welcome to Check out this week's events and meetups to find out what's happening near you! Upcoming Events September 17: September 18: Upcoming Meetups September 15: September 15: September 16: September 16: September 16: September 17: September 16: September 16: September 17: September 16: September 14: That's it for this week. Stay tuned for Elastic happenings next week! - The Elastic Team P.S. if you're interested in hosting a meetup or are giving a talk about Beats, Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic? - Kiratech Event, Trax Tech, and LinkedIn Meetups"}
{"index":{}}
{"author":"Joe Fleming","category":"Releases","publish_date":"2015-09-08T00:00:00.000Z","url":"/blog/kibana-4-1-2-now-available","seo_title":"Kibana 4.1.2 Now Available","content":" Kibana 4.1.2 is now available, and you can . This is a bugfix/maintenance release for the 4.1 branch Of particular note in this release, we fixed a lot of labeling, including labels on filters and visualizations, and related to scripted fields, range aggregations and pie charts. A mostly complete list of fixes include, in no particular order: ","locales":"","title":"Kibana 4.1.2 Now Available"}
{"index":{}}
{"author":"Samir Bennacer","category":"Engineering","publish_date":"2015-09-11T00:00:00.000Z","url":"/blog/hot-warm-architecture","seo_title":"","content":" The current blog applies to Elasticsearch versions 1.x and 2.x. If you are looking for “Hot-Warm” Architecture in Elasticsearch 5.x click When using Elasticsearch for larger time-data analytics use cases, we recommend using time-based indices and a tiered architecture with 3 different types of nodes (Master, Hot-Node and Warm-Node), which we refer to as the \"Hot-Warm\" architecture.Each node has their own characteristics, which are described below.Master nodesWe recommend running 3 dedicated master nodes per cluster having dedicated master nodes which run in their own JVM increases stability and resilience as they are not affected by garbage collection that can affect other types of nodes. These nodes do not handle requests and do not hold any data, and therefore only require less resources (such as CPU, RAM and Disk)Hot data nodesHot data nodes are designed to perform all indexing within the cluster and will also hold the most recent daily indices that generally tend to be queried most frequently. As indexing is a very IO intensive operation, these servers need to be powerful and backed by attached SSD storage.Warm data nodesWarm data nodes are designed to handle a large amount of read-only indices that are not queried frequently. As these indices are read-only, warm nodes tend to utilise very large attached spinning disks instead of SSDs.Elasticsearch needs to know which servers contain the hot nodes and which server contain the warm nodes. This can be achieved by assigning arbitrary tags to each server.For instance, you could tag the node with , or you could start a node using .And the nodes on the warm zone are \"tagged\" with or you could start a node using The box_type parameter is completely arbitrary and you could name it whatever you like. These arbitrary values will be used to tell Elasticsearch where to allocate an index.We can ensure that today’s index is on our SSD boxes by creating it with the following settings:PUT /logs_2015-08-31 { \"settings\": { \"index.routing.allocation.require.box_type\" : \"hot\" } }After few days if the index no longer needs to be on our strongest boxes, we can move it to the nodes tagged as warm by updating its index settings:PUT /logs_2014-08-31/_settings { \"index.routing.allocation.require.box_type\" : \"warm\" }Now how can we achieve that using Logstash :We need update the index templates to include allocation filtering \"index.routing.allocation.require.box_type\" : \"hot\" so that any new indices are created on the nodes that are tagged with hot.Example:{ \"template\": \"logstash-*\", \"settings\": { \"index.refresh_interval\": \"5s\", \"index.routing.allocation.require.box_type\": \"hot\" }, \"mappings\": { \"_default_\": { \"_all\": { \"enabled\": true, \"omit_norms\": true }, \"dynamic_templates\": [ { \"message_field\": { \"match\": \"message\", \"match_mapping_type\": \"string\", \"mapping\": { \"type\": \"string\", \"index\": \"analyzed\", \"omit_norms\": true } } }, { \"string_fields\": { \"match\": \"*\", \"match_mapping_type\": \"string\", \"mapping\": { \"type\": \"string\", \"index\": \"analyzed\", \"omit_norms\": true, \"fields\": { \"raw\": { \"type\": \"string\", \"index\": \"not_analyzed\", \"ignore_above\": 256 } } } } } ], \"properties\": { \"@version\": { \"type\": \"string\", \"index\": \"not_analyzed\" }, \"geoip\": { \"type\": \"object\", \"dynamic\": true, \"properties\": { \"location\": { \"type\": \"geo_point\" } } } } } } }When indexes are no longer being written to and not being searched frequently they no longer need to be on the hot nodes. We can move them to the nodes tagged as warm by updating their index settings: \"index.routing.allocation.require.box_type\" : \"warm\".Elasticsearch will automatically migrate the indices over to the warm nodes. Finally we recommend running an optimize on the migrated indices. It would be a bad idea to optimize the index while it was still allocated to the strong boxes, as the optimization process could swamp the I/O on those nodes and impact the indexing of today’s logs. But the medium boxes aren’t doing very much, so we a","locales":"","title":"“Hot-Warm” architecture"}
{"index":{}}
{"author":"Michael McCandless","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2015-09-08T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2015-09-08","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top NewsNew from infrastructure guru : ' Command Line Debugging w/The _cat API' — elastic (@elastic) Elasticsearch Core Apache Lucene Watch This Space, where we'll share more news on the whole ELK ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - September 8 2015"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2015-09-07T00:00:00.000Z","url":"/blog/2015-09-07-where-in-the-world-is-elastic","seo_title":"Where in the World is Elastic, September 7 2015","content":" Welcome to  And September has started! See what's going on in Elastic meetups and events land this week!Upcoming EventsSeptember 9-10: September 9-11: - Our own is giving a  on Wednesday, September 9 from 8:30a.m. - 12:00 p.m.Upcoming MeetupsSeptember 9: September 9: September 10: September 10: September 10: September 8: September 8: September 9: That's it for this week. Stay tuned for Elastic happenings next week! - The Elastic Team P.S.   if you're interested in hosting a meetup or are giving a talk about Beats, Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic? - September 7, 2015"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2015-09-14T00:00:00.000Z","url":"/blog/elasticsearch-1-7-2-released","seo_title":"Elasticsearch 1.7.2 released","content":" Today, we are happy to announce the bug fix release of , based on . This is the latest stable release. Users are advised to upgrade if they find themselves affected by any of the bugs which have been fixed.You can .Previous blog posts about the 1.7 series:This release contains a number of bug fixes including:Please , try it out, and let us know what you think on Twitter () or on our . You can report any problems on the . ","locales":"","title":"Elasticsearch 1.7.2 released"}
{"index":{}}
{"author":"Daniel Palay","category":"User Stories","publish_date":"2015-09-10T00:00:00.000Z","url":"/blog/a-post-with-a-view-elastic-at-viewpost","seo_title":"","content":" At Elastic, we're constantly engaged with our customers and users to learn more about how they're using our software stack. I recently had the opportunity to learn more about , a leading provider of online invoice payment software, who uses Elasticsearch to power search for its financial application suite, allowing their customers to find specific business partners, look up invoice payables and receivables, and quickly navigate around the application. Because of the sensitive nature of the invoice and payment data, Viewpost aims to meet and exceed security regulations such as ISO 9000 and SSAE 16. They had been trying to use a community­-created Elasticsearch plugin to secure their data, but found it complicated to set up, maintain, and verify. After , Elastic’s security plugin, was released, Viewpost was able to upgrade their deployment within a few weeks, leveraging tightly integrated authentication, encryption, and role-based access control to to meet their security requirements.Here's a snippet from my conversation with Tommy Bollhofer – Viewpost's resident Database Architect – who gives a bit more insight into how Viewpost uses Elasticsearch and Shield to power their SaaS financial application:Database Architect. We are responsible for architecture, design, support, maintenance, administration, and optimization of relational and non-relational distributed database systems.Several uses cases for double wild card search functionality within the application lead us to review both Elasticsearch and Solr. We chose Elasticsearch for many reasons, including the speed at which Elasticsearch responds to user feedback and incorporates new features into the product, and that it's platform agnostic which allows us to run local instances of Elasticsearch on our development machines.Security is the foundation to our product and we wanted to ensure we had both TLS and authentication enabled with our initial rollout of Elasticsearch. Prior to Shield, the only viable option was Jetty. The installation, configuration, maintenance, and integration of Elasticsearch using the Jetty plug-in was very cumbersome. It lacked support for LDAP integration, logging, etc. We started using the Shield plug-in during its initial beta release. It exceeded our expectations and we were able to quickly transition from Jetty to Shield with ease.Elasticsearch was one of the first non-relational distributed database systems in our environment which in and of itself was a big step forward. The rich feature set in Elasticsearch, in conjunction with the support we received, allowed us to move very quickly, delivering a rich, full-fledged search experience to our customers. ","locales":"","title":"A Post with a View: Elastic @ Viewpost"}
{"index":{}}
{"author":"Michael Rupp","category":"User Stories","publish_date":"2015-09-10T00:00:00.000Z","url":"/blog/paymill-how-elasticsearch-is-our-swiss-army-knife-for-data","seo_title":"PAYMILL: How Elasticsearch is our Swiss Army Knife for Data","content":" At PAYMILL we provide an easy-to-use, reliable payment service for online merchants, which is a quite challenging task. As you directly deal with your customers’ money everything has to be top-notch, stable and fast. Starting off with a fast growing monolithic code base and a SQL database we grew very fast within the last three years, both in data size and code complexity. As for the latter part we decided to go for a more service-oriented approach. We had to think about which tool could help us to handle big amounts of data for various use cases and fits into our architectural concept. Elasticsearch quickly came into our focus as it is very easy to setup, provides a JSON based, RESTful API and all kinds of libraries, which makes it easy to use from numerous services at once. Elasticsearch - Our Use Case To get into the technology we first used it as a kind of buffer for an export between our transactional MySQL database and Salesforce. To prevent long running or blocking queries from MySQL we moved small, immutable chunks of data to Elasticsearch every minute and then, in another process, pushed this data to Salesforce, which can take quite a while. This task was the perfect starting point as we got into fundamental concepts of Elasticsearch, learned how to use Elastica, the respective PHP library, and learned our first lessons in a not so critical process. As we were satisfied with the results we decided to go one step further with Elasticsearch and use it for our application logging. As a payment service provider you have to follow very strict rules when it comes to logging and traceability and you have to make sure not to lose any information. Basically you have to store every login, every action and every API call within your system and keep this information for years to fulfil all compliance rules. To achieve this we developed a scalable, failsafe service, which stores all application level logging information into Elasticsearch and makes it accessible to our administration and support via a front end. This is still an on-going project as we want to integrate Logstash and Kibana there in the future but we already learned a lot about index and type management, data storage and data retrieval. This again worked out very nicely and we quickly came up with another use case for Elasticsearch. We offer a frontend to our customers, called the Merchant Centre, which gives them access to their account and data. One of its key features is a dashboard to show the most important business numbers at a glance and include some analytic graphs. Elasticsearch in the Merchant Centre As SQL based analytics can get slow and block tables very easily, Elasticsearch and its extensive aggregation functionality seemed to be the perfect fit to solve this problem for us. We integrated it as the data storage component in an ETL process and set up the data retrieval within our Merchant Centre to utilize our own access control mechanisms. Everyone knows how important load time of web pages is for a customer, so we were quite uncertain if the real time aggregation would work fast and stable enough for our needs. But Elasticsearch did not disappoint us here as it delivers the values really fast even for more complex aggregations. You can get a glance at the new dashboard in the picture above. It is not yet online but soon to be released, so stay tuned. Within the last months the Elastic technology stack really became one of the core components of our service architecture and serves us in many ways whenever we have to handle big, immutable datasets. We use it to log, search and analyse data both internally and for the customer. Our system administration set up the whole ELK stack to handle server logs, we plan to finally utilize it for the search in the Merchant Centre and the data science team thinks about Ela","locales":"de-de","title":"PAYMILL: How Elasticsearch is our Swiss Army Knife for Data"}
{"index":{}}
{"author":"Dale McDiarmid","category":"Engineering","publish_date":"2015-09-08T00:00:00.000Z","url":"/blog/when-and-how-to-percolate-2","seo_title":"When and how to Scale and Optimise Percolator for Elasticsearch","content":" In last week's blog \"\" we discussed the key differences between Watcher and Percolation and how to determine which is applicable to your use case. We followed this with an overview of the scaling properties of Percolator. In this week's post, we'll discuss the tools available for improving and optimising Percolator performance. Filters Filters represent the initial and preferred mechanism for reducing the percolation query set and thus making linear execution acceptable. Filters work by attaching metadata to the percolator queries. When percolating a document, a normal Elasticsearch filter is also included. This identifies a reduced subset of queries which need to be evaluated using the process in last week's post. Although filtering incurs some overhead, this cost is comparable to the use of Elasticsearch filters in standard search use cases, which scales significantly better on large index sizes. Candidates for filters are typically metadata fields, which exist on both the queries and percolator documents. Although not essential, these would ideally allow your queries to be grouped into approximately evenly distributed subsets. However, any metadata that allows subsets of queries to run should be considered for use in filtering. Let's consider the use case discussed in last week's post, where a shard contains 1 million percolation queries. Evaluating each of these against a document takes around 4.2 seconds. However, suppose we modify our percolator queries such that our category id is attached as metadata. We, in turn, pass the categories associated with a product, as a filter, when percolating its document. PUT /best_buy/.percolator/2 { \"query\": { //query }, \"category_id\": \"pcmcat209400050001\", \"user\": \"0dc7a91b9efb4f91e2a454907aeb9596fcb43bc2\" } POST /best_buy/.percolator/_percolate { \"query\": { \"bool\": { \"filter\": { \"terms\": { \"category_id\": [ \"cat00000\", \"abcat0300000\", \"pcmcat165900050023\" ] } } } }, \"doc\": { //document }, \"size\": 10 } See for further details. Across a corpus of 1 million queries, the category id has a cardinality of approx. 1500. These are not uniformly distributed across our data, with users expressing more interest in some categories than others. The following diagram illustrates the results of repeating the tests described in \"When and How To Percolate - Part 1\", with filtering applied using the category id. Adding the category id as a filter results in a significant performance increase. 1 million percolation queries now evaluate in around 100ms. Filtering requires: We recognise that filtering is not always possible. For example, in some instances users have the requirement to always evaluate all queries against a document(e.g. for an admin-focused use case). Alternatively, there may just be an absence of appropriate metadata for filters. Sharding and Routing Sharding of percolator indexes provides a mechanism to take advantage of additional resources and thus improve performance. By splitting percolate queries across multiple shards, the number of percolator queries requiring evaluation in each is reduced. Given the document is percolated on each shard in parallel, the total latency experienced therefore becomes the performance on the slowest shard - thus increasing throughput through parallel execution of smaller tasks. Once sharded, users can consider optimising further using routing. This techniques ensures documents are only percolated on the the shards which contain possible candidate queries for linear evaluation. To achieve this, percolator queries need to be partitioned by a custom routing value at index time. This same value is also used when passing a document to ensure it is only executed on the required shard - thus saving resources and increasing possible throughput. Multiple values can also be specified as a comma separated string, allowing a query and document to be indexed and executed on more th","locales":"","title":"When and How To Percolate - Part 2"}
{"index":{}}
{"author":"Tudor Golubenco","category":"News","publish_date":"2015-09-04T00:00:00.000Z","url":"/blog/beyond-packets-elastic-beats-beta3","seo_title":"","content":" We are excited to announce a new release for the Beats, and , our trusted shippers of operational data. It’s not just Packetbeat anymore, now we’ve added Topbeat, which you can use to monitor your systems’ resources and processes. And don’t worry, we’ve continued to improve Packetbeat as well. In fact, thanks to you -- our community -- we have increased the pace in improving Packetbeat. Here are the highlights of the new release. Topbeat It’s like the top command that you know from the Linux/Unix shell, but sends the data periodically to Elasticsearch. It captures system wide data like the system load, free/used memory or disk stats as well as per process stats. You can monitor all the processes running on your system or just a subset. Oh, and it’s not just for Linux, it works on Windows and OS X too. Guess what, using Kibana to monitor your operating system metrics is a lot of fun! DNS support in Packetbeat DNS support is probably our most requested protocol for Packetbeat. stepped up and added generic support for UDP protocols to Packetbeat and then DNS support on top of it. Check out the screenshot for some of the interesting facts Packetbeat reveals. Memcache support in Packetbeat Memcache is another commonly requested feature. With this release Packetbeat fully supports it, including the binary and text protocols, over UDP and over TCP. Here is a screenshot showing statistics about the Memcache errors that Packetbeat found: Windows support We now officially support Windows. It used to be possible to run Packetbeat on Windows but it wasn’t an easy task. With this release we took several steps to make it simpler. This includes running as a native Windows service and logging to files. It’s also easier now, with Packetbeat, to and pick the one you want to use for network traffic capturing. We plan to go even further with improving the Windows support, including adding a GUI installer to makes things even easier. Stay tuned. Wrong libpcap version no more We’ve completely reworked our build system and now we can statically compile against C libraries. This means libpcap is no longer a runtime dependency for Packetbeat on Linux. This means Packetbeat and Topbeat binaries work not only on the officially supported Linux distributions but also on any other Linux distribution as long as glibc is newer than 2.11. Bonus: Developer Guides! Lots of you asked for developer guides for how to create your own Beats or for how to add a new protocol to Packetbeat. We have now published two fairly comprehensive guides: one for and one for to Packetbeat. If you have an idea for a new Beat or have a question about any of the above, please open a topic on the . Here are the download links for and . ","locales":"","title":"Beyond packets: Elastic Beats 1.0.0-beta3 released"}
{"index":{}}
{"author":"Tyler Langlois","category":"Engineering","publish_date":"2015-09-02T00:00:00.000Z","url":"/blog/elasticsearch-command-line-debugging-with-cat","seo_title":"Elasticsearch Command Line Debugging With The _cat API","content":" One of the most useful utilities for investigating Elasticsearch from the command line is the . Whereas the usual Elasticsearch API endpoints are ideal for consuming JSON from within a fully-fledged programming language, the cat API (as its name would imply) is especially suited for command-line tools.In we've explored some of the different endpoints for the API. We can build upon that and the existing plethora of command-line utilities to build even more useful patterns that we can combine for simple (and effective) monitoring and debugging use cases.A Primer for your CatA basic familiarity with the cat API is helpful before reading on. In particular: Diving Into The HeapAn oft-asked question is how to debug messages. You've taken , but after some time of normal use, heap usage grows again and instability ensues. How can you dig further?There are lots of good resources to track this sort of resource utilization: offers commercial monitoring, and there's many of as well. However, when you're debugging a red cluster at the last minute, you need immediate options. What tools can you reach for easily?The cat API offers many endpoints, and piping a command which retrieves heap metrics into can quickly answer the question, \"Which node is experiencing the most memory pressure right now?\"$ curl -s 'localhost:9200/_cat/nodes?h=host,heap.percent' | sort -r -n -k2 es02 71 es00 60 es01 59We can see that node is using 71% of the JVM heap. Following the pipeline: Coupled with other utilities like and , we can find both over- and under-utilitized nodes in very large clusters very quickly.This is useful, but we can do more. It would be nice if we could query heap usage at a more granular level in order to determine what, exactly, is using space on our nodes.It turns out we can:$ curl -s 'localhost:9200/_cat/nodes?h=name,fm,fcm,sm,qcm,im&v' name fm fcm sm es01 781.4mb 675.6mb 734.5mb es02 1.6gb 681.3mb 892.2mb es00 1.4gb 620.1mb 899.4mb: These are abbreviated column names for (fm), (fcm), and (sm). Other fields exist as well, consult for additional information.From this we can see that is consuming a fairly large part of our node's memory. Armed with this knowledge, mitigations such as increased use of doc_values can aid in shrinking that aspect of heap usage.Gazing Into The Thread PoolMany Elasticsearch operations take place in thread pools, which are useful when inspecting what your cluster is busy doing. During peak times, the thread pool can be a useful reflection of what operations (searching, indexing, etc.) are keeping machines busy.The cat API is useful here, too. By default it returns common thread pools' active, queued, and rejected pools, which can often help pinpoint requests that are backing up into queued pools under heavy load. Consider this generic output:$ curl -s 'localhost:9200/_cat/thread_pool' es03 10.xxx.xx.xxx 0 0 0 0 0 0 1 0 0 elk00 10.xx.xxx.xxx 0 0 0 0 0 0 1 0 0 es00 10.xx.xx.xxx 0 0 0 0 0 0 0 0 0: the table headers are omitted here, but the numbers following node IPs are the , , and pools for the , , and thread pools, respectively.This cluster is serving a single search request, which isn't terribly exciting. However, what if your cluster is having problems and you need to closely watch operations? A command can help here:$ watch 'curl -s localhost:9200/_cat/thread_pool | sort -n -k1' executes the command every 2 seconds by default. We sort on the first column to keep ordering consistent, and usefully highlights field values as they change so we can keep a close eye on thread pools as they change, so it's easy to spot problems such as a deluge of search requests getting queued if users are hitting the cluster hard.Diffing Indices for Fun and ProfitAnother common use case is migrating data from one cluster to another: there are several ways to do this including snapshots and utilities like logstash. With","locales":"","title":"Elasticsearch Command Line Debugging With The _cat API"}
{"index":{}}
{"author":"Pier-Hugues Pellerin","category":"Engineering","publish_date":"2015-09-03T00:00:00.000Z","url":"/blog/logstash-metadata","seo_title":"Metadata use cases in Logstash 1.5","content":" With the release of Logstash 1.5 we have added the ability to add to an event. The difference between regular event data and metadata is that metadata is not serialized by any outputs. This means any metadata you add is transient in the Logstash pipeline and will not be included in the output. Using this feature, one can add custom data to an event, perform additional filtering or add conditionals based on the metadata while the event flows through the Logstash pipeline. This will simplify your configuration and remove the need to define temporary fields. To access the metadata fields you can use the standard field syntax: [@metadata][foo] Use CasesLets us consider some use cases to illustrate the power of . In all our use cases, will be using the rubydebug and the stdout output to check our transformation, so make sure you are correctly defining the output codec with the option set to true. The codec used in the stdout output is currently the only way to see what is in at output time. output { stdout { codec => rubydebug { metadata => true } } } Date filterSince logs arrive in a wide variety of formats, grok is used to extract them, and the date filter to convert them to ISO8601 and overwrite the field with the timestamp from the log event. It happens frequently that users omit to remove the source timestamp field after the conversion and overwrite, though. Here's a rough example of how the new field could be used with the date filter and prevent a temporary timestamp field from making it into Elasticsearch: grok { match => { \"message\" => '%{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{HTTPDATE:[@metadata][timestamp]}\\] “%{WORD:verb} %{DATA:request} HTTP/%{NUMBER:httpversion}” %{NUMBER:response:int} (?:-|%{NUMBER:bytes:int}) %{QS:referrer} %{QS:agent}' } } date { match => [ \"[@metadata][timestamp]\", \"dd/MMM/YYYY:HH:mm:ss Z\" ] } Before Logstash 1.5, you would remove the redundant field by adding the line into the date filter as I outlined above. Theoretically, that will be a slower operation than this one. That makes using the field a performance booster! The field act like a normal field and you can do all the operations or filtering on it. Use them as a scratchpad if you don't need to persist the information. # Log sample: # 213.113.233.227 - server=A id=1234 memory_load=300 error_code=13 payload=12 event_start=1417193566 event_stop=1417793586input { file { sincedb_path => '/dev/null' path => \"/source/test.log\" start_position => 'beginning' } } filter { grok { match => { \"message\" => \"%{IP:ip} - %{DATA:[@metadata][components]}$\" } } kv { source => \"[@metadata][components]\" } date { match => [\"event_start\", \"UNIX\"] target => \"event_start\" } date { match => [\"event_stop\", \"UNIX\"] target => \"event_stop\" } ruby { code => \"event['@metadata']['duration'] = event['event_stop'] - event['event_start']\" } if [@metadata][duration] > 100 { mutate { add_tag => \"slow_query\" add_field => { \"[@metadata][speed]\" => \"slow_query\" } } } else { mutate { add_field => { \"[@metadata][speed]\" => \"normal\" } } } } output { stdout { codec => rubydebug { metadata => true } } } Elasticsearch outputSome plugins leverage the use of the metadata, like the elasticsearch input. It allows you to keep the document information in a predefined field. This information is available to various parts of the Logstash pipeline, but will not be persisted in Elasticsearch documents. input { elasticsearch { host => \"localhost\" # Store ES document metadata (_index, _type, _id) in metadata docinfo_in_metadata => true } } output { elasticsearch { document_id => \"%{[@metadata][_id]}\" index => \"transformed-%{[@metadata][_index]}\" type => \"%{[@metadata][_type]}\" } } Create your own id from your event dataOut of the box, Elasticsearch provides an efficient way to create unique IDs for every documents that you are inserting. In most cases, you should let Elasticsearch gen","locales":"","title":"Make Your Config Cleaner and your Log Processing Faster with Logstash Metadata"}
{"index":{}}
{"author":"Michael McCandless","category":"","publish_date":"2015-09-01T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2015-09-01","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top NewsElasticsearch 2.0.0-beta1 released, with over 2,500 PRs from 469 contributors, get it while its hot! — elastic (@elastic) Elasticsearch Core Apache Lucene Watch This Space, where we'll share more news on the whole ELK ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - September 1 2015"}
{"index":{}}
{"author":"Spencer Alger","category":"News","publish_date":"2015-08-31T00:00:00.000Z","url":"/blog/kibana-4-2-beta-1-i-heard-you-like-betas","seo_title":"Kibana 4.2 Beta 1","content":" Have you been trying out the brand new ? Do you want to use Kibana with your test installation? Well, have I got news for you! Today we're releasing a beta for your beta, Kibana 4.2.0-beta1.This release offers support for the first Elasticsearch 2.0 beta along with a handful of features and bug fixes. If you are too excited to stick around, go right ahead and . If you are interested in what you are downloading however, lets take a look at some of the new features.Persistent ColorsStarting in the 4.2 beta, generated colors are bound to the data they represent. This means that you no longer have to worry about the same color meaning multiple things on a single dashboard, hooray! — Thanks ! Server StatusIn order to help people get a better idea of why Kibana may not be able to run, we now include a status page which can be accessed at any time, but also shows up automatically when there is a problem. For instance, if the Kibana server loses connectivity with Elasticsearch: Filter using the legendEach visualization's legend is a useful way to see the groups that data falls into, but now it's also a great way to filter a visualization by one of those groups! Just click on any of the labels and a filter will be added to the filter bar at the top of the page. This works across an entire dashboard, but combine it with the introduced in 4.1 and your filter now applies to all of Kibana.Olson timezone supportOne of the features we are excited about in Elasticsearch 2.0 is the enhanced timezone support in the aggregation. Support for Olson timezone identifiers allows Elasticsearch to more accurately understand the UTC offset of every single event, even when a histogram covers a shift in UTC offset like the start of daylight savings time. This means that Kibana can better show when events happened in your local timezone. Structural ImprovementsKibana 4.2 also introduces several structural improvements including: to all of the contributors who have helped us pull this beta together. There are many other features, tweaks, and bug fixes that we couldn't mention here and they wouldn't be possible without you.If you have questions (or want to help answer some) please check out the  forums. If you find a bug, please file it on . Make something great? Tell about your #kibana dashboards on twitter. Happy data hunting! ","locales":"","title":"Kibana 4.2 Beta 1: I Heard You Like Betas"}
{"index":{}}
{"author":"Dale McDiarmid","category":"Engineering","publish_date":"2015-08-31T00:00:00.000Z","url":"/blog/when-and-how-to-percolate-1","seo_title":"When and How to Scale Percolator","content":" As we move beyond search at Elastic, we are seeing a growing number of the requirements to take action based on changes in your data. Back in June, we released the first version of to bring alerting and notification functionality to the Elastic product stack, based on the principle that users and administrators should be able to alert on anything for which they can can create a query. Many of you who are familiar with the existing capabilities of Elasticsearch perceived some functional overlap with the percolate API [reference: ] and have expressed confusion as to when each is applicable. Although both match documents against queries and can be used to create alerting-type functionality, they each provide discrete capabilities and solve distinct use cases. This post aims to clarify when each is appropriate, before discussing the sizing and scaling considerations for percolator. Should I Watch or Percolate?Traditionally, you design documents based on your data, store them into an index, and then define queries via the search API in order to retrieve them. Percolator works in the opposite direction. First, you store queries into an index and then, via the percolate API, you match individual documents against these queries. Percolator provides the capability to ascertain which queries a discrete document would match. It does not consider the corpus of documents and thus does not allow the observation of trends (e.g. using aggregations to spot if avg volume of requests have gone up in last hour). A common misconception for those new to Percolator is that alerting is a side-effect of inserting documents, similar to a database trigger. This is not the case. There is no requirement to index percolated documents and any alerting-type functionality is left to the user. Percolator is also not limited to alerting use cases. For example, queries can represent categories and the percolation process is then used to classify documents which may or may not be inserted into Elasticsearch. By indexing the queries, Percolator demonstrates linearly scalability with respect to the number of registered queries, modulated by the number of concurrent shards processing each request.   Because the Percolator evaluates a single document at a time, it is less suited for applications that must compare multiple documents at once. Furthermore, Percolator brings a real-time capability to matching in comparison to the scheduled nature of Watcher. Percolator is well suited to use cases where the requirement focuses on the need to spot new documents of interest in isolation to the corpus in real-time. Comparatively, Watcher periodically schedules pre-defined queries for executing against a document corpus before sending alerts and notifications on matches. It is able to consider either properties of the entire corpus or even a subset of documents (e.g. those recently indexed, thus providing trend identification). Functionally, Watcher takes this further by allowing the user to take actions on the matches to the above question without the requirement to write code. Watcher exhibits excellent scaling properties with respect to both the total document corpus size and the number of new documents. It is, however, less suited to requirements where there is a need for real time matches. Consider the following examples,  An e-commerce website, wishes to provide the capability for users to save searches. When a new item is listed, those users whose searches match the product are notified that the item is available for purchase. Approximately 1000 new products are listed per minute. The site has approximately 3 million users, who on average save 0.5 queries each. Searches for products consist of a product type, key terms, price range and geographical distance. Users should be notified within 1 minute of a new product being listed. A high number of queries preclude Watcher without significant hardware investment. The ","locales":"","title":"When and How To Percolate - Part 1"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2015-08-31T00:00:00.000Z","url":"/blog/2015-08-31-where-in-the-world-is-elastic","seo_title":"Where in the World is Elastic, August 24 2015","content":" Welcome to  As we conclude the month of August, we continue to have meetups occurring worldwide. Read on to see where we are!Upcoming MeetupsAugust 31: September 2: September 3: That's it for this week. Stay tuned for Elastic happenings next week! - The Elastic Team P.S.   if you're interested in hosting a meetup or are giving a talk about Beats, Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic? - August 31, 2015"}
{"index":{}}
{"author":"Costin Leau","category":"Engineering","publish_date":"2015-08-27T00:00:00.000Z","url":"/blog/elasticsearch-for-apache-hadoop-2-2-0-m1","seo_title":"","content":" Hot on the heels of Elasticsearch 2.0.0-beta1 (check it out!), we are pleased to announce the of Elasticsearch for Apache Hadoop, or simply ES-Hadoop, and . ES-Hadoop 2.2.0-m1 This first milestone in the 2.2 branch introduces, among bug fixes, several new features: Want to use the new goodies in Elasticsearch 2.0 in Spark or Hadoop? That’s what ES-Hadoop is here for! Note that compatibility with Elasticsearch 1.x is preserved in ES-Hadoop 2.2.x. At start-up, ES-Hadoop checks whether there’s another version used in the job and properly informs the user. This helps the not-so-uncommon case of multiple versions being picked arbitrarily from different sources (task vs jar vs runtime vs Java classpath). While ES-Hadoop does provide data formatting, it now detects whether one is provided (such as ) and uses that instead allowing richer formatting of s. Future versions might be extended to support the new package in Java 8. Those building the source manually will notice the system has been upgraded to the latest stable version, resulting in some nice speed improvements and simplifications (such as being able to use Java 8 for building the Scala modules). ES-Hadoop 2.1.1 Alongside the milestone, ES-Hadoop 2.1.1 was . It is the latest stable version of ES-Hadoop and contains important bug-fixes and improvements such as: The operations in Elasticsearch Spark have been pushed-down resulting in instant response no matter how big the . Fatal network errors are detected early and reported better, simplifying diagnosis. A previous bug sometimes prevented Pig support from being used across Hadoop versions. This has now been addressed. Feedback ES-Hadoop 2.0.x and 2.1.x users are recommended to upgrade to while those interested in trying out Elasticsearch 2.2.0, should take ES-Hadoop for a spin. Let us know what you think on Twitter (@elastic) or on . You can report any problems on the GitHub . ","locales":"","title":"Elasticsearch for Apache Hadoop 2.2.0-m1 and 2.1.1"}
{"index":{}}
{"author":"Clinton Gormley","category":"Engineering","publish_date":"2015-08-25T00:00:00.000Z","url":"/blog/elasticsearch-unplugged","seo_title":"Elasticsearch unplugged - Networking changes in 2.0","content":" You start Elasticsearch on your laptop. You issue a quick to clear out yesterday’s experiments. Then you notice the plaintive cries issuing from the mouths of your developer colleagues and you wonder what pain they are suffering…Elasticsearch has always been friendly and approachable. Testing out how a multi-node cluster works is as easy as starting up a few instances on your laptop: they auto-discover each other using multicast, form a cluster, and start sharing the load. But at times Elasticsearch has been a bit too friendly. Try starting Elasticsearch on your laptop at a conference, and you could easily find yourself participating in a 100 node cluster.The soon-to-be-released 2.0.0-beta1 comes with some networking changes that make Elasticsearch choosier about who it talks to, while still maintaining the easy out-of-the-box developer experience. ","locales":"","title":"Elasticsearch unplugged - Networking changes in 2.0"}
{"index":{}}
{"author":"Martin Johansson","category":"User Stories","publish_date":"2015-08-27T00:00:00.000Z","url":"/blog/improving-user-intelligence-with-the-elk-stack-at-sca","seo_title":"Improving User Intelligence with the ELK Stack at SCA","content":" is a leading global hygiene and forest products company, employing around 44,000 people worldwide. The Group (all companies within SCA) develops and produces sustainable, and . Sales are conducted in about 100 countries under . Each brand each has its own website and its own search.At SCA we use Elasticsearch, Logstash, and Kibana to record searches, clicks on result documents and user feedback, on both the intranet and external sites. We also collect qualitative metrics by asking our public users a question after showing search results: “Did you find what you were looking for?” The user has the option to give a thumbs up or down and also write a comment.What is logged?All search parameters and results information is recorded for each search event: the query string, paging, sorting, facets, the number of hits, search response time, the date and time of the search, etc. Clicking a result document also records a multitude of information: the position of the document in the result list, the time it took from search to click and various document metadata (such as URL, source, format, last modified, author, and more). A click event also gets connected with the search event that generated it. This is also the case for feedback events.Each event is written to a log file that is being monitored by Logstash, which then creates a document from each event and pushes them to Elasticsearch where the data is visualized in Kibana.Why?Due to the extent of information that is indexed, we can answer questions from the very simple, such as “What are the ten most frequent queries during the past week?” and “Users who click on document X, what do they search for?” to the more complex like “What is the distribution of clicked documents’ last modified dates, coming from source S, on Wednesdays? The possibilities are almost endless!The answers to these questions allow us to tune the search to meet the needs of the users to an even greater extent and deliver even greater value. Today, we use this analysis for everything from adjusting the relevance model, to adding new facets or removing old ones, or changing the layout of the search and result pages.Experienced value – more than “just” logsRecording search and click events are common practice, but at SCA we have extended this to include user feedback, as mentioned above. This increases the value of the statistics even more. It allows an administrator to follow up on negative feedback in detail, e.g. by recreating the scenario. It also enables implicitly evaluated trial periods for change requests. If a statistically significant increase in the share of positive feedbacks is observed, then that change made it easier for users to find what they were looking for. We can also find the answer to new questions, such as “What’s the feedback from the users who experience zero hits?” and “Are users more likely to find what they are looking for if they use facets?\"And server monitoring as well!Benefits of the ELK Stack What this means for SCA is that they get a search that is ever improving. We, the developers and administrators of the search system, are no longer in the dark regarding what changes actually change things for the better. The direct feedback loop between the users and administrators of the system creates a sense of community, especially when users see that their grievances are being tended to. Users find what they are looking for to a greater and greater extent, saving them time and frustration.ConclusionWe rely heavily on Elasticsearch, Logstash and Kibana as the core of our search capability, and for the insight to continually improve. We're excited to see what the 2.0 versions bring. The challenge is to know what information you are after and create a model that will meet those needs. Getting the ELK platform up and running at SCA","locales":"de-de","title":"Improving User Intelligence with the ELK Stack at SCA"}
{"index":{}}
{"author":"Michael McCandless","category":"","publish_date":"2015-08-25T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2015-08-25","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top NewsWhy did we remove the Delete by Query API in 2.0? — elastic (@elastic) Elasticsearch Core Apache Lucene Watch This Space, where we'll share more news on the whole ELK ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - August 25 2015"}
{"index":{}}
{"author":"Christian Strzadala","category":"Engineering","publish_date":"2015-08-28T00:00:00.000Z","url":"/blog/ttl-documents-shield-and-found","seo_title":"","content":" Elasticsearch allows for many types of document mappings. An interesting mapping is the or mapping.  This mapping allows us to set a expiry time for a document. Once the time has past, the expired documents are deleted.  Note that in the current version of Elasticsearch, is deprecated. This post will look at the current mapping and how you can try to avoid using them. Let's define the mappingLet’s say we are creating an index structure for a store called that has regular specials that end at various time periods. Here is an example mapping with a for a regular document, in this case called POST /crazy-shop { \"mappings\": { \"special\": { \"properties\": { \"message\": { \"type\": \"string\" } }, \"_ttl\": { \"enabled\": true, \"default\": \"10s\" } } } } The is set to and defaults to . We can test this out by indexing the following document: PUT /crazy-shop/special/1 { \"message\" : \"An awesome new special\" } We can get this document with the following request: GET /crazy-shop/special/1 which will return: { \"_index\": \"crazy-shop\", \"_type\": \"special\", \"_id\": \"1\", \"_version\": 1, \"found\": true, \"_source\": { \"message\": \"An awesome new special\" } } So after 10 seconds, this document should be deleted, right? . How does expiring a document actually work?Elasticsearch has a background system process that will check for all expired documents and add them to a bulk delete request. This process by default runs every 60 seconds. This means that setting the for a document to anything under 60 seconds won’t actually delete the document until 60 seconds has past. This interval can be adjusted though, through the setting , so you can adjust this if needed. Delete operations are expensiveDeleting documents doesn’t remove them from the index, but rather marks them as and filters them out at query time. Large amounts of bulk deletions can result in large Lucene index segments and in turn a large amount of data to handle until segment merges occur and deleted documents are finally removed from the index. Refer to for more information around Elasticsearch and Lucene indexes. With this in mind, our advice is to try and avoid using TTL. The expense that TTL causes Elasticsearch can in most circumstances be avoided using time based indices. The suggested re-implementation is to use a combination of two things: You may think your data model doesn’t allow for time-based indices or there’s some overhead around managing time-based indices. We’ll demonstrate a modified data model below and there are already available for handling time-based indices. Example time based indicesContinuing our example, below is an example that allows for: An alias is also included to use a constant index name. PUT /_template/crazy-shop-template { \"order\": 0, \"template\": \"crazy-shop-*\", \"mappings\": { \"special\": { \"properties\": { \"message\": { \"type\": \"string\" }, \"expiry\": { \"type\": \"date\", \"format\": \"yyyy-MM-dd'T'HHmmssZ\" } } } }, \"aliases\": { \"crazy-shop\": {} } } We can add some documents to an index with a date in the index name which will create a new index with the document added to it: PUT /crazy-shop-2015-08-24/special/1 { \"message\" : \"An awesome new special\", \"expiry\" : \"2015-08-24T202000+1000\" } PUT /crazy-shop-2015-08-24/special/2 { \"message\" : \"Another awesome new special\", \"expiry\" : \"2015-08-24T203000+1000\" } Next, we can query the index using a filter for the field with a timestamp, which will filter out documents we don’t need anymore. POST /crazy-shop/_search { \"query\": { \"filtered\": { \"query\": { \"match_all\": {} }, \"filter\": { \"range\": { \"expiry\": { \"gte\": \"2015-08-24T202500+1000\" } } } } } } We can delete the daily index manually with the following request: DELETE /crazy-shop-2015-08-24 Or use tools like to automatically delete indices. Again, deleting the entire index is cheaper than having lots of deleted documents in an index. When time based indices may not workThere may be times when the use of time based indices may not work for you","locales":"","title":"TTL Documents, Shield and Found"}
{"index":{}}
{"author":"Haley Eshagh","category":"User Stories","publish_date":"2015-08-25T00:00:00.000Z","url":"/blog/describe-elasticsearch","seo_title":"What is Elasticsearch? Between the Curly Braces.","content":" Imagine you’re walking down the street. You’re thinking about query optimization, upgrading your cluster, what you should have for lunch — you know, the usual. Suddenly, you’re stopped by someone, your average Joe, and he asks, “Hey, what is this Elasticsearch thing I keep hearing about?” What would you say? How would you describe Elasticsearch? Before you rack your brain for a good answer, we captured a few responses on video at Elastic{ON}. We weren't sure how people would answer, but, boy, we’re glad we asked. Elasticsearch can be many things to many people. It helps , , , (lunch problem solved!), and more. While Elasticsearch certainly started as a search engine, it’s become more than just that. Elasticsearch helps you connect the proverbial dots and make sense of your data — whether it’s server logs, mortgage rates, genetic code, or the complete works of Shakespeare. So, now it's your turn. How would you describe Elasticsearch? Give it a go on Twitter (with the added bonus of a 140-character limit) using #elastic. Lastly, we’d like to say thanks to all of the folks who dropped by our recording studio (OK, maybe it was a trailer in a parking lot — we improvised) at Elastic{ON} to share their stories. We had a fantastic time and look forward to hearing more at an upcoming event. () ","locales":"","title":"Between the Curly Braces: Describe Elasticsearch"}
{"index":{}}
{"author":"Jon Budzenski","category":"Engineering","publish_date":"2015-08-24T00:00:00.000Z","url":"/blog/kibana-custom-field-formatters","seo_title":"","content":" Kibana 4.1 introduced a new feature called field formatters, giving us the ability to visually convert fields on the fly.  Field formatters provide a great way to display data in a different way without changing how it is stored.  For a more in depth overview of what field formatters do, you can read a previous blog post . The goal for this post is to walk through the steps of creating a new formatter.  We will start by going over the field formatter API, then implement a basic formatter that will draw attention to the word \"error\", and finally generalize our solution. Getting StartedInstructions on pulling down the Kibana development environment can be found at the . Starting from the Kibana root directory, field formatters are stored in . The resulting folder structure looks like: /stringify |--type  //Contains different formatters |--icons |--editors  //HTML used by the formatter to request or display additional information |--__tests__ |--register.js //Every formatter is registered with the application here In Kibana 4.1, formatters can be found at . If using 4.1, paths in the code samples will need to be updated. Let’s create a new file called in the folder and add some initialization code: define(function (require) { return function HighlightFormatProvider(Private) { var _ = require('lodash'):  var FieldFormat = Private(require('ui/index_patterns/_field_format/FieldFormat')):  _.class(Highlight).inherits(FieldFormat):  function Highlight(params) { Highlight.Super.call(this, params):  } Highlight.id = 'highlight':  Highlight.title = 'Highlight':  Highlight.fieldType = ['string']:  Highlight.prototype._convert = { text: _.escape, html: _.escape }:  return Highlight:  }:  }):  A class extending FieldFormat is implemented for each field format.   is used internally by Kibana to keep track of the formatter and should not conflict with any other formatter id.   is the displayed text when selecting from a list of formatters, and describes what types of fields this formatter is available for. is where the formatting happens. It has methods for converting both text and html. The text method is used for tooltips, filters, legends, and axis markers. The html method is used for search tables. Both take a field value as input, and return what we want displayed visually. If you want both methods to behave the same, can be assigned as a function instead. Adding code to draw attention to errors looks like: Highlight.prototype._highlight = function (val, replace) { return _.escape(val).replace(/(error)/g, replace):  }:  Highlight.prototype._convert = { text: function(val) { return this._highlight(val, function convertToUpperCase(match) { return match.toUpperCase():  }):  }, html: function(val) { return this._highlight(val, '<mark>$&</mark>'):  } }:  Any time a field has the text \"error\", we either wrap it in a mark element or convert it to uppercase depending on whether HTML or text is requested. It's important to use here, we want to avoid HTML injection and cross-site scripting attacks. We also need to register this field formatter with the application. In we add: fieldFormats.register(require('ui/stringify/types/Highlight')):  In the future, we (the Kibana team) will be working on providing this functionality as a plugin, which will help simplify the registration process. We should now be able to add Highlight as a field formatter on fields with a string type! Testing it on the Discover page shows us it is working: GeneralizeThis works, but what if we want to highlight more than just errors? Instead of making a new field formatter for every input, let’s match against an inputted regular expression. In the editor folder, add a new file called with the following: <div class=\"form-group\"> <label>Pattern</label> <input class=\"form-control\" ng-model=\"editor.formatParams.pattern\"/> </div> Back in Highlight.js we need to define as our editor and update our method to use its input field as our pat","locales":"","title":"Writing Custom Field Formatters for Kibana"}
{"index":{}}
{"author":"Alexander Reelsen","category":"","publish_date":"2015-08-18T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2015-08-18","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top NewsSee how GuideStar, with help from , aggregates, analyzes, & provides data for grantmakers & benefactors: — GuideStar USA (@GuideStarUSA) Elasticsearch Core Apache Lucene Watch This Space, where we'll share more news on the whole ELK ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - August 18 2015"}
{"index":{}}
{"author":"Michael McCandless","category":"Engineering","publish_date":"2015-08-18T00:00:00.000Z","url":"/blog/core-delete-by-query-is-a-plugin","seo_title":"","content":" As of Elasticsearch's 2.0.0-beta1 release we have the previous core implementation of the and with a new . Here we explain why we did this and how the plugin's implementation differs from the previous core implementation. If you use the Delete by Query API, after upgrading to 2.0, just install the plugin and then  : bin/plugin install delete-by-query Why did we do this? We take the quality of Elasticsearch's core APIs seriously, and the previous implementation of Delete by Query had major issues that could not easily be fixed: In contrast, the has a fully safe implementation: it runs to find all ids matching the query, and then uses the to delete them. This implementation is necessarily slower, especially if the query deletes many documents. Be sure to test your application if you delete many documents using this API, and consider switching to a different approach where you can . The  describes more details about the motivation and differences in the new implementation.A minimal Elasticsearch core Switching to a plugin was not an easy decision: many users were able to use the faster core Delete by Query without issue. Still, the danger was always there, and a non-trivial number of users did hit the serious problems above. Furthermore, Elasticsearch's core must remain reliable and lean. Any feature which can be built on top of the other core APIs really does not belong in the core, especially if it's buggy. All features in the core should be ironclad, and Delete by Query, despite its popularity and high performance, simply wasn't.We take resiliency and quality seriously enough to make hard tradeoffs like this one, when necessary.Deleting a Mapping is gone has also because it could lead to index corruption if the same field names were later re-used in a new type but with different mappings. However, you can still delete all documents for a given type using the plugin with a  against that type, or strongly consider changing your approach to use a separate index instead of different types within one index. ","locales":"","title":"The Delete by Query API Is now a plugin"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2015-08-17T00:00:00.000Z","url":"/blog/2015-08-17-where-in-the-world-is-elastic","seo_title":"Where in the World is Elastic, August 17 2015","content":" Welcome to  It is summer time but still we have a few meetups scheduled this week. Read on to see where we are!Upcoming EventsAugust 22-23: - Don't miss talk on on Sunday, August 23 at 10 a.m. & check out the Elastic bouncy castle, too!Upcoming MeetupsAugust 19: August 19: August 22: August 19: August 20: That's it for this week. Stay tuned for Elastic happenings next week! - The Elastic Team P.S.   if you're interested in hosting a meetup or are giving a talk about Beats, Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic? - August 17, 2015"}
{"index":{}}
{"author":"Michael McCandless","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2015-08-12T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2015-08-12","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top NewsSame topic, new expert tips! . presents 'Staying in Control w/Moving Avgs - Pt 2.' — elastic (@elastic) Elasticsearch Core Apache Lucene Watch This Space, where we'll share more news on the whole ELK ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - August 12 2015"}
{"index":{}}
{"author":"Alexander Reelsen","category":"","publish_date":"2015-08-04T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2015-08-04","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top NewsStaying in Control with Moving Avgs: outlier detection via new pipeline aggs — elastic (@elastic) Elasticsearch Core Apache Lucene Watch This Space, where we'll share more news on the whole ELK ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - August 04 2015"}
{"index":{}}
{"author":"Zachary Tong","category":"Engineering","publish_date":"2015-08-12T00:00:00.000Z","url":"/blog/staying-in-control-with-moving-averages-part-2","seo_title":"","content":" Last week, using the new pipeline aggregations.  The demonstration used a very simple dataset, where the trend was very flat, and the spike was very obvious.  In fact, it was so obvious you could probably catch it with a simple threshold. This week, we'll show how the same control chart can be used in more tricky scenarios, such as constantly increasing linear trends, or cyclic/seasonal data ","locales":"","title":"Staying in Control with Moving Averages - Part 2"}
{"index":{}}
{"author":"Zachary Tong","category":"Engineering","publish_date":"2015-08-03T00:00:00.000Z","url":"/blog/staying-in-control-with-moving-averages-part-1","seo_title":"","content":" In manufacturing and business processes, there is a common tool called a . Created in 1920 by Dr. Walter Shewhart, a control chart is used to determine if a process is \"in control\" or \"out of control\".   At the time, Dr. Shewhart was working at Bell Labs trying to improve the signal quality of telephone lines.  Poorly-machined components was a leading cause of signal degradation, so improving manufacturing processes to produce more uniform components was a critical step in improving signal quality. We can graph this and see that the spike (yellow) shoots up past the control limit (green).  In a real system, this is when you send out an alert or email.  Or maybe something more drastic, since this is a nuclear reactor we are modeling : ) ","locales":"","title":"Staying in Control with Moving Averages - Part 1"}
{"index":{}}
{"author":"Clinton Gormley","category":"News","publish_date":"2015-08-26T00:00:00.000Z","url":"/blog/elasticsearch-2-0-0-beta1-released","seo_title":"Elasticsearch 2.0.0-beta1 released","content":" Today, we are excited to announce the release of , based on . This release contains over 2,500 pull requests from 469 committers. 850 of the pull requests are completely new to 2.0. : This is a beta release and is intended for testing purposes only. There is no guarantee that Elasticsearch 2.0.0-beta1 will be compatible with Elasticsearch 2.0.0 GA. You can . Elasticsearch 2.0.0-beta1 has some cool new changes like: Besides the above, there are hundreds of incremental changes in both Elasticsearch and Lucene which make Elasticsearch 2.0 safer, better, and easier. Have a look at these blog posts for more information about this release: ","locales":"","title":"Elasticsearch 2.0.0-beta1 released"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2015-08-24T00:00:00.000Z","url":"/blog/2015-08-24-where-in-the-world-is-elastic","seo_title":"Where in the World is Elastic, August 24 2015","content":" Welcome to  It is summer time but still we have a few meetups scheduled this week. Read on to see where we are!Upcoming MeetupsAugust 24: August 26: August 26: August 26: August 26: That's it for this week. Stay tuned for Elastic happenings next week! - The Elastic Team P.S.   if you're interested in hosting a meetup or are giving a talk about Beats, Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic? - August 24, 2015"}
{"index":{}}
{"author":"Pier-Hugues Pellerin","category":"Releases","publish_date":"2015-08-20T00:00:00.000Z","url":"/blog/logstash-1-5-4-and-1-4-5-released","seo_title":"Logstash 1.5.4 and 1.4.5 released","content":" We are announcing the  of logstash 1.5.4 and 1.4.5 which fixes important security issues. Our recommendation is to upgrade immediately if you are using either of the following features: After the release of 1.5.3, users encountered an issue where Logstash Forwarder was unable to communicate to Logstash instance because of SSL/TLS certificate validation errors. This has been fixed. Typically used to connect two Logstash instances. In such deployments, one Logstash instance is used to collect logs from a webserver and securely transmit them to a central Logstash instance to perform additional filtering and storing. Security FixesWhen using SSL/TLS functionality, Lumberjack output from Logstash 1.5.3 and prior versions did not validate certificate presented by the Logstash instance acting as a server. This exposes a man in the middle vulnerability. We have been assigned  for this issue and have added this vulnerability to our .  Users of Logstash Forwarder are not affected by this particular vulnerability Enhancements Added the ability to update existing ES documents and support of upsert  -- if document doesn't exist, create it (). Thanks to  for contributing this enhancement! output { if [use_case] == \"doc_upsert\" { elasticsearch { host => \"elasticsearch\" protocol => \"http\" action => \"update\" document_id => \"%{[uid]}\" doc_as_upsert => true } } else if [use_case] == \"doc_static_upsert\" { elasticsearch { host => \"elasticsearch\" protocol => \"http\" action => \"update\" document_id => \"%{[uid]}\" upsert => '{\"static_field\": \"demo\"}' } } else if [use_case] == \"doc_dynamic_upsert\" { elasticsearch { host => \"elasticsearch\" protocol => \"http\" action => \"update\" document_id => \"%{[uid]}\" upsert => '{\"use_case\": \"%{[use_case]}\", \"dynamic\": { \"fieldC\": \"%{[dynamic_field][fieldC]}\"}}' } } } Bug fixes Below is a list of bug fixes in core and plugins. For a full list, please check the . FeedbackPlease  Logstash 1.5.4 and let us know what you think on Twitter () or on our forum. You can report any problems on the GitHub  page. ","locales":"","title":"Logstash 1.5.4 and 1.4.5 released"}
{"index":{}}
{"author":"João Alves","category":"User Stories","publish_date":"2015-08-17T00:00:00.000Z","url":"/blog/how-blueliv-uses-the-elastic-stack-to-combat-cyber-threats","seo_title":"","content":" Why the ELK stack? Most companies that are defending themselves against these attacks use some kind of Security Information and Event Management (SIEM) software that allows them to aggregate and correlate data. This software allows them to set up dashboards in order to quickly visualize the information. Elasticsearch, Logstash, and Kibana are a really great toolsets for our clients: Given these characteristics and how well these tools work seamlessly together, many companies take advantage of these technologies as a SIEM. Therefore, it makes a lot of sense to offer our cyber threat intelligence feeds via a Logstash input plugin that allows users to receive real-time insights about cyber threats in just a couple of minutes. In order to get our data feeds into the ELK stack, we developed a Logstash input plugin that periodically collects them for you, letting you focus on the data analysis and making your company more secure. Logstash output configuration allows us to use different indexes to save different information (for instance, bot IPs and crime servers), which makes the dashboard visualization and creation easier. Another critical characteristic of Elasticsearch (taking into account that we currently analyze and collect information about millions of crime servers and infected IPs) is its stunning performance. It indexes all this data and lets our clients search against it quickly. Protect your network with the ELK stack and Blueliv Every day, millions of people worldwide are affected by cyber attacks. This means that your company's safety and therefore your privileged information may be compromised. With Blueliv Logstash input plugin, you can start to monitor and get insights about cyber threats. Our ELK users will be able to access the Blueliv's global intelligence such as malware distribution domains, C&Cs, phishing campaigns, exploit kits, backdoors, infected IPs, affected operating systems, and more at a glance using Kibana dashboards. To get started, we offer a free API for crime servers that contains a subset of our unique cyber threat intelligence, as well as a 14-day trial of our full-featured feeds. A lot of companies (banks, insurance companies, pharmaceuticals, etc) manage sensitive information about their clients and their own business. For such companies, these information leaks may have a huge impact at the financial and customer level, besides the damage to the company's reputation and brand. On the other hand, for companies with hundreds of thousands of employees, distributed across the world, it is really difficult to enforce security policies based on “common-sense”. Attackers know this very well, and will use social engineering techniques (such as phishing attacks that replicate legitimate websites) or other malware distribution techniques, in order to trick the users and thus obtaining information from the infected user, such as credentials, confidential documents, etc. The need to quickly identify, prevent, or mitigate these attacks arises. Blueliv continuously scours and analyses hundreds of sources to provide unique intelligence about verified online crime server conducting malicious activity, infected bot IPs, malware hashes and hacktivism activities. The feeds are offered as an easy to buy solution that provides high-impact results rapidly. The user can understand what attack vectors malicious actors are using, understand potential indicators of compromise (IoC) and deploy mitigation solutions. Although many of the above described companies performed some kind of log analysis, they do not have access to real-time cyber threat insights that would allow them to take action. That is why we launched . If our clients already used the ELK stack for Log analysis it would be easier to install a plugin using the technologies they already know and are used to working with. Taking action against malicious IPs and domains One of o","locales":"","title":"How Blueliv Uses the Elastic Stack to Combat Cyber Threats"}
{"index":{}}
{"author":"Wesley Hall","category":"User Stories","publish_date":"2015-08-13T00:00:00.000Z","url":"/blog/more-signal-less-noise-with-elasticsearch","seo_title":"","content":" By the time you finish reading this sentence, the Signal platform will have received, extracted, analysed, classified and indexed a volume of text equivalent in size to the complete works of Shakespeare. This is a process that continues, in real time, 7 days a week, 365 days a year, allowing users to intelligently strategize and make data-driven decisions. The Importance of Search  As a startup, and lacking the resources of the well-known search behemoths, we needed to create the kind of infrastructure that could manage this data and make it available instantly, and we needed to do this on a budget. Since a primary function of our product is search and discovery, what we really needed was a highly-effective, open source text search infrastructure component that could be relied upon to scale up with our increasing data volumes. We found this component in the fantastic Elasticsearch system. The Signal software system is effectively a large scale data processing pipeline that runs sophisticated text analytics routines. As we receive our incoming stream of news articles and blog posts, each is processed through this pipeline, extracting references to important entities such as organisations, people and places. Articles are classified into topics using advanced models built using our machine learning technology. Finally, articles are grouped into clusters of where they described related events or stories. The result of this processing pipeline is large volumes of text (up to 7 million individual articles per day and growing) with associated meta-data describing the meaningful entities and concepts that have been detected by our system. Once this data has been prepared by our pipeline system it is indexed into an Elasticsearch cluster consisting of 15 fairly substantial, cloud-based server instances making it available for search, exploration and discovery by our users. The decision to place Elasticsearch at the centre of our infrastructure was really a matter of coming to the realisation that search is not just a feature of our product:  it  our product. Our entire offering is based around providing users with the ability to locate relevant information. Our system divided neatly between the extraction of this relevant information and the ability to make effective use of it. Why Elasticsearch?  Previously, search had been much more of a secondary feature as we focused on the delivery, inbox-style, of the relevant articles to the interested users. The shortcomings of this approach quickly became obvious, however, as we began to consider where our users may want to go next. The feeds themselves offer an excellent jumping off point into further exploration and discovery of the available content but this service was difficult to provide without an effective search system. Up until this point we had been using the AWS cloud search product to provide the search capabilities of our system, but cloud search lacked the advanced features that we needed such as complex aggregation or sophisticated monitoring and control. We needed something with these features and more… Elasticsearch was an obvious forerunning candidate to provide the infrastructure required for this project. Having worked (albeit briefly) with Elastic CTO Shay Banon as part of an eBay project, I was already aware of the degree of technical competency found in the Elasticsearch software. I knew that Elastic really would mean  and having confidence in the initial creators is something that takes me a long way when it comes to my technology selection process. We began a proof of concept implementation towards the end of 2014, and like many successful PoCs this eventually grew in to our V2 product. Reimagining the View on Data Reimagining our product as being a multi-faceted search system has changed its nature and how we approach its evolution. We move forward by providing our users with different views and perspectives on their d","locales":"","title":"More Signal, Less Noise with Elasticsearch"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2015-08-10T00:00:00.000Z","url":"/blog/2015-08-10-where-in-the-world-is-elastic","seo_title":"Where in the World is Elastic, August 10 2015","content":" Welcome to  It is summer time but still we have a few meetups scheduled this week. Read on to see where we are!Upcoming MeetupsAugust 10: August 12: August 13: August 11: That's it for this week. Stay tuned for Elastic happenings next week! - The Elastic Team P.S.   if you're interested in hosting a meetup or are giving a talk about Beats, Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic? - August 10, 2015"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2015-08-03T00:00:00.000Z","url":"/blog/2015-08-03-where-in-the-world-is-elastic","seo_title":"Where in the World is Elastic, August 3 2015","content":" Welcome to  Read on for the full overview of this week's events and meetups happening all around the world!  Upcoming Events August 1-6: - Join a community speaker for his talk on , Wednesday, August 6, 12:45 p.m. - 3:15 p.m. August 4-5: - say hi to who is attending in the hallway track and ask any question you may have around ELK! August 6-9:  - and again, will be present and ready for any questions you may have. Upcoming Meetups August 4: August 5: August 6: August 6: August 4: August 6: That's it for this week. Stay tuned for Elastic happenings next week - there's much to come!  - The Elastic Team P.S.   if you're interested in hosting a meetup or are giving a talk about Beats, Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic? - August 3, 2015"}
{"index":{}}
{"author":"Daniel Palay","category":"User Stories","publish_date":"2015-08-13T00:00:00.000Z","url":"/blog/better-data-makes-a-better-world","seo_title":"","content":" There’s a lot of talk in the tech industry of having the next big thing that’s going to change the world. I think we can all agree there are some times where ‘changing the world’ is an understatement - think about what you would do without your smartphone or access to something like Google, GPS navigation, or even Facebook and Twitter. More often than not though, it’s a mixture of hope and hyperbole and I think the world is starting to catch on - look no further than the current trend of American television commercials (including with Jeff Goldblum) satirizing our industry.  But every once in a while, we’re lucky enough to come across an organization where even hyperbole doesn’t do justice for their work.  From their Williamsburg, Virginia headquarters, the team at GuideStar (with a little help from the Elastic platform) aggregates, analyzes, and provides data for grant makers and benefactors all over the world. Through their free and subscription-based services, GuideStar makes it easier for prospective donors to find reputable charities and nonprofits (like man animal shelters across the United States participating in this weekend’s campaign) to which they could donate their time and money.  Even more than that, GuideStar is revolutionizing the way nonprofits can now benchmark their own effectiveness within their area of influence by providing data to customers on how similarly focused nonprofits are making an impact around the world. Using this data, nonprofits are now more able to make critical adjustments on the fly which can translate into instant improvements to operations and effectiveness within their communities. I had the pleasure of meeting Shane Ward - GuideStar’s Data Architect - this past February, and it didn’t take long to see his passion for changing the world come through. Don’t take my word for it though, just let our latest customer spotlight be your guide as Shane narrates you through their daily journey to change the world by leveraging Elasticsearch to provide . ","locales":"","title":"Better Data Makes For A Better World"}
{"index":{}}
{"author":"Jay Modi","category":"Releases","publish_date":"2015-08-10T00:00:00.000Z","url":"/blog/shield-1-3-2-released","seo_title":"","content":" Today, we are pleased to announce the bugfix release of Shield 1.3.2. This Shield release contains several bug fixes, most notably fixes for forwarding audit events to remote clusters and ensuring that node startup is not aborted due to a LDAP connection failure. For a full list of fixes, take a look at the . We recommend that existing Shield users upgrade.For a new installation, download Shield 1.3.2 :  to upgrade from a prior version of Shield, please follow the . ","locales":"","title":"Shield 1.3.2 Released"}
{"index":{}}
{"author":"Tal Levy","category":"Engineering","publish_date":"2015-08-03T00:00:00.000Z","url":"/blog/logstash-jdbc-input-plugin","seo_title":"","content":" Ever want to search your database entities from Elasticsearch? Now you can use Logstash to do just that! In this blog we introduce the , which has been created to import data from any database that supports the . Below, we show you few examples of using this plugin. Getting StartedInstallationbin/plugin install logstash-input-jdbc Driver SupportPopular databases like Oracle, Postgresql, and MySQL have compatible JDBC drivers that can be used with this input. This plugin does not come packaged with any of these JDBC drivers out of the box, but is straightforward to download. You can then configure the plugin to use the desired jdbc driver library. The setting and are used to load the library path and the driver's class name. Lets get started with the examples! Example 1: Simple Postgres InputHere is an example of how you get started reading from a local  database. As a prerequisite,  the Postgresql JDBC drivers to use with the plugin. Setting Up The DatabaseBefore we get started, let's create a table called and populate it with some contacts! create table contacts ( After this runs, here are the contents in the database in table form. Logstash ConfigurationWe can go ahead and output all these events to the console with this sample Logstash configuration: # file: simple-out.conf input {     jdbc {         # Postgres jdbc connection string to our database, mydb         jdbc_connection_string => \"jdbc:postgresql://localhost:5432/mydb\"         # The user we wish to execute our statement as         jdbc_user => \"postgres\"         # The path to our downloaded jdbc driver         jdbc_driver_library => \"/path/to/postgresql-9.4-1201.jdbc41.jar\"         # The name of the driver class for Postgresql         jdbc_driver_class => \"org.postgresql.Driver\"         # our query         statement => \"SELECT * from contacts\"     } } output {     stdout { codec => json_lines } } Now we can run Logstash and see the results! Awesome, we read data from Postgresql! Up next, we will demonstrate two examples of how you may use this plugin in the context of Elasticsearch. Example 2: Synchronizing Data In Your Table To ElasticsearchIn the case that we are using our database as an input source for Elasticsearch, we may be interested in keeping our existing documents in-sync with our data as the database undergoes updates. In this case, we can simply index our rows in Elasticsearch with unique ids such that any time we re-index them, they will just update. This way, we prevent Elasticsearch from assigning a new ID for each record and generating duplicates! Let's do a quick check to see that \"Sam\" was indexed into Elasticsearch So far we just saw how to use a query to fetch results from a database query, but what if we want to update our index with new changes? What if some of our contacts changed emails, or we want to update someone's last name? Here is a sequence of changes that we can apply to our table and later verify the behavior we want in the resulting Elasticsearch index. Now we can run Logstash with the same configuration. When we do the same query as before, we will  notice that our document containing Sam has been updated and is now Using this method, we can re-index our table into Elasticsearch without ending up with duplicates. One thing to note is that we are not able to capture deletes to documents under this scheme. Example 3: MusicBrainz Demo is an open music database containing up-to-date information about artists, their works, and everything in-between. You can learn more at .  MusicBrainz graciously hosts a biweekly data dump of their database . This data is 1.8GB with information about around 18 million tracks How to get the MusicBrainz data You must first run your own mirror of the MusicBrainz database. This can be achieved using a tool called . The project's repo has instructions on syncing","locales":"","title":"INSERT INTO LOGSTASH SELECT DATA FROM DATABASE"}
{"index":{}}
{"author":"Konrad Beiske","category":"Engineering","publish_date":"2015-07-30T00:00:00.000Z","url":"/blog/packetbeat-on-found","seo_title":"Packetbeat on Found","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud. Packetbeat is the tool that lets you ship your network data into Elasticsearch so you can visualize them in Kibana. How Does It Work?Packetbeat taps into the network stack on the machine it's running on, analyzes all the traffic for known protocols and extracts useful information from each packet, before shipping that information into Elasticsearch. Currently it supports HTTP, PostgreSQL, MySQL, Redis and Thrift. Packetbeat is a competitor to Logstash, you say? Well, yes and no. There is some overlap between Packetbeat and Logstash, they both feed data into Elasticsearch, but their use cases are still slightly different. Packetbeat focuses on network traffic while Logstash focuses on logs. Both are capable of extracting metrics and Packetbeat also has a sibling called Filebeat that can consume log files. Filebeat can either feed logs directly into Elasticsearch or forward them to Logstash for centralised preprocessing. InstallingInstalling Packetbeat is very straightforward. Download the package matching your platform from and install it with your package system or simply extract the archive if you opted for one of the or files. ConfiguringOnce Packetbeat is installed, it is time to edit . The Debian and the RPM packages install this file directly to , but the generic packages just leave it in the directory you extracted it to. ShipperThere are three noteworthy properties in this section, , and . The attribute is meant to uniquely identify the server Packetbeat is monitoring and it defaults to the host name. You can safely leave it blank or just give a name that makes sense to you. The attribute allows you to specify tags that will be applied to all data originating from this server. Use it for what makes sense to you, but tags like test, staging, production, database or application are not uncommon. In the end the tags are useful when you want to filter data from different groups of servers. InterfacesA short section, but no less important. This is where you specify how Packetbeat will integrate with the networking stack on the server. At minimum, you should specify which devices to monitor. On my Mac, I used something as simple as this: interfaces: device: en0 The device names are platform specific and you should choose what matches your platform. Run in your terminal for a list of devices. On Macs your Wi-Fi and your ethernet card are usually named either or and Linux and are common. There is also a special name called where Packetbeat will connect with all available devices. Packetbeat supports a few more options in this section, each with different trade-offs that are out of scope for this article, but before deploying into production, have a look at . ProtocolsPacketbeat currently supports monitoring HTTP, PostgreSQL, MySQL, Redis and Thrift. For all protocols there is one mandatory parameter, . For demo purposes, declaring ports and protocols in use is usually sufficient, but in particular for the HTTP protocol there are a few things to be aware of. The parameter allows you to specify form fields in GET or POST requests that should not be indexes. This is important since some fields may contain sensitive data like passwords in clear text. A very common setup is to have a standard HTTP server, like Apache or Nginx in front of your application server and then let this server handle things like SSL termination. One disadvantage of this is that it hides the client IP that request originated from. Luckily, both Apache and Nginx and most other servers you would want to use in such a scenario support adding the client IP in a header on each request. Packetbeat can then be configured with the parameter to extract this information. Terminating SSL before it reaches the application server is also the only way to let Packetbeat monitor services provided over HTTPS. OutputLike Logstas","locales":"","title":"Packetbeat on Found"}
{"index":{}}
{"author":"Jean-Pierre Paris","category":"User Stories","publish_date":"2015-07-31T00:00:00.000Z","url":"/blog/how-elasticsearch-helped-orange-to-build-out-their-website-search","seo_title":"","content":" A web search engine is not a monolithic application. The picture below shows that there are many different types of data that contribute to building a useful and attractive response page such as a newspage (a), “did you know” section (b), French web documents (blue) with a sub-section of associated articles (red) (c) , videos (d), and the buzz (e). In addition to the web search index, we have developed specialized “vertical engines” for each type of data, like weather, recent films, or sports-related news. Each of these vertical engines are built on a dedicated corpus that is much smaller than the main French web source of documents. a) b) c) d) e) A History of Our Legacy Since its beginning more than 15 years ago, the Orange French search engine — has evolved by developing its own technology and by integrating open source frameworks. Back in 2013, each vertical engine was built on proprietary solutions or older versions of the Sphinx search engine. Moreover, every single vertical engine ran on its own set of hardware, including redundancy. This was expensive and difficult to manage, but even worse was that the complexity increased every time a new vertical engine, or new feature, had to be deployed. Last, but not least, the job of the team that runs these engines became more and more complex because of the various technologies in use. After living with this complexity for years, we realized that we needed to reduce the number of different technologies and improve our ability to quickly add new features. Finding a New Universal Engine to Power Our Search Platform Selecting a new technology on which to base all of our vertical engines was a difficult, but important choice for us. Different teams have developed the vertical engines, and they each had been working with different search technologies. To encourage them to switch, we had to simplify the interface, and ensure that the technology we selected could meet the many technical requirements, like high availability, high throughput and low latency. During summer 2013, we evaluated both Elasticsearch and Solr. It quite quickly came down to Elasticsearch mainly because of the consistent, comprehensive API and the fact that it was designed from its beginning to be, well, elastic. Elasticity — horizontal scalability — was one of the key requirements for our migration. Even though our initial roll-out was starting with a single vertical engine, we were selecting a technology on which all of our future vertical engines and features would be built. As such, the technology had to be able to scale to handle new vertical engines, but it also had to be able to grow along with our user base. The first Elasticsearch cluster went live at the end of 2013, with just 2 indexes and less than one million documents. Today, we have 3 clusters, the biggest having 50 million documents on 20 virtual machines (8CPU, 20GB RAM and 100GB HDD). The primary size of these indices is 150GB, and we're able to process hundreds of requests per second with latency rates under 200ms, all while running on VMs rather than dedicated hardware. Moving away from our legacy interface was made easy by the Elasticsearch REST JSON interface as JSON parsing and HTTP clients are easy to develop in almost every language. Moreover, Elastic provides client libraries for mainstream languages, which simplifies our interaction with Elasticsearch even more by hiding the low level of JSON parsing and HTTP interactions. Thanks to the simple client libraries, our internal customers are now moving to Elasticsearch for the vertical engines. Because we can rapidly deploy new Elasticsearch indexes and clusters, it is easier than ever for our internal teams to create new vertical engines, new features, and handle more data. Based on our experience so far, we are confident that we can operate Elasticsearch in a demanding environm","locales":"fr-fr","title":"How Elasticsearch Helped Orange to Build Out Their Website Search"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2015-07-29T00:00:00.000Z","url":"/blog/elasticsearch-1-7-1-and-1-6-2-released","seo_title":"","content":" Today, we are pleased to announce bug fix releases of based on , and . These releases contain a fix for a rarely occurring but important bug which can cause data loss. .You can download them and read the full changes list here:The bug in question () can, in very rare circumstances involving multiple simultaneous node failures or restarts, cause all copies of a shard to be deleted from the cluster. This bug was introduced in 1.5.0.This release also contains a fix for CIDR mask conversion for IPv4 addresses, and a bug that prevented Shield users from using the more-like-this API, plus a few more changes whose details can be found in the .Please , try it out, and let us know what you think on Twitter () or on our . You can report any problems on the . ","locales":"","title":"Elasticsearch 1.7.1 and 1.6.2 released"}
{"index":{}}
{"author":"Leslie Hawthorn","category":"","publish_date":"2015-07-29T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2015-07-29","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top News — elastic (@elastic) Elasticsearch Core Apache Lucene Watch This Space , where we'll share more news on the whole ELK ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - July 29 2015"}
{"index":{}}
{"author":"Joe Fleming","category":"Engineering","publish_date":"2015-07-31T00:00:00.000Z","url":"/blog/stick-with-it-pinned-filters","seo_title":"","content":" One of the new features that was added to Kibana in version 4.1 is something we call . Pinned Filters allows you to craft filters in one part of Kibana and bring those filters with you to all the other parts. It started as an enhancement idea from a user in the IRC channel, and now we can't imagine using Kibana without it. Creating Pinned Filters is a simple, 1-click operation. You start by creating some filters, for example, by .Next, you need to decide what filters you'd like to take with you. Once you've decided, simply hover over the filter and click the pin icon, and you'll see a pin icon show up on the filter. This icon lets you quickly see which filters are pinned. You can click on the pin icon again to un-pin the filter. If you've got a lot of filters and you simply want to pin them all, you could spend your time clicking on each one, but there's a faster way. If you click on the  menu in the filter bar, it will open a menu that allows you manipulate  of the filters, simply click on .Now that you've got one or more pinned filters, when you go to another app, like Visualize, the filters will follow along. It's that simple!Generally, users use filters to tease out interesting records in their data set, and pinned filters let them get more out of that effort. For example, after filtering down data in a visualization, they can take those filters over to Discover and see the records responsible for that visualization. That concludes our crash course in Pinned Filters, now go forth and pin! ","locales":"","title":"Stick With It - Pinned Filters in Kibana"}
{"index":{}}
{"author":"Jay Modi","category":"Releases","publish_date":"2015-07-29T00:00:00.000Z","url":"/blog/watcher-1-0-1-released","seo_title":"","content":" Today we are announcing the release of Watcher 1.0.1! This is a bugfix release and we recommend that all users of Watcher upgrade. Read on for more details and download it . bug fixes Watcher 1.0.1 fixes a compatibility issue with Elasticsearch 1.6.1 and 1.7.2, which were earlier today. enhancements now support specifying a time zone to be used when computing the names of the indices. The default is UTC. Previously, the computation was fixed to always use UTC when computing the names of the indices. upgrading Please refer to the in the Watcher documentation. feedback We would love to hear any feedback or questions that you may have in the category of our forums. ","locales":"","title":"Watcher 1.0.1 Released"}
{"index":{}}
{"author":"Andrew Cholakian","category":"Engineering","publish_date":"2015-07-29T00:00:00.000Z","url":"/blog/introducing-logstash-http-poller","seo_title":"","content":" I’m pleased to announce the release of a brand new Logstash input: . With this new input you’ll be able to repeatedly poll one or more HTTP endpoints and turn the response into Logstash events. There are a number of practical uses for this plugin, like: The syntax for this plugin is dead simple to boot, as seen in the example below: input { http_poller { # List of urls to hit # URLs can either have a simple format for a get request # Or use more complex HTTP features urls => { some_service => \"http://localhost:8000\" some_other_service => { method => \"POST\" url => \"http://localhost:8000/foo\" } } # Maximum amount of time to wait for a request to complete request_timeout => 30 # How far apart requests should be interval => 60 # Decode the results as JSON codec => \"json\" # Store metadata about the request in this key metadata_target => \"http_poller_metadata\" } } output { stdout { codec => rubydebug } } This request will put the HTTP responses of the polled endpoints into the field and provide metadata like response timing and HTTP response headers in the field. Using HTTP Poller to monitor website statusWe’ll start with a simple example:  using HTTP Poller to monitor whether a given URL is up, down, or responding slowly. The following Logstash config will hit a webserver on . If you don’t have one up, you can start one that takes a variable length of time to return a JSON response by running in your console (assuming you have ruby installed and have installed the sinatra gem with ). After that, try running Logstash with the sample config below. The sample config has been heavily annotated to make reading it easy, even for a complete logstash novice. Using the config below, you can generate Kibana charts like the one just underneath this paragraph, showing the ratio of slow to fast requests to your service over time. If you have Elastic's set up, you can use that to automatically send you alerts when you receive slow requests as well input { http_poller { urls => { \"localhost\" => \"http://localhost:8000\" } automatic_retries => 0 # Check the site every 10s interval => 10 # Wait no longer than 8 seconds for the request to complete request_timeout => 8 # Store metadata about the request in this field metadata_target => http_poller_metadata # Tag this request so that we can throttle it in a filter tags => website_healthcheck } } filter { # The poller doesn't set an '@host' field because it may or may not have meaning # In this case we can set it to the 'name' of the host which will be 'localhost' # The name is the key used in the poller's 'url' config if [http_poller_metadata] { mutate { add_field => { \"@host\" => \"%{http_poller_metadata[name]}\" } } } # Classify slow requests if [http_poller_metadata][runtime_seconds] and [http_poller_metadata][runtime_seconds] > 0.5 { mutate { add_tag => \"slow_request\" } } # Classify requests that can't connect or have an unexpected response code if [http_request_failure] or [http_poller_metadata][code] != 200 { # Tag all these events as being bad mutate { add_tag => \"bad_request\" } } if \"bad_request\" in [tags] { # Tag all but the first message every 10m as \"_throttled_poller_alert\" # We will later drop messages tagged as such. throttle { key => \"%{@host}-RequestFailure\" period => 600 before_count => -1 after_count => 1 add_tag => \"throttled_poller_alert\" } # Drop all throttled events if \"throttled_poller_alert\" in [tags] { drop {} } # The SNS output plugin requires special fields to send its messages # This should be fixed soon, but for now we need to set them here # For a more robust and flexible solution (tolerant of logstash restarts) # Logging to elasticsearch and using the Watcher plugin is advised mutate { add_field => { sns_subject => \"%{@host} is not so healthy! %{@tags}\" sns_message => '%{http_request_failure}' codec => json } } } } output { # Catch throttled messages for request failures # If we hit one of these, send the output to stdout # as w","locales":"","title":"HTTP Poller, Opening up a New World for Logstash"}
{"index":{}}
{"author":"Tudor Golubenco","category":"Engineering","publish_date":"2015-07-27T00:00:00.000Z","url":"/blog/mongodb-monitoring-with-packetbeat-and-elasticsearch","seo_title":"Monitoring MongoDB with Packetbeat and Elasticsearch","content":" Since the recently released 1.0.0-beta2 version, Packetbeat can understand the MongoDB wire protocol. This means you can now use Packetbeat and Elasticsearch to monitor the performance of your MongoDB servers. You can play with a live demo of the associated Kibana dashboard . The way Packetbeat works is that it captures the network traffic, correlates the request with the response, and inserts a document into Elasticsearch describing each MongoDB query or operation seen on the wire. You can then use Kibana to discover errors or slow queries and to visualize things like “Top slowest MongoDB queries”, “Response time percentiles for a particular collection”, or the “Number of writes with unacknowledged write concern”. But more about these visualizations later, let’s start by giving credit where credit is due. Based on a community contributionThe beauty of having a system like Packetbeat be open source is that anyone can add support for the protocols they need. By making it easy to add new protocols and encouraging community contributions, we’re not only extending Packetbeat’s list of supported protocols faster, but we benefit from having experts in each technology providing insights. The open source nature also means that folks can implement support for niche or proprietary protocols. While we hoped that people will start creating their own protocol modules for Packetbeat, we didn’t expect it to happen this quickly. We don’t have yet a proper developer guide for new protocols and the APIs are scarcely documented and may be inconsistent here and there. But all of this didn’t stop to submit the pull request for adding MongoDB support just a few days after we announced that . His pull request was well documented, contained unit and integration tests and even a simple to future protocol module writers. As if he didn’t do enough already, we asked Alban to contribute to this blog post as well. He sent us the following quote: Thank you Alban for your excellent work and for leading the way to more community contributions! Mongo’s wire protocolLet’s do a quick dive in the details of the MongoDB wire protocol, so you get a better understanding of what data Packetbeat captures. It is a fairly friendly protocol to a passive decoder like Packetbeat. Messages and individual fields have lengths, so it is easy to jump over the fields that we don’t understood or we don’t need. There is little contextual state so it is easy to understand what is happening even when capturing only part of the conversion. Also, because a single serialization protocol is used in all messages ( - binary JSON), it was easy for us to leverage an existing Go library to decode them. Namely, we use the BSON implementation from the driver, written by Gustavo Niemeyer. One thing to note is that prior to version 2.6, the write operations didn’t actually require a response from the server. By default, however, the drivers called the GetLastError method to acknowledge that the write was successful.  This was , which introduced a new wire protocol for insert, update and delete commands so that a response is always sent from the server and it doesn’t require a second command. Currently Packetbeat only supports the newer version of the protocol, so write commands from versions prior to 2.6 are not captured. We (or you!) can implement this in the future if there is demand. Another interesting aspect is that in a MongoDB cluster the same wire protocol is used between the three server types: mongos, config server and data shards servers. The same protocol is also used for replication between shard nodes. This means you can use Packetbeat for advanced troubleshooting of what is going on inside the cluster! To capture the traffic inside the cluster, you need to install Packetbeat not only on the routing nodes but also on the shard nod","locales":"","title":"Monitoring MongoDB with Packetbeat and Elasticsearch"}
{"index":{}}
{"author":"Leslie Hawthorn","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2015-07-22T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2015-07-22","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Elasticsearch Core Apache Lucene Watch This Space, where we'll share more news on the whole ELK ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - July 22 2015"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2015-07-20T00:00:00.000Z","url":"/blog/2015-07-20-where-in-the-world-is-elastic","seo_title":"Where in the World is Elastic, July 20 2015","content":" Welcome to Upcoming EventsJuly 20-24: , Portland, Oregon - Stop by our booth #311 to meet our local team and see live demos. July 20-26: - Our own will be giving a talk about Beyond the Basics with Elasticsearch on Friday, July 24 at 11 a.m. Don't miss it!Upcoming MeetupsJuly 21: July 22: July 23:July 23: July 21:July 22: July 22: July 23: That's it for this week. Stay tuned for Elastic happenings next week - there's much to come! - The Elastic TeamP.S.:   if you're interested in hosting a meetup or are giving a talk about Beats, Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic? - July 20, 2015"}
{"index":{}}
{"author":"Chris Earle","category":"Engineering","publish_date":"2015-07-24T00:00:00.000Z","url":"/blog/support-in-the-wild-my-biggest-elasticsearch-problem-at-scale","seo_title":"Support in the Wild: My Biggest Elasticsearch Problem at Scale","content":" As a Support Engineer at Elastic, I come across a lot of different issues from our customers, ranging from development questions surrounding Kibana to helping a user to understand why their Elasticsearch cluster had issues. In this blog post, I want to describe the number one problem that I run into with users running Elasticsearch as they begin to scale their workloads. Without fail, when a user approaches me with issues running Elasticsearch, whether it is a customer or not, I point them to this issue. Java Heap PressureElasticsearch has so many wildly different use cases that I could not write a reasonably short blog post describing what can and cannot consume memory. However, there is one thing that constantly stands out above all of the other concerns that you might have while running an Elasticsearch cluster at scale. For the users that I help, is the problem that is the most likely to cause their cluster's instability. Fielddata is the bane of my existence and it's the most frequent cause of the highest severity issues that I handle with our customers. Understanding FielddataThe inverted index is the magic that makes Elasticsearch queries so fast. This data structure holds a sorted list of all the unique terms that appear in a field, and each term points to the list of documents that contain that term: Term: Docs: 1 2 3 4 5 ---------------------------- brown X X X fox X X quick X X ---------------------------- Search asks the question: What documents contain term in the field? The inverted index is the perfect structure to answer this question: look up the term of interest in the sorted list and you immediately know which documents match your query. Sorting or aggregations, however, need to be able to answer this question: What terms does Document 1 contain in the field? To answer this, we need a data structure that is the opposite of the inverted index: Docs: Terms: ---------------------------- 1 [ brown ] 2 [ quick ] 3 [ brown ] 4 [ brown, fox, quick ] 5 [ fox ] ---------------------------- This is the purpose of fielddata. Fielddata can be generated at query time by reading the inverted index, inverting the term <-> doc data structure, and storing the results in memory. The two major downsides of this approach should be obvious: Because loading fielddata is costly, we try to do it as seldom as possible. Once loaded, we keep it in memory for as long as possible. By default, fielddata is loaded on demand, which means that you will not see it until you are using it. Also, by being loaded per segment, it means that new segments that get created will slowly add to your overall memory usage until the field's fielddata is evicted from memory. Eviction happens in only a few ways: While the first two ways will cause the memory to be evicted, they're not useful in terms of solving the problem because they make the index unusable. Segment merging is happening in the background and it is not a way to clear fielddata. The fourth and fifth ways are unlikely to be a long term solution because they do not prevent fielddata from being reloaded. The sixth option, evicting fielddata when the cache is full, leads to different issues: one request triggers fielddata loading for one field and the next request triggers loading for another, causing the first field to be evicted. This causes memory thrashing and slow garbage collections, and your users suffer from very slow queries while they wait for their fielddata to be loaded. Simply put, once fielddata becomes a problem, then it stays a problem. Why Fielddata is BadAt small scales, you can generally get away with fielddata usage without even realizing that you are using it. In highly controlled environments, you may even enjoy that specific fields are being loaded into memory for theoretically faster access. However, almost without fail, you are bound","locales":"","title":"Support in the Wild: My Biggest Elasticsearch Problem at Scale"}
{"index":{}}
{"author":"John O'Brien","category":"User Stories","publish_date":"2015-07-23T00:00:00.000Z","url":"/blog/api-ing-us-government-540-co-story","seo_title":"","content":" The US Federal Government is teeming with data… all types of data. Just in the area of business operations alone, there is , , and being generated by each agency - and while much of this data is publicly available (or becoming increasingly more available as data transparency continues to be a focus of the administration), it resides in various places across the government landscape and is often difficult to access, understand or use.As our government and enterprise clients continue to strive to use data to help them understand their business operations and improve decision making, we are often called upon to build applications, “” or analytic tools to help answer questions, provide visibility and automate data analysis to inform decisions makers.During our engagements, we often found ourselves harvesting the same procurement data, the same budget data, etc. to “paint different pictures” for clients, especially when a flexible way of asking questions of the data wasn’t readily available.  We also kept hearing about other teams having to start from scratch on the data harvest phase as well - acting as a barrier to achieving the goals of extracting meaningful information from the data.In a very similar way that the government likes to talk about “common operating pictures” (ok, that may actually be more of a DoD term) we began to envision a “common operational data” platform that would: Our solution - FedAPILeveraging our knowledge of the domain and data, we cranked up the R&D budget, ordered a few cases of 5 Hour Energy, spun up our AWS servers and hacked out beta, with Elasticsearch acting as the heartbeat and began to build out various harvesters for data we often found being reused, difficult to parse and/or of high value to our clients and friends. To date, FedAPI currently harvests and exposes: In a nutshell, it was pretty simple because of ElasticsearchElasticsearch provided such a powerful indexing and search capability, that really much of FedAPI is a proxy to an Elasticsearch cluster that calls upon Elasticsearch’s powerful search API endpoint (or what we often call the “magic wand endpoint”) to respond to requests. We also added a little code to record “event” records when data changes to allow us to see when data is changing and what has changed - which is often very important to see deltas over time - and have begun to pilot Kibana 4 with to more quickly show the data to our clients to allow them to start to better understand the “common operational data” that is ready to use in FedAPI. What’s next?FedAPI now acts as both and a . As a , we often point to FedAPI as an example / reference of how to leverage Elasticsearch when clients are looking to modernize how they store and expose data within their architecture.  A recent example of this can be found in our to DTIC for a new “master data repository” earlier this year. As a , we have begun to use FedAPI  in our application builds as a source of data and other external parties / partners have begun to do the same.  We have also begun to explore doing private, on-prem installs of the platform to help jumpstart internal data indexing efforts. This is forcing us to continue to evolve the APIs, improve documentation, harvest new data and is driving us towards a refactor to make things a little faster (our fault, not Elastic!). Have a problem you want to solve with data?Leveraging what we have learned with FedAPI and Elasticsearch, we know we can add more data and help answer more questions.If you have a data challenge just contact us, we’d love to hack out a solution for you. ","locales":"","title":"API-ing the US Government: A 540.co Story"}
{"index":{}}
{"author":"Jay Modi","category":"Releases","publish_date":"2015-07-21T00:00:00.000Z","url":"/blog/shield-1-3-1-and-1-2-3-released","seo_title":"","content":" We have released Shield 1.3.1 and Shield 1.2.3 today. These are bug fix releases that fix a serialization bug in Shield 1.3.0 and Shield 1.2.2 that prevented rolling upgrades when using message authentication. Read below for all of the details and then download it . Message AuthenticationShield provides support for through the use of a shared system key. In Shield 1.2.2 and 1.3.0, a bug was introduced that caused the message authentication to fail when communicating with nodes running Shield 1.2.1 or earlier. During a rolling upgrade from Shield 1.2.1 to Shield 1.2.2 or 1.3.0, the nodes running the newer version would not be able to join the cluster and would see “tampered signed text” log messages. Upgrading from Shield 1.2.1 or earlierIf you have not upgraded to Shield 1.2.2 or 1.3.0, then you can simply follow the normal and upgrade to Shield 1.2.3 or Shield 1.3.1. Upgrading from Shield 1.2.2 or 1.3.0 The first step is to determine if you are using . If you have not configured the system-key on all nodes, you are not using message authentication:  please follow the normal . If you are using the message authentication feature with Shield 1.2.2 or 1.3.0, then a will be necessary. The existing Shield plugin should be uninstalled and the new plugin should be installed on each node while they are stopped as documented in the documentation. FeedbackWe would love to hear any feedback and questions that you may have via the category in our forums. ","locales":"","title":"Shield 1.3.1 and 1.2.3 Released"}
{"index":{}}
{"author":"Leslie Hawthorn","category":"","publish_date":"2015-07-16T00:00:00.000Z","url":"/blog/this-week-in-elastic-2015-07-16","seo_title":"","content":" Welcome to ! In this weekly round up post, we bring you news on all aspects of the ELK Stack, including updates on Elasticsearch, Logstash and Kibana development, tips and tricks and upcoming events. Getting started w/Watcher -alerting for ? Join on Jul 21 for live demos, real use cases+more — elastic (@elastic) Elastic News Useful blog posts, how tos, tutorials and articles from the wide world of all the ELK stack ecosystem. We just released beta2 of our NEST extension that adds support for GA! — Martijn Laarman (@Mpdreamz) Support for MongoDB, multiple Elasticsearch nodes and other goodies in the new Packetbeat 1.0.0-beta2 — Packetbeat (@packetbeat) Slides and VideosIf you're a PHP developer,  just gave a presentation on . You can check out the accompanying demo application . Where to find Us We'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking about the ELK stack or hosting a meetup, Australia FranceThe Elastic Paris Meetup will get together on July 29 to talk Fuzzy Name Matching in Elasticsearch. to save your seat. Germany Hong Kong will be speaking at the , coming up July 31 - August 2 in Hong Kong. Stay tuned for more details on his talk! IndiaThe Big Data Analytics on Cloud Meetup group of Hyderabad will get together on August 1 for a Hands on ELK Workshop. to attend.  IsraelFor our second Tel Aviv meetup in July, you can hear about Elasticsearch and Fuzzy Matching, Fighting Crime with Elasticsearch and Unstructured Data Analytics and more. to attend this meetup on July 22. Japanタイトル決めました（中身はまだだけど）/ 【東京】【女性限定】Javaでデータ解析　基本のキ！ - Java女子部 | Doorkeeper - — Jun Ohtani (@johtani) 今回は、CTOでありElasticsearchの生みの親であるShayが来日してトークします。 / 第11回elasticsearch勉強会 - — Jun Ohtani (@johtani) Korea[Elastic Seoul Meetup - 엘라스틱 서울 밋업] 모임을 온오프믹스에 개설하였습니다. 많이 참석해주세요! — JongMin Kim 김종민 드미트리 (@kimjmin) South AfricaThe Elasticsearch South Africa group will convene in Jo'Burg on July 29 for a Gentle Introduction to Elasticsearch. to save your seat. SpainEuroPython 2015 heads to Bilbao this year, featuring two talk from : and . You can learn more about Honza's talks in the :  EuroPython runs July 20-26. United Kingdom United States - East United States - West UruguayThe Montevideo Big Data and Data Science Meetup will get together on August 6 for talks on Elasticsearch. to save your seat. Where to Find You  If you're a regular reader, you're thinking of skipping this section. You may even be thinking to yourself, yes of course I will drop a note on Twitter when I am giving a talk on all things ELK. That's awesome, because we'd like to showcase every meetup, conference presentation and workshop on Elasticsearch, Logstash, and Kibana happening worldwide. And now, we've made it even easier for you to get support for your meetup! Head on over to  ! (And we'll still totally send you swag if you're giving a talk on anything ELKy at a conference.) Oh yeah, we're also  . If you'd like us to find you for employment purposes, just . We care more about your skill set and passion for Elasticsearch, Kibana, and Logstash than where you rest your head. Trainings If you are interested in training on Elastic's product line, we have courses coming up in: ","locales":"","title":"This Week in Elastic - Better query execution in Elasticsearch 2.0"}
{"index":{}}
{"author":"Suyog Rao","category":"Releases","publish_date":"2015-07-22T00:00:00.000Z","url":"/blog/logstash-1-5-3-and-1-4-4-released","seo_title":"","content":" We are announcing the of Logstash 1.5.3 and 1.4.4. In addition to fixing defects, these releases address important security vulnerabilities. Our recommendation is to upgrade immediately if you are using either of the following plugins: Security Fixes Lumberjack Input Security Vulnerability Logstash 1.5.2 and prior versions are vulnerable to a SSL/TLS security issue called the attack. If you are using the Lumberjack input, FREAK allows an attacker to successfully implement a man in the middle attack, intercepting communication between the Logstash Forwarder agent and Logstash server. Both 1.5.3 and 1.4.4 release include a patch which resolves this issue. We have been assigned for this issue and added this vulnerability to our . Elasticsearch Vulnerabilities Logstash 1.5.2 and prior versions were packaged with Elasticsearch releases which are vulnerable to Remote code execution vulnerability () and Directory traversal vulnerability (). These binaries are used in Elasticsearch output specifically when using the and protocol. Both 1.5.3 and 1.4.4 are packaged with Elasticsearch version 1.7.0 which has been to address these vulnerabilities. Note that users of protocol are not vulnerable to these attacks. Bug Fixes Below we highlight some bug fixes and enhancements in this release. For a full list, please check the Feedback Please Logstash 1.5.3 and let us know what you think on Twitter (@elastic) or on our . You can report any problems on the GitHub . ","locales":"","title":"Logstash 1.5.3 and 1.4.4 released"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2015-07-16T00:00:00.000Z","url":"/blog/elasticsearch-1-7-0-and-1-6-1-released","seo_title":"","content":" Today, we are pleased to announce the release of based on , and the bugfix release of . These releases contain a security fix and .You can download them and read the full changes list here:The 1.7.0 release will be the last feature release in the 1.x series. All future features will go into Elasticsearch 2.0 or later.Elasticsearch 1.7.0 is a small release, but contains two important security fixes and two important features which will improve cluster stability and recovery: ","locales":"ja-jp","title":"Elasticsearch 1.7.0 and 1.6.1 released"}
{"index":{}}
{"author":"Michael McCandless","category":"Engineering","publish_date":"2015-07-16T00:00:00.000Z","url":"/blog/lucene-jvm-bugs","seo_title":"","content":" A few weeks ago,  from the  notified us on the Lucene developer's list that . This happens every few weeks and is a familiar routine for us by now: we quickly upgraded the Lucene  jobs at both  and  build servers so that Lucene's  would stress the new JDK snapshot. But, we were then suddenly bombarded by all sorts of exotic failures !  dug into the intermittent failures and found that they often happened when the test used Lucene's \"spoon feeding reader\" (). This helpful test-only class wraps any incoming and randomly chops up the incoming large blocks of characters into small randomly sized chunks, much like how you would spoon-feed a baby. Its purpose is to tickle any buffering bugs such as . Lucene has also had various exciting buffering bugs in its tokenizers in the past, but this time  caught a bug in the JVM, specifically in ! Robert eventually boiled the failing test down to a  which finally led to . The issue was quickly fixed (thank you!), but  to see just how hairy it is for the JVM to implement the seemingly innocent ! This is like pulling off the volume knob on your car radio only to discover it has a small nuclear reactor inside. This collaboration between the OpenJDK team and Lucene developers is win/win: new versions of OpenJDK (and of course , nearly the same thing) get more extensive testing before being unleashed to the world and Lucene users gain some confidence that there are no specific Java bugs causing horrible things like silent index corruption such as . You can see . The Lucene community also  affecting us, and  gave a . There is one Oracle developer who really stands out in resolving the scary JVM bugs we discover: on behalf of , I'd like to extend a warm thank you to . We are perpetually in awe of Vladimir because somehow, with even the most cryptic and difficult Lucene test failures, iterating with  or Robert or Uwe or sometimes all three, Vladimir can stare at heaps and heaps of assembly code created by the hotspot compiler and understand and fix the JVM bugs. We are not sure how he does it but he always does! The silver liningThings have not always been so rosy. This  for  in Oracle's first Java 1.7.0 release caused , which . But the silver lining in this unfortunate event was the closer collaboration and squashed bugs we see today, not just in  but also  that Rory notifies on new JDK snapshot builds. Still, things can and should be better. Inexplicably, . This is insane and self-defeating: why on earth would any serious open-source project want to put false barriers for users to report and iterate on problems? This can only hurt the quality of your software. We have no choice but to work around this commercial silliness by resorting to  emails to OpenJDK team members and lists when our tests find problems, such as   on , still causing occasional SEGVs in Lucene tests today. Robert  to an email (and ) to the  instead of opening an issue himself. We are allowed to report incidents at , but they remain invisible until approved or somehow moved to . Dark ages! Second, isolating new JVM bugs is horribly time consuming. For example,  but only Dawid has had time to dig in a bit to try to isolate the bugs to small test cases (more volunteers welcome!). It's spooky that even now (), we  that consistently passes Lucene's tests. The frequent build failure emails to the  for bugs that are not in fact Lucene's cause additional noise and confusion to most readers on that list who don't necessarily understand that we are testing early-access Java builds. At times we feel like a strange extension of Oracle's QA team! IBM's J9 JDK joins the funIBM has its own , and we used to include it in Lucene's tests rotation, but there were too many JVM bugs, such as this , causing test failures. Long ago, we never succeeded in getting IBM's attention to resolve them. But then this  led to a renewed effort, and thanks again to Robert we now h","locales":"","title":"Apache Lucene helps squash JVM bugs"}
{"index":{}}
{"author":"Leslie Hawthorn","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2015-07-15T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2015-07-15","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Elasticsearch Core Apache Lucene Watch This Space , where we'll share more news on the whole ELK ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - July 15, 2015"}
{"index":{}}
{"author":"Adrien Grand","category":"Engineering","publish_date":"2015-07-15T00:00:00.000Z","url":"/blog/better-query-execution-coming-elasticsearch-2-0","seo_title":"","content":" It’s time to forget everything you knew about queries and filters: Elasticsearch 2.0 will make much better decisions by itself instead of relying on users to formulate an optimized query. This change will be almost invisible at the API level but let’s dive into the internal changes that made it possible. Most changes that are mentioned in this blog post have been done in Lucene 5.0, 5.1 and 5.2 and will be integrated in Elasticsearch 2.0. The query/filter mergePrevious versions of Elasticsearch used to handle queries and filters as different citizens: you had queries that could match and score, and filters that could only match and were cacheable. In Elasticsearch 2.0, queries and filters are the same internal object, and these objects can be configured to score documents, or to skip scoring. Then the query DSL makes sure to propagate the information correctly: for instance the clause of a query would need to produce scores if the query needs to produce scores too, while a clause would never need to produce scores since it may only be used for filtering documents out. In order to make the query DSL more consistent with this change, : clauses are like clauses except that they do not contribute to the score. This means that the following query: { “filtered” : { “query”: { query definition }, “filter”: { filter definition } } } should now be replaced with { “bool” : { “must”: { query definition }, “filter”: { filter definition } } } Note that the query DSL is still backward compatible in spite of this change: if you try to run a query, it will parse as a query internally. However, we would encourage you to migrate to the new syntax as the query will be removed in a future release. While this might look like a minor change, this is actually very useful for us. For instance, we used to have 3 queries/filters that could perform conjunctions: the query, the filter and the query, and it happens that they all computed conjunctions in a slightly different way. The fact that we only have the query now allows us to perform optimizations in a more robust way, and in particular to leverage two-phase execution efficiently. Two-phase executionThe previous filter API could be consumed in two ways: either using iterators over matching documents, or using an optional random-access API that allowed to check if a particular document matched the filter or not. Everything is good so far, except that the best way to consume a filter depended on which kind of filter you had: for instance the filter was more efficient when using the random-access API while the filter was more efficient using the iterator API. This was quite a nightmare to optimize and was the root cause why the filter on the one hand and the and filters on the other hand performed differently. To fix this issue, Lucene introduced a new two-phase iteration API, which consists of an approximation phase that can quickly iterate over a superset of the matching documents, and a verification phase that can be used to check if a document in this superset actually matches the query. A good example of use of this API is the phrase query: if you search for “quick fox”, a good approximation would be to search for documents that contain both “quick” and “fox”, and then as a verification step we could read positions to see if we can find these terms at consecutive positions. Why is it useful? Imagine that you are searching for “quick fox” and applying a filter as well. The fact that we can dissociate the approximation from the verification phase allows us to first intersect the approximation with the filter, so that we will verify positions on a smaller set of documents. This two-phase iteration pattern also applies to other queries: for instance geo-distance queries can use a bounding box as an approximation and a distance computation as a verification, and filters that only make sense in a random-acces","locales":"","title":"Better query execution coming to Elasticsearch 2.0"}
{"index":{}}
{"author":"Shelby Switzer","category":"User Stories","publish_date":"2015-07-15T00:00:00.000Z","url":"/blog/solving-the-local-data-problem-elasticsearch-docker-compose","seo_title":"","content":" Open data at the local government level may seem “smaller” but actually faces bigger challenges than those at the national level.  While there are increasing efforts to open government data sets nationally, cities, towns, counties, and even states are often left behind. These smaller entities don't have the resources to implement APIs for their data, and community tech groups and developers who want to use this data constantly encounter obstacles ranging from fragmented, non-uniform data to finding (and funding) API hosting and maintenance.But what if we could throw all that data in a box that's easy to open, close, and move around? What if we could bypass traditional solutions requiring infrastructure for hosting and maintenance? Enter Docker and Elasticsearch, and a simple three-layer, API-in-a-box solution that any developer can immediately turn on with . The Need Local brigades of (CfA) meet regularly across the country and even the world to bring together citizens from different backgrounds to improve their community and make government more accessible. This means routinely working with data provided by governments, typically in the form of Excel files, CSVs, other spreadsheets, and even PDFs, all containing interesting data such as government employee salaries or tax digests documenting business registration. These data sets can consist of anywhere between ten rows and a few thousand rows — what I call “small data.” Developers working on these projects come from very different backgrounds with varying development environments, and must determine the best way to use this data and make it accessible for not only themselves but for other citizens and projects. The projects typically don’t have funding, so they often can’t be hosted, even though most engineers are familiar with interacting with and building clients for HTTP APIs.In sum, any solution, or set of solutions, to these various projects and their small data needs must: The Solution While Elasticsearch is typically thought of in the context of big data and huge amounts of text, it rose to the top of the list for solutions to working with the small data we encounter in civic hacking. The spreadsheets of data we often work with are incomplete, have only text data types, contain duplicates, and contain fields that no one (not even the government officials who provided the sources) can explain but should probably stick around in case they become useful later. Elasticsearch has a RESTful JSON API that is familiar to most developers and very intuitive to work with. Its powerful functions allow for the easy sorting, de-duping, and searching of data without actually changing or deleting raw data.We can then put a hypermedia API layer over Elasticsearch so that we have a uniform interface for different data sets that is not only consumable by generic hypermedia clients but also exposes the queries that are available for this data. For example, if my data has Name, Lat, Long, and Business Type fields, then a hypermedia API can expose those queries simply through the JSON response for the resource, so that anyone consuming this API knows what’s possible even without documentation.But Elasticsearch and a hypermedia API need hosting just like any other database or server, so how can we use them for a solution that can be simply set up anywhere and not need paid hosting? Enter Docker.With a Docker container for Elasticsearch and another for the hypermedia API, and Docker Compose to link and start them together smoothly, we can create an API experience that is uniform, uses remote data, and has powerful search and data manipulation functionality, all without needing to be hosted anywhere beyond your local laptop.The final process is made up of these four easy steps: With an API-in-a-Box solution using Elasticsearch and Docker, anyone, anywhere, once they have the raw data, only ne","locales":"","title":"Solving the local data problem with Elasticsearch and Docker Compose"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2015-07-13T00:00:00.000Z","url":"/blog/2015-07-13-where-in-the-world-is-elastic","seo_title":"","content":" Welcome to Check out our events happening this week. It's summer time and at least Europe is calming down a bit. But it's picking up soon again with more events and meetups coming as of August/September.Upcoming EventsJuly 16-17:  - Meet our team in the hallway or perhaps we'll see you in the open space zone where we made a proposal on the ELK stack. Upcoming MeetupsJuly 14: July 14: July 14: July 14: July 15: July 14: July 16: July 13: July 14: That's it for this week. Stay tuned for Elastic happenings next week - there's much to come! - The Elastic TeamP.S.:   if you're interested in hosting a meetup or are giving a talk about Beats, Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic? - July 13, 2015"}
{"index":{}}
{"author":"","category":"","publish_date":"2015-07-09T00:00:00.000Z","url":"/blog/introducing-logstash-input-http-plugin","seo_title":"","content":" Logstash has been missing a way to receive data through HTTP requests, but not anymore! The  plugin is now available for everyone! Also, starting with , it is included as one of the default plugins that are distributed with the official release. So what exactly does the logstash-input-http plugin do?When you configure this plugin in the section, it will launch a HTTP server and create events from requests sent to this endpoint. This means you can send notification data to Logstash using webhooks, thereby integrating your existing applications with Logstash’s rich plugin ecosystem! How do I use this plugin?By default it will bind the webserver to all hosts (\"0.0.0.0\") and open the TCP port 8080 but it's possible configure these settings: input { http { host => \"127.0.0.1\" # default: 0.0.0.0 port => 31311 # default: 8080 } } That's all you need! What about security?You can configure basic authentication by setting a username and password. All requests done to Logstash will then have to set the right credentials or receive a 401 response. Only correctly authenticated requests will produce an event inside of Logstash. For SSL, it is necessary to specify the path to a that contains the certificate that clients use to validate the server. Here's an example: input { port => 3332 user => myuser password => \"$tr0ngP4ssWD!\" ssl => on keystore => \"/tmp/mykeystore.jks\" keystore_password => \"keystore_pass\" } OK, now show me this plugin in action!Step 1 - starting Logstash with http input: bin/logstash -e \"input { http { } } output { stdout { codec => rubydebug} }\" Step 2 - That's it! To test it, let's issue two requests: % curl -XPUT 'http://127.0.0.1:8080/twitter/tweet/1' -d 'hello' % curl -H \"content-type: application/json\" -XPUT 'http://127.0.0.1:8080/twitter/tweet/1' -d '{ \"user\" : \"kimchy\", \"post_date\" : \"2009-11-15T14:12:12\", \"message\" : \"trying out Elasticsearch\" }' Result in Logstash: { \"message\" => \"hello\", \"@version\" => \"1\", \"@timestamp\" => \"2015-05-29T14:49:00.392Z\", \"headers\" => { \"content_type\" => \"application/x-www-form-urlencoded\", \"request_method\" => \"PUT\", \"request_path\" => \"/twitter/tweet/1\", \"request_uri\" => \"/twitter/tweet/1\", \"http_version\" => \"HTTP/1.1\", \"http_user_agent\" => \"curl/7.37.1\", \"http_host\" => \"127.0.0.1:8080\", \"http_accept\" => \"*/*\", \"content_length\" => \"5\" } } { \"user\" => \"kimchy\", \"post_date\" => \"2009-11-15T14:12:12\", \"message\" => \"trying out Elasticsearch\", \"@version\" => \"1\", \"@timestamp\" => \"2015-05-29T14:49:04.105Z\", \"headers\" => { \"content_type\" => \"application/json\", \"request_method\" => \"PUT\", \"request_path\" => \"/twitter/tweet/1\", \"request_uri\" => \"/twitter/tweet/1\", \"http_version\" => \"HTTP/1.1\", \"http_user_agent\" => \"curl/7.37.1\", \"http_host\" => \"127.0.0.1:8080\", \"http_accept\" => \"*/*\", \"content_length\" => \"110\" } } You can see that in the second request, since the content-type was , the body was deserialized and expanded to the event root (notice the fields \"user\", \"post_date\" and \"message\"). Show me more concrete examples of how to use it!Because, real world examples make everything clearer! Elastic Watcher IntegrationIn this section, we’ll show you how to integrate -- the new Elasticsearch plugin for alerting and notification -- with Logstash. notifications to Logstash via this input provides you a powerful toolset to further transform notifications and use Logstash’s rich collection of . Imagine that you have indices with Apache logs, and now we want to get a periodic update of how many requests are resulting in a 404 (Not Found) response. The required steps for this are: Here we go! 1. Installing Watchercd elasticsearch-1.5.2 bin/plugin -i elasticsearch/watcher/latest bin/plugin -i elasticsearch/license/latest bin/elasticsearch # restart the server 2. Creating a watchThe Watcher plugin for elasticsearch provides an API to create and manipulate scheduled tasks, or \"watches\". A will query the data in the elasticsearch cluster according to its schedule, look for certain scenar","locales":"","title":"Introducing the Logstash HTTP input plugin"}
{"index":{}}
{"author":"Leslie Hawthorn","category":"","publish_date":"2015-07-08T00:00:00.000Z","url":"/blog/this-week-in-elastic-2015-07-08","seo_title":"","content":" Welcome to ! In this weekly round up post, we bring you news on all aspects of the ELK Stack, including updates on Elasticsearch, Logstash and Kibana development, tips and tricks and upcoming events. - hosted - is here. Join us on July 15 for a live webinar, demo, & Q&A. — elastic (@elastic) Elastic News Useful blog posts, how tos, tutorials and articles from the wide world of all the ELK stack ecosystem. Gearing up 4 the Tel Aviv meetup.View from the Gigya offices are amazing.Thx for hosting us. — Boaz Leskes (@bleskes) Slides and VideosAnd you can enjoy on from last week's JDEV Conference .Where to find Us We'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking about the ELK stack or hosting a meetup, Australia FranceThe Elastic Paris Meetup will get together on July 29 to talk Fuzzy Name Matching in Elasticsearch. to save your seat.Germany Hong Kong will be speaking at the , coming up July 31 - August 2 in Hong Kong. Stay tuned for more details on his talk!IsraelFor our second Tel Aviv meetup in July, you can hear about Elasticsearch and Fuzzy Matching, Fighting Crime with Elasticsearch and Unstructured Data Analytics and more. to attend this meetup on July 22.Japan大阪でもやります！スピーカー募集中です！/ Elasticsearch勉強会 in 大阪 - elasticsearch勉強会 | Doorkeeper - — Jun Ohtani (@johtani) 出張Elasticsearch勉強会第2弾！ / Elasticsearch勉強会 in 京都 - elasticsearch勉強会 | Doorkeeper - — Jun Ohtani (@johtani) South AfricaWe are a proud swag sponsor of the . If you happen to be in Johannesburg on July 11, pop in for knowledge and some cool Elastic goodies. SpainEuroPython 2015 heads to Bilbao this year, featuring two talk from : and . You can learn more about Honza's talks in the :  EuroPython runs July 20-26. SwitzerlandThe Elasticsearch Switzerland Meetup group will get together on July 14 to talk Elasticsearch Under the Hood with . to save your seat. United KingdomThe London Elastic Meetup will get together on July 21 for two talks, including on using Kibana 4 to analyze data from the Ministry of Transportation. to save your seat - we're hosting at the Elastic London office!United States - East United States - West Where to Find You  If you're a regular reader, you're thinking of skipping this section. You may even be thinking to yourself, yes of course I will drop a note on Twitter when I am giving a talk on all things ELK. That's awesome, because we'd like to showcase every meetup, conference presentation and workshop on Elasticsearch, Logstash, and Kibana happening worldwide. And now, we've made it even easier for you to get support for your meetup! Head on over to  ! (And we'll still totally send you swag if you're giving a talk on anything ELKy at a conference.) Oh yeah, we're also  . If you'd like us to find you for employment purposes, just . We care more about your skill set and passion for Elasticsearch, Kibana, and Logstash than where you rest your head. Trainings If you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elastic - July 08, 2015"}
{"index":{}}
{"author":"Leslie Hawthorn","category":"","publish_date":"2015-07-07T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2015-07-07","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top NewsEverything you want to know about Found’s hosted service - register here: — Found by Elastic (@foundsays) Elasticsearch Core Apache LuceneWatch This Space , where we'll share more news on the whole ELK ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - July 7, 2015"}
{"index":{}}
{"author":"Adrien Grand","category":"Engineering","publish_date":"2015-07-02T00:00:00.000Z","url":"/blog/store-compression-in-lucene-and-elasticsearch","seo_title":"","content":" Back in 2011 if you asked Lucene to index and store some text content, odds are high that your inverted index would take about 30% of the size the original data while the document store (also called “stored fields”) would take a bit more than 100%. Why more than 100%? Because the document store would just store your documents sequentially on disk without any compression and even add some overhead, for instance separators between fields. If you wanted to add compression, your only option was to compress field values yourself before sending them to Lucene. Unfortunately, compression algorithms don’t like short content much. They prefer longer content where they have more opportunities to identify patterns and take advantage of them for compression. Actually if you compress a short string, it will likely be larger than the original string due to the overhead of the compression container. So if you were indexing structured content, you pretty much had no options for store compression. Lucene 4 and codecsBut then in 2012 perspectives changed after the release of Lucene 4.0. One of the major highlights of Lucene 4.0 was the new codec API, which gives developers a framework that makes experimentation with file formats and backward compatibility easier. The latter point in particular is important: if you want to change the index format, all you need to do is to build a new codec and make it the new default. Since segments register the codec that has been used to write them, old segments would still work by using the read API of the previous codec while new segments would be written using the new codec. This has been a very important change since it allowed us to perform drastic changes of the index format in minor releases in a backward-compatible way. By the way, out of the eleven 4.x Lucene releases, 6 of them changed the index format! In particular in Lucene 4.1, the codec changed in order to automatically compress the document store. It works by grouping documents into blocks of 16KB and then compresses them together using , a lightweight compression algorithm. The benefit of this approach is that it also helps compressing short documents since several documents would be compressed into a single block. However in order to read a single document, you need to decompress the whole block. It generally does not matter as decompressing 16KB for 100 documents with LZ4 is still faster than running a non-trivial query or even just seeking on a spinning disk for these 100 documents. Better compression with DEFLATEThe good news is that we brought even more improvements to the document store in Lucene 5.0. More and more users are indexing huge amounts of data and in such cases the bottleneck is often I/O, which can be improved by heavier compression. Lucene 5.0 still has the same default codec as Lucene 4.1 but now allows you to use (the compression algorithm behind zip, gzip and png) instead of LZ4, if you would like to have better compression. We know this is something which has been long awaited, especially by our logging users. Opting in for better compression is just a matter of setting the setting to . For instance, the following API call would create an index called that trades stored field performance for better compression: curl -XPUT ‘localhost:9200/my_index’ -d ‘{ “settings”: { “index.codec”: “best_compression” } }’ Hot and cold nodesThis new option opens new perspectives when it comes to managing hot and cold data: with time-based indices, it is common that indices become queried less often the older they get since new data tend to be more interesting than old data. In order to remain cost-effective in such cases, a good practice is to leverage elasticsearch to assign new indices to beefy machines with fast CPUs and disks, and old indices to cheaper machines that have plenty of disk space. Now you can also enable better compression on the cold nodes by setting ","locales":"","title":"Store compression in Lucene and Elasticsearch"}
{"index":{}}
{"author":"Shaunak Kashyap","category":"","publish_date":"2015-07-02T00:00:00.000Z","url":"/blog/logstash-lines-2015-07-02","seo_title":"","content":" Welcome back to ! In these weekly posts, we'll share the latest happenings in the world of Logstash and its ecosystem. Ever wanted to use Logstash Forwarder with Docker? Here is a two-part article series explaining how to do just this: And from our very own , here is part one of a three-part blog series on Kafka and Logstash integration:  Logstash Core and Plugins That's this week in . Come back next week for more Logstash news! ","locales":"","title":"The Logstash Lines — Logstash Forwarder with Docker, a new input plugin, and more!"}
{"index":{}}
{"author":"Colin Goodheart-Smithe","category":"","publish_date":"2015-07-08T00:00:00.000Z","url":"/blog/out-of-this-world-aggregations","seo_title":"","content":" One of the most visible features coming in 2.0 are the Pipeline Aggregations. This is an extension of the current Aggregations framework, to allow additional computations to be performed on top of the results of aggregations. This allows users to ask questions such as “What is the maximum average monthly price?” from a date histogram and “How many new users are signing up each day?” from a date histogram showing total user count each day. Internally these aggregation are run after the other aggregations have completed on the coordinating node. This means Pipeline Aggregators are able to use the final results of their sibling aggregations but are not able to go back to the shards to query the index. The first of these new types are: In this post I will show some of these new aggregations in action. To help demonstrate, I have downloaded spacecraft trajectory data from for the Voyager 1 and Voyager 2 spacecraft. Each document in my index represents the position of one of the spacecraft on a particular day. The data spans from the launch of the Voyager missions in September 1977 to the present day and also includes projected trajectories up to 2020. Below is an example of a document in Elasticsearch: { \"_index\": \"helioweb\", \"_type\": \"voyager1\", \"_id\": \"1977-257\", \"_score\": 1, \"_source\": { \"seLon\": 353.8, \"objectName\": \"voyager1\", \"seLat\": 0.2, \"year\": 1977, \"date\": \"1977-09-14T10:28:25.178Z\", \"radAU\": 1.02, \"dayOfYear\": 257 } } and are the latitude and longitude of the object in the respectively, and is the radial distance of the object from the Sun measured in . In this post we will mainly be concentrating on the , , and fields. So first let’s plot the average radial distance of Voyager 1 over time. This can be done with the existing aggregations by defining the following aggregations: { ... \"aggs\": { \"histo\": { \"date_histogram\": { \"field\": \"date\", \"interval\": \"month\" }, \"aggs\": { \"avg_distance\": { \"avg\": { \"field\": \"radAU\" } } } } } } The graph we get from the results looks pretty uninteresting: Basically all we can find out from this is that since launch Voyager 1 has been travelling further and further away from the Sun. Not exactly surprising for a long distance space probe! But what does Voyager 1’s radial speed over time look like? This isn’t something you can currently do in Elasticsearch. The indexed documents only contain the radial distance of Voyager 1 so its speed cannot be plotted directly. With the new Derivative Aggregation we can take the derivative of that distance to calculate the radial speed each month: { ... \"aggs\": { \"histo\": { \"date_histogram\": { \"field\": \"date\", \"interval\": \"month\" }, \"aggs\": { \"avg_distance\": { \"avg\": { \"field\": \"radAU\" } }, \"speed\": { \"derivative\": { \"buckets_path\": \"avg_distance\" } } } } } } Pipeline aggregations look very similar to the other aggregations in the request, but instead of having a parameter to reference which in the document they will work on, they have a which references which aggregation to use in their calculation. The resulting aggregation tree for the above request looks like this: { ... \"aggregations\": { \"histo\": { \"buckets\": [ { \"key_as_string\": \"1977-09-01T00:00:00.000Z\", \"key\": 241920000000, \"doc_count\": 25, \"avg_distance\": { \"value\": 1.0328 } }, { \"key_as_string\": \"1977-10-01T00:00:00.000Z\", \"key\": 244512000000, \"doc_count\": 31, \"avg_distance\": { \"value\": 1.1867741935483873 }, \"speed\": { \"value\": 0.15397419354838737 } }, … ] } } } You can see that the Derivative Aggregation has added a new value to the all but the first bucket. The first bucket is left empty as we require two values to compute the derivative, the current value and the previous value. Since there is no previous value for the first bucket, we can’t compute a derivative for it. Plotting the speed against time gives us a much more interesting graph: Here we can see that Voyager 1’s radial speed dropped significantly twice, once around 1979 and once around 1981. Loo","locales":"","title":"Out of this world aggregations"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2015-07-06T00:00:00.000Z","url":"/blog/2015-07-06-where-in-the-world-is-elastic","seo_title":"Where in the World is Elastic, July 6 2015","content":" Welcome to Check out this weeks events and meetups and join us if one of them is happening near you!Upcoming EventsJuly 8-9:  - Come by our booth, talk to the Elastic team and grab a cool piece of swag on the way!July 10-11: - Say hi to who will be giving an introduction to Elasticsearch as part of Adrian Carr's talk \"Amazing Speed - Elasticsearch for the .NET Developer\" on Friday, July 10, at 4:05 p.m. Feel free to ask him questions thereafter, he's around for the duration of the event.July 7: - Come say hi at the Elastic/Mimacom booth (#33) to check out cool demos and talk all things ELK!July 11:  - Check out community organiser Jurgen du Toit's talk on \"StaticElastic: An Elasticsearch and Backbone SPA\" at 2 p.m.Upcoming MeetupsJuly 7: July 8: July 8: July 7: July 8: That's it for this week. Stay tuned for Elastic happenings next week - there's much to come! - The Elastic TeamP.S.:   if you're interested in hosting a meetup or are giving a talk about Beats, Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic? - July 06, 2015"}
{"index":{}}
{"author":"Aaron Mildenstein","category":"Engineering","publish_date":"2015-07-02T00:00:00.000Z","url":"/blog/curator-3-2-0-released","seo_title":"","content":" Curator 3.2.0 is here!  While this release is an incremental improvement over 3.1.0 and 3.0.0, there are some noteworthy things we just had to let you know about:What’s new?Some of the changes included in 3.2.0 are:Let’s take a look at some of these!Synced Flush SupportWith the release of came a highly anticipated feature: , which allows your cluster to recover more quickly.  The best part of this feature is that your indices will automatically have the benefits of this feature applied as soon as they go inactive (5 minutes with no indexing).  However, there may be times when you need to shut down immediately, but want to have your indices recover more quickly via a synced flush.  If you run into such a scenario, Curator can help:curator seal indices --all-indicesOf course, you can use all of the regular parameters, too!Curator will detect if your cluster is 1.6+ and will automatically perform a synced flush on indices before they are closed.Alias CreationIn all prior versions of Curator, an alias had to already exist in order to add or remove indices to it.  Now, starting with 3.2.0, if the alias does not exist, Curator will create it with the indices you specify.The --skip-repo-validation FlagBeginning with Elasticsearch 1.4, creating a snapshot repository would result in Elasticsearch testing all nodes to ensure each had read/write access to the shared filesystem, be it NFS, S3, Azure, HDFS, etc.  An to manually test write access also came with it.  This API call is used by Curator as a test before snapshots as a safety check, ensuring that all nodes still have write access to the repository.  This has helped some users find errors that might have resulted in incomplete or broken snapshots.  However, some users reported intermittent errors with this validation check, usually as a result of the test timing out for some reason, rather than that the file system was unavailable.Because it is a safety check to ensure availability, it is not recommended that you use the --skip-repo-validation flag unless you are completely certain that all nodes have functional read/write access to the repository and that the errors are the result of intermittent network timing issues.Experimental: SSL Certificate ValidationThis was a requested feature.  If you’re using SSL to connect to your Elasticsearch instance (you’re using , for example), if you have signed certificates you may see warnings that your certificate is invalid, even though it valid.  With the new validation feature, these false-positive warnings should go away.  In order to use this feature you need to be using Curator 3.2.0 and have the Python module installed.  This can be installed easily by running:pip install certifiWith the certifi module installed, SSL certificate validation should be automatic.  This feature is experimental, so please report any issues you may encounter at One more thing…All these features make for a great release.  But this isn’t all that we have cooked up for you.Package RepositoriesCurator is a Python-based product, which can complicate deployment on servers without installed.  The call for package-based deployment has been heard, and answered!: If you have already installed Curator by way of , you should continue to use .  These packages provide Python modules which may otherwise conflict with modules installed via or .We even have (64bit only)!  These binaries are compiled using the fantastic tool, .  As a result, you no longer need to have Python installed on Windows to use Curator.  Unzip the package, and run the Curator exe file in a CMD or PowerShell window.ConclusionWe hope you like the new features, and the availability of packages!  With the release of packages, and package repositories, Curator is now even easier to obtain and deploy—wherever you may need it.  ","locales":"","title":"Curator 3.2.0 Released"}
{"index":{}}
{"author":"Leslie Hawthorn","category":"","publish_date":"2015-07-01T00:00:00.000Z","url":"/blog/this-week-in-elastic-2015-07-01","seo_title":"This Week in Elastic: July 1, 2015","content":" Welcome to ! In this weekly round up post, we bring you news on all aspects of the ELK Stack, including updates on Elasticsearch, Logstash and Kibana development, tips and tricks and upcoming events. We’ve made (hosted ) more awesome. Now w/our new Premium & Standard plans — elastic (@elastic) Elastic News Useful blog posts, how tos, tutorials and articles from the wide world of all the ELK stack ecosystem. Slides and Videos \\Where to find Us We'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking about the ELK stack or hosting a meetup, AustraliaOur team will be in the exhibits hall at , on July 16 & 17. We're also proposing an open space on the ELK Stack, so perhaps we'll see you in the open space zone, too.France Germany IndiaThe Practical Big Data Science Meetup will host a hands on training on the ELK stack for log analysis and monitoring on July 4. to save your seat.Israel Japan大阪でもやります！スピーカー募集中です！/ Elasticsearch勉強会 in 大阪 - elasticsearch勉強会 | Doorkeeper - — Jun Ohtani (@johtani) 出張Elasticsearch勉強会第2弾！ / Elasticsearch勉強会 in 京都 - elasticsearch勉強会 | Doorkeeper - — Jun Ohtani (@johtani) New ZealandThe next meeting of the Elasticsearch ELK NZ meetup is coming to Wellington on July 2. We're still lining up the agenda, but for now to save your seat. PolandJoin in Poznan for . The conference runs July 2-4. South AfricaWe are a proud swag sponsor of the . If you happen to be in Johannesburg on July 11, pop in for knowledge and some cool Elastic goodies. SpainEuroPython 2015 heads to Bilbao this year, featuring two talk from : and . You can learn more about Honza's talks in the :  EuroPython runs July 20-26.SwitzerlandThe Elasticsearch Switzerland Meetup group will get together on July 14 to talk Elasticsearch Under the Hood with . to save your seat.UkraineThe L'viv DevOps Meetup will get together on July 4 for their second installment of the CoreOS Fest Conference review, featuring a CoreOS with Kubernetes demo: ElasticSearch Cluster on local machine and AWS. to save your seat. United Kingdom United States - East United States - West Where to Find You  If you're a regular reader, you're thinking of skipping this section. You may even be thinking to yourself, yes of course I will drop a note on Twitter when I am giving a talk on all things ELK. That's awesome, because we'd like to showcase every meetup, conference presentation and workshop on Elasticsearch, Logstash, and Kibana happening worldwide. And now, we've made it even easier for you to get support for your meetup! Head on over to  ! (And we'll still totally send you swag if you're giving a talk on anything ELKy at a conference.) Oh yeah, we're also  . If you'd like us to find you for employment purposes, just . We care more about your skill set and passion for Elasticsearch, Kibana, and Logstash than where you rest your head. Trainings If you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elastic - July 01, 2015"}
{"index":{}}
{"author":"Shay Banon","category":"News","publish_date":"2015-07-01T00:00:00.000Z","url":"/blog/we-just-made-found-more-awesome","seo_title":"","content":" It has been 4 months since , and they have been wonderful. The teams, now working together, broke ground when it comes to providing the best hosted Elasticsearch (and more) offering out there (even I played a ).All this hard work and unique opportunity to work closely together has resulted in today, when we launch . Found Standard, everything that you know and love about Found today with additional features and a  lower price point, and Found Premium, with SLA-driven support and in the future, Shield and Watcher on top of that.Found StandardFound is awesome, with dedicated Elasticsearch clusters, easy scaling, security built in, hourly pay as you go, and much more. We think this is the go-to solution when someone is looking for hosted Elasticsearch, and we want to make sure we make it available and affordable for everybody.Starting today, we have reduced the prices of Found considerably, one can easily get started with hosted Elasticsearch for .While reducing the prices is one important step in the right direction, we want to make sure everybody running on Found will have the best experience possible. Together with reducing the prices, we are also happy to announce that Found will come with , and .One of the important features of Found is the ability to choose on how many data centers your cluster will be deployed for high availability. Your data is important, and we want to make sure we can help you make the right choice. To reflect it, our pricing  your cluster is deployed on.We also, as you probably guessed, love Kibana, and we think Kibana is a groundbreaking way to visualize data in Elasticsearch. Kibana 4, our latest version of Kibana, comes with a server side component, which means it incurs an additional toll in order to provide it as a service. In the same spirit, and thanks to the incredible infrastructure the Found team has built and the close work with the Kibana team, we are also happy to say that any hosted Elasticsearch cluster will get its own  starting July 15th.Found PremiumWe are also happy to announce that we have taken our subscriptions offering around our open source products and now provide it on top of Found Standard, and we are calling it Found Premium.If you want SLA-based support, compared to forum based support, from the team that built the products, you now have that option. Whether it's a critical event and you need help, or looking for guidance and advice on how to best use our products to anticipate problems, we are here for you.In the near future, as it is part of our subscription offerings, we will also make , our one stop security product for Elasticsearch, and , our alerting product, available for no additional charge to Found Premium customers.It is humbling to see how our teams have worked together to make sure we can make all of this possible, and roll all this wonderful work to you, our users. The future is exciting. I am incredibly proud and we hope you like it – to learn more and get your questions answered! ","locales":"","title":"We Just Made Found More Awesome"}
{"index":{}}
{"author":"Suyog Rao","category":"Releases","publish_date":"2015-06-30T00:00:00.000Z","url":"/blog/logstash-1-5-2-released","seo_title":"","content":" Logstash 1.5.2 is now available for ! This is a bug fix release which resolves an important issue in file input. Check the for full details! http input plugin We are also announcing the availability of a new , which is bundled with 1.5.2. This is a general purpose input for sending data to Logstash through http(s). Interesting use cases, you ask? If your existing applications support webhooks, it's now possible to send actionable data through this input to leverage Logstash's rich plugin ecosystem. For example, your Continuous Integration platform such as Jenkins can push build failure notifications to Logstash and index them in Elasticsearch. At the same time you can page developers for critical build failures using conditionals and the PagerDuty output. Watcher integration More interestingly, using our recently announced product, you can set up notifications triggered from data in your Elasticsearch cluster to be escalated through , notify co-workers on and archived to for long term storage. All this can be accomplished in a single (and simple) Logstash configuration. Watch out for a more detailed blog post about this plugin and its use cases. Bug Fixes Fixed a critical bug where new files added to a directory being watched would cause an error. This issue also happens when wildcards were used to watch files matching a certain pattern (). String interpolation is widely used in LS to create keys combining dynamic values from extracted fields. For example, this is used to create an Elasticsearch index name based on the timestamp of an event. We added an optimization which compiles this interpolation template on first use and caches in-memory for subsequent uses. Our performance testing show about 20% increase in throughput for configs that involve a lot of date processing and use field reference syntax ) This input was not handling backpressure properly from downstream plugins and would not timeout client connections, causing clients to constantly reconnect and eventually cause Logstash to run out of memory. We added a circuit breaker to stop accepting new connections when we detect this situation. Please note that `max_clients` setting introduced in v0.1.9 has been deprecated. This setting temporarily solved the problem by configuring an upper limit to the number of LSF connections (). Please Logstash 1.5.2 and let us know what you think on Twitter () or on our . You can report any problems on the . ","locales":"","title":"Logstash 1.5.2 released"}
{"index":{}}
{"author":"Leslie Hawthorn","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2015-06-30T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2015-06-30","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top NewsHave an upgrade on the horizon? Get some tips on leveling up your cluster from next week: — elastic (@elastic) Elasticsearch Core Apache Lucene Watch This Space , where we'll share more news on the whole ELK ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - June 30, 2015"}
{"index":{}}
{"author":"Shaunak Kashyap","category":"Engineering","publish_date":"2015-06-30T00:00:00.000Z","url":"/blog/kibana-4-1-field-formatters","seo_title":"","content":" One of the new features in Kibana 4.1 that we are  excited about is field formatters. Field formatters allow you to display your data in Kibana the way you prefer to  it, regardless of how it is actually  in Elasticsearch. For instance, if you are storing date values in Elasticsearch and would like them to be displayed in mm/dd/yyyy format, you can easily do that now, courtesy of field formatters! Here's how you can start formatting your fields in Kibana : Watch the following short screencast as it walks through the steps listed above. In this example, a field with a raw number value representing bytes of some data is being formatted using the Bytes field formatter: But field formatters can be used for more than formatting bytes values. You can create links, insert images (see screenshot below), and much more.Here is an example of using http status cats to represent the HTTP status codes from the log files: And here's what it ends up looking like: Go ahead. Give it a whirl. ","locales":"","title":"Field Formatters in Kibana 4.1"}
{"index":{}}
{"author":"Andrei Stefan","category":"","publish_date":"2015-06-30T00:00:00.000Z","url":"/blog/elasticsearch-logging-secrets","seo_title":"","content":" Like every other software out there, Elasticsearch provides some insight into what's happening with its internals while running. This is an informative process and can reveal quickly if something bad happened (in the form of an error message), or it can decisively determine the cause of a performance issue or why the cluster is behaving in a way users might not like. The Basics Main Elasticsearch logs are written to file. For this file the default level is INFO, thus being sufficient for a rather moderate amount of information and, at the same time, not create a huge log file. Logging settings are specified in the logging.yml configuration file. While this is a convenient way of having the log settings in a single place, having to modify this file on every node every time you want to change the settings is painful because it requires nodes restart and unnecessary downtime.Good news! Elasticsearch allows you to  .The same applies to  the second type of logging that it provides: . These provide a very useful insight into how fast or slow queries/indexing process/fetches are performing. The slowlog file will show the query itself, on which shards it ran and how much time it took to complete. For the logs, there is a list of \"levels\" which indicate how granular the logs will be. In the order of granularity from low to high, we have: WARN, INFO, DEBUG, TRACE. Setting the log level to WARN will display only WARN logging messages. On the other hand, setting the level to INFO will display both WARN and INFO logging messages, so this is a \"cumulative\" set of log messages. Similarly, for TRACE every log message in Elasticsearch will be printed in the log files. Setting the level to TRACE will create a considerable amount of chatter. Dynamically changing the root logger level This is not necessarily a tricky task :-), but for completeness sake and to warm up before the following tips&tricks let's first (the one from which all other loggers inherit). You can run a query like the following, while the cluster is running of course: PUT /_cluster/settings {\"transient\":{\"logger._root\":\"DEBUG\"}} Very simple and quick. Now you should see some more interesting things in the logs, some messages that weren't available with INFO or WARN levels. Dynamically changing the logger level per Java package Many times it is very useful to see logging messages that are related to a certain part or functionality of Elasticsearch. Increasing the logging level globally in all modules in Elasticsearch would result in a potentially very large log file, so Elasticsearch allows  . . Let's take, for example, the package . It contains classes that deal with. If you have an issue with nodes joining/leaving the cluster, or with master election, this would be the package to monitor in your logs: PUT /_cluster/settings {\"transient\": {\"logger.discovery.zen\":\"TRACE\"}} You notice in the command that \"logger\" is again present and, yes, it should always be. But, how did we choose \"discovery.zen\". You remember I mentioned above that the logging level can be set per Java package. Well, \"discovery.zen\" is just that, without the already present \"org.elasticsearch\". What if you are interested only in the master node fault detection messages? The Java package containing classes for master fault detection is . And the Elasticsearch command would be: PUT /_cluster/settings {\"transient\": {\"logger.discovery.zen.fd\":\"TRACE\"}} You got the picture. : Slowlog settings at index level You remember that I mentioned , without nodes or cluster restart. This is valid for slow logs, as well. Imagine you have several tens of indices in your cluster and you suspect that one of the older indices is performing poorly. You'd want to investigate this index more closely and monitor the query response times. The first approach would be to and grep the resulting slowlog file for your index name. This will work, indeed, but why having to grep for the desired index name? Instead, : PUT /test_index/_s","locales":"","title":"Elasticsearch Logging Secrets"}
{"index":{}}
{"author":"Clinton Gormley","category":"News","publish_date":"2015-06-29T00:00:00.000Z","url":"/blog/elasticsearch-2.0.0.beta1-coming-soon","seo_title":"","content":" We are gearing up to release which takes advantages of all of the improvements available in . This forthcoming release will deliver a few awesome user facing features such as: Pipeline AggregationsThe ability to run aggregations such as derivatives, moving average, and series arithmetic on the results of other aggregations. This functionality was always do-able on the client-side, but pushing the computation into Elasticsearch makes it easier to build more powerful analytic queries, while simplifying client code considerably. It opens up the potential for predictive analytics and anomaly detection. Query/Filter mergingFilters are no more. All filters clauses have now become query clauses instead. When used in , they have an effect on relevance scoring and, when used in , they simply exclude documents which don’t match, just like filters do today. This restructuring means that query execution can be automatically optimized to run in the most efficient order possible. For instance, slow queries like phrase and geo queries first execute a fast approximate phase, then trim the results with a slower exact phase. In filter context, frequently used clauses will be cached automatically whenever it makes sense to do so. Configurable store compressionThe setting allows you to choose between the LZ4 compression for speed (), or DEFLATE for reduced index size (). This is particularly useful for logging, where old indices can switch to before being optimized. Blog posts about the above topics will follow shortly. ","locales":"de-de,fr-fr,ja-jp","title":"Elasticsearch 2.0.0.beta1 coming soon!"}
{"index":{}}
{"author":"Clinton Gormley","category":"Engineering","publish_date":"2015-06-29T00:00:00.000Z","url":"/blog/great-mapping-refactoring","seo_title":"","content":" One of the biggest sources of pain for users of Elasticsearch today is the ambiguity that exists in type and field mappings. This ambiguity can result in exceptions at index time, exceptions at query time, incorrect results, results that change from request to request, and even index corruption and data loss. In the quest to make Elasticsearch more solid and predictable, we have made a number of changes to make field and type mappings stricter and more reliable. In most cases, we only enforce the new rules when creating new indices in Elasticsearch v2.0, and we have provided a backwards compatibility layer which will keep your old indices functioning as before. However, in certain cases, such as in the presence of conflicting field mappings as explained below, we are unable to do so.  If the data in these indices is no longer needed, then you can simply delete the indices, otherwise you will need to reindex your data with correct mappings. Changing how mappings work is not a decision that we take lightly. Below I explain the problems that exist today and the solutions that we have implemented. Deleting mappings Finally, it is no longer possible to delete a type mapping (along with the documents of that type). Even after removing a mapping, information about the deleted fields continues to exist at the Lucene level, which can cause index corruption if fields of the same name are added later on. You can either leave mappings as they are or reindex your data into a new index. Preparing for 2.0 ","locales":"","title":"The Great Mapping Refactoring"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2015-06-29T00:00:00.000Z","url":"/blog/2015-06-29-where-in-the-world-is-elastic","seo_title":"Where in the World is Elastic, June 29, 2015","content":" Welcome to Summer is almost here (it technically is already :), so things may calm down a little bit in event and meetup land. W still have some things going on this week, so see whether one of these awesome events is happening near you!Upcoming EventsJune 30 - July 3:  - will give at talk about  from 11 -11:45 AM and will also give a   from 2-5:30 PM on July 1.July 2 - 4:  - has some knowledge to share around Polyglot persistence at 4 PM on July 2.Upcoming MeetupsJune 30:  (joint event with Meteor South Bay)June 30: June 30: June 30: June 30: July 2: July 1: July 1: July 2: July 4:  That's it for this week. Stay tuned for Elastic happenings next week - there's much to come! - The Elastic TeamP.S.:   if you're interested in hosting a meetup or are giving a talk about Beats, Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic? - June 29, 2015"}
{"index":{}}
{"author":"Spencer Alger","category":"Engineering","publish_date":"2015-06-29T00:00:00.000Z","url":"/blog/kibana-4-1-1-released","seo_title":"","content":" Today we are releasing a quick update to Kibana 4.1 which fixes a critical bug preventing server error messages from rendering properly as well as a few other small bugs.Grab the new release here: bug fixes ","locales":"","title":"Kibana 4.1.1 Released"}
{"index":{}}
{"author":"Shaunak Kashyap","category":"","publish_date":"2015-06-26T00:00:00.000Z","url":"/blog/logstash-lines-2015-06-26","seo_title":"","content":" Welcome back to ! In these weekly posts, we'll share the latest happenings in the world of Logstash and its ecosystem. This week, we have a couple of interesting blog posts to share from the Logstash community:Logstash Core and Plugins That's this week in . Come back next week for more Logstash news! ","locales":"","title":"The Logstash Lines — Stories from the community, performance improvements, and more!"}
{"index":{}}
{"author":"Uri Boness","category":"News","publish_date":"2015-06-25T00:00:00.000Z","url":"/blog/watcher-1-0-released","seo_title":"","content":" We are thrilled to announce the first GA release of Watcher 1.0. Watcher was built to play a key role in the Elastic product stack and bring alerting and notification functionality to Elasticsearch. Elasticsearch itself is an amazing system that enables fine grained exploration of data... massive amounts of data. And now with Watcher, much of this data exploration can be automated. Watcher enables you to register to watch your data and act on events of interest. A Watch consists of four main parts - , , and . The following pseudocode describes the roles of these different parts in the watch execution: Trigger fires an event e watch.execute(e) payload = input.load(e) if condition.eval(payload) for each action in actions action.execute(e, payload) Lets look at at how this simple execution model translates to a real-world scenario. In this example, we assume you use Elasticsearch to index and store events. These events can be log entries from various systems that are fed by  or perhaps network monitoring events fed by . You don't index these events just for the sake of storing them:  in most cases, the main reason for indexing them is to gain insight into what exactly goes on in the various systems you are monitoring. Part of this overview is to pick up on misbehaving systems, in which case, you want to do it as close to real time as possible. So lets look at a simple example for a watch that looks for error events and if/when found, sends a notification to the appropriate stakeholders. Assuming you have Elasticsearch already installed, the first thing you need to do is install Watcher. Watcher is a standard plugin to Elasticsearch and as such, you install it in the same way you install any other plugin: bin/plugin -i elasticsearch/license/latest For Watcher to work, you will also need to install the License plugin, but don't worry, you don't need a license to test watcher and play around with it, a 30 days trial license is automatically installed and opens up Watcher's full functionality. If you would like to extend the trial license, just contact us and we will be happy to extend it. bin/plugin -i elasticsearch/watcher/latest Now that you have Watcher installed, start Elasticsearch and add your first watch: <span class=\"pln\">curl -XPUT 'http://localhost:9200/_watcher/watch/error_status' {</span> \"trigger\"<span class=\"pln\">: {</span> \"schedule\"<span class=\"pln\"> : { </span>\"interval\"<span class=\"pln\"> : </span>\"5m\"<span class=\"pln\"> } },</span> \"input\"<span class=\"pln\"> : {</span> \"search\"<span class=\"pln\"> : {</span> \"request\"<span class=\"pln\"> : {</span> \"indices\"<span class=\"pln\"> : [</span> \"<events-{now/d}>\"<span class=\"pln\">,</span> \"<events-{now/d-1d}>\"<span class=\"pln\"> ],</span> \"body\"<span class=\"pln\"> : {</span> \"query\"<span class=\"pln\"> : {</span> \"filtered\"<span class=\"pln\"> : {</span> \"query\"<span class=\"pln\"> : { </span>\"match\"<span class=\"pln\"> : {</span> \"status\"<span class=\"pln\"> :</span> \"error\"<span class=\"pln\"> }},</span> \"filter\"<span class=\"pln\"> : {</span> \"range\"<span class=\"pln\"> : {</span> \"_timestamp\"<span class=\"pln\"> : {</span> \"from\"<span class=\"pln\"> :</span> \"now-5m\" <span class=\"pln\">}}} } } } } } },</span> \"condition\"<span class=\"pln\"> : {</span> \"compare\" <span class=\"pln\">: { </span>\"ctx.payload.hits.total\"<span class=\"pln\"> : { </span>\"gt\"<span class=\"pln\"> : 0 }} },</span> \"actions\"<span class=\"pln\"> : {</span> \"email_admin\"<span class=\"pln\"> : {</span> \"email\"<span class=\"pln\"> : {</span> \"to\"<span class=\"pln\"> : </span>\"admin@domain\"<span class=\"pln\">,</span> \"subject\"<span class=\"pln\"> : </span>\"Error Events\"<span class=\"pln\">,</span> \"priority\"<span class=\"pln\"> : </span>\"high\"<span class=\"pln\">,</span> \"body\"<span class=\"pln\"> :</span> \"Found {{ctx.payload.hits.total}} erroneous events\"<span class=\"pln\"> } } } }</span> Once registered, the above watch will be triggered and be executed every 5 minutes. When executed the input searches for events with an error status (note the index name pattern that ","locales":"","title":"Watcher 1.0 Goes GA"}
{"index":{}}
{"author":"Shaunak Kashyap","category":"","publish_date":"2015-06-22T00:00:00.000Z","url":"/blog/logstash-lines-2015-06-22","seo_title":"","content":" Welcome back to ! In these weekly posts, we'll share the latest happenings in the world of Logstash and its ecosystem. This week, gain a deeper understanding of how to use the ever-popular from Greg Medford. Here's his presentation on the topic from the just-concluded Monitorama conference in Portland: from Logstash Core and Plugins That's this week in . Come back next week for more Logstash news! ","locales":"","title":"The Logstash Lines — Deep dive into grok, install plugins offline, and more!"}
{"index":{}}
{"author":"Costin Leau","category":"News","publish_date":"2015-06-24T00:00:00.000Z","url":"/blog/elasticsearch-for-apache-hadoop-2-1-spark-storm-and-more","seo_title":"Elasticsearch for Apache Hadoop 2.1 GA: Spark, Storm and More","content":" Elasticsearch for Apache Hadoop, affectionately known as ES-Hadoop, enables Hadoop users and data-hungry businesses to enhance their work-flows with a full-blown search and analytics engine, in real-time. We are thrilled to announce the GA release of . This GA release has been the result of 4 successful Betas: , , , , plus , over the last 10 months. We would like to thank the community and all our users for their valuable feedback during the Beta process. Version 2.1 embraces the emerging, real-time components in the Hadoop ecosystem, (in particular and ), beefs up security support by adding SSL/TLS transport encryption with both HTTP and PKI authentication, and introduces a YARN module. All while preserving , extending the number of supported Hadoop , and gaining new certifications. Plus, we're still bringing you all this functionality with only a single JAR file to download, no dependencies required. In fact, upgrading to 2.1 is simply a matter of updating the ES-Hadoop JAR. However, to help those with longer release cycles, we have also released , the last maintenance release for the 2.0.x branch. It contains important bug-fixes without introducing any new features. Native Integration with Spark and Spark SQL Since its announcement, Apache Spark has taken the 'Big Data' world by storm. While ES-Hadoop 2.0 provides support for Spark through its Map/Reduce functionality, 2.1 goes beyond that, providing Java and Scala APIs tightly integrated with Spark Core and Spark SQL. This means one can easily index and analyze data through Elasticsearch in real-time directly from Spark. Simply put, through ES-Hadoop, Spark treats Elasticsearch indexes as s (Resilient Distributed Dataset) and s. Furthermore, the connector can take Spark and index it or run SQL queries on top Elasticsearch indexes. In good ol' ES-Hadoop tradition, the developer never has to leave her environment:  the same Spark API, conventions and data types work transparently on top of Elasticsearch. import org.elasticsearch.spark._ val sc = new SparkContext(conf) // collection of data val numbers = Map(\"one\" -> 1, \"two\" -> 2, \"three\" -> 3) val airports = Map(\"arrival\" -> \"Otopeni\", \"SFO\" -> \"San Fran\") sc.makeRDD(Seq(numbers, airports)).saveToEs(\"spark/docs\") // case classes case class Trip(departure: String, arrival: String, days: long) val upcomingTrip = Trip(\"OTP\", \"SFO\", 10) val lastWeekTrip = Trip(\"MUC\", \"OTP\", 3) val rdd = sc.makeRDD(Seq(upcomingTrip, lastWeekTrip)) sc.saveToEs(rdd, \"spark/trips\") Also available in Spark SQL as (or <code>SchemaRDD for those on Spark 1.1-1.2): // as a DataFrame val df = sqlContext.read().format(\"org.elasticsearch.spark.sql\").load(\"spark/trips\") df.printSchema() // root //|-- departure: string (nullable = true) //|-- arrival: string (nullable = true) //|-- days: long (nullable = true) val filter = df.filter(df(\"arrival\").equalTo(\"OTP\").and(df(\"days\").gt(3)) And also in pure SQL: CREATE TEMPORARY TABLE trips USING org.elasticsearch.spark.sql OPTIONS (path \"spark/trips\") SELECT departure FROM trips WHERE arrival = \"OTP\" and days > 3 The integration is not only elegant and out of the developer's way, but also quite efficient:  ES-Hadoop connector pushed down all of Spark SQL queries translating them into , thus effectively executing the queries directly on Elasticsearch and leveraging its lightning fast search capabilities:  both of the queries above are translated by the connector at runtime into: { \"query\" : { \"filtered\" : { \"query\" : { \"match_all\" : {} }, \"filter\" : { \"and\" : [{ \"query\" : { \"match\" : { \"arrival\" : \"OTP\" } } }, { \"days\" : { \"gt\" : 3 } } ] } } } } Note that as with all the other integrations, through ES-Hadoop all the reads and writes in the example above are executed in parallel across the index shards, for what we call partition-to-partition architecture. 2.1 supports Spark 1.0 through 1.4,","locales":"","title":"Elasticsearch for Apache Hadoop 2.1 GA: Spark, Storm and More"}
{"index":{}}
{"author":"Leslie Hawthorn","category":"","publish_date":"2015-06-24T00:00:00.000Z","url":"/blog/this-week-in-elastic-2015-06-24","seo_title":"This Week in Elastic: June 24, 2015","content":" Welcome to ! In this weekly round up post, we bring you news on all aspects of the ELK Stack, including updates on Elasticsearch, Logstash and Kibana development, tips and tricks and upcoming events. for Apache 2.1 GA is here. Now w/native support for Spark, Storm + more — elastic (@elastic) Elastic News Useful blog posts, how tos, tutorials and articles from the wide world of all the ELK stack ecosystem. NEST and .NET 1.6 just released! — Greg Marzouka (@gregmarzouka) Happy Wednesday. Puppet module 0.9.7 released with bugfixes and hot new features. Grab it at — Richard pijnenburg (@Richardp82) Slides and Videos Where to find UsCome say hello to at Amsterdam. We have cool swag and hot information for you! — Shaunak Kashyap (@shaunak) We'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking about the ELK stack or hosting a meetup, Australia ChileJoin the Lenguajes dinámicos meetup group in Santiago tonight, June 24, to talk Elasticsearch and Ruby. to save your seat. France GermanyJoin our team in the exhibits hall at on July 9. We'll be there to answer all your questions and give you some lovely Elastic goodies. India Israel will visit the Tel Aviv Elastic User Group on July 8 to talk Resiliency and Elasticsearch. to save your seat. KoreaThe next Korea Elasticsearch Study Session will be held on July 1. to attend. The Netherlands returns to the Pakhuis de Zwijger June 24-26. You can visit us at our booth or hear from on her latest DevOps and Culture topic, . Rumor has it that the amazing BBQ will be reprised this year, so miss this one! New ZealandThe next meeting of the Elasticsearch ELK NZ meetup is coming to Wellington on July 2. We're still lining up the agenda, but for now to save your seat. Poland and will attend Devoxx Poland 2015 taking place June 22-25 in Krakow. Don't miss their talk on June 25 at 1:30 PM: . And last but not least, join in Poznan for . The conference runs July 2-4. South AfricaWe are a proud swag sponsor of the . If you happen to be in Johannesburg on July 11, pop in for knowledge and some cool Elastic goodies. Spain UkraineThe L'viv DevOps Meetup will get together on June 27 for CoreOS Fest Conference review, featuring a CoreOS with Kubernetes demo: ElasticSearch Cluster on local machine and AWS. to save your seat. United Kingdom United States - East United States - West Where to Find You  If you're a regular reader, you're thinking of skipping this section. You may even be thinking to yourself, yes of course I will drop a note on Twitter when I am giving a talk on all things ELK. That's awesome, because we'd like to showcase every meetup, conference presentation and workshop on Elasticsearch, Logstash, and Kibana happening worldwide. And now, we've made it even easier for you to get support for your meetup! Head on over to  ! (And we'll still totally send you swag if you're giving a talk on anything ELKy at a conference.) Oh yeah, we're also  . If you'd like us to find you for employment purposes, just . We care more about your skill set and passion for Elasticsearch, Kibana, and Logstash than where you rest your head. Trainings If you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elastic - June 24, 2015"}
{"index":{}}
{"author":"Leslie Hawthorn","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2015-06-23T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2015-06-23","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top NewsFaster Analytics through Elasticsearch (written by me) — Harold Neal (@metacreek) Elasticsearch Core Apache LuceneWatch This Space , where we'll share more news on the whole ELK ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene: Faster Analytics through Elasticsearch"}
{"index":{}}
{"author":"Uri Boness","category":"News","publish_date":"2015-06-19T00:00:00.000Z","url":"/blog/watcher-1-0-RC1-released","seo_title":"","content":" ","locales":"","title":"Watcher 1.0 RC1 - Try it Now!"}
{"index":{}}
{"author":"Tal Levy","category":"Engineering","publish_date":"2015-06-23T00:00:00.000Z","url":"/blog/logstash-kafka-intro","seo_title":"","content":" As you may of heard, we added support with Logstash 1.5! What is included? Both input and output plugins! This blog is a first in a series of posts introducing various aspects of the integration between Logstash and Kafka. Today, we’ll go over some of the basics. For documentation on all the options provided you can look at the plugin documentation pages: The Apache Kafka defines Kafka as: \"Kafka is a distributed, partitioned, replicated commit log service. It provides the functionality of a messaging system, but with a unique design.\" Why is this useful for Logstash? Kafka is quickly becoming the de-facto data-bus for many organizations and Logstash can help enhance and process the messages flowing through Kafka. Another reason may be to leverage Kafka's scalable persistence to act as a message broker for buffering messages between Logstash agents. Here, we will show you how easy it is to set up Logstash to read and write from Kafka. You may follow for launching a local Kafka instance. Once launched, you can go ahead and create a test topic we will use in the examples. # create \"logstash_logs\" topic $ bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic logstash_logs Kafka comes with a simple console producer to help quickly test writing to Kafka. In some ways, it is even easier to use Logstash as a replacement for that tool! We can use the input plugin to allow us to write messages to a specific Kafka topic. $ bin/logstash -e \"input { stdin {} } output { kafka { topic_id => 'logstash_logs' } }\" Logstash Kafka output plugin uses the official Kafka producer. All of its options are exposed to the plugin. One important option that is important is the which defines acknowledgment semantics around how many Kafka Brokers are required to acknowledge writing each message. By default, this is set to -- this means that the producer never waits for an acknowledgement. This option provides the lowest latency but the weakest durability guarantees. Setting this to , the producer will wait for an acknowledgement from the leader replica. is the safest option, where it waits for an acknowledgement from all replicas that the data has been written. More details surrounding other options can be found in the plugin’s page and also . To verify that our messages are being sent to Kafka, we can now turn on our reading pipe to pull new messages from Kafka and index them into using Logstash's output plugin. $ bin/logtash -e \"input { kafka { topic_id => 'logstash_logs' } } output { elasticsearch { protocol => http } }\" The Kafka input plugin uses the high-level consumer under the hoods. Each instance of the plugin assigns itself to a specific consumer group (“logstash” by default). This way we leverage the partitioning properties of consuming data from Kafka as is done in the high-level consumer. By leveraging these consumer groups we can simply launch multiple logstash instances to scale the read throughput across the partitions. Kafka implements a consumer rebalancing algorithm to efficiently distribute partitions across newly introduced consumers. Consumer offsets are committed to Kafka and not managed by the plugin. You may want to replay messages -- if that is the case, offsets can be disregarded and you may read from the beginning of a topic by using the configuration option. Storage of consumer offsets is defaulted to Zookeeper. More details surrounding other options can be found in the plugin’s documentation page. Logstash processing pipelines can grow very complex and cpu-intensive as more plugins like grok are introduced. By default, Logstash implements a back-pressure mechanism wherein inputs are blocked until the later processing units are free to accept new events. Under this scheme, input events are buffering at the source. This may be a problem for inputs which do not natively support buffering of sent messages, and may create additional resource constraints on inputs lik","locales":"","title":"Kafka and Logstash 1.5 Integration"}
{"index":{}}
{"author":"Livia Froelicher","category":"User Stories","publish_date":"2015-06-18T00:00:00.000Z","url":"/blog/shield-q-and-a-how-dhl-uses-shield-to-secure-mission-critical-logistics-data-in-elasticsearch","seo_title":"","content":" At Elastic, one of our greatest pleasures is learning and sharing how our customers are utilizing our software to achieve various organizational and personal goals.  Earlier this year we released  to make it super easy for anyone to add security to their Elasticsearch deployment (try it for yourself with ).  Today, we are excited to share how DHL Supply Chain, a global leader in the logistics industry, has experienced success with Shield. DHL Supply Chain offers a variety of supply chain services in over 220 countries, including assembly and packaging, warehousing, and transportation. In order to provide customers visibility across their supply chain, DHL integrates its Warehouse Management and Transportation Management System with its customers’ ERP systems. The messages that pass through DHL LINK, DHL's integration layer, are indexed in Elasticsearch, allowing DHL’s customers to search through multiple terabytes of data to find the latest status of a specific EDI order message using a PO number or other reference to see that it has been sent successfully to the customer (e.g. the status of the EDI message is “sent”). ","locales":"","title":"Shield Q&A: How DHL Uses Shield to Secure Mission Critical Logistics Data in Elasticsearch"}
{"index":{}}
{"author":"Jay Modi","category":"Engineering","publish_date":"2015-06-24T00:00:00.000Z","url":"/blog/shield-1-3-and-1-2-2-released","seo_title":"","content":" Today we’re excited to announce Shield 1.3 and Shield 1.2.2! Read below for all of the details and then download it . Shield 1.2.2 is a bugfix release, please refer to the for details on what has been fixed. Shield 1.3 is the latest feature release and is our first release to introduce a new realm! Shield 1.3 also includes a new output for auditing and several other enhancements. Here are the highlights: pki realmThe is the first new realm to be introduced since Shield was released and is a very important realm. We received a lot of feedback from users who wanted to directly authenticate their application servers without storing user credentials. In many of these cases, the PKI realm can be used in place of storing and passing credentials. The PKI realm uses X.509 certificates for authentication and maps the distinguished name (DN) to a user via the . index output for auditingAn index based output for has been added. This output allows indexing of audit events into the current cluster or a . This means that the audit logs can now be searched and analyzed using elasticsearch out of the box. For more details on configuring the index based auditing, please refer to the . Here's an example Kibana dashboard based on the audit data:breaking changesShield 1.3 does contain a few breaking changes, though in most cases, upgrading to Shield 1.3 will not require any additional changes. The first breaking change is that the and hashing algorithms have been removed as options for setting. If you are using either of these, please specify one of the other or remove this setting altogether to fall back on the default, . Additionally, the file now only supports password hashes. The tool has always generated hashes, so as long as this tool is used, there will be no issues when upgrading to Shield 1.3. other changesRefer to the Shield 1.3 for the full list of changes including bug fixes and other enhancements. upgradingPlease refer to the of the Shield documentation. feedbackWe would love to hear any feedback that you may have via the category in our forums. ","locales":"","title":"Shield 1.3 and 1.2.2 Released"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2015-06-22T00:00:00.000Z","url":"/blog/2015-06-22-where-in-the-world-is-elastic","seo_title":"","content":" Welcome to This week we're visiting 4 continents. Read on for the full list of events and meetups below. We hope you'll join us if one is on near you!Upcoming EventsJune 21-25:   - Co-located with Red Hat Summit in Boston, this developer focused conference will feature on The Democratization of DevOps. You can at on June 23 at 2:45 PM.June 22-25: , Washington DC: Join our team in the exhibits hall to get your questions answered. June 23-26: , Boston - Come say hi at our booth to talk all things Elastic and grab a cool piece of swag!June 22-26: -  will give a talk on on Tuesday, June 23, 11:50 AM -12:50 PM. and will present on  on Wednesday, June 24, at 3:10 PM.. David and Joao will also give a on Thursday, June 25 from 1:00 p.m. - 5:00 p.m. June 24-26:  - Don't miss talk on , Thursday, June 25 at 12:00p.m. We also have a booth so make sure to stop by!Upcoming MeetupsJune 23: June 24: June 24: June 24: June 25: June 24:  (Santiago)June 23: June 23: (Moscow)June 24: June 24: June 25: June 25: June 27: June 28: That's it for this week. Stay tuned for Elastic happenings next week - there's much to come! - The Elastic TeamP.S.:   if you're interested in hosting a meetup or are giving a talk about Beats, Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic: RedHat Summit Boston & DevOps Days Amsterdam"}
{"index":{}}
{"author":"Costin Leau","category":"News","publish_date":"2015-06-17T00:00:00.000Z","url":"/blog/es-hadoop-2-1-rc1-released","seo_title":"","content":" The first release candidate of Elasticsearch for Apache Hadoop (ES-Hadoop) 2.1 is . Exactly 50 days after Beta4, RC1 completes the features scheduled for 2.1, improves the stability of the code and polishes the documentation. The goodies are described below:  however we understand if you want to grab the binaries right away from the or . Spark SQL Push Down2.1 RC1 not only supports the just released Spark 1.4 but also introduces support for Spark SQL. That is, through ES-Hadoop, Spark SQL is translated to Elasticsearch so the operations are actually to the storage and thus efficiently executed so that only the needed results are returned back to Spark.  For example: SELECT reason FROM trips WHERE id>=1 AND airport=\"OTP\"is translated into: { \"query\" : { \"filtered\" : { \"query\" : { \"match_all\" : {} }, \"filter\" : { \"and\" : [{ \"query\" : { \"match\" : { \"airport\" : \"OTP\" } } }, { \"range\" : { \"gte\" : 1 } } ] } } } } This significantly increases performance and also again minimizes the amount of I/O, memory and CPU required when working against Elasticsearch. Note that the translation applies even if the user has a query defined (on the for the and can be configured to work on (default) and terms so, whatever the Elasticsearch index configuration, one can get started right away. Moverover in terms of usage, ES-Hadoop not only implements all of Spark SQL API but also the so all of Spark SQL (including table creation, insertion and ) are fully supported: CREATE TABLE esIndex USING org.elasticsearch.spark.sql OPTIONS (path \"spark/docs\") INSERT OVERWRITE TABLE esIndex SELECT * FROM existingSparkTable // Spark 1.4 style val df = sqlContext.read.load(\"examples/src/main/resources/users.parquet\") df.select(\"name\", \"favorite_color\").write.mode(SaveMode.ErrorIfExists) .format(\"org.elasticsearch.spark.sql\").save(\"users/colors\") Note that the excellent Spark compatibility (1.0-1.4) is . Security improvementsOn the security front, the SSL/TLS handshake has been improved so that protocol errors are better diagnosed and properly exposed to the user. In addition ES-Hadoop officially supports (Public Key Infrastructure) authentication and, to ease setup, features a dedicated documentation on Security. Also the HDFS snapshots/restore has been improved to better work in Kerberos environments and the acquisition reworked to minimize the number of client connections within the cluster. Last but not least, the various libraries have been upgraded while maintaining backwards compatibility - this includes upgrading to Apache Storm 0.9.5, Apache Hive 1.2, Apache Pig (0.15 - if you are using it in a Hadoop 1.x environment) and of course, Elasticsearch 1.6 (do upgrade to it). FeedbackLet us know what you think about RC1! We love to hear from you on , or the . ( works too). Cheers, ","locales":"","title":"Elasticsearch for Apache Hadoop 2.1 RC1 is out"}
{"index":{}}
{"author":"Leslie Hawthorn","category":"User Stories","publish_date":"2015-06-17T00:00:00.000Z","url":"/blog/elasticsearch-opentracker-leadboxer","seo_title":"","content":" is an analytics company focusing on website and mobile app user behavior. Today, our 1000s of clients from webmasters to corporate, financial and governmental institutions have become addicted to seeing exactly who, and how, people are using their online services in real-time, letting them track users, improve client experience, and increase conversion rates. Read on to learn how Opentracker.net and our experiences with Elasticsearch led us to create , a sophisticated web and mobile based lead capture and qualification system.In our early stages, like a lot of other companies founded during this period, Opentracker’s technology was built around MySQL. While providing a great starting point, MySQL suffered from inherent bottlenecks relative to scaling and structuring big data. As NoSQL solutions began appearing, we explored them and eventually settled on Cassandra, which resolved many of our data maintenance, redundancy, storage, and scaling issues. The one thing it did not provide us with, however, was real-time search. Real-time searches are important because every client has specific insights they want to extract from their data, and they need the flexibility to find exactly what they need to improve their business. Some clients are interested in users from certain locations, so that they can focus their on-boarding efforts by sales region, while others are interested in finding users on conversion pages that pre-qualify the visitors with interest in a specific product or service. Others are looking for users that match a specific lead qualification criteria in order to increase the chance of making sales, allowing sales agents to target potentials more effectively, and make warmer calls. In order to provide this functionality, we experimented with Apache Solr and Elasticsearch solely for the purpose of running real-time searches on both web and mobile app user data. Elasticsearch worked smoothly and presented an easy learning curve, while its schemaless structure allowed our clients to search through high volumes of unstructured data with flexibility, stability, and good response times. After implementing Elasticsearch, we started getting the same request from our B2B users: \"Show me the companies who are visiting, and help me get in touch with them.\" An opportunity presented itself: to develop a new service that could provide intelligence in the process of generating and capturing leads. We branched the technology that targeted the identification of companies, and . LeadBoxer’s value proposition? Identify sales leads from online activity, then qualify the leads based on metrics the client finds important. (You can check out the service at by setting up a trial account.) After implementing Elasticsearch in Opentracker, we realised we could use it to power LeadBoxer’s basic functionality by manipulating the scoring and ordering algorithms. Our guess was that we could utilize the and to help sort and rank leads by importance. At the time, documentation on the subject was limited and distributed through many different community channels, so we started to develop our own API to query Elasticsearch in order to create a lead scoring system that could be customized and generated by our customers. We were taking a risk. We thought that search engines were designed for searching through docs, not qualifying sales leads! The big question was would it work? The short answer, is . We now use Elasticsearch to deliver useful, qualified, and beautifully-designed lead results to sales and marketing teams. Our customers can adjust sliding weights to influence a lead score preference, and the preferences are converted into percentile values. The values are stored and converted to a formula in the API call, which in turn translates the formula to an Elasticsearch query. The Elasticsearch query returns the leads, ranked and prioritised based on the for","locales":"","title":"Elasticsearch: Powering Real-Time Mobile and Web Analytics for LeadBoxer and Opentracker"}
{"index":{}}
{"author":"Leslie Hawthorn","category":"","publish_date":"2015-06-17T00:00:00.000Z","url":"/blog/this-week-in-elastic-2015-06-17","seo_title":"","content":" Welcome to ! In this weekly round up post, we bring you news on all aspects of the ELK Stack, including updates on Elasticsearch, Logstash and Kibana development, tips and tricks and upcoming events. Rivers have been removed in code base. Time to switch to other ETL guys! Try for example! — David Pilato (@dadoonet) Elastic News Useful blog posts, how tos, tutorials and articles from the wide world of all the ELK stack ecosystem. Getting started w/? Join on June 24 for an in-depth webinar & Q&A — elastic (@elastic) Slides and VideosFor all you Apache Hadoop lovers out there, Elasticsearch <3s MapR-DB An introduction to Elasticsearch for Apache Hadoop from yesterday's Elastic Silicon Valley Meetup Our first ever Developer Hangout: on Logstash and the Path from Open Source Contributor to Full-Time Employee An introduction to Elasticsearch (日本語で) We're a global community, dontcha know. Check out this slide deck in West-Vlaams, a Flemish dialect Where to find Us. and getting ready to show off some \"dazzling dashboards\" at - join us upstairs! — elastic (@elastic) We'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking about the ELK stack or hosting a meetup, AustraliaOur team will be in the exhibits hall at , on July 16 & 17. We're also proposing an open space on the ELK Stack, so perhaps we'll see you in the open space zone, too. CanadaJoin and in Toronto on June 23 for an open Q&A session. to join them at the Elasticsearch Toronto Meetup. ChileJoin the Lenguajes dinámicos meetup group in Santiago on June 24 to talk Elasticsearch and Ruby. to save your seat. France GermanyJoin our team at on July 19. They'll be on the show floor to answer all of your questions. India ItalyJoin the community in Reggio Emilia for the FazLab Fest! On this Saturday, June 20 - a full day's talk on the Elastic platform and related technologies. are on meetup.com. The Netherlands Reg now: Netherlands Meetup will get together 17 June w/ talks from & dev Nik Everett — Leslie Hawthorn (@lhawthorn) New ZealandThe next meeting of the Elasticsearch ELK NZ meetup is coming to Wellington on June 30. We're still lining up the agenda, but for now to save your seat. Poland and will attend Devoxx Poland 2015 taking place June 22-25 in Krakow. Between the two of them, they'll deliver three talks: And last but not least, join is Poznan for . The conference runs July 2-4. South AfricaWe are a proud swag sponsor of the . If you happen to be in Johannesburg on July 11, pop in for knowledge and some cool Elastic goodies. Spain TaiwanThe Taipei D3.js meetup will convene on June 18,including a lightning talk on Elasticsearch and Kibana. to attend. UkraineThe L'viv DevOps Meetup will get together on June 27 for CoreOS Fest Conference review, featuring a CoreOS with Kubernetes demo: ElasticSearch Cluster on local machine and AWS. to save your seat. United KingdomJoin and for Manchester Geek Nights, where you'll hear about Elasticsearch in Action and Building Entity Centric Indexes. to save your seat. Can't join the meetup? No worries. The fine folks organizing the meetup have set up a livestream. Stay tuned to watch live! United States - East United States - West Where to Find You  If you're a regular reader, you're thinking of skipping this section. You may even be thinking to yourself, yes of course I will drop a note on Twitter when I am giving a talk on all things ELK. That's awesome, because we'd like to showcase every meetup, conference presentation and workshop on Elasticsearch, Logstash, and Kibana happening worldwide. And now, we've made it even easier for you to get support for your meetup! Head on over to  ! (And we'll still totally send you swag if you're giving a talk on anything ELKy at a conference.) Oh yeah, we're also  . If you'd like us to find you for employment purposes, just . We care more ","locales":"","title":"This Week in Elastic: Elasticsearch for Apache Hadoop 2.1 RC1 released"}
{"index":{}}
{"author":"Leslie Hawthorn","category":"","publish_date":"2015-06-10T00:00:00.000Z","url":"/blog/this-week-in-elastic-2015-06-10","seo_title":"","content":" Welcome to ! In this weekly round up post, we bring you news on all aspects of the ELK Stack, including updates on Elasticsearch, Logstash and Kibana development, tips and tricks and upcoming events. ICYMI: 's 1st ever dev hangout w/ Fri, 12 June 11 AM PDT/8 PM CEST. Watch live: — Leslie Hawthorn (@lhawthorn) Elastic News Useful blog posts, how tos, tutorials and articles from the wide world of all the ELK stack ecosystem. I'll talk about Elasticsearch under the hood and speed during the first german webinar next Tuesday at 3pm — Alexander Reelsen (@spinscale) Slides and Videos Where to find Us We'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking about the ELK stack or hosting a meetup, AustriaThe .NET Community Austria Meetup will get together on June 11 to talk all about the ELK stack. to save your seat. BelgiumThe PHP wvl group will convene in Roeselare on June 16. The speakers will be presenting on Elasticsearch and Symfony. to save your seat. CanadaJoin our and in Toronto on June 23 for an open Q&A session. to join them at the Elasticsearch Toronto Meetup. FranceYou can see our very own on at Breizhcamp this week, or these awesome speakers on Elasticsearch: Université Kibana et Elasticsearch : . Analyse de données temps réel avec la suite et Adelean.— Lucian Precup (@lucianprecup) Plus, we have the 3rd Elastic France meetup coming up in Lyon on June 23. to save your seat. And last but not least, join in Bordeaux. The conference runs June 30-July 3. GermanyIf you'll be attending the on June 12, make sure to say hello to in the hallway track! India ","locales":"","title":"This Week in Elastic: First dev hangout &  Logstash 1.4.3 released"}
{"index":{}}
{"author":"Rashid Khan","category":"Engineering","publish_date":"2015-06-10T00:00:00.000Z","url":"/blog/kibana-4-1","seo_title":"","content":" Kibana 4.1 is now available for your data dicing, stat splicing, log munching, chart crunching enjoyment. In addition to a fork load of great new features, we've put a big focus on performance in Kibana 4.1, improving responsiveness across the board and reducing memory consumption by as much as 10x. Kibana 4.1 does more, and does it all more quickly. Can't wait? . Otherwise, dig into the juicy details below. Axis AwesomenessSpikes. You deal with them often. Server load hits the ceiling, prices jump for a moment. Kibana 4.1 brings support for logarithmic axis in the line chart. Logarithmic scales allow you to smooth out those big spikes so that the important details aren't unduly impacted by one big event. Speaking of smooth, we've introduced smoothing to line and area charts. Sure, smooth lines are a lie, but they're your lie and they sure do look good! Bubble chartsBut thats not all we've done with line charts. We've also introduced a new metric dimension for line charts called **dot size**. This new metric lets us define the size of the dots placed on the line based on some other metric. For example, is the bandwidth used to serve large files potentially impacting my ability to handle more users? New aggregationsTo help you utilize those new visualization modes we've built support for more elasticsearch aggregations including and . Map MadnessIn addition to improvements to how we communicate the nature of the map projection, we added the ability to render a heat maps. And because heat maps are built using HTML canvas, instead of SVG, they're super fast, enabling you to crank up the precision of the aggregation. Plus, when you need to drill in, we're added geographic filters. Draw and box and we'll filter your request down to just the region you're interested in. Put a pin in itSpeaking of filters, we're introducing the concept of **pinned filters**. We just can't stop using this feature, I don't know how we lived with out it. Imagine you're on a dashboard and you've filtered the view down to using a combination of filters. Now you want to jump back to discover and dig into the documents, or create a new visualization looking at only that filtered data. With filter pinning you can. Pinning a filter means it sticks with you no matter where you go in the app. So if you create a filter in Discover and pin it, it will still be there when you head over to the Dashboard.  And when you're on that dashboard, don't miss that new option when you go to save. If you're a Kibana 3 user you may remember that your time filter was always saved with the dashboard. We've brought that option back in Kibana 4.1: Select and Kibana will return you to right where you left off when you load the dashboard. Field formattingWe've always wanted you to see your data how you want it. While aggregations allow us to compose complex, expressive, visualizations, we wanted to be able to do that same thing with the actual values of fields. Thus, we've introduced field formatters in Kibana 4.1. Now you can continue to index your data just how you'd like it, as numbers, or unformatted strings, and let Kibana do the rest. For example, format numbers as bytes, or URL strings as links. URL formatting even support templates allowing you to use Kibana data to create custom links to other applications. And so much moreThat isn't even close to everything in Kibana 4.1. You'll discover improved functionality around every corner. We've streamlined where we can and reduced unnecessary configuration whenever possible. We've fixed dozens of bugs and added important management features, like importing and exporting schemas, and fine grained timeout configuration. Download Kibana 4.1 and give it a shot! EngageOf course Kibana is a product of your input and insight. We wanted to make it easier for you to stay up to date, try out the latest and great, and contribute to the project. We've begun publishing code to let you ride the knife edge. Have a question? Join us on the new . Have a bug? . Make someth","locales":"","title":"Kibana 4.1"}
{"index":{}}
{"author":"Leslie Hawthorn","category":"","publish_date":"2015-06-09T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2015-06-09","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top NewsVideo of my talk is now online: Algorithms that power Lucene and Elasticsearch — Adrien Grand (@jpountz) Elasticsearch Core Apache Lucene Watch This Space , where we'll share more news on the whole ELK ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene: Algorithms that power Lucene and Elasticsearch"}
{"index":{}}
{"author":"","category":"","publish_date":"2015-06-16T00:00:00.000Z","url":"/blog/logstash-1-5-1-released","seo_title":"","content":" We are pleased to announce the release of Logstash 1.5.1. This is a bug fix release:  please check the for details or head to our page.We would like to highlight few of the bug fixes here:FeedbackPlease download Logstash 1.5.1 and let us know what you think on Twitter () or on our . You can report any problems on the . ","locales":"","title":"Logstash 1.5.1 released"}
{"index":{}}
{"author":"Shaunak Kashyap","category":"","publish_date":"2015-06-15T00:00:00.000Z","url":"/blog/logstash-lines-2015-06-15","seo_title":"","content":" Welcome back to ! In these weekly posts, we'll share the latest happenings in the world of Logstash and its ecosystem. This week, David Pilato shows us how easy it is to use Logstash — instead of using rivers, which were recently deprecated — to index tweets into Elasticsearch: New post: Indexing Twitter in with — David Pilato (@dadoonet) Logstash Core and Plugins That's this week in . Come back next week for more Logstash news! ","locales":"","title":"The Logstash Lines — JDBC input plugin, 1.4.3 release and more!"}
{"index":{}}
{"author":"Uri Boness","category":"Engineering","publish_date":"2015-06-10T00:00:00.000Z","url":"/blog/watcher-beta-2","seo_title":"","content":" We're pleased to announce Watcher 1.0.0-Beta2!This release of Watcher includes bug fixes and enhancements to features introduced in Beta1.But we didn’t stop there. Since the release of Beta1, we’ve received wonderful feedback from all of you trying out Watcher. We’re delighted with the activity in the  and I can honestly say that your questions have helped us improve Watcher in every aspect - functionality, usability and documentation.And yes, we are talking the talk and walking the walk. We took some of your wonderful suggestions and included them in this release. For more details, please read the .Installing Beta2 is as easy as it was installing Beta1. You can use the same beta key you received in the Beta1 registration email, but this time appending  `-Beta2` to your key:Haven’t registered yet? and try Watcher out! ","locales":"","title":"Watcher Beta 2 - Fixes, Features, Feedback"}
{"index":{}}
{"author":"Suyog Rao","category":"Engineering","publish_date":"2015-06-09T00:00:00.000Z","url":"/blog/logstash-1-4-3-released","seo_title":"","content":" We are pleased to announce the availability of Logstash 1.4.3. This is a bug fix release for the 1.4 series which fixes a few important security vulnerabilities. Our recommendation is to upgrade to 1.4.3 if you are using either of the following plugins:Elasticsearch 1.1.1 vulnerabilityLogstash 1.4.2 was bundled with Elasticsearch 1.1.1, which is vulnerable to . These binaries are used in Elasticsearch output specifically when using the node protocol. Since a node client joins the Elasticsearch cluster, the attackers could use scripts to execute commands on the host OS using the node client's URL endpoint. With 1.4.3 release, we are packaging Logstash with Elasticsearch 1.5.2 binaries which by default disables the ability to run scripts. This also affects users who are using the configuration option in the Elasticsearch output which starts a local embedded Elasticsearch cluster. This is typically used in development environment and proof of concept deployments. Regardless of this vulnerability, we strongly recommend not using embedded in productionNote that users of transport and http protocol are not vulnerable to this attack.Logstash Forwarder with Lumberjack input/outputThe combination of Logstash Forwarder and Lumberjack input (and output) was vulnerable to the attack in SSLv3 protocol. We have disabled SSLv3 for this combination and set the minimum version to be TLSv1.0. We have added this vulnerability to our CVE page and are working on filling out the CVE.Thanks to Tray Torrance, Marc Chadwick, and David Arena for reporting this.File output vulnerabilityAn attacker could use the File output plugin with in the path option to traverse paths outside of Logstash directory. This technique could also be used to overwrite any files which can be accessed with permissions associated with Logstash user. This release sandboxes the paths which can be traversed using the configuration. We have also disallowed use of dynamic field references if the path options is pointing to an absolute path.We have added this vulnerability to our and are working on filling out the CVE. We would like to thank Colin Coghill for reporting the issue and working with us on the resolution.Other fixesFixed an issue in Elasticsearch output which was not correctly releasing socket connections. This fix was in the HTTP client library, so any plugins using this should benefit from this resolution ()Please check the or proceed to our page. ","locales":"","title":"Logstash 1.4.3 released"}
{"index":{}}
{"author":"Shaunak Kashyap","category":"The Logstash Lines","publish_date":"2015-06-05T00:00:00.000Z","url":"/blog/logstash-lines-2015-06-05","seo_title":"","content":" Welcome back to ! In these weekly posts, we'll share the latest happenings in the world of Logstash and its ecosystem. This week, our very own David Pilato shows us how to reindex data in Elasticsearch using Logstash: New post: Reindex With — David Pilato (@dadoonet) And here is an awesome presentation from Logstash engineer João Duarte, on how you can setup a burglar alert system using Logstash! Logstash Core and Plugins That's this week in . Come back next week for more Logstash news! ","locales":"","title":"The Logstash Lines: How to reindex data in Elasticsearch using Logstash"}
{"index":{}}
{"author":"Shaunak Kashyap","category":"The Logstash Lines","publish_date":"2015-05-29T00:00:00.000Z","url":"/blog/logstash-lines-2015-05-29","seo_title":"","content":" Welcome back to ! In these weekly posts, we'll share the latest happenings in the world of Logstash and its ecosystem. This week,  from Chartbeat has some useful Logstash deployment and scaling tips for us: Our very own drops some knowledge on deploying and scaling at Chartbeat: — Chartbeat (@Chartbeat) Logstash Core and Plugins That's this week in . Come back next week for more Logstash news! ","locales":"","title":"The Logstash Lines: Logstash deployment and scaling tips"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2015-05-26T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2015-05-26","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top NewsWe were among the first to use and contribute to … now, more than 1,500 government websites use — Taha Kass-Hout (@DrTaha_FDA) Elasticsearch Core Apache LuceneWatch This Space , where we'll share more news on the whole ELK ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene: Lucene 5.2.0 is coming"}
{"index":{}}
{"author":"Leslie Hawthorn","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2015-06-02T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2015-06-02","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top NewsHow uses + to improve their criminal justice data inventory — Leslie Hawthorn (@lhawthorn) Elasticsearch Core Apache LuceneWatch This Space , where we'll share more news on the whole ELK ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene: The second release candidate for 5.2.0 is out"}
{"index":{}}
{"author":"Jettro Coenradie","category":"","publish_date":"2015-05-29T00:00:00.000Z","url":"/blog/found-geo-points-and-elasticsearch-percolator","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud. Elasticsearch has some very good support for geo locations. You can store locations, use distances to filter and even use distances to group using aggregations. Elasticsearch also has something called geo shapes - no worries, we'll explain later on what they are - to select locations within a certain area. If you combine these geo shapes with filters and something called the percolator, you can create a basic classification system. Curious? Read on to learn about the Elasticsearch percolator and geo support in Elasticsearch. To explain the concepts we use a sample application based on spring boot and angularjs. Before we jump into the technical stuff, we'll talk about the data lifecycle that you need for all data projects. ","locales":"","title":"Combining Geo Points With the Elasticsearch Percolator"}
{"index":{}}
{"author":"Shay Banon","category":"News","publish_date":"2015-05-27T00:00:00.000Z","url":"/blog/welcome-packetbeat-tudor-monica","seo_title":"","content":" I have been following Packetbeat for quite some time, being one of those projects I just love. Packetbeat is a lightweight network packet analyzer that parses different protocols (HTTP, MySQL, Postgresql, Redis, Thrift-RPC), extracts the relevant information from them, ships the data to Elasticsearch and uses Kibana to visualize it.  It has grown to become quite successful, with a strong following. These type of projects are what make open source such a great platform for innovation. Here at Elastic we are building products like Elasticsearch, Kibana, and Logstash, and a project is born using them in a wonderful new direction. ","locales":"","title":"Welcome Packetbeat, Tudor & Monica"}
{"index":{}}
{"author":"Antonio Bonuccelli","category":"Engineering","publish_date":"2015-05-26T00:00:00.000Z","url":"/blog/elasticsearch-2015-leap-second","seo_title":"","content":" We're approaching that time of the year when a day will actually be longer than a normal day, by just one second.That is what the scientific community refers to as the “Leap Second.”A Leap Second is a one-second adjustment that is occasionally applied to Coordinated Universal Time (UTC) in order to keep its time of day close to the mean solar time, or UT1.The next scheduled occurrence of leap second will be on June 30, 2015 23:59:60 UTC.While the leap second is certainly an interesting subject for discussions among scientists, it has proven to be a big headache for an IT infrastructure.The most recent occurrence of the leap second in 2012 disrupted operations for many major companies, bringing down services by causing high cpu usage on production servers.The main problem actually sits in Linux kernel layer. Most Linux vendors have published guides explaining what you need to do (if anything) to test and prepare for the upcoming leap second:Please refer to your distribution's support channels if it is not listed above and for additional details. Elastic recommends you follow your distribution's recommended guidelines for handling this issue in order to avoid any further complications that may arise from unofficial or ad-hoc fixes.Additionally, some of the major cloud providers have also published their own information which you should read if you are running on one of their platforms: ","locales":"","title":"Elasticsearch and the 2015 Leap Second"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2015-06-15T00:00:00.000Z","url":"/blog/2015-06-15-where-in-the-world-is-elastic","seo_title":"","content":" Welcome to We're continuing the summer with many events and meetups worldwide. Read on to find out if there is an event happening near you:Upcoming EventsJune 15-17: , Portland, Oregon- Logstash team lead will give a lightning demo on Wednesday, June 17 at 10:40 AM.June 16: , London - Stop by our booth (#1) in the exhibit hall to talk to our local team and pick up some swag. Also don't miss at 12:30 where will show a cool Kibana 4 demo, plus we've got a guest speaker from the Met Office, too!June 18-19: , We've got hot Dutch waffles at our booth (#19/#20) - make sure you don't leave without having tried one! Also, we have an entire track dedicated to Elasticsearch on Thursday, June 18 with the following talks:Upcoming MeetupsJune 15: June 16: June 16: June 16: (Richmond, VA)June 16:  June 17: June 17: June 18: June 18: June 20: June 16: (Roeselare, Belgium)June 17: June 18: June 20: June 20: (Hyderabad, India)That's it for this week. Stay tuned for Elastic happenings next week - there's much to come! - The Elastic TeamP.S.:   if you're interested in hosting a meetup or are giving a talk about Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic: Big Data Analytics London & GOTO Amsterdam"}
{"index":{}}
{"author":"Clinton Gormley","category":"Engineering","publish_date":"2015-06-09T00:00:00.000Z","url":"/blog/elasticsearch-1-6-0-released","seo_title":"","content":" Today, we are pleased to announce the release of , based on . This is the latest stable version of Elasticsearch and is packed with awesome new features: You can read the full changes list and . ","locales":"ja-jp","title":"Elasticsearch 1.6.0 released"}
{"index":{}}
{"author":"Robyn Bergeron","category":"Kurrently in Kibana","publish_date":"2015-06-05T00:00:00.000Z","url":"/blog/kurrently-kibana-2015-06-05","seo_title":"","content":" Welcome to , your source for keeping up with the Kibana project. With information on project releases, work in progress, and learning resources, this weekly update helps you satisfy your around what's been happening in Kibana! dashboard for at Stage 1 — Tanya Bragin (@tbragin) Kibana 4.1: The QA Train Moves ForwardQA continues on Kibana 4.1. We’ve found a number of issues in IE and Firefox, though fortunately recent discoveries have all been fairly small. You can see what we have left to tackle . Performance TweaksOn the plus side, we’ve sped things up a bit as a side effect of all the aforementioned cross browser testing time: Plus... Lots of Fixes! All eyes on demo! Monitoring tweets! — Valentina Sipovica (@pvalentinaq) See You Next Week!Did you know? Elasticsearch and Apache Lucene, Logstash and Kibana all have weekly updates published to the Elastic blog. Tune in next week to to stay in the ! ","locales":"","title":"Kurrently in Kibana: Performance Tweaks and Big Fixes for 4.1"}
{"index":{}}
{"author":"Sébastien Muller","category":"User Stories","publish_date":"2015-06-04T00:00:00.000Z","url":"/blog/norways-postal-service-moves-to-elastic-from-fast","seo_title":"","content":" ","locales":"","title":"Norway's Postal Service Moves to Elastic from FAST"}
{"index":{}}
{"author":"Leslie Hawthorn","category":"","publish_date":"2015-06-03T00:00:00.000Z","url":"/blog/this-week-in-elastic-2015-06-03","seo_title":"","content":" Welcome to ! In this weekly round up post, we bring you news on all aspects of the ELK Stack, including updates on Elasticsearch, Logstash and Kibana development, tips and tricks and upcoming events. Always wanted to ask our devs (almost anything)? Join our Dev Hangouts Series! brings you the scoop — elastic (@elastic) Elastic News Useful blog posts, how tos, tutorials and articles from the wide world of all the ELK stack ecosystem. Couldn’t catch our live and Beyond webinar yesterday? Catch & now on demand! — elastic (@elastic) Slides and VideosPssst! We visited CERN last week to learn about all the ways they're using the ELK stack and for our team to tell the scientists there who didn't know us more about our technology. , featuring the ever amazing and . But wait, there's more! Where to find Us We'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking about the ELK stack or hosting a meetup, AustriaThe .NET Community Austria Meetup will get together on June 11 to talk all about the ELK stack. to save your seat. CanadaThe Montreal Monitoring Meetup will get together on June 8 to talk about the ELK stack, amongst many other tools. to save your seat. France During 2015, will show us how to add to legacy applications in a blink ! — RivieraDEV (@RivieraDEV) Université Kibana et Elasticsearch : . Analyse de données temps réel avec la suite et Adelean.— Lucian Precup (@lucianprecup) GermanyIf you'll be attending the on June 12, make sure to say hello to in the hallway track!IrelandJoin for an expert level session at NoSQL Matters Dublin on . The conference takes place on June 4. IndiaThe Big Data Analytics on Cloud Meetup in Hyderabad will host a free workshop on Elasticsearch on June 20. to attend the workshop. JapanPlease join all members of Elastic's team in Japan at in Tokyo on June 3-5. We will have a booth in the exhibits hall and will be very happy to answer any questions you may have. The Netherlands Reg now: Netherlands Meetup will get together 17 June w/ talks from & dev Nik Everett — Leslie Hawthorn (@lhawthorn) PolandIf you'll be in Katowice, Poland on June 11, join the AWS Poland User Group to talk Elasticsearch on AWS. Register now to save your place. David Pilato and will attend Devoxx Poland 2015 taking place June 22-25 in Krakow. Between the two of them, they'll deliver three talks: SpainThe Madrid Open Analytics Meetup will get together on June 25 to talk big data before your holidays, including Elasticsearch. to join them. (No holiday included, but definitely great knowledge sharing.) United Kingdom South Wales Elasticsearchers: a meetup with a talk by Honza Král , Elasticsearch developer: All welcome. — DjangoCon Europe (@DjangoConEurope) United States - East United States - West Where to Find You  If you're a regular reader, you're thinking of skipping this section. You may even be thinking to yourself, yes of course I will drop a note on Twitter when I am giving a talk on all things ELK. That's awesome, because we'd like to showcase every meetup, conference presentation and workshop on Elasticsearch, Logstash, and Kibana happening worldwide. And now, we've made it even easier for you to get support for your meetup! Head on over to  ! (And we'll still totally send you swag if you're giving a talk on anything ELKy at a conference.) Oh yeah, we're also  . If you'd like us to find you for employment purposes, just . We care more about your skill set and passion for Elasticsearch, Kibana, and Logstash than where you rest your head. Trainings If you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elastic: New discussion forums are live!"}
{"index":{}}
{"author":"Robyn Bergeron","category":"Kurrently in Kibana","publish_date":"2015-05-29T00:00:00.000Z","url":"/blog/kurrently-kibana-2015-05-29","seo_title":"","content":" Welcome to , your source for keeping up with the Kibana project. With information on project releases, work in progress, and learning resources, this weekly update helps you satisfy your around what's been happening in Kibana! tv commercial showing some promising results already! — jwijgerd (@jwijgerd) Kibana 4.1: QA Time! All of the planned pulls have been merged and we’re into QA. You can track our triaging of QA-identified issues Heat Map Merged The new geo heat map has been merged. You turn it on in the tab of the tile map, along with settings to tune it to just your use case. It brought with it a normalization of the tooltips used on the heat map. At the moment they’re using the geo_point field formatter, which we plan to improve on. This also makes sure that the field formatters are used to display the value of the result of a metric aggregation. Refactored Filter Functions We found some instances in which adding and removing filters could create duplicate history entries. We did some refactoring here and added a bunch of test coverage as well. Merged deb and rpm PackagingThanks to , we now have the ability to create rpm and deb packages for Kibana 4.1. We'll let you know as soon as they hit the repository! Plus This Stuff: We have great news: Packetbeat is joining ! — Packetbeat (@packetbeat) See You Next Week!Did you know? Elasticsearch and Apache Lucene, Logstash and Kibana all have weekly updates published to the Elastic blog. Tune in next week to to stay on All The Things!. ","locales":"","title":"Kurrently in Kibana: Kibana 4.1 QA time"}
{"index":{}}
{"author":"Leslie Hawthorn","category":"","publish_date":"2015-05-27T00:00:00.000Z","url":"/blog/this-week-in-elastic-2015-05-27","seo_title":"","content":" Welcome to ! In this weekly round up post, we bring you news on all aspects of the ELK Stack, including updates on Elasticsearch, Logstash and Kibana development, tips and tricks and upcoming events. Elastic NewsCommunity + one a-mazing data scientist concur: you want to hear each others real world stories. We're here to help! — Leslie Hawthorn (@lhawthorn) Useful blog posts, how tos, tutorials and articles from the wide world of all the ELK stack ecosystem. Thought you might like my mention of ELK here: :)— Samuel Scott (@samueljscott) Slides and Videos Where to find Us (Don't let that tomorrow bit fool you. : ) TOMORROW! is coming to talk about elasticsearch, logstash and kibana. Come get your learn on! — San Antonio DevOps (@SADevOps) We'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking about the ELK stack or hosting a meetup, AustriaThe .NET Community Austria Meetup will get together on June 11 to talk all about the ELK stack. to save your seat. France During 2015, will show us how to add to legacy applications in a blink ! — RivieraDEV (@RivieraDEV) Université Kibana et Elasticsearch : . Analyse de données temps réel avec la suite et Adelean.— Lucian Precup (@lucianprecup) GermanyThe Elastic team will be out in force at Berlin Buzzwords, including 3 talks, a booth on the show floor and a special conference meetup. You can for the conference and RSVP for . Buzzwords runs May 31 - June 3. Bonus: meet the newest members of our team, and of Packetbeat fame! IrelandJoin Mark Harwood for an expert level session at NoSQL Matters Dublin on . The conference takes place on June 4. JapanPlease join all members of Elastic's team in Japan at in Tokyo on June 3-5. We will have a booth in the exhibits hall and will be very happy to answer any questions you may have. The Netherlands Technical Expert Aurélien Foucret of talks about ElasticSearch at , more info: — Meet Magento NL (@mm15nl) Reg now: Netherlands Meetup will get together 17 June w/ talks from & dev Nik Everett — Leslie Hawthorn (@lhawthorn) Poland David Pilato and will attend Devoxx Poland 2015 taking place June 22-25 in Krakow. Between the two of them, they'll deliver three talks: United Kingdom South Wales Elasticsearchers: a meetup with a talk by Honza Král , Elasticsearch developer: All welcome. — DjangoCon Europe (@DjangoConEurope) United States - East United States - West On stage now: Rob Tice, Director at talking about complex modelling of rich text data in — elastic London (@elastic_london) Where to Find You  If you're a regular reader, you're thinking of skipping this section. You may even be thinking to yourself, yes of course I will drop a note on Twitter when I am giving a talk on all things ELK. That's awesome, because we'd like to showcase every meetup, conference presentation and workshop on Elasticsearch, Logstash, and Kibana happening worldwide. And now, we've made it even easier for you to get support for your meetup! Head on over to  ! (And we'll still totally send you swag if you're giving a talk on anything ELKy at a conference.) Oh yeah, we're also  . If you'd like us to find you for employment purposes, just . We care more about your skill set and passion for Elasticsearch, Kibana, and Logstash than where you rest your head. Trainings If you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elastic: Welcome Packetbeat!"}
{"index":{}}
{"author":"Sonja Winter","category":"","publish_date":"2015-05-26T00:00:00.000Z","url":"/blog/community-health-metrics-part-one","seo_title":"","content":" Hi all! I'm Sonja Winter, a data scientist/statistician, currently helping Elastic analyze their community data. In my daily work, I use the ELK stack to explore the data I work with, so I am excited to now be getting insights from the ELK stack into the data from the same community that helped build it. This post will be the first in a series of short updates on Elastic’s community development and engagement. Any open source project thrives on the engagement and contributions of their community, and the ELK stack is no different. Therefore, it is important to gain insight into the factors that play a role in engaging and motivating this community. Hosting meetups is a popular method of getting the community together to share knowledge and for the projects’ developers to figure out their users’ praise and pain points. So how do you get more people to register for your meetup events? The analysis I will show you today will demonstrate that getting someone from your community to speak to your community at a meetup event, will result in a higher registrant-to-member ratio.The fine folks at Elastic have been monitoring meetup activity for all events taking place since October 2011. Since then, a total of 649 Elastic related events have been organised and tracked via meetup.com:  plus, there are many more user group meetings happening than are reflected on just the meetup.com site. For 138 of these events, we have data on the origin of the speaker of the event, and we know that the event was hosted by an Elastic-focused meetup group.In total, 83 events had community speakers (60.1%) and 55 events had Elastic employee speakers (39.9%):As a proxy for the popularity of an event, I computed the registrants-to-member ratio, which equals the number of registrants to an event divided by the total number of members of that meetup group. A ratio of 1 indicates that all members of the meetup group went to the event. A ratio < 1 indicates that some members of the group went to the event and a ratio > 1 indicates that more people than members of the group went to the event. The box-plot figure below shows that the average registrants-to-member ratio (the thick line in each box) is higher when the speaker is a community member than when the speaker is from Elastic Inc.. More specifically, for a community speaker, the average ratio is 0.69, meaning that for every 1 member, 0.69 members will register for the event (or for every 100 members, 69 will register). For an Elastic speaker, the average ratio is 0.58, meaning that for every 1 member, 0.58 members will register for the event (or for every 100 members, 58 will register). Thus, on average, this data indicates that an event with a community speaker will attract 11 per 100 members more than an event with a Company speaker.To explore whether this difference is also statistically significant, I performed a Poisson regression. Since a Poisson regression is used for count data, I first multiplied the ratio by 100 and rounded to the nearest integer. This number now represents the number of registrants per 100 members. The data is overdispersed (z = 3.85, p < .001:  see also the histogram below), meaning that the variance of the data is larger than its mean. You can also see this in the box-plot by looking at the outliers (the dots). Because of this, I chose to perform a negative binomial regression analysis. The results indicate that the difference between a community and company speaker is not statistically significant (Est. = 0.19, 95% CI = -0.17 - 0.53, p = .291). This is not surprising, as the variance around the mean is very large (you can see this in the box-plot by looking at the length of the whiskers [lines] and the dots that indicate very high values). To make the estimate more meaningful, we take its exponent, which results in an incidence rate. For every 1 registrant per 100 members for an event with a company speaker, 1.21 members p","locales":"","title":"Elastic Community Health Metrics: Why *You* Should Share Your Stories"}
{"index":{}}
{"author":"Robyn Bergeron","category":"","publish_date":"2015-05-22T00:00:00.000Z","url":"/blog/kurrently-kibana-2015-05-22","seo_title":"","content":" Welcome to , your source for keeping up with the Kibana project. With information on project releases, work in progress, and learning resources, this weekly update is a great way to learn what's been happening in Kibana! Starting to get the hang of . E.g. to dig for place names matched to multiple places & vice versa. — Rainer Simon (@aboutgeo) Kibana 4.1: The Road to Release Almost all of the major features for Kibana 4.1 have been merged with the exception of heat maps, which are almost there. We snuck in one more thing this past week, but really it was mostly squashing bugs and lots of intensive pull review in preparation for release. Added a pause button for auto refresh Quite simply pauses the auto refresh without the need to expand the time picker and turn it off. Sometimes its the little things, eh? We also added one-click access to the refresh interval selector whenever the time picker is open and switched up how the tabs work for a cleaner, easier to use, design. (Also, I keep thinking “paws” when I type “pause” because has a house full of kittens and can’t stop posting pictures of those fluffy little balls of pure cuddliness and furniture destruction.) The new server architecture is ready for review In addition to playing with kittens, @simianhacker has been simian hacking hard on a new architecture for the Kibana server that enables a totally awesome plugin architecture that we’ll leverage to take Kibana, and other Elasticsearch-enabled (and enabling!) apps to the next level. Check out the write up over on the . It's like looking into the future! Also, we: Once again is my hero: holds so much promise for usability & query optimization!— Rashid Khan (@rashidkpc) See You Next Week!Did you know? Elasticsearch and Apache Lucene, Logstash and Kibana all have weekly updates published to the Elastic blog.We'll bring you the newest, mind-blowing stuff in next week's . ","locales":"","title":"Kurrently in Kibana: The Road to Releasing 4.1"}
{"index":{}}
{"author":"Uri Boness","category":"News","publish_date":"2015-05-20T00:00:00.000Z","url":"/blog/watcher-beta-goes-public-you-know-for-alerting","seo_title":"","content":" Today we’re delighted to release the first public beta of – our commercial alerting and notification product. In the same spirit of Shield, Watcher is a standalone plugin for Elasticsearch. Elasticsearch enables you to extract invaluable knowledge and insights from changes in your data, in realtime. But with the fast moving markets we all operate in today, it is not enough. In order for you to stay ahead of the game, you also need to do something with that knowledge – take actions that will directly translate to tangible values. And act fast. Act at realtime. This is where Watcher complements Elasticsearch. While with Elasticsearch alone you need to be “proactively proactive” when it comes to analyzing your data, Watcher enables you to be “reactively proactive.” Hmm… what? Yes! With Elasticsearch alone you need to continually be there to run the analytics, interpret the results and act on them. With Watcher that is no longer the case. Watcher will monitor the data for you, act on the changes and/or notify you about important events – freeing you up to focus on other important tasks, while letting you stay ahead of the game. Here are a few examples of tasks you can define in Watcher: Watcher was designed to serve all the scenarios above and many more. And we’re very eager to see all the different use cases you use it for. Next, I’ll show how you’d go about using Watcher. But first, if you haven’t registered to the beta program, don’t miss out and . Once registered, you will receive the installation instructions by email, and as you will find out, installing Watcher is as easy as installing any other Elasticsearch plugin. How do I work with Watcher?Watcher adds a new set of APIs to Elasticsearch to manage . It’s probably best to explain the concept of a with an example. In this example, suppose you are indexing your web traffic logs in Elasticsearch. Let's first create a index and enable the _timestamp mappings, such that a timestamp will be associated with each log event that we’ll index. PUT logs { \"mappings\" : { \"event\" : { \"_timestamp\" : { \"enabled\" : true } } } } Now that we have the logs index ready, let’s add our first . The immediate thing that comes to mind in this setup is the ability to pick up on errors in your logs as close to realtime as possible. In other words, you would like to watch newly indexed logs and look for any log events with an error status. The following snippet registers a new watch that searches for any errors in the log entries in the last 5 minutes: PUT /_watcher/watch/error_status { \"trigger\" : { \"schedule\" : { \"interval\" : \"5s\" } }, \"input\" : { \"search\" : { \"request\" : { \"indices\" : [ \"logs\" ], \"body\" : { \"query\" : { \"filtered\" : { \"query\" : { \"match\" : { \"status\" : \"error\" }}, \"filter\" : { \"range\" : { \"_timestamp\" : { \"from\" : \"now-5m\" }}} } } } } } } } If you look closer at the watch definition above, you’ll see that we defined this watch with a schedule such that it will be executed every 5 seconds (I chose 5 seconds here just for the sake of the example, a more realistic value would have been 5 or 10 minutes). The input loads data for the watch (what we like to refer to as ). It does that by executing a search request over the index, and looks for events in the last 5 minutes with an status. Once this watch is added to Watcher, the schedule will immediately kick in and start ticking. Every 5 seconds the watch will execute and will search for errors. Although it looks like nothing is happening, in the background Watcher records all these executions in indices. And just like any other indices in Elasticsearch, you can explore them using the normal search API. Executing the following search request will return the last 10 watch executions: GET /.watch_history*/_search { \"sort\" : [ { \"execution_result.execution_time\" : \"desc\" } ] } OK… Searching for errors and then searching the watch history is a good start, but ","locales":"","title":"Watcher Beta Goes Public! You Know, For Alerting..."}
{"index":{}}
{"author":"Leslie Hawthorn","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2015-06-16T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2015-06-16","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top NewsGetting started w/? Join on June 24 for an in-depth webinar & Q&A — elastic (@elastic) Elasticsearch Core Apache LuceneWatch This Space , where we'll share more news on the whole ELK ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene: 5.2.1 bugfix release is out"}
{"index":{}}
{"author":"John-Henry Gross","category":"News","publish_date":"2015-06-16T00:00:00.000Z","url":"/blog/partner-spotlight-search-technologies-launches-aspire-for-elasticsearch","seo_title":"","content":" Today, I am excited to announce that Search Technologies, an Elastic partner has released Aspire for Elasticsearch. Aspire is a content access and processing framework specifically designed for unstructured data. It enables content from a variety of content repositories to be acquired, cleaned, normalized and enriched, as part of Elasticsearch implementations. I encourage you to download , it is free for a 45-day trial.Connectors and SecurityLet’s talk a little bit about how Aspire is being used in conjunction with Elasticsearch. A large management consulting firm needed to connect web content, several large databases, SharePoint 2010, and SharePoint 2013 to index in Elasticsearch. Elastic offers Logstash, which focuses on structured log and event data, not unstructured data like office documents and web content. Aspire for Elasticsearch comes pre-packaged with connectors for files systems, RDB and web content. This enables users to start indexing that content immediately and add such as Box.com, Documentum, Salesforce, etc. as needed. And since Aspire uses a modular framework, these connectors can be licensed and plugged in at any time. This management consulting firm needed to retrieve access control lists (ACLs) to maintain document-level security throughout the Elasticsearch application as well as extract metadata and content from SharePoint. Aspire SharePoint connectors support this requirement. Both Aspire and Shield address security concerns, however, it is important to understand the different and complementary roles they play.  Shield provides the foundation-level of security needed to run Elasticsearch in production - protecting your cluster with a username/password and providing more advanced features like encryption and role-based access controls. Aspire provides support for document level security, what a user has the rights to see from a query based on the source repositories ACLs. The need for content processingBy its nature, unstructured content is prone to be inconsistent with incorrect or missing metadata, poor granularity, extraneous content and erratic term usage. Content processing prior to indexing is critical to the success of search and analytics applications utilizing unstructured data. For example, a large recruiting customer’s Elasticsearch solution required extensive content processing. They had inconsistent date formats along with upper and lower case usage in candidate resumes, causing poor search results.Aspire is used to process the content to normalize the date formats and case usage to solve this challenge. In some cases, multiple resumes come in XML blocks and need to be separated into individual documents. Aspire identifies the individual resumes and is able to capture relevant entities (location, titles, company names, etc.) in resumes based on rules (patterns, capitalization, etc.) to create metadata for later analysis for Elasticsearch. An innovative use of content processing in this solution is using Aspire’s integration with Hadoop to support vector creation for a document matching feature.Learn MoreAspire Enterprise for Elasticsearch, just like Marvel and Shield is initially provided as a time restricted “try and buy” and can easily be converted to full licensing at any time. This is a full feature offering including:For more details on functionality or to download, go to ","locales":"","title":"Partner Spotlight: Search Technologies Launches Aspire for Elasticsearch"}
{"index":{}}
{"author":"Sonja Winter","category":"","publish_date":"2015-06-10T00:00:00.000Z","url":"/blog/community-health-metrics-part-two","seo_title":"","content":" In conclusion, spreading the word about your company and products by talking at developer-centric conferences boosts subsequent meetup activity in that area. If you want your product to be adopted by the entire world, make sure you send your developers to speak all over the world! ","locales":"","title":"Elastic Community Health Metrics: Sending Your Devs to Conferences is a Good Thing"}
{"index":{}}
{"author":"Rashid Khan","category":"Engineering","publish_date":"2015-06-09T00:00:00.000Z","url":"/blog/kibana-4-0-3","seo_title":"","content":" Kibana 4.0.3 is now available! Can't wait to read the rest?  now. Otherwise read on for details. As you know, we’re constantly working to build new, awesome features into Kibana. If you didn’t know that, you should get out the weekly  series. At the same time we’re building new stuff, we’re working to stabilize, secure and enhance the features you already know and love. Today we’re releasing Kibana 4.0.3 with over two dozen fixes and tweaks to make everyday Kibana life better. We also undertook an extensive string sanitization audit and identified a few potential security issues worth fixing. For details see our . We strongly encourage everyone to upgrade to Kibana 4.0.3, and to stay tuned to this blog for more in the near future! ","locales":"","title":"Kibana 4.0.3 is available"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2015-06-08T00:00:00.000Z","url":"/blog/2015-06-08-where-in-the-world-is-elastic","seo_title":"","content":" Welcome to  From San Jose to a mini 'Tour de France' plus Poland, we have another geographical roller coaster of awesome meetups and conferences coming up. Read on to find out if there is an event happening near you: Upcoming Events June 9-11: - Come visit us at our booth (#S1) to meet some of our local developers, and learn more about how to explore your data and get real-time results. June 8: - Stop by our booth in the exhibit hall to talk to our local team and pick up some swag. June 10-12: - will be giving a talk on  on Thursday, June 11 at 4:30 p.m. June 11-12:  - David will be on tour in South of France reprising his talk on , Friday, June 12, 2:40 p.m. June 12: - Alexander Reelsen will be attending in the hallway track. So if you'd like to talk the ELK stack and more, drop him a ! Upcoming Meetups June 8: June 8: June 10: June 11: June 11: June 11: That's it for this week. Stay tuned for Elastic happenings next week - there's much to come!  - The Elastic Team P.S.:   if you're interested in hosting a meetup or are giving a talk about Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic: Hadoop Summit San Jose & dotScale Paris"}
{"index":{}}
{"author":"Robyn Bergeron","category":"Engineering","publish_date":"2015-06-03T00:00:00.000Z","url":"/blog/newsletter-and-hangouts","seo_title":"","content":" Here at Elastic, the fantastic folks on our Developer Relations team are not so different from the amazing people in our community in terms of background. Some of us have been developers, some have been in ops — and we’ve all felt the pain of trying to keep track of what’s going on with our favorite tools. As open source software users and contributors, we’ve accumulated quite a few list subscriptions over time, and… well, let’s just say we actually celebrate “Mailing List Password Reminder Day” each month with a little bit of a grin. And with so many sources of information coming in — Twitter, forums, blog posts, you name it — filtering out what is useful can be a challenge. We know the pain. And we want to make that easier for you —- at least when it comes to Elastic-related content. Which is why the Developer Relations team at Elastic is bringing you two new ways to keep in touch with what’s going on in the Elastic community: a weekly newsletter, with content curated for our dev audience, and a weekly, live video hangout session with one of the Elastic Developers. Our goal: to bring you concise, relevant, useful content — deliver it to you in an easily consumable format — and to be authentic and fun in doing so.  (That last part is important — we never want to sound like we’re, you know, stuffy or annoying.) Delivering AwesomeThe Developer Newsletter will hit your inbox once a week. . (And so is unsubscribing — but we know you’ll never want to!) Bonus points: When you sign up for this newsletter, *the newsletter is all you will get.* (Unless you specify otherwise at some other point in time with us, that is!) No added advertisements, calls, announcements, or otherwise. Double Bonus Points: The Developer Newsletter is your pass, quite literally, to our new Developer Hangouts. Live Developer Hangouts, Exclusively for Newsletter Subscribers Every week, we’ll be hosting a video hangout with a fabulous guest star from the Elastic engineering team, for approximately half an hourish —-- consisting partially of a mini-presentation from our guest star, a bit of Q & A from the host, and plenty of time at the end reserved for an “Ask Me Anything” session — where you can ask the guest (almost) anything, and get an answer. Want to participate? Access to the live hangout  is granted exclusively to our newsletter subscribers only — which means that if you have questions, you’ve got to be on our list. Every week, the developer newsletter will contain all the info you’ll need for the hangout — including the guest, the topic, your link to participate, and information about connecting via IRC to ask All The Things! Of course, though, we want to give you a little taste, just so you know how awesome our hangouts are — and this means that our first hangout won’t require newsletter subscription. So be sure to keep your eyes open for information about our first hangout next Friday, June 12, with . Aaron’s one of our many awesome developers working on Logstash, and he’ll be talking about how his involvement in open source projects and passion for contributing got him on the track to a full-time job working at Elastic on Logstash. As your first host, I’ll be sure to get the scoop on all the things Aaron been working on lately, too. But after that? . Sorry, folks! And don’t you worry, folks outside of my UTC -7 time zone: we recognize that Elastic has a community that stretches far beyond the west coast of the United States. We’ll be rotating the days and times of day in which we hang — so everyone gets an opportunity to attend. So what are you waiting for? Get today — and get your full-fledged, “first edition” newsletter starting next week. ","locales":"","title":"Introducing (Awesome) Developer Hangouts and our Developer Newsletter"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2015-06-01T00:00:00.000Z","url":"/blog/2015-06-01-where-in-the-world-is-elastic","seo_title":"","content":" Welcome to  From Berlin to Tokyo, this week is full with great events and meetups once again! Read on to find out if we'll be near you:Upcoming EventsJune 1 - June 2:  - Stop by our booth in the exhibit hall to talk to our local team and pick up some swag.May 31 - June 5: - Make sure to say hello to Honza Kral in the hallway track. Our cool meetup featured below takes place on June 4, don't miss it!June 1-3: - We have a full host of Elastic talks coming up:June 3-5: - David Pilato will be giving a talk on  on Wednesday, June 3, 12:00 p.m - 12:45 p.m.June 4: - Mark Harwood will give a talk on 11:30 a.m. - 12:15 p.m.June 3-5:  - Don't miss the chance to meet our pretty new team in Japan. We'll be at booth #14 and ready for any Elastic questions you may have. Upcoming Meetups June 2: June 4 : June 5:  June 3: June 4: That's it for this week. Stay tuned for Elastic happenings next week - there's much to come!  - The Elastic Team P.S.:   if you're interested in hosting a meetup or are giving a talk about Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic: MongoDB World & Berlin Buzzwords"}
{"index":{}}
{"author":"Njal Karevoll","category":"","publish_date":"2015-05-26T00:00:00.000Z","url":"/blog/found-dive-into-elasticsearch-storage","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud. In this article we'll investigate the files written to the data directory by various parts of Elasticsearch. We will look at node, index and shard level files and give a short explanation of their contents in order to establish an understanding of the data written to disk by Elasticsearch. ","locales":"","title":"A Dive into the Elasticsearch Storage"}
{"index":{}}
{"author":"Shaunak Kashyap","category":"The Logstash Lines","publish_date":"2015-05-22T00:00:00.000Z","url":"/blog/logstash-lines-2015-05-22","seo_title":"","content":" Welcome back to ! In these weekly posts, we'll share the latest happenings in the world of Logstash and its ecosystem. We shipped 1.5.0 last week but the team didn't waste any time getting their work organized for future versions of Logstash.  Epic ticket triage for Logstash complete. ~550 tickets read, ~150 moved or otherwise updated. Whew! — @jordansissel (@jordansissel) If you are curious, here's what's cooking for versions and . While the Logstash team works their magic, here's some Logstash reading for this week: That's this week in . Come back next week for more Logstash news! ","locales":"","title":"The Logstash Lines: Versions 1.5.1 and  preview"}
{"index":{}}
{"author":"Haley Eshagh","category":"User Stories","publish_date":"2015-05-21T00:00:00.000Z","url":"/blog/opowering-energy-efficiency-with-elastic","seo_title":"","content":" It’s not every day we think about our energy consumption. Do you know what you paid on your last electricity bill? Or what your thermostat is set to right now? Less often, we think about how to reduce it — and even then, how to take action. Opower is a company that has made a business out of it. In fact, they’ve helped utility companies help their customers (i.e., you and me, the energy consumer) save over 4 terawatt-hours, roughly the equivalent of taking San Francisco off the power grid for an entire year. So what does Elastic have to do with Opower? Well, a few months back, we were bumbling around the Internets (like you do) looking for cool uses of our software, and came across an from Opower engineer Ben Siemon, describing their use of Elasticsearch. We had to know more. A few phone calls later, Ben and his team invited us for a visit. We packed up our gear, jumped on a plane, and rolled into the Opower offices in Arlington, VA, ready to record. One by one, we got the full story from the developers who work with Elasticsearch every day to build and maintain their internal segmentation tool and UI, Utonium. (, if you're curious.) Utonium puts data and insight into the nontechnical hands of Opower employees who then decide what type of energy advice will have the most impact on given segments of a utility company’s customer population. I won’t go into all of the details (that’s what the video above is for), but will note that throughout the process of filming the Opower team, it was clear that they’re on a mission to make the world a better place. Yes, that sounds cliché, but it’s honest. Currently, we only have one planet to call home, and we’re limited to the resources it provides. If it’s possible to leverage the power of data to use those resource more wisely, the more power to Opower. So, sit back and relax for a few minutes as Opower shares their Elasticsearch story. And, to all the Opower folks out there, a tip of the hat to you — keep up the good work. ","locales":"","title":"Opower(ing) Energy Efficiency with Elastic"}
{"index":{}}
{"author":"Leslie Hawthorn","category":"","publish_date":"2015-05-20T00:00:00.000Z","url":"/blog/this-week-in-elastic-2015-05-20","seo_title":"","content":" Welcome to ! In this weekly round up post, we bring you news on all aspects of the ELK Stack, including updates on Elasticsearch, Logstash and Kibana development, tips and tricks and upcoming events. Proud to have my second PR into Elasticsearch. Minor changes, but the community has been great and makes me want to contribute more— Nick Canzoneri (@nick_canz) Elastic News Useful blog posts, how tos, tutorials and articles from the wide world of all the ELK stack ecosystem. Just announced: Beta (alerting for ) goes public. Try it today! — elastic (@elastic) Slides and Videos Slides from my talk at on Migrating from Hibernate Search to Elasticsearch — Christian Strzadala (@cstrzadala) Slides from my , Logstash & Kibana talk at last week: — Robin Moffatt (@rmoff) Where to find Us On tonight at 7 PM: LAX meetup (Venice, CA) on logging AWS Cloudwatch & Cloudtrail data. RSVP now: — Leslie Hawthorn (@lhawthorn) We'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking about the ELK stack or hosting a meetup, Algeria will take the stage at Voxxed Days Algiers to present An Introduction to the ELK Stack. This first ever conference takes place May 23, and Samir's talk is on for 2:00 PM. AustriaThe .NET Community Austria Meetup will get together on June 11 to talk all about the ELK stack. to save your seat. Canada France During 2015, will show us how to add to legacy applications in a blink ! — RivieraDEV (@RivieraDEV) Germany IrelandJoin Mark Harwood for an expert level session at NoSQL Matters Dublin on . The conference takes place on June 4. Israel We've got dueling meetups in Tel-Aviv Yafo on May 26: JapanPlease join all members of Elastic's team in Japan at in Tokyo on June 3-5. We will have a booth in the exhibits hall and will be very happy to answer any questions you may have. The NetherlandsTechnical Expert Aurélien Foucret of talks about ElasticSearch at , more info: — Meet Magento NL (@mm15nl) Poland will take the stage at 6:30 PM on May 26 to talk Beyond the Basics with Elasticsearch at the . The event runs May 25-26 in Warsaw. (PS - We've also heard a rumor that the organizers will be offering Schema-Free beer on Elastic. Raise a glass with Honza if we're right!) Spain Join for the next Elasticsearch Barcelona Meetup on May 21, where you'll learn all about Moving to Production with Elasticsearch: Best Practices. to save your seat. SwedenThe Stockholm Elasticsearch Meetup will get together on May 27 to hear about two use cases. to save your seat. SwitzerlandThe Elasticsearch Zurich Meetup group will convene on May 27 to hear about Elastic's security product, Shield. to attend. United Kingdom South Wales Elasticsearchers: a meetup with a talk by Honza Král , Elasticsearch developer: All welcome. — DjangoCon Europe (@DjangoConEurope) United States - East United States - West The Elastic family: Big data search, discovery and analytics (p 20-22) - by Christoffer Vig — Found by Elastic (@foundsays) Where to Find You  If you're a regular reader, you're thinking of skipping this section. You may even be thinking to yourself, yes of course I will drop a note on Twitter when I am giving a talk on all things ELK. That's awesome, because we'd like to showcase every meetup, conference presentation and workshop on Elasticsearch, Logstash, and Kibana happening worldwide. And now, we've made it even easier for you to get support for your meetup! Head on over to  ! (And we'll still totally send you swag if you're giving a talk on anything ELKy at a conference.) Oh yeah, we're also  . If you'd like us to find you for employment purposes, just . We care more about your skill set and passion for Elasticsearch, Kibana, and Logstash than where you rest your head. Trainings If you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elastic: Watcher Beta goes public"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2015-05-19T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-and-apache-lucene-2015-05-19","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top NewsCheck out Mike McCandless’ latest deep dive…\"Hunting Tricky Apache bugs.\" — elastic (@elastic) Elasticsearch Core Apache Lucene ","locales":"","title":"This Week in Elasticsearch and Apache Lucene: Hunting Tricky Apache Lucene bugs"}
{"index":{}}
{"author":"Aaron Mildenstein","category":"Engineering","publish_date":"2015-05-19T00:00:00.000Z","url":"/blog/how-to-check-logstashs-pulse","seo_title":"","content":" Have you ever wondered if Logstash was sending data to your outputs? There's a brand new way to check if Logstash has a \\\"pulse.\\\" Introducing the It’s bundled with Logstash 1.5 so you can start using it immediately! Why? Logstash currently has a single pipeline. All events generated by inputs travel through the block, and then out of Logstash through the block.Even if you have multiple outputs and are separating events using conditionals all events pass through this single pipeline. If any one of your outputs backs up, the entire pipeline stops flowing. The heartbeat plugin takes advantage of this to help you know when the flow of events slows, or stops altogether.How?The heartbeat plugin sends a message at a definable interval. Here are the options available for the message configuration parameter:ExamplesBe sure to assign a to your heartbeat events. This will make it possible to conditionally act on these events later on.\\\"ok\\\" MessagePerhaps you only want to know that Logstash is still sending messages. Your monitoring system can interpret an \\\"ok\\\" received within a time window as an indicator that everything is working. Your monitoring system would be responsible for tracking the time between \\\"ok\\\" messages.I can send the default \\\"ok\\\" message every 10 seconds like this:input { heartbeat { interval => 10 type => \\\"heartbeat\\\" } # ... other input blocks go here } The events would look like this:{\\\"message\\\":\\\"ok\\\",\\\"host\\\":\\\"example.com\\\",\\\"@version\\\":\\\"1\\\",\\\"@timestamp\\\":\\\"2015-03-18T17:05:24.696Z\\\",\\\"type\\\":\\\"heartbeat\\\"} {\\\"message\\\":\\\"ok\\\",\\\"host\\\":\\\"example.com\\\",\\\"@version\\\":\\\"1\\\",\\\"@timestamp\\\":\\\"2015-03-18T17:05:34.696Z\\\",\\\"type\\\":\\\"heartbeat\\\"} {\\\"message\\\":\\\"ok\\\",\\\"host\\\":\\\"example.com\\\",\\\"@version\\\":\\\"1\\\",\\\"@timestamp\\\":\\\"2015-03-18T17:05:44.695Z\\\",\\\"type\\\":\\\"heartbeat\\\"} Epoch timestampPerhaps your monitoring system uses unix timestamps to track event timing (like Zabbix, for example). If so, you can use the epoch timestamp in the clock field to calculate the difference between \\\"now\\\" and when Logstash generated the heartbeat event. You can calculate lag in this way. This may be especially useful if you inject the heartbeat before events go into a broker, or buffering system, like Redis, RabbitMQ, or Kafka. If the buffer begins to fill up, the time difference will become immediately apparent. You could use this to track the elapsed time--from event creation, to indexing--for your entire Logstash pipeline.This example will send the epoch timestamp in the field:input { heartbeat { message => \\\"epoch\\\" interval => 10 type => \\\"heartbeat\\\" } # ... other input blocks go here }The events would look like this:{\\\"clock\\\":1426698365,\\\"host\\\":\\\"example.com\\\",\\\"@version\\\":\\\"1\\\",\\\"@timestamp\\\":\\\"2015-03-18T17:06:05.360Z\\\",\\\"type\\\":\\\"heartbeat\\\"} {\\\"clock\\\":1426698375,\\\"host\\\":\\\"example.com\\\",\\\"@version\\\":\\\"1\\\",\\\"@timestamp\\\":\\\"2015-03-18T17:06:15.364Z\\\",\\\"type\\\":\\\"heartbeat\\\"} {\\\"clock\\\":1426698385,\\\"host\\\":\\\"example.com\\\",\\\"@version\\\":\\\"1\\\",\\\"@timestamp\\\":\\\"2015-03-18T17:06:25.359Z\\\",\\\"type\\\":\\\"heartbeat\\\"} Sequence of numbersThis example makes it easy to immediately check if new events are occurring because the clock will continuously increase.input { heartbeat { message => \\\"sequence\\\" interval => 10 type => \\\"heartbeat\\\" } # ... other input blocks go here }The events would look like this:{\\\"clock\\\":1,\\\"host\\\":\\\"example.com\\\",\\\"@version\\\":\\\"1\\\",\\\"@timestamp\\\":\\\"2015-03-18T17:08:13.024Z\\\",\\\"type\\\":\\\"heartbeat\\\"} {\\\"clock\\\":2,\\\"host\\\":\\\"example.com\\\",\\\"@version\\\":\\\"1\\\",\\\"@timestamp\\\":\\\"2015-03-18T17:08:23.027Z\\\",\\\"type\\\":\\\"heartbeat\\\"} {\\\"clock\\\":3,\\\"host\\\":\\\"example.com\\\",\\\"@version\\\":\\\"1\\\",\\\"@timestamp\\\":\\\"2015-03-18T17:08:33.029Z\\\",\\\"type\\\":\\\"heartbeat\\\"}OutputNow let's add a conditional to send this to our monitoring system, and to our other outputs:output { if [type] == \\\"heartbeat\\\" { # Define the output block for your monitoring system here } else { # ... other output blocks go here } } Of course, if you do want your heartbeat messages to be indexed alongside your log data, you are free to do so.ConclusionThe new heartbeat plugin provides a simple, but effective way to monitor the\\xC2","locales":"","title":"How to check Logstash's pulse"}
{"index":{}}
{"author":"Roy Russo","category":"User Stories","publish_date":"2015-05-19T00:00:00.000Z","url":"/blog/elasticsearch-at-predikto","seo_title":"","content":" Predikto processes large volumes of data for asset-intensive industries in order to predict equipment failure. This information allows customers to pro-actively apply prescriptive maintenance, thereby avoiding downtime. The Predikto Enterprise Platform combines applies advanced data science and machine learning techniques and algorithms for customers across industries such as oil & gas, rail, fleet, manufacturing, and data gathered from the Industrial Internet of Things. It processes large volumes of data from disparate sources in heterogeneous formats, something that Elasticsearch is well-suited to store and query on. Elasticsearch is a key component of the Predikto Enterprise Platform. Horizontal scalability, product maturity, and tooling all factored in the decision to adopt Elasticsearch at Predikto. Mapping Distributed AssetsGeoJSON support is an important feature of Elasticsearch. For example, a railroad company may want to predict asset failure on moving locomotives, train cars, and even train car doors to prevent unnecessary stops that cost millions of dollars annually. Predikto factors the location of the asset (sometimes an actively moving asset like a train or fleet vehicle) and correlates the location with weather forecast data also stored in Elasticsearch, as weather often plays a role in device failure. Since Elasticsearch supports the GeoJSON standard out-of-the-box, it allowed Predikto to standardize sensor and device location data in GeoJSON format as well. Now, correlating latitude and longitude coordinates with weather data is a trivial matter. The other benefit to having all location data standardized with GeoJSON and stored in Elasticsearch was that it made visualization simple. The Predikto user interface includes a large map, allowing users to visualize their assets as they move in real-time and highlight those assets that are likely to fail. Heavy use of the aggregations API enables a clean way to do time-series modeling,especially with features like terms, date_histogram, and top hits APIs. Standardizing the format of geo-location data helped remove the custom transformation of  location data for the user interface and the predictive analysis process. Integrating Elasticsearch with Spark for big data ETLThere is no widely-adopted standard format for sensor data, although many IoT vendors are working on one. This means that the massive amounts of data flowing in to the Predikto system are in any manner of format, from CSV, TSV, log line, xml, and even some legacy formats that defy logic at first glance. All of this data must be transformed into a standard format in order for our machine learning algorithms and predictive analytics models to process. This is where Apache Spark comes in. Being able to process large batches of data in memory, while reading and writing from and to Elasticsearch using the es-hadoop library maintained by Elastic enabled Predikto to have smooth ETL processes that improved performance from the previous, disk-bound ETL process by a factor of 10. Massaging the original data into the Predikto standard format often requires substantial processing power. In this instance, Elasticsearch, as a primary datasource, is having to perform heavy read operations, and finally, heavy write operations from dozens of Spark workers asynchronously during the ETL process. The last point above helped remove a stumbling-block we ran in to when writing asynchronously from many Spark workers to Amazon S3. Because of S3’s eventual-consistency object store, the Spark master was unable to combine to worker-written file parts, as they were not immediately visible, thus causing the entire Spark process to fail. In the past, we remedied the issue by saving the output to a mounted EBS volume, and then copying the file to S3. This was error-prone and not performant at scale. Instead, we make heavy use of the bulk API to insert documents from Spark workers now in to Elastic","locales":"","title":"Elasticsearch in Big Data Predictive Analytics at Predikto"}
{"index":{}}
{"author":"Robyn Bergeron","category":"","publish_date":"2015-05-18T00:00:00.000Z","url":"/blog/openstack-summit-elk","seo_title":"","content":" If you're a fan of the ELK stack and you happen to be in Vancouver, British Columbia this week for the , good things are coming your way! As it so often happens with well-loved open source projects, enthusiastic community members find ways to marry the technologies of the projects they love and make them work together. The OpenStack project community is no exception here, and the numbers of ways in which various pieces of the ELK stack are being incorporated in myriad ways with OpenStack -- either as parts of deployments, incorporated in OpenStack projects, used as part of the OpenStack project infrastructure, and likely other ways I haven't heard about just yet.  What does this mean? As an attendee, there are a variety of options this week at the OpenStack summit to learn and discuss how the ELK stack can work with OpenStack. Hooray! And to save you, lovely reader, from having to scroll endlessly through the incredible schedule of amazing content to choose from this week, I've put together your quick guide to finding the ELK love you want to hear about while at Summit. Birds of a Feather As the author of this post, I'll admit that I may be a bit biased: I definitely think you should come to the ELK Stack Birds-of-a-Feather session this week -- because I will be your lovely and gracious host. Titled \"\" -- we'll be gathering in the East building on Wednesday, May 20th, from 3:30 - 4:10pm. Operating in a traditional Birds of a Feather fashion, the content of this session very much depends on your participation!  One of the great things about bringing people together at open source conferences is that it enables opportunities for collaboration. And even with so many different and useful ways to combine the ELK stack and OpenStack, chances are that if you have an idea in mind, others may be as well. If you're interested in seeing the ELK stack be part of the Murano application catalog, or looking for Heat templates for the ELK stack, or want to figure out the best practices for shipping your logs in an OpenStack deployment, this is the place to be! So: Bring your questions, your ideas, your problems you're trying to solve -- let's bring people together, and make magic happen. Also: If you're looking for Logstash or Elasticsearch stickers.... I hear they'll be making an appearance here. So be sure to swing by. Wednesday, May 20: In addition to the aforementioned BoF, the following sessions are also running on Wednesday, and thankfully, not overlapping with each other (thanks, awesome organizers!): : 1:50 - 2:20pm This session, presented by Michael Factor and Dmitry Sotnikov of IBM's Haifa Research Lab, will be showing how they've used open source tools (including Elasticsearch, Logstash, and Kibana!) to identify performance bottlenecks in Swift. : 5:20 - 6:00pm Andy McCrae of the Rackspace Private Cloud team will be sharing tips from the operator's point of view on making logs useful and accessible, even in an environment with numerous OpenStack services running across many hosts -- and examples of how to do it with the ELK stack. Thursday, May 21 : 9:00-10:30am Jordan Callicoat and Christopher Woodward of Rackspace will help you eliminate the late-week conference blues of missing doing actual things on your computer with this interactive workshop. Learn how to deploy the ELK stack in an OpenStack environment, and get some hands-on experience using the Elasticsearch API and Kibana to get information about users, instances, and services in your OpenStack deployment.  : 9:50-10:30am Gabriel Hurley will be discussing the universe of possibilities around feeding OpenStack data directly into Elasticsearch -- and the types of problems to be solved in doing just that. He'll also be covering how to integrate Elasticsearch into your cloud, sharing example queries, and sharing upcoming challenges in embarking on such a project. Be sure to say hi! With such a great schedule, I know it's hard sometimes to be","locales":"","title":"ELK Stack Crossing: Where to find the ELK in OpenStack Summit"}
{"index":{}}
{"author":"Michael McCandless","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2015-05-12T00:00:00.000Z","url":"/blog/2015-05-12-this-week-in-elasticsearch-and-apache-lucene","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top News Polyglot meetup next Wed night two talks \"Scaling Elasticsearch & Autosuggest in Lucene\" — Tavis Rudd (@tavisrudd) Elasticsearch Core Apache Lucene Watch This Space , where we'll share more news on the whole ELK ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene:Meetup: Scaling Elasticsearch & Autosuggest in Lucene"}
{"index":{}}
{"author":"Leslie Hawthorn","category":"News","publish_date":"2015-05-12T00:00:00.000Z","url":"/blog/join-the-conversation","seo_title":"","content":" As we’ve begun to scale up development on three different open source projects, we’ve found mailing lists to be a difficult solution for dealing with all of our needs for community support. We’ve had multiple mailing lists going, one dedicated to Elasticsearch and another to Logstash, plus numerous lists for speakers of languages other than English. We've found these many vectors of entry can be confusing for new folks trying to figure out where to go to ask a question.We’ve also found our lists are becoming noisy in the “good problem to have” kind of way. As we’ve seen more user adoption, and across such a wide variety of use cases, we’re getting widely different types of questions asked. It can be hard to separate signal from noise on a general list. Plus, not all of our users are comfortable with mailing lists.At Elastic, we love to solve our users' problems. We've found a solution that we hope will work well for all of our community members, so head on over to our discussion forum at . Join the conversation and let us know what you think of the new communication tool. Lover of mailing lists, but not a fan of web forums? No problem! Create a user profile on the forum - you can also do this simply by using your GitHub, Facebook, Twitter or Google Apps credentials, plus plain old sign in with email address - and do a quick one time set up of your preferences for email notifications. Once that's done, you can interact with the forums solely via email .Use, share and enjoy. And, of course, and how we can improve your experience! ","locales":"","title":"Join the Conversation: Discuss.Elastic.Co"}
{"index":{}}
{"author":"Christoffer Vig","category":"","publish_date":"2015-05-11T00:00:00.000Z","url":"/blog/found-analyzing-weblogs-with-elasticsearch","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud.Using Logstash and Kibana on Found by ElasticThis is part one of a two post blog series, aiming to demonstrate how to feed logs from IIS into Elasticsearch and Kibana via Logstash, using the hosted services provided by Found by Elastic. This post will deal with setting up the basic functionality and securing connections. will show how to configure Logstash to read from IIS log files, and how to use Kibana 4 to visualize web traffic. ","locales":"","title":"Analyzing Weblogs with Elasticsearch in the Cloud, Part 1"}
{"index":{}}
{"author":"Robyn Bergeron","category":"Kurrently in Kibana","publish_date":"2015-05-18T00:00:00.000Z","url":"/blog/kurrently-kibana-2015-05-18","seo_title":"","content":" Welcome to , your source for keeping up with the Kibana project. With information on project releases, work in progress, and learning resources, you can on this weekly update to let you know about the latest Kibana goodness coming your way! Data loaded in via Logstash and analyzed with Kibana. is in ELK stack heaven. — Michael Rainey (@mRainey) Kibana 4.1: Getting Closer... The team has been plugging away at the last few , mostly trying to get some tile map stuff cleaned up so we can merge it. Lately, we: Kibana Forum Just like the other projects here at Elastic, Kibana now has a forum for discussions and questions that was recently launched. This awesome new service lets us keep Github for getting work done, while fostering a community of Kibana experts to kick around questions, ideas and solutions in a totally async and persistent fashion. , and enjoy! Elasticsearch Stuff We’re Giddy About: _path. We’ve bandied this idea around for over a year and it's almost ready to use. Pull: Purpose: `path` allows us to specify what data we want from a request in a super granular way. For example, let’s say is retrieving unique web requests over time:  a date_histogram agg, with a cardinality agg inside of it. ``` curl -XGET 'localhost:9200/logstash-*/_search’ -d ‘ { “aggs”: { “2”: { “date_histogram”: { “field”: “@timestamp”, “interval”: “30s”, }, “aggs”: { “1”: { “cardinality”: { “field”: “request” } } } } } } ``` We’re going to get back something like this for every bucket. So, you know, like 1000 of these. ``` { \"key_as_string\" : \"2015-04-25T07:06:30.000Z”, // We don’t need this \"key\" : 1429945590000, // Oooo important \"doc_count\" : 6716, // Nope \"1\" : { \"value\" : 441 // Yep } }, ``` So this thing was 4 keys, only 2 of which we actually need, and one of them is a big long string. Move enough of those over the wire and it's a whole lot of data. Not to mention the memory considerations of parsing all those and keeping around a big honking object on which we’ll only ever access half the properties. Ok, now let's bring into the fray: ``` curl -XGET 'localhost:9200/logstash-*/_search?_path=aggregations.2.buckets.1,aggregations.2.buckets.key -d [All that other stuff from above] ``` We get back a nice clean result with only the stuff we need. ``` { \"key\" : 1429945740000, \"1\" : { \"value\" : 1 } } ``` It's something like a 68% reduction in data moved over the wire. And it's not just the search API -- it applies to every API. If you need to do something like enumerate the hostnames of every node in your cluster, it's more like a 99% reduction. Sweet. My first data visualization in with and . Exchange message's send and received — Farhaz Hofman (@farhaz) See You Next Week!Did you know? Elasticsearch and Apache Lucene, Logstash and Kibana all have weekly updates published to the Elastic blog. We'll bring you the newest, stuff in next week's . ","locales":"","title":"Kurrently in Kibana - May 18, 2015"}
{"index":{}}
{"author":"Leslie Hawthorn","category":"","publish_date":"2015-05-13T00:00:00.000Z","url":"/blog/2015-05-13-this-week-in-elastic","seo_title":"","content":" Welcome to ! In this weekly round up post, we bring you news on all aspects of the ELK Stack, including updates on Elasticsearch, Logstash and Kibana development, tips and tricks and upcoming events. : You know, for Alerting (Coming Soon)...Get details & sign up for the webinar now — elastic (@elastic) Elastic News Useful blog posts, how tos, tutorials and articles from the wide world of all the ELK stack ecosystem. New article from one of the Found founders: A Dive into the Elasticsearch Storage — Found by Elastic (@foundsays) Slides and Videos Where to find Us If you're in Denver & interested in , come hear me on things I wish I'd known as a newbie ES app dev: — Shaunak Kashyap (@shaunak) We'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking about the ELK stack or hosting a meetup, Algeria will take the stage at Voxxed Days Algiers to present An Introduction to the ELK Stack. This first ever conference takes place May 23, and Samir's talk is on for 2:00 PM. Australia The Melbourne Search Users Group will meet up on May 19 for a talk from Movideo: Migrating from Hibernate Search to Elasticsearch. to save your seat. We're pretty sure our very own will be joining you, too! Canada Czech Republic Our very own will be on hand for Q&A on Elasticsearch in Prague on May 7. You can RSVP for this talk . France The Elasticsearch France Meetup returns to Paris on May 26. so you know when we open up RSVPs! During 2015, will show us how to add to legacy applications in a blink ! — RivieraDEV (@RivieraDEV) Germany Israel We've got dueling meetups in Tel-Aviv Yafo on May 26: The Netherlands Johan Guldmyr will host a talk on dCache and the ELK stack at the at 2:20 PM on May 19. The Workshop runs May 18-20 in Amsterdam. Poland will take the stage at 6:30 PM on May 26 to talk Beyond the Basics with Elasticsearch at the . The event runs May 25-26 in Warsaw. Portugal The Merge Lisbon Meetup Group will get together on May 14 to talk about the ELK stack. to hear from our very own Logstash engineering team member . Spain Join for the next Elasticsearch Barcelona Meetup on May 21, where you'll learn all about Moving to Production with Elasticsearch: Best Practices. to save your seat. Turkey Voxxed Days Istanbul is coming up on May 9. Join to learn all about Advanced Search for Your Legacy Application Using Elasticsearch. David at 11:30 AM. United Kingdom For folks in Cardiff, Wales, your welcome to attend our Elasticsearch Meetup hosted at DjangoCon EU. Make sure to pop by the conference to say hello to Honza Kral in the hallway track. runs May 31 - June 5. South Wales Elasticsearchers: a meetup with a talk by Honza Král , Elasticsearch developer: All welcome. — DjangoCon Europe (@DjangoConEurope) United States - East United States - West Where to Find You  If you're a regular reader, you're thinking of skipping this section. You may even be thinking to yourself, yes of course I will drop a note on Twitter when I am giving a talk on all things ELK. That's awesome, because we'd like to showcase every meetup, conference presentation and workshop on Elasticsearch, Logstash, and Kibana happening worldwide. And now, we've made it even easier for you to get support for your meetup! Head on over to  ! (And we'll still totally send you swag if you're giving a talk on anything ELKy at a conference.) Oh yeah, we're also  . If you'd like us to find you for employment purposes, just . We care more about your skill set and passion for Elasticsearch, Kibana, and Logstash than where you rest your head. Trainings If you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elastic: Watcher, alerting for Elasticsearch, is coming soon"}
{"index":{}}
{"author":"Christoffer Vig","category":"","publish_date":"2015-05-11T00:00:00.000Z","url":"/blog/found-analyzing-weblogs-with-elasticsearch-part-2","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud.Using Logstash and Kibana on Found by ElasticThis is part two in the series about using Elasticsearch, Kibana and Logstash with Found by Elastic. In the , we used Logstash 1.5 and Kibana 4.02 to communicate with Elasticsearch in the cloud. This time we will use Logstash to feed logs from a web search application running on the Microsoft web server IIS (Internet Information Service) into Elasticsearch, and use Kibana to show some nice graphs. ","locales":"","title":"Analyzing Weblogs with Elasticsearch in the Cloud, Part 2"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2015-05-18T00:00:00.000Z","url":"/blog/2015-05-18-where-in-the-world-is-elastic","seo_title":"","content":" Welcome to  We'll be all over the world this week for our meetups and events, from Denver to Taiwan. Read on to find out if we'll be near you:Upcoming EventsMay 20-21: Stop by our booth at  in Broomfield, Colorado. Aaron Mildenstein will be giving a talk titled on Wednesday, May 20th in Track 2, from 3:45 PM- 4:15 PM, make sure to check it out!May 23: Samir Bennacer will be giving an at from 2:00 PM- 3:00 PM in Hall: Salle Algiers.Upcoming Meetups May 18: May 18: May 19: May 19: May 20: May 20: May 20: May 20: May 20:May 21: May 21: May 18: May 19: That's it for this week. Stay tuned for Elastic happenings next week - there's much to come!  - The Elastic Team P.S.:   if you're interested in hosting a meetup or are giving a talk about Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic: Gluecon Colorado & Voxxed Days Algiers"}
{"index":{}}
{"author":"Michael McCandless","category":"Engineering","publish_date":"2015-05-18T00:00:00.000Z","url":"/blog/lucene-tricky-bugs","seo_title":"","content":" Recently we tracked down two particularly tricky Lucene backwards compatibility bugs ( and ), serious enough to trigger a . The series of events leading to these bug fixes is quite illustrative, not only in how seriously we take index corruption issues (no matter how rare) but also in how randomized tests and alert us to potential issues and how debugging tricky test failures in healthy open source projects works these days. The story begins with , noticing a spooky-looking test failure in one of , asking me to dig. The test in question, , is a great test: it unpacks old indices from pre-built ZIP files across all prior Elasticsearch releases and then verifies that the latest version of Elasticsearch starts up and searches and indexes against these indices properly. Almost always the test passes, but every so often it would fail, with a scary exception showing the index had somehow become corrupt. The corruption seemed to happen when Lucene 4.10.3 first kissed a 3.x index ('s first commit to the index).Like Lucene, Elasticsearch tests are heavily randomized, but provide a reproducible seed to recreate the failure. Unfortunately, when I ran the test with this seed, with the same Java version and JVM options, it refused to fail. Grrr.This sometimes happens, e.g. if the test relies on how concurrent threads are scheduled, so I fired up a dedicated Python script to \"beast\" the test. The script efficiently runs a single test case over and over, bypassing the normally costly Maven dependency checking and compilation steps and just executing the test directly across multiple JVMs. Still, it would not fail for me, after more than 24 hours of beasting. Why not? Unable to reproduce the failure, but still seeing it occasionally fail in Jenkins, we retrenched. First, multiple developers stared at the , and we uncovered which could possibly explain the failure we were seeing:  at least it produced the same exception. Excited that we had possibly found the issue, we to track it. But then, on further digging, Elasticsearch was not re-using index shard directories in this test, so that could not explain the failure. Still, this was a nice separate bug to uncover and fix and this shows a common pattern in open-source software: a test failure, or a new bug, causes sudden excitement and scrutiny by multiple developers on the code in question, and this often uncovers further issues to fix and the project moves forward. We saw a similar pattern with , also fixed in Lucene 4.10.4. Retrenching again, we fell back on looking for any patterns to the 3 or 4 failures we had seen, and one thing we noticed is it was always the 0.20.6 test index (Lucene 3.6.2) that failed, despite other 3.x indices in the test. Why? One unusual thing about that particular test index was that it was much larger than the others, containing many more segments, which confused me, and I asked about this on the issue. quickly replied, explaining that he had regenerated that one index to make it \"beefier\" to better test another issue he had fixed in the past. Perhaps something unique about this index tickled a bug that the other indices didn't? Simon also suggested turning on verbose file deletion logging for this test, a feature to Elasticsearch for precisely this kind of situation. Then we waited for the next failure, which came a couple days later, and it clearly indicated where Lucene was incorrectly deleting the file! Armed with that important information, and then going back and staring at that code some more, we finally isolated the situation that could cause the corruption: it only happens on Lucene 4.x’s first kiss to a 3.x index, if concurrent merges are also running and if the threads are scheduled by the JVM \"just right\". This explains why only the 0.20.6 index would fail: it was the only 3.x index with many segments that needing merging!  It also explains why the test would not easily reproduce: it relied on the specific timing of concurrent me","locales":"","title":"Hunting Tricky Apache Lucene Bugs"}
{"index":{}}
{"author":"Shaunak Kashyap","category":"The Logstash Lines","publish_date":"2015-05-15T00:00:00.000Z","url":"/blog/logstash-lines-2015-05-15","seo_title":"","content":" Welcome back to ! In these weekly posts, we'll share the latest happenings in the world of Logstash and its ecosystem. After 500+ commits, 1.5.0 GA is released. Details on integration & more here — elastic (@elastic) The highlights of this release include: Read more in Many thanks to our community contributors and the amazing Logstash team at Elastic. Here are some of their happy faces after the 1.5 release: Logstash remote release party with the team. <3 — @jordansissel (@jordansissel) That's this week in . Come back next week for more Logstash news! ","locales":"","title":"The Logstash Lines: Logstash 1.5 is here!"}
{"index":{}}
{"author":"Suyog Rao","category":"News","publish_date":"2015-05-14T00:00:00.000Z","url":"/blog/logstash-1-5-0-ga-released","seo_title":"","content":" We are excited to announce the GA release of Logstash 1.5.0! 500 plus commits and 10 months in the making, this is one of our biggest releases. A big thank you to all our users for the many contributions, feedback, GitHub issues, and for trying out the pre-releases. Jump to our page to get going, or check the for details. HighlightsThe main themes of 1.5.0 are plugin management, performance improvements, and Apache Kafka integration. Here are the highlights: Plugins SeparationLogstash has a rich collection of over 165 plugins (inputs, filters, outputs, and codecs). With 1.5.0, we are taking a step closer to making plugin management even better for our users. We have added infrastructure to easily install, update, and remove plugins on top of Logstash. For example, to install the S3 output plugin: $LS_HOME/bin/plugin install logstash-output-s3 This has all the scoop about plugin changes.New pluginsAs we iterated on 1.5.0, the feedback we received for plugin changes was overwhelming. This is evident with the contribution of many new plugins -- Heartbeat input, CouchDB changes input, Slack input, RSS input, JMX input just to name a few. Check out it is to create and publish plugins! Performance ImprovementsLogstash 1.5.0 is much faster. Let's highlight two areas where performance has gone way up: In this release, we have increased the throughput of the popular grok filter in some patterns by 100%. Put another way, you can process more data through Logstash when using the grok filter. In our benchmark testing, we compared throughput in 1.5.0 and 1.4.2 by processing 6.9 million entries of Apache Web access log lines using the grok pattern. Throughput in 1.5.0 increased from 34K events per second (eps) to 50K eps. Both tests were run on an eight-core machine with eight worker threads in Logstash. These tests we run with a single grok filter and measured throughput of events processed in the pipeline using a stdin input and stdout output. Please note that overall performance will vary with hardware and Logstash configuration used. We implemented JSON serialization and deserialization using the JrJackson library, which improved the throughput by over 100%. In our previously mentioned performance tests, we sent 500,000 JSON events (1.3KB in size) and measured a throughput increase from 16K eps to 30K eps. With events 45KB in size, throughput increased from 850 eps to 3.5K eps. Improved SecurityWe have improved the security of the Elasticsearch output, input, and filter by adding authentication and transport encryption support. For instance, with the HTTP protocol you can configure SSL/TLS to enable encryption and HTTP basic authentication to provide a username and password while making requests. These capabilities will enable Logstash to natively integrate with the Elastic's security product. Apache Kafka IntegrationIn scaling Logstash deployments, can be used as an intermediate message buffer to store data between the shipping instances and indexing instances. In 1.5.0, we have added built-in support for the Logstash Kafka input and output plugin that was originally developed by . We added the codec and support to easily plug in your serialization/deserialization mechanism. All this makes it easy to consume events stored in Kafka, enrich them, and analyze them using Elasticsearch, Logstash, and Kibana. Windows ExperienceThis release made many improvements for running Logstash and plugin-related infrastructure on Windows, which was degraded since the 1.4.2 release. We resolved issues related to initial setup and upgrade, and fixed important bugs in the file input plugin. DocumentationPreviously, Logstash documentation was hosted on , which made it cumbersome to find information when working with the rest of the ELK stack. We have moved documentation for 1.5.0 and all future releases to the Elastic website under the . Fixes/Enhancements1.5.0 contains a number of important enhancements and bug fixes. A detailed list can be found in the . Wh","locales":"","title":"Logstash 1.5.0 GA released"}
{"index":{}}
{"author":"Shay Banon","category":"News","publish_date":"2015-05-11T00:00:00.000Z","url":"/blog/watcher-you-know-for-alerting-coming-soon","seo_title":"","content":" With simple REST APIs exposed over HTTP, Elasticsearch is a platform that encourages integration and automation. Whether using the percolator API, direct integration with infrastructure monitoring systems like Nagios, or cron-job scripts that run queries and take action, users have always been able to build notification and alerting systems on top of Elasticsearch. As downloads of Elasticsearch have grown and its use cases have matured and expanded, we have heard frequent requests from our users and customers for integrated alerting and notification capabilities with a simple API to help them detect changes and anomalies in their growing, diverse data sets. Today, after spending many hours with our customers understanding their needs, and many months designing and coding, we are thrilled to announce . Watcher is a flexible, powerful product that will allow Elasticsearch users to get insights and take action on changes in their data more efficiently across a wide range of use cases. Whether you're , or , the ability to easily push notifications based on changes in your data will be a game changer. Like all of our products, Watcher is built using public Elasticsearch extension points. This means you can install it on your existing Elasticsearch cluster and it's easy to get started. How It Works: The Anatomy of a “Watch\"Interested in 404 errors? Low disk space? Or want to know the exact moment when social sentiment for your latest campaign takes takes off on Twitter? Just define it as a query using the full power of the Elasticsearch query language, including aggregations. Now that Watcher knows what to look for, set a threshold worthy of an alert — maybe you only care about 404 errors if they occur 50% more frequently than average. To craft more sophisticated conditions, scripting is supported.Choose how often Watcher runs your queries and checks the condition. It's easy to define simple schedules — run every minute, hour, or day. For more complex scheduling needs, cron syntax is also supported.  If your conditions are met, Watcher can send a custom email, push data to external systems like PagerDuty via WebHook, or take the results of your query and store them in Elasticsearch.Learn More If you would like to learn more, please join us for the , happening on Wednesday, May 20. Uri Boness and Steve Kearns will give a detailed overview, including installation, configuration, and a live demo. And when you sign up, you can also opt into the Watcher beta. Lastly, I am happy to announce that Watcher will be free for existing and future subscription customers. We invest heavily in the success of our customers, from insights provided by our developers during production and development support, to our growing family of products, and we couldn't be happier to provide them our latest product at no additional cost.  ","locales":"","title":"Watcher: You Know, For Alerting (Coming Soon)"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2015-05-11T00:00:00.000Z","url":"/blog/2015-05-11-where-in-the-world-is-elastic","seo_title":"","content":" Welcome to  A calm week on the events side but we have some cool meetups coming up this week! Read on to find out where we'll be:Upcoming Meetups May 12: May 13: May 12: May 12: May 13:  May 14: That's it for this week. Stay tuned for Elastic happenings next week - there's much to come!  - The Elastic Team P.S.:   if you're interested in hosting a meetup or are giving a talk about Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic: Meetups in Chicago, Berlin, Vienna, and more"}
{"index":{}}
{"author":"Shaunak Kashyap","category":"The Logstash Lines","publish_date":"2015-05-08T00:00:00.000Z","url":"/blog/2015-05-08-logstash-lines","seo_title":"","content":" Welcome back to ! In these weekly posts, we'll share the latest happenings in the world of Logstash and its ecosystem. 1.5 RC4 now available with lumberjack input, filter, and other improvements! — elastic (@elastic) Other Logstash Core Improvements was super excited about the logstash-core gem which is used by the pipeline. This gem encapsulates only the pipeline logic now — we moved out all the helpers, development dependencies, start-up scripts and such. This simplifies artifact packaging, and fixes issues with flaky dependencies. All good stuff! Wondering how best to scale Logstash? Look no further than this awesome talk by our very own : That's this week in . Come back next week for more Logstash news! ","locales":"","title":"The Logstash : Logstash 1.5 RC4 is hot off the presses!"}
{"index":{}}
{"author":"Martijn Laarman","category":"Engineering","publish_date":"2015-05-08T00:00:00.000Z","url":"/blog/nest-1-5-released","seo_title":"","content":" Today we are happy to release NEST and Elasticsearch.Net 1.5 which brings us to feature parity with the new features introduced in . A couple weeks later than our usual feature parity release cadance, but the .NET team was on the road in the states, which we'll report on later in this blog post. Put plainly: NEST 1.4.3 was an Elasticsearch 1.5 compatibilty release, NEST 1.5 is a complete feature parity release.  The Bad! Before we get the good news show rolling, let's start with the bad that has been fixed in this 1.5 release. We broke .NET 4.0 support in our Nuget package meta-data causing folks on .NET 4.0 not see any update past NEST 1.3.3.  We are very happy that this is now fixed and folks still on .NET 4.0 can now upgrade to 1.5! If you had an alternative id configured on your POCO either through or on , the method we constructed to get the id out of your POCO was not cached, causing it to not take advantage of cached reflection.   We are happy this is now fixed but upset we had to include a section in this blog post to begin with! Inner Hits This feature allows you to get the matching parents, childs or nested objects back separately but in a single search request. Imagine for a second that we're indexing royal families.  Instead of indexing the complete graphs, we'll index Kings, Princes, Dukes, Earls and Barons separately. The mapping for which might look something like this in NEST: var create = this.Client.CreateIndex(\"royalty\", c => c .NumberOfReplicas(0) .NumberOfShards(1) .AddMapping<King>(m => m.MapFromAttributes()) .AddMapping<Prince>(m => m.MapFromAttributes().SetParent<King>()) .AddMapping<Duke>(m => m.MapFromAttributes().SetParent<Prince>()) .AddMapping<Earl>(m => m.MapFromAttributes().SetParent<Duke>()) .AddMapping<Baron>(m => m.MapFromAttributes().SetParent<Earl>()) ):  When we are searching for s we'd also like to get back the king he's serving, inner hits allows you to do this! var results = this.Client.Search<Prince>(s => s .Index(\"royalty\") .Query(q => q .HasParent<King>(hp => hp .Query(qq => qq.MatchAll()) .InnerHits() ) ) ):  To get back each hit's king (remember a document can only have one parent) you can use the following C# code: foreach(var hit in results.Hits) { var king = hit.InnerHits[\"king\"].Documents<King>().FirstOrDefault():  } Here we specified as part of the query DSL, but you can also specify more deeply nested inner hits structures as part of the request.  Be sure to read the  for more information. IDocument In order to implement inner hits, we had to support lazy document serialization. Because the inner hits on the response is basically an where can be of any unrelated type, we had to come up with a way to defer deserialization until you, the programmer, can get a hold of the dictionary and know what types you want to get out of it. To solve this, inner hits are mapped to , a special construct that you can call on later to deserialize into the type of your pleasing. The  earlier is a convenience method on the inner hits meta data object to do so for all the inner hits. A cool side effect is that you can now also use this on using as and defer determening what type to deserialize to in your application code. Variables in suffix expressions Best explained through an example. var sortSuffix = \"raw\":  var path = Property.Path<ElasticsearchProject>(p => p.Name.Suffix(sortSuffix)):  var resolved = client.Infer.PropertyPath(path):  will be now be and so much more... Please see our for all the fixes and new features that went into this release, and as always a shout out to everyone who submitted a PR or reported an issue. We can't thank you enough! .NET team US tour The .NET team within Elasticsearch (  & ) spent 8 jam-packed days together doing meetups, trainings and hashing out our roadmap for the rest of the year. .NET Fringe First stop: Portland, Oregon, Where we attended the amazing , a grassroots .NET OSS conference. We are still a bit buzzed by the good vibes we felt at t","locales":"","title":"NEST 1.5 Released"}
{"index":{}}
{"author":"Gabriel Moskovicz","category":"Engineering","publish_date":"2015-05-08T00:00:00.000Z","url":"/blog/how-to-handle-elasticsearch-virtualization","seo_title":"","content":" To have a better understanding of the challenges we may deal with when using Elasticsearch in a virtualized environment, we need to change the focus from conventional hardware problems to a more complex view. The purpose of this article is to uncover some common issues you might experience using Elasticsearch in virtual environments. A Brief History Way before Elasticsearch appeared, the concept of virtualization was taking its place as a first class citizen in computing. Virtualization refers to the act of creating a virtual (not an actual) version of something, including, among others, virtual-computer hardware platforms, operating systems, storage devices, or computer network resources. Virtualization was born in the late 1960s and early 1970s, when IBM created the CP-40/CMS (Conversational Monitor System) as a method of logically dividing the system resources provided by mainframe computers between different applications. Afterwards, the meaning of the term broadened to what currently is: full virtual machine (VM) implementations and control of processing, network and memory, all working together seamlessly in the cloud. Existing Platforms There are various existing platforms to handle Elasticsearch in virtual environments, all of which are different between them. Generally, the three main platforms we see used for Elasticsearch are: Finally, as a different way to handle our Elasticsearch virtualized infrastructure,  is a hosted and fully managed Elasticsearch Software as a Service (SaaS). Found provides a fast, scalable, reliable and easy to operate search service hosted for you in the cloud. The Architecture As an example of how complex a virtualized architecture can be, and all the points we have to understand to manage Elasticsearch on a virtual environment, we can take a brief look into VMware's vSphere architecture. VMware vSphere is used to transform entire datacenters into a single cloud computer infrastructure, virtualizing and aggregating the main physical hardware resource across multiple systems and providing virtual resources to the datacenter. VMware vSphere consists of multiple component layers such as: Although the architecture is complex, no matter which virtualization solution we use, we will have tools that makes it very easy to manage entire datacenter or clusters. Those tools can help us to easily allocate storage and networking to the physical nodes, parcel out resource allocation (CPU, memory, disk and network bandwidth) as needed, monitor datacenter status, and more. The tools will allow us to configure and setup Elasticsearch in a virtual environment exactly as required depending on our needs. Regardless, we need to take care around some issues that can crop up with CPU, memory and disk utilization. Handling Resources There are various ways to achieve the goal of running Elasticsearch in a virtualized environment. Each platform and solution, whether is cloud-based or not, has his own complexity and difficulty for configuring and running. Handling resources is the key area for achieving success. CPU Every virtualization solution has limits regarding CPU usage. A physical processor core can support up to 32 virtual CPUs (vCPU) in both vSphere 6 and Azure, and 36 vCPU in Amazon EC2. As we increase CPU allocation on cloud providers, we will increase the cost for each instance. Elasticsearch uses Java, so we will need to handle a Java Virtual Machine (JVM) within our virtual environment. A good approach for JVM's is to have a minimum of two CPU's, one to handle garbage collection and JVM administration, and the other to handle the application processing. A good way to handle CPU usage is to monitor CPU utilization inside the VM using . If Elasticsearch is using a lot of  CPU resources inside the VM, it may be worth considering increasing the number of available vCPUs. Memory As well as CPU limits, there are limits for the amou","locales":"","title":"Virtualization and Elasticsearch: Best Practices"}
{"index":{}}
{"author":"Shaunak Kashyap","category":"Engineering","publish_date":"2015-05-07T00:00:00.000Z","url":"/blog/kibana-4-video-tutorials-part-4","seo_title":"","content":" For our next instalment of Kibana 4 video series, we bring you a tutorial on how to embed Kibana 4 visualizations into your webpage.  This seemingly daunting proposition is easier than ever with Kibana 4! If you have suggestions for future videos or would like to discuss any other Kibana topic, visit our new Kibana discussion category on ! ","locales":"","title":"Kibana 4 Video Tutorials, Part 4"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2015-05-04T00:00:00.000Z","url":"/blog/2015-05-01-where-in-the-world-is-elastic","seo_title":"","content":" Welcome to  Check out our shows coming up this week featuring tons of cool content provided by our own Elastic experts. Read on to find out more... Upcoming Events May 4-5: - Come say hi at our booth and look out for the 5 minute demo session by . May 6-9: : Orem, Utah - We have four talks at this event, as well as a table so stop by to meet us! May 5: : Riga, Latvia - Ruslan Zavacky will be giving a presentation on all things Elasticsearch: \"You know, for Search\".  to save your seat! May 6-7: : London, UK - Our own Hadoop expert Costin Leau will be speaking about , Thursday, May 7, 10:55 a.m. - 11:35 a.m. We also have a booth there, so come say hi at booth number 408 and grab one of our brand new stickers! May 6: Comperio Breakfast: Oslo, Norway - This event is hosted by our partner firm Comperio and will take place from 8:30 a.m. to 10:30 a.m.. Our Solutions Architect   will be giving an introduction to the ELK stack. Register for free. May 9: : Istanbul, Turkey - will give a talk on , 11:30 a.m. Upcoming Meetups May 7: May 6: May 6: May 7: That's it for this week. Stay tuned for Elastic happenings next week - there's much to come!  - The Elastic Team P.S.:   if you're interested in hosting a meetup or are giving a talk about Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic: Elastic{ON}15 & Qcon London"}
{"index":{}}
{"author":"Leslie Hawthorn","category":"","publish_date":"2015-04-29T00:00:00.000Z","url":"/blog/2015-04-29-this-week-in-elastic","seo_title":"","content":" Welcome to ! In this weekly round up post, we bring you news on all aspects of the ELK Stack, including updates on Elasticsearch, Logstash and Kibana development, tips and tricks and upcoming events. Awesome: Maptimize demo, attributes last 1M tweets around the world. — Christopher Shields (@Cshields2015) Elastic News Useful blog posts, how tos, tutorials and articles from the wide world of all the ELK stack ecosystem. Just released: for Hadoop 2.1 Beta 4 with new features, improvements, and more — elastic (@elastic) Just released a new version of DSL, a high-level client. Lot of improvements and new features — Honza Král (@HonzaKral) Slides and Videos How to sync transformed data from to using Transporter — DigitalOcean (@digitalocean) Where to find Us We'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking about the ELK stack or hosting a meetup, Algieria will take the stage at Voxxed Days Algiers to present An Introduction to the ELK Stack. This first ever conference takes place May 23, and Samir's talk is on for 2:00 PM. Australia The Melbourne Search Users Group will meet up on May 19 for a talk from Movideo: Migrating from Hibernate Search to Elasticsearch. to save your seat. We're pretty sure our very own will be joining you, too! Austria The Vienna Elasticsearch Meetup will convene on May 12 to hear from on Elasticsearch Under the Hood. to save your seat. Canada If you're attending the upcoming Open Stack Summit in Vancouver, make sure to catch the session. OpenStack Summit runs May 18-22. Czech Republic Our very own will be on hand for Q&A on Elasticsearch in Prague on May 7. You can RSVP for this talk . Denmark France The Elasticsearch France Meetup returns to Paris on May 26. so you know when we open up RSVPs! Germany Israel We've got dueling meetups in Tel-Aviv Yafo on May 26: Latvia Ruslan Zavacky will present on all things Elasticsearch at LatCraft's NoSQL and Data event on May 5. to save your seat and get introduce to Elasticsearch! Norway Our partner firm, Comperio Search, will host a breakfast learning session on all things Elasticsearch on May 6. You'll hear from Elastic's Solutions Architect amongst other speakers. Attendance is free of charge, but . Portugal The Merge Lisbon Meetup Group will get together on May 14 to talk about the ELK stack. to hear from our very own Logstash engineering team member . South Africa Elasticsearch Jo'burg meetup 29 April at Elasticsearch is not just for search. Come explore with us! — Jurgens du Toit (@jrgns) Spain Turkey Voxxed Days Istanbul is coming up on May 9. Join to learn all about Advanced Search for Your Legacy Application Using Elasticsearch. David at 11:30 AM. United Kingdom We just announced the meetup around London with our expert ! Wed, May 6, 8pm. — Livia Froelicher (@LivFroe) United States - EastThe Detroit Python User Group will convene, , April 29 to talk Python and Elasticsearch.  to attend.The DevOps and Automation New Jersey Meetup will get together on May 12 to hear from to save your place.The Detroit Elastic User Group will hold its inaugural meeting on May 27, where Peter Kim will reprise his talk from the DevOps and Automation New Jersey Meetup. to save your seat. United States - WestJoin the folks from LinkedIn for a meetup on their use of the ELK stack, , April 29, which they are kindly hosting in their Mountain View, California office. of Elastic will also be speaking about Logstash and Apache Kafka. to save your space.Attending the ? You can hear from our very own and Tyler Langlois on various topics, from starting your career in open source to all things ELK stack. Plus, we'll have a booth on the show floor so you can get all of your questions answered. Open West runs May 7-9 in Orem, Utah.The Orange County MongoDB Users Group will get together on May 7 to talk Elasticsearch and Mon","locales":"","title":"This Week in Elastic: ICYMI: Elasticsearch 1.5.2 and 1.4.5 were released"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2015-04-27T00:00:00.000Z","url":"/blog/2015-04-27-where-in-the-world-is-elastic","seo_title":"","content":" Welcome to Europe, America and Africa are our three continents for this week.Check out all the places we are heading to!Upcoming EventsApril 28: - , our Director of Product Management will be giving a talk around April 29-30: - will be talking about , April 30, 3:30 p.m. - 4:20 p.m.Upcoming MeetupsApril 29: April 29: April 29: April 30: April 29: That's it for this week. Stay tuned for Elastic happenings next week - there's much to come! - The Elastic TeamP.S.:  if you're interested in hosting a meetup or are giving a talk about Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic: Human Language Technology Conference, VA"}
{"index":{}}
{"author":"Robyn Bergeron","category":"Kurrently in Kibana","publish_date":"2015-05-08T00:00:00.000Z","url":"/blog/kurrently-kibana-2015-05-08","seo_title":"","content":" Welcome to , your source for keeping up with the Kibana project. With information on project releases, work in progress, and learning resources, we're betting you'll need a to add up all the things you'll learn each week! Exotic and stealth paramiko based SSH bruteforce from 94.79.33.21/30 in Russia. Visualization done in Kibana 4: — Eric Leblond (@Regiteric) Kibana 4.1: Progress, it's progressing! We're making progress on 4.1. There's some pretty and we want to make sure they get the attention and proper review they so rightfully deserve. Here's a couple highlights that have gotten some of that attention this week: Pinned Filters Pinned filters allow you to specify a filter as one of your BFFs. A pinned filter is always there for you, no matter which application you switch to next. From Dashboard back to Discover, they are there, by your side (in your filter bar), filtering your searches. But, unlike your BFF who is, frankly, starting to get a bit irritating and should probably make a few more friends and stop smothering you, pinned filters go away with the click of an icon: Automatic Precision on Maps We've always allowed you to configure your geohash_grid precision, but now Kibana is taking it back. I mean, if you let it. (Please, it'll be worth it!) As you zoom in and out on a map Kibana will change the precision automagically for you. In combination with the recently merged pull to only draw that which can be seen, this really jazzes up interactions with maps. A Fun Review of the Kibana 4 Client Side Code Gill Barr took the time to really dig into the Kibana 4 code and give us a kind review. And Many Other Things! 4.1 enables display of images and URLs inline. Love it! — Robin Moffatt (@rmoff) See You Next Week! As you may have seen, we're now publishing weekly updates on Elasticsearch and Apache Lucene, Logstash and Kibana. We'll bring you the newest, stuff in next week's . ","locales":"","title":"Kurrently in Kibana: Features of the 4.1 Release"}
{"index":{}}
{"author":"Leslie Hawthorn","category":"","publish_date":"2015-05-06T00:00:00.000Z","url":"/blog/2015-05-06-this-week-in-elastic","seo_title":"","content":" Welcome to ! In this weekly round up post, we bring you news on all aspects of the ELK Stack, including updates on Elasticsearch, Logstash and Kibana development, tips and tricks and upcoming events. Join us tomorrow as we investigate Scaling w/ & in a .NET environment — Jamie Turner (@PCAJamie) Elastic News Useful blog posts, how tos, tutorials and articles from the wide world of all the ELK stack ecosystem. Cool! 's Polysearch2 is a text-mining system for biomedical research — Zachary Tong (@ZacharyTong) Slides and Videos Hacking on for Philly's new transit system – Indego Bike Share — jamestyack (@jamestyack) Where to find Us We'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking about the ELK stack or hosting a meetup, Algieria will take the stage at Voxxed Days Algiers to present An Introduction to the ELK Stack. This first ever conference takes place May 23, and Samir's talk is on for 2:00 PM. Australia The Melbourne Search Users Group will meet up on May 19 for a talk from Movideo: Migrating from Hibernate Search to Elasticsearch. to save your seat. We're pretty sure our very own will be joining you, too! Austria The Vienna Elasticsearch Meetup will convene on May 12 to hear from on Elasticsearch Under the Hood. to save your seat. Canada If you're attending the upcoming Open Stack Summit in Vancouver, make sure to catch the session. OpenStack Summit runs May 18-22. Czech Republic Our very own will be on hand for Q&A on Elasticsearch in Prague on May 7. You can RSVP for this talk . Denmark France The Elasticsearch France Meetup returns to Paris on May 26. so you know when we open up RSVPs! During 2015, will show us how to add to legacy applications in a blink ! — RivieraDEV (@RivieraDEV) Germany Israel We've got dueling meetups in Tel-Aviv Yafo on May 26: The Netherlands Johan Guldmyr will host a talk on dCache and the ELK stack at the at 2:20 PM on May 19. The Workshop runs May 18-20 in Amsterdam. Poland will take the stage at 6:30 PM on May 26 to talk Beyond the Basics with Elasticsearch at the . The event runs May 25-26 in Warsaw. Portugal The Merge Lisbon Meetup Group will get together on May 14 to talk about the ELK stack. to hear from our very own Logstash engineering team member . Spain Join for the next Elasticsearch Barcelona Meetup on May 21, where you'll learn all about Moving to Production with Elasticsearch: Best Practices. to save your seat. Turkey Voxxed Days Istanbul is coming up on May 9. Join to learn all about Advanced Search for Your Legacy Application Using Elasticsearch. David at 11:30 AM. United Kingdom We just announced the meetup around London with our expert ! Wed, May 6, 8pm. — Livia Froelicher (@LivFroe) For folks in Cardiff, Wales, your welcome to attend our Elasticsearch Meetup hosted at DjangoCon EU. Make sure to pop by the conference to say hello to Honza Kral in the hallway track. runs May 31 - June 5. United States - EastThe DevOps and Automation New Jersey Meetup will get together on May 12 to hear from to save your place.The Elastic Chicago Meetup will convene on May 13 to discuss Performance Tuning for Elasticsearch. to save your place. For folks in Chicago interested in all things .NET, you may also want to on May 20 for an Introduction to the ELK stack.The Detroit Elastic User Group will hold its inaugural meeting on May 27, where Peter Kim will reprise his talk from the DevOps and Automation New Jersey Meetup. to save your seat. United States - WestAttending the ? You can hear from our very own and Tyler Langlois on various topics, from starting your career in open source to all things ELK stack. Plus, we'll have a booth on the show floor so you can get all of your questions answered. Open West runs May 7-9 in Orem, Utah.The Orange County MongoDB Users Group will get together on May 7 to talk Elasticsearch and MongoDB. to save your ","locales":"","title":"This Week in Elastic: Shield versions 1.2.1 and 1.1.1 released"}
{"index":{}}
{"author":"Shaunak Kashyap","category":"The Logstash Lines","publish_date":"2015-05-01T00:00:00.000Z","url":"/blog/2015-05-01-logstash-lines","seo_title":"","content":" Welcome back to  ! In these weekly posts, we'll share the latest happenings in the world of Logstash and its ecosystem. In case you missed it, we last week, found and are planning an RC4 very soon. Stay tuned! Logstash Core and Plugins From the Community We had presentations from LinkedIn's Tin Le and Elastic's Tal Levy () at a meetup last Wednesday - Talking , , and with — Kurt Hurtado (@kurtado) And for all you Docker and ELK enthusiasts out there - Automating Docker Logging: ElasticSearch, Logstash, Kibana, and Logspout — Ben Dixon (@TalkingQuickly) That's this week in . Come back next week for more Logstash news! ","locales":"","title":"The Logstash Lines: RC4 release coming soon"}
{"index":{}}
{"author":"Jay Modi","category":"Engineering","publish_date":"2015-04-29T00:00:00.000Z","url":"/blog/shield-1-2-1-and-1-1-1-released","seo_title":"Shield 1.2.1 and 1.1.1 Released","content":" Today, we are pleased to announce the bugfix releases of Shield 1.2.1 and Shield 1.1.1. For a new installation, download it :  to upgrade from prior versions of Shield, please follow the . These Shield releases contain several bug fixes, most notably a fix that ensures  works properly on clusters secured by Shield. We recommend that existing Shield users upgrade to ensure the health of their clusters. ","locales":"","title":"Shield 1.2.1 and 1.1.1 Released"}
{"index":{}}
{"author":"Costin Leau","category":"","publish_date":"2015-04-28T00:00:00.000Z","url":"/blog/elasticsearch-hadoop-2-1-0-beta4-released","seo_title":"","content":" The 4th Beta of Elasticsearch for Apache Hadoop (aka es-hadoop) has been . This release adds a plethora of new features and enhancements to the connector in various areas: Results returned as JSON Since Beta4, data read from Elasticsearch can be returned in JSON format through property (in a highly efficient manner). This is useful for serialization purposes such as saving information to the disk or sending it over the wire. And as always, the feature is available in all library integrations. Obtain document metadata Additionally, it is now possible to return the for each document, alongside the actual source information. Whether one is interested in an index type or a document version or the remaining time-to-live, this information is now made available without any extra network cost. Inclusion / Exclusion of fields On the front, it is now possible to specify what fields to be included or excluded for data about to be written to Elasticsearch. This makes it quite handy not only for doing quick transformation of the data but also specifying document metadata without storing it: # extracting the id from the field called 'uuid' es.mapping.id = uuid # specifying a parent with id '123' es.mapping.parent = <123> # combine include / exclude for complete control # include es.mapping.include = u*, foo.* # exclude es.mapping.exclude = *.description Client-node routing For clusters in restrained environments, it is now possible to use the connector through . That is, rather than accessing the cluster data nodes directly, the connector will use the client nodes instead (which do need to have the HTTP(S) port opened) and ask those to do the work on its behave. This will impact parallelism as the connector will not communicate directly with the nodes however, unless a lot of data is read/written and locality is not of importance, the performance penalty is insignificant. Spark improvements The various libraries have been upgraded and enhanced however by far the most updates were applied to the Spark integration. Spark 1.2 and 1.3 are officially supported in es-hadoop Beta4 - both Core and SQL. Unfortunately, due to some in Spark SQL, Elasticsearch Hadoop provides now two different versions - one for Spark 1.0-1.2 and another for Spark 1.3 (and hopefully higher). Core users can transparently migrate between them however those using Spark SQL need to adapt from to the newly introduced API. Speaking on Spark SQL, the DataSource API is in both styles (Spark SQL 1.2 and 1.3) so one can use the connector in a fully declarative fashion: val dataframe = sql.load(\"spark/index\", \"org.elasticsearch.spark.sql\") Further more, through the API, the connector is able to understand the Spark SQL operations applied on it and thus it is able to push down to the store through optimizing the queries made. Various enhancements were made such as introducing  (to allow metadata to be specified separately for each document, at runtime) and (to return the data in unprocessed, JSON format), out-of-the-box indexing of Scala case classes and JavaBeans and also, providing binaries not just for Scala 2.10 (the default) but also Scala 2.11 (make sure to use the 2_11 suffix). Strata London Next week, Elastic will be attending Strata London. If you're interested in Elasticsearch or the ELK stack, please drop by our booth (#408). Further more, you're cordially invited on Thursday at 10:55 to hear about by yours truly.  Also join us for the (please RSVP, seats are limited) on Wednesday, May 6th, in London at the new Elastic office. We look forward to your feedback on – you can find the binaries are available on the and the new features explained in the . As always, you can on GitHub. ","locales":"","title":"Elasticsearch Hadoop 2.1.0.Beta4 released"}
{"index":{}}
{"author":"Shaunak Kashyap","category":"The Logstash Lines","publish_date":"2015-04-24T00:00:00.000Z","url":"/blog/2015-04-24-logstash-lines","seo_title":"","content":" Welcome back to ! In these weekly posts, we'll share the latest happenings in the world of Logstash and its ecosystem. More news today! Happy to announce 1.5.0 RC3 is now available w/performance improvements, bug fixes & more — elastic (@elastic) Logstash Core Recent fixes for 1.5 RC3 include: ","locales":"","title":"The Logstash Lines: Logstash 1.5.0 RC3 released"}
{"index":{}}
{"author":"Suyog Rao","category":"Engineering","publish_date":"2015-05-07T00:00:00.000Z","url":"/blog/logstash-1-5-0-rc4-released","seo_title":"","content":" A short post to announce the 4th release candidate for Logstash. This release fixes important bugs which were reported with RC3. Head over to the for the details or RC4!Bug Fixes: FeedbackPlease RC4, try it out, and provide your feedback by opening an , contributing pull requests on GitHub or to us. ","locales":"","title":"Logstash 1.5.0 RC4 released"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2015-05-05T00:00:00.000Z","url":"/blog/2015-05-05-this-week-in-elasticsearch-and-apache-lucene","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top News although not mentioned, is the powerhouse behind our analytics cloud… w00t! — Matt Lenda (@mattTheLenda) Elasticsearch Core Apache Lucene Watch This Space , where we'll share more news on the whole ELK ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This week in Elasticsearch and Apache Lucene: Elasticsearch and the MarsCuriosity analytics cloud"}
{"index":{}}
{"author":"Robyn Bergeron","category":"Kurrently in Kibana","publish_date":"2015-05-01T00:00:00.000Z","url":"/blog/kurrently-kibana-2015-05-01","seo_title":"","content":" Welcome to , your source for keeping up with the Kibana project. With information on project releases, work in progress, and learning resources, we hope you'll find this weekly update to be pretty... ! soo useful to be able to index days/gigabytes of accesslogs from S3 in mere minutes and immediately usable in Kibana. ftw— Christian Westman (@westmaaan) Kibana 4.1: Maps, maps, and more maps! This past week the Kibana team decided to tackle the map visualization and make some progress on the outstanding 4.1 issues, as well as finding ways to just make things plain better. You can check out the larger scope of everything planned in the , but here's what we've been we've been up to recently: Heat Map is Close to Merge-able We're still cleaning up some code issues, but the functionality is largely there. We also took the opportunity to improve the tooltip positioning for all of the other maps. The tooltip on the map now works by looking for the closest point, with a margin, and throwing a tooltip on it. This means you no longer need to hover directly over the SVG, a tricky task at high precision levels. Adding Filter Support to the Map The first phase of tile map filters is under review. This allows you to draw a rectangle on the map to form a geo_bounding_box filter. Later phases will allow for drawing circles and polygons. Map filters coming in 4.1. Super useful with 's pinned filter feature. — Rashid Khan (@rashidkpc) Other Map-related Stuff Progress on Non-Map Things, Too! See You Next Week! As you may have seen, we're now publishing weekly updates on Elasticsearch and Apache Lucene, Logstash and Kibana. We'll bring you the latest and greatest in next week's . ","locales":"","title":"Kurrently in Kibana: Heat Map Visualization in 4.1"}
{"index":{}}
{"author":"Michael McCandless","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2015-04-28T00:00:00.000Z","url":"/blog/2015-04-28-this-week-in-elasticsearch-and-apache-lucene","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top NewsElasticsearch Core Apache Lucene How to sync transformed data from to using Transporter — DigitalOcean (@digitalocean) Watch This Space , where we'll share more news on the whole ELK ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This week in Elasticsearch and Apache Lucene: Elasticsearch Core and Lucene tips"}
{"index":{}}
{"author":"Daniel Palay","category":"User Stories","publish_date":"2015-04-24T00:00:00.000Z","url":"/blog/elasticon-video-of-the-week-verizon","seo_title":"","content":" Bhaskar Karambelkar is a Senior Security Data Scientist on the Security Research Team at Verizon Business, where he works on next-generation security research. Over the past three years, Bhaskar and his colleagues have scaled Elasticsearch from simple a log analytics proof of concept, to a 128-node production system capable of handling up to 20 billion documents a day. Here's Bhaskar's video and some of his reflections from his time as a speaker at Elastic{ON}. To watch Bhaskar's full Elastic{ON} talk, follow this or check out his slides below: You can share or find Bhaskar on Twitter . ","locales":"","title":"Elastic{ON} Video of the Week: Scaling Elasticsearch for Production at Verizon: 500 Billion Documents & Counting"}
{"index":{}}
{"author":"Suyog Rao","category":"Engineering","publish_date":"2015-04-23T00:00:00.000Z","url":"/blog/logstash-1-5-0-rc3-released","seo_title":"","content":" We have a new release candidate () for 1.5.0. After RC2 was released last month, we got a lot of input from the community. Among them were reports of performance regressions, which has been the central focus of this release. We also fixed important bugs so we decided to provide an update before making 1.5.0 generally available. Check out the for details. The highlights: Performance regression Users reported a drop in throughput performance when compared to 1.4.2. This was especially evident in configurations using conditionals in filters and outputs. We tweaked the code generation in these pipeline stages and made it more efficient. has the details including numbers. Shutdown issuesFixed a bug which manifested in Logstash not shutting down gracefully. The root cause for this issue was pipeline not sending the right shutdown signal to the plugins. (,  ) Java Options Added the ability to append extra JVM options while running Logstash. Setting the LS_JAVA_OPTS environment variable will add to the default options. This enhancement also provides the ability to completely replace the JVM options. For example, users may change the GC algorithm if they wish and use their own instead of the out of the box settings. (#  Grok Filter regression Fixed an issue where coercion of int and floats in Grok pattern was causing empty fields to be created (in some cases). This was a side effect of the performance fixes made previously for the Grok library. () File Input Users mentioned permission issues (with respect to sinceDB) during Logstash shutdown. Fixed this and other corruption issues while writing file position to sinceDB in the Filewatch gem. (, , ) As always, thanks to all our users for the valuable feedback and contributions. We are in the home stretch for releasing 1.5.0. Please download , try it out, and send your feedback by opening an  on GitHub or to us.  ","locales":"","title":"Logstash 1.5.0 RC3 released"}
{"index":{}}
{"author":"Leslie Hawthorn","category":"","publish_date":"2015-04-22T00:00:00.000Z","url":"/blog/2015-04-22-this-week-in-elastic","seo_title":"","content":" Welcome to ! In this weekly round up post, we bring you news on all aspects of the ELK Stack, including updates on Elasticsearch, Logstash and Kibana development, tips and tricks and upcoming events. Just analyzing some space data… no big deal — Chris Cowan (@simianhacker) Elastic News Useful blog posts, how tos, tutorials and articles from the wide world of all the ELK stack ecosystem. Hosting meetup at the office tonight. Organizer Amit covers solving problems zero downtime✔️ — Rangle.io Inc. (@rangleio) Slides and Videos Query DSL: Not Just for Wizards. Webinar on 28/4, pres. by wizard — Found by Elastic (@foundsays) Where to find Us We'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking about the ELK stack or hosting a meetup, Algiers will take the stage at Voxxed Days Algiers to present An Introduction to the ELK Stack. This first ever conference takes place May 23, and Samir's talk is on for 2:00 PM. Australia . and myself will also be at the Sydney on the 23rd, answering all your + + questions — Mark Walkom (@warkolm) Austria The Vienna Elasticsearch Meetup will convene on May 12 to hear from on Elasticsearch Under the Hood. to save your seat. Canada If you're attending the upcoming Open Stack Summit in Vancouver, make sure to catch the session. OpenStack Summit runs May 18-22. Denmark Germany Hungary The Craft Conference will be held April 22-24 in Budapest. If you're heading to the conference, make sure to catch talk . Latvia Ruslan Zavacky will present on all things Elasticsearch at LatCraft's NoSQL and Data event on May 5. to save your seat and get introduce to Elasticsearch! Norway Our partner firm, Comperio Search, will host a breakfast learning session on all things Elasticsearch on May 6. You'll hear from Elastic's Solutions Architect amongst other speakers. Attendance is free of charge, but . Portugal The Merge Lisbon Meetup Group will get together on May 14 to talk about the ELK stack. to hear from our very own Logstash engineering team member . South Africa Elasticsearch Jo'burg meetup 29 April at Elasticsearch is not just for search. Come explore with us! — Jurgens du Toit (@jrgns) Spain If you're heading to the Spring I/O Conference in Barcelona, make sure to catch  talk . The conference runs April 29-30. Turkey Voxxed Days Istanbul is coming up on May 9. Join to learn all about Advanced Search for Your Legacy Application Using Elasticsearch. David at 11:30 AM. United Kingdom We just announced the meetup around London with our expert ! Wed, May 6, 8pm. — Livia Froelicher (@LivFroe) United States - East The Human Language Technology Conference is coming up in Herndon, Virginia on April 28. If you're attending the conference, make sure to see  presentation . The Detroit Python User Group will convene on April 29 to talk Python and Elasticsearch.  to attend. United States - West The Los Angeles Elasticsearch Meetup is getting together , April 22. ! The Enterprise Search and Analytics Meetup group in Silicon Valley will get together on April 23 to talk Exploring Real-Time Aggregations and an Introduction to Elasticsearch and the ELK stack. The speakers for the evening will be two Elastic folks, , Software Engineer, and , Solutions Architect. to save your seat. The Dallas Elasticsearch Meetup has been rebooted! Join the next meetup on April 23 for an Introduction to Elasticsearch. to save your seat. Join the folks from LinkedIn for a meetup on their use of the ELK stack on April 29, which they are kindly hosting in their Mountain View, California office. of Elastic will also be speaking about Logstash and Apache Kafka. to save your space. Attending the ? You can hear from our very own and Tyler Langlois on various topics, from starting your career in open source to all things ELK stack. Plus, we'll have a booth on the show floor so","locales":"","title":"This Week in Elastic: The True Story Behind Elasticsearch Storage Requirements"}
{"index":{}}
{"author":"Clinton Gormley","category":"Engineering","publish_date":"2015-04-27T00:00:00.000Z","url":"/blog/elasticsearch-1-5-2-and-1-4-5-released","seo_title":"Elasticsearch 1.5.2 Released","content":" We would like to announce security bugfix releases of and , both based on . You can download them and read the full changes list here: For blog posts about past releases see: You can read about all of the changes that have been made in the  and  release notes, but the security issue is explained below: Directory traversal vulnerability found All Elasticsearch versions prior to 1.5.2 and 1.4.5 are vulnerable to a directory traversal attack that allows an attacker to retrieve files from the server running Elasticsearch. This vulnerability is not present in the initial installation of Elasticsearch. The vulnerability is exposed when a “site plugin\" is installed. Elastic's Marvel plugin and many community-sponsored plugins (e.g. Kopf, BigDesk, Head) are site plugins. Elastic Shield, Licensing, Cloud-AWS, Cloud-GCE, Cloud-Azure, the analysis plugins, and the river plugins are not site plugins. We have been assigned for this issue. Versions 1.5.2 and 1.4.5 have addressed this vulnerability, and we advise all users to upgrade. Users that do not want to upgrade can address the vulnerability in several ways, but these options will break any site plugin: Thanks to John Heasman of DocuSign for reporting this issue. Other notable changes Some important changes have been back-ported to v1.4.5: Please , try it out, and let us know what you think on Twitter (). You can report any problems on the . ","locales":"","title":"Elasticsearch 1.5.2 and 1.4.5 Released"}
{"index":{}}
{"author":"Robyn Bergeron","category":"Kurrently in Kibana","publish_date":"2015-04-24T00:00:00.000Z","url":"/blog/kurrently-kibana-2015-04-24","seo_title":"","content":" Welcome to , your source for keeping up with the Kibana project. With information on project releases, work in progress, and learning resources, we hope you'll find this weekly update to be pretty... ! Kibana dashboard demo with and by at — Lionel Porcheron (@lporcheron) Kibana 4.1: What's the scoop? You can check out the larger scope of everything planned in the , but here's what we've been doing in the past week or so: Separate View Options Tab We're starting to get to the point in some visualizations where you're doing less with the data, and more with your view of the data. Previously we had a section called “view options\" at the bottom of the aggregation builder. We're changing that in 4.1 and adding a new tab called “Options\" that will let you configure things like map style, dot sizes and other options that aren't directly related to the aggregation you're running. We've also moved a few buttons around in order to undo the effect this change would have on vertical space utilization. Field Formatting Updates We decided that field formatters needed a few more options, like per-field number precision. So we're breaking field configuration out into its own screen. This will have the 1-2 punch effect of both simplifying the field listing screen AND allowing us to have a more powerful interface to field configuration. It's a good thing. (Pull pending)  Not Kibana, But Is Amazing (Or, the death of timestamped indices) Rashid has been whistling and smiling half the day because of . Why you ask? This API will allow incredibly fast access to the min and the max of a field without using aggregation. Big Deal, you say? Yeah, it's TOTALLY a big deal. Kibana will have instant access to the min and max of the timestamp fields of every index. This means instead of needing to generate a list of possible indices with their corresponding time, e.g. the old logstash-YYYY.MM.DD, we'll just look up the indices containing timestamps between X and Y, because it's cheap. This could mean the death of timestamped indices. You just set your pattern in Kibana to logstash-* and we'll handle the optimization. And MOAR! So much more. See You Next Week! As you may have seen, we're now publishing weekly updates on Elasticsearch and Apache Lucene, Logstash and Kibana. We'll bring you the latest and greatest in next week's . ","locales":"","title":"Kurrently in Kibana: 4.1 Feature Preview, Part 2"}
{"index":{}}
{"author":"Tanya Bragin","category":"News","publish_date":"2015-04-23T00:00:00.000Z","url":"/blog/logstash-user-survey-results","seo_title":"","content":" In case you hadn't heard about it, we asked all of our community members to tell us more about how they use Logstash, including praise and pain points. We ran the survey for ~4 weeks and had just shy of 250 respondents! We have been poring over the results and have already learned a lot that will prove to be extremely valuable as we continue to improve Logstash. And today, we're very excited to share the results with you. In this blog post, we'll describe the survey methodology, share our summary of the results, and provide links to the raw results in case anyone is interested in doing more analysis. (And if you do, make sure to !)  All charts in this blog are based on survey data analysis and visualization using the Elastic stack. Check out the \"Raw Result Data\" section at the end of this blog for more information on how to set that up yourself. Response Highlights For those of you that can't wait to find out the juicy bits, here are a few highlights: : All percentage calculations are based of the total number of survey respondents. Since every question was multiselect, percentages do not add up to 100%. Want to read about the rest of results? Interested to see it in charts? Read on! Survey Methodology We ran this survey for about four weeks. It was promoted on the logstash-users mailing list, the Elastic company blog, the Elastic Twitter account, and at the Elastic{ON} conference. In all, we had 242 respondents to the survey, with the majority coming in after the first official blog and Tweet. The survey consisted of 17 questions, which combined multi-select and write-in answers (see survey in “preview mode\" ). Although all of the questions were optional, most respondents completed almost all of the questions and provided ample write-in feedback. The questions ranged from use case information and environment details to desired future product functionality. The survey was anonymous, unless the participants chose to share their contact information with our Developer Relations team. In addition, by taking the survey, participants agreed to share the anonymized results of the survey with the community. Result Details Without further ado, here are the questions we asked and the answers you gave us. 94% of respondents use Logstash in a log aggregation use case based on Elasticsearch. In addition, 22% also send logs to other systems, and 16% aggregate other time-series data, such as Twitter and call detail records. And the most-interesting-use-case award goes to the author of the following write-in answer: Cool! 74% of respondents are running Logstash in production, 57% in development, and 16% are just trying it out. 78% of respondents deploy Logstash on premise, 22% in AWS, 2% in Azure. Other public cloud providers include Rackspace, Google Compute Engine, and DigitalOcean. OpenStack got some mentions among private cloud technologies. Linux leads as the operating system of choice for Logstash deployments with 93% of respondents. 16% of respondents are running Logstash on Windows and 7% on Mac OS X.  Top Linux distributions in use by the survey respondents are Ubuntu (23%), CentOS (18%), RHEL (12%), and Debian (10%). The majority of respondents (55%) reported deployments with <10K events/sec. However, a significant number are running with 50K events/sec, 100K events/sec, 500 events/sec, and even >1M events/sec. Most respondents are running between 1 and 10 Logstash servers, with some outliers in the teens and beyond. A couple of users reported several hundred Logstash servers, but we were not sure if those users counted Logstash Agent deployments on endpoints. Methods for getting data into Logstash vary, but various approaches for shipping logs from endpoints (Logstash Forwarder, Logstash Agent, nxlog, log-courier, Beaver) lead when added together. These are followed by following Logstash inputs in this order: file, syslog, TCP, UDP, Log4j, Twitter, Kafka, and S3. 40% of respondents do not use queueing software to buffer","locales":"","title":"Logstash User Survey Results"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2015-04-21T00:00:00.000Z","url":"/blog/2015-04-21-this-week-in-elasticsearch-and-apache-lucene","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top NewsThe real deal on disk capacity requirements from + : . — elastic (@elastic) Elasticsearch Core Apache Lucene Watch This Space , where we'll share more news on the whole ELK ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This week in Elasticsearch and Apache Lucene: 5.1.0 is released!"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2015-04-20T00:00:00.000Z","url":"/blog/2015-04-20-where-in-the-world-is-elastic","seo_title":"","content":" Welcome to You want the answer? Just read on for the full list of events and meetup happenings this week. We hope to see you at one of these lovely places!Upcoming EventsApril 21-23:  - Say hi to  &  at the Elastic/Inovex booth for demos and more!April 21-23:  - Don't miss out on talk about , Thursday, April 23, 3:45 p.m. - 4:45 p.m.April 22-24:  - Join  for his talk on , Thursday, April 23, 11:55 a.m. - 12:40 p.m.April 22:  - Say hi to , who will be in the hallway track. Ask him about all things ELK and AWS.April 23: - Our local support engineer Mark Walkom will also be attending this event and happy to speak to you about all things ELK and Puppet.Upcoming MeetupsApril 20: April 21: April 22: April 23: April 23: April 20: April 21: April 22: April 22: That's it for this week. Stay tuned for Elastic happenings next week - there's much to come! - The Elastic TeamP.S.:  if you're interested in hosting a meetup or are giving a talk about Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic: JAX Mainz & AWS Summit Sydney"}
{"index":{}}
{"author":"Robyn Bergeron","category":"Kurrently in Kibana","publish_date":"2015-04-17T00:00:00.000Z","url":"/blog/kurrently-kibana-2015-04-17","seo_title":"","content":" Welcome to , your source for keeping up with the Kibana project. With information on project releases, work in progress, and learning resources, we hope you'll find this weekly update to be pretty... ! Working on a year in review (on IRC) blog post, getting to play with some ElasticSearch and Kibana! — matthewarkin (@matthewarkin) Kibana 4.1: What's the scoop? In the , we shared you all the glorious details about our planning for the 4.2 feature set. Since then, we've been hard at work on the 4.1 release. You can check out the larger scope of everything planned in the , but here's what we've been doing in the past week or so: Import/Export saved objects definitions. This allows you to export and share definitions of searches, visualizations and dashboards. It also lets you edit the raw JSON schema and import the object. (Pull pending) Add and sort columns from the dashboard. Previously columns and sort were tied to the saved search. If you wanted to change them you had todo so from Discover by re-saving the search with your new columns and sort. This changes allows you to change those parameters on a dashboard-by-dashboard basis, while still sharing the search between them. Field formatter pull request ready for review! A long time ask, this allows the user to apply transformation functions to the view of a field. For example, setting the precision of a number, or displaying it as bytes. Or taking a string and mapping it to a link. Phase 1 will have pre-defined functions, phase 2 will like be our first foray into a defined plugin structure and will allow users to define their own functions and views for fields. (Pull pending) Sorting on scripted fields. We've long been able to display and filter scripted fields. Now you can sort on them too. This means the only place you can't use scripted fields now is in the query bar. (Pull pending) Added an indication of current time. Super useful for folks with delayed data, or data that goes “into the future\". It draws a faint red line on the chart at “now\". This was designed in such a way that we can place multiples of these in arbitrary locations in the future, making them useful chart markers at a later time. (Pull pending) Snapshot builds. Snapshot build artifacts are now available for when you're jonesin' for the latest and greatest thing. Get them . And MOAR! So much more. In case you missed it... No joke! Kibana 4.0.2 was released on April 1, with a number of important bug fixes and small updates to make the Kibana 4 experience even better. Check out our for more information. Tutorials: You want 'em, we got 'em. After a bit of a hiatus during what we call \"The Elastic{ON} Period,\" we're now back in business with the newest episode in our WILDLY POPULAR Kibana 4 video tutorial series. In this third installment, we share with you -- giving you the tips you need to be a geographic-point-association pro! (Sorry if that title is a mouthful. Don't worry, though, you'll be awesome nonetheless.) Filed under: Pics, Or It Didn't Happen An anonymous reporter from the Phoenix, Arizona office shares that some of the Kibana team members at Elastic may have been delighted to be assisting in the \"renovations\" of some sections of the building, taking great pride in hitting stuff with hammers. So never fear, dear readers, it appears to be true: even though it seems like they must never stop, the Kibana team does get away from the desk now and then! :) See You Next Week! As you may have seen, we're now publishing weekly updates on Elasticsearch and Apache Lucene, Logstash and Kibana. We'll bring you the latest and greatest in next week's . If you're using , you should try Kibana. Amazing way to visualise the data in Elasticsearch. — Nicolas Widart (@NicolasWidart) ","locales":"","title":"Kurrently in Kibana: 4.1 Feature Preview, Part 1"}
{"index":{}}
{"author":"Peter Kim","category":"Engineering","publish_date":"2015-04-17T00:00:00.000Z","url":"/blog/elasticsearch-storage-the-true-story","seo_title":"","content":" UPDATE: The \"sequel\" to this blog post titled  was posted on September 15, 2015 which runs these tests against the more recent Elasticsearch 2.0beta1. Don't forget to read that after getting through this one! Introduction One of our responsibilities as Solutions Architects is to help prospective users of the ELK stack figure out how many and what kind of servers they'll need to buy to support their requirements. Production deployments of the ELK stack vary significantly. Some examples of use cases we've spoken to people about include: You can run a legitimate mission-critical Elasticsearch deployment with just 1 server or 200 servers. You may need the ability to ingest 1 million documents per second and/or support thousands of simultaneous search queries at sub-second latencies. Or your needs may be significantly more modest because you're just getting the website/mobile app for your startup off the ground. So in response to the question, “How much hardware will I need to run Elasticsearch?\", the answer is always, “It depends.\" For this blog post, we'll focus on one element of hardware sizing: figuring out the amount of disk required. Also, we'll be using log data as our test data set. Indexing logs, many different ways A typical log message can be anywhere between 200 bytes and 2000 bytes or more. This log message can contain various types of data: Even if the raw log message is 500 bytes, the amount of space occupied on disk (in its indexed form in Elasticsearch) may be smaller or larger depending on various factors. The best way to start making rough estimates on how much disk you'll need is to do some testing using representative data. Apparently, there's word going around that the data volume in Elasticsearch experiences significant expansion during the indexing process. While this can be true due to Elasticsearch performing text analysis at index-time, it doesn't have to be true, depending on the types of queries you expect to run and how you configure your indexing accordingly. It's certainly not an “all or nothing\" scenario – you can configure certain text fields to be analyzed and others to not be analyzed, in addition to tune other parameters which can have a significant impact on disk utilization. A common question asked with regards to disk usage is whether Elasticsearch uses compression – Elasticsearch does utilize compression but does so in a way that minimizes the impact on query latency. One thing to look forward to is that will allow some configurability in compression.  To analyze or to not_analyze As mentioned above, the textual analysis performed at index time can have a significant impact on disk space. Text analysis is a key component of full text search because it pre-processes the text to optimize the search user experience at query time. Fields can be configured to be analyzed, not be analyzed, retain both analyzed and non_analyzed versions and also be analyzed in different ways. A great introduction to the analysis process in Elasticsearch can be found in .  Are you _all in? The _all field is a field, which by default, contains values of all the fields of a document. This is extremely convenient when the user doesn't know the field(s) in which a value occurs so they can search for text without specifying a field to search against. However, there will be additional storage overhead if all of a document's fields are indexed as a part of the _all field in addition to being indexed in its own field. More information about the _all field can be found here: . Doc values One additional lever that can have a significant impact on disk usage is doc values. Doc values are a way to reduce heap memory usage, which is great news for people running applications that require memory-hungry aggregations and sorting queries. However, enabling doc values results in additional on-disk data structures to be created at index time which result in larger index fil","locales":"","title":"The true story behind Elasticsearch storage requirements"}
{"index":{}}
{"author":"Daniel Palay","category":"User Stories","publish_date":"2015-04-14T00:00:00.000Z","url":"/blog/elasticon-video-of-the-week-digitalgovsearch","seo_title":"","content":" Loren Siebert serves as the technical lead for DigitalGov Search, the U.S. General Services Administration's service that powers the search box for 1,500 government websites.  From helping the public find necessary paperwork on transactional pages like the Internal Revenue Service and the Immigration and Naturalization Service to helping nature lovers find photos of remote national parks on the Department of the Interior's website, Loren's work has helped revolutionize the way that everyday Americans are able to access the treasure troves of information contained in the web properties of the US Government. Here's Loren's video and some of his reflections from his time as a speaker at Elastic{ON}.  To watch Loren's full Elastic{ON} talk, follow this or check out his slides below: You can share this or find Loren on Twitter .  ","locales":"","title":"Elastic{ON} Video of the Week: The ELK & The Eagle: Search & Analytics for the US Government"}
{"index":{}}
{"author":"Leslie Hawthorn","category":"The Logstash Lines","publish_date":"2015-04-16T00:00:00.000Z","url":"/blog/2015-04-16-logstash-lines","seo_title":"","content":" Welcome back to ! In these weekly posts, we'll share the latest happenings in the world of Logstash and its ecosystem. Reminder: Join tomorrow (Apr. 16) as he brings you our “: 0-60 in 60” webinar — elastic (@elastic) Logstash Core nice! full house for my presentation :) — Colin Surprenant (@colinsurprenant) That's this week's line on Logstash. Stay tuned for RC3! ","locales":"","title":"The Logstash Lines:  0-60 in 60” Webinar"}
{"index":{}}
{"author":"Nikos Fertakis","category":"","publish_date":"2015-04-16T00:00:00.000Z","url":"/blog/how-skroutz-uses-elasticsearch-to-gather-insights-on-user-behaviour","seo_title":"","content":" Skroutz.gr is a Greek price comparison website that enables its users to filter through a variety of more than 7.5 million products in more than 1.5  thousand shops. The site has been in business  for almost 10 years and, in the process, has spinned off a sister site in Turkey, Alve.com. Skroutz will expand into the UK market in the near future with the Scrooge.co.uk site. Being one of the top-3 sites in Greece, and with monthly traffic of more than 12 million visits from more than 4 million unique visitors, it is only natural that we would like to learn something about the way our visitors use our platform. Which product categories or product category families (e.g. “Technology\") are the most popular? How does time of the season affect each product categories' popularity? Which are our users' most favourite products? Which are the top products of any category right now or a year ago? The most popular shops? Can we drill-down into these results to find the most popular product in a particular shop last summer? From another point of view, maybe we would like to gain some insights about how well our search engine is working. For example, which are the most popular searches on our platform and which are the most popular searches in any specific category? We could also try to identify problematic searches by examining those that return zero results, such as which zero-result query is used most often by our users? Is our search engine performing well? Do our users find what they look for in the first results page, or do they paginate endlessly? How much do they use our search filters? We could obviously go on, but at this point it should be painfully obvious that insights of this kind are of great value and can play a crucial role in the short and long-term development of our product. Certainly, all of the above didn't come to us as some kind of epiphany. Lots and lots of work had been done previously to make sense of the mountains of logs that we keep amassing. With the danger of oversimplifying, we can describe the previous approach as a set of task-specific Ruby scripts that would run in regular intervals, then process the most recent logs and persist the results inside time-stamped MongoDB documents. The results of these computations were presented to users in custom dashboards using libraries such as high charts. Much thought was also put into creating the logs that we use as input. These are text files with lines in JSON format that record the subset of the website activity that is of interest to us. Each line corresponds to a specific user action and contains information such as the type of that action (e.g. “product view\", “search\", or “browsing\"), a timestamp, the exact URI and also action-specific fields such as category or product ID, search query etc. Unfortunately, our approach had its limitations. Creating a new dashboard required a lot of  effort because many layers of the system would have to be touched.Usually, the MongoDB collections required a new schema so that query performance would be acceptable, which in turn meant altering existing scripts or creating new ones to convert the input to the required state. This process translated into time spent on designing, implementing and testing those scripts and also beefing up our MongoDB servers so they could handle the increased workload. Years of repeating the above process whenever a need for a new dashboard arose had left us with a system that was unmaintainable and of debatable value. Sure, it did the number crunching correctly, but the actual presentation of the data was subpar, with the information hidden inside huge tables and dense plots. Any requested change could only be performed by the very few developers that were familiar with the systems inner workings, and even they found the task daunting. All in all, we used to spend more time developing the platform than ana","locales":"","title":"How Skroutz uses Elasticsearch to gather insights on user behaviour"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2015-04-13T00:00:00.000Z","url":"/blog/2015-04-13-where-in-the-world-is-elastic","seo_title":"","content":" Welcome to Where are we this week? From the US to the UK, Spain, Poland, Belgium, Germany, Japan, Denmark, Israel and Australia, we are covering the world with a bunch of great events and meetups!Upcoming EventsApril 12-14: - and will be giving a workshop on  on Sunday, April 12 from 1 p.m. to 5 p.m.April 13: - Our own puppet expert, , will be attending. Make sure to say hi to him in the hallway track! April 15-16: - will give a talk on , Wednesday, April 15, 2:30 p.m. - 3:10 p.m.April 18-19: - is one of the great organizers behind this event where people come together to talk about all things software development. Say hello to him while you're there, and thank him and the other volunteer team for their great work putting the camp together!Upcoming MeetupsApril 13: April 13: April 13: (live streaming the Elastic Portland Meetup)April 14: April 15: April 15: April 16: April 13: April 13: April 14: April 13: April 15: April 14: That's it for this week. Stay tuned for Elastic happenings next week - there's much to come! - The Elastic TeamP.S.:  if you're interested in hosting a meetup or are giving a talk about Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic? Do Not Fringe Portland &Hadoop Summit Brussels"}
{"index":{}}
{"author":"Leslie Hawthorn","category":"","publish_date":"2015-04-15T00:00:00.000Z","url":"/blog/2015-04-15-this-week-in-elastic","seo_title":"","content":" Welcome to ! In this weekly round up post, we bring you news on all aspects of the ELK Stack, including updates on Elasticsearch, Logstash and Kibana development, tips and tricks and upcoming events. Looking to get started with ? Join on Apr 16 for our next live webinar: 'Logstash: 0-60 in 60' — elastic (@elastic) Elastic News Useful blog posts, how tos, tutorials and articles from the wide world of all the ELK stack ecosystem. writes about how picked search tech while building on — Rif Kiamil (@rifkiamil) Slides and Videos Where to find Us We'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking about the ELK stack or hosting a meetup, will be attending tomorrow meetup at our brand new office at Mountain View and doing a Q&A — Shay Banon (@kimchy) For folks in NYC: please join & for tonight's meetup on & .NET. RSVP: — Leslie Hawthorn (@lhawthorn) Australia Join us for the April Sydney ELK Meetup on April 22 for some pretty amazing talks on Docker and ELK plus Elasticsearch + Cassandra + Kafka + Spark in a Windows Environment. to save your seat.  . and I will be at Sydney on the 22nd. Come say hi and ask us your + on questions! :) — Mark Walkom (@warkolm) . and myself will also be at the Sydney on the 23rd, answering all your + + questions — Mark Walkom (@warkolm) Austria The Vienna Elasticsearch Meetup will convene on May 12 to hear from on Elasticsearch Under the Hood. to save your seat. Belgium The EU edition of will be held this year in Brussels April 15-16. , lead developer of Elasticsearch for Apache Hadoop, gave his presentation Real Insights, Real-time: Bridging Batch and Real-time Systems for Anomaly Detection already today, but you can still say hello to him in the hallway track. Canada Join the Elasticsearch Toronto User Group on April 21 to talk about Zero Downtime for Elasticsearch in Production. to save your seat. Denmark The Copenhagen .NET User Group will get together on May 12 to talk Serilog and Elasticsearch in .NET. to save your seat. France The Toulouse DevOps meetup will get together on April 20 to talk about Transforming Your Logs into Insights with the ELK stack. to save your seat. Germany Hungary The Craft Conference will be held April 22-24 in Budapest. If you're heading to the conference, make sure to catch talk . India Oh btw, I'm talking about and Kibana at this Saturday - — Ninad Pundalik (@ni_nad) The Madras Java User Group will be holding their Developer Day on April 21. The schedule updates are still in progress, but you'll be treated to a talk on Elasticsearch and Kibana as part of the event program. to attend the event, which is free of charge for all attendees. Israel The Java Israel User Group of Tel-Aviv Yafo will convene on April 19 to discuss Real World Data Pipeline Technologies, including Elasticsearch. The meetup is nearly full, so to save one of the few remaining seats. Portugal The Merge Lisbon Meetup Group will get together on May 14 to talk about the ELK stack. to hear from our very own Logstash engineering team member . South Africa Elasticsearch Jo'burg meetup 29 April at Elasticsearch is not just for search. Come explore with us! — Jurgens du Toit (@jrgns) Spain If you're heading to the Spring I/O Conference in Barcelona, make sure to catch  talk . The conference runs April 29-30. Thailand First Elasticsearch Meetup in Thailand on Wednesday, April 22, 2015 by Thai Programmer Network Register here ... — UP1 (@somkiat) Turkey Voxxed Days Istanbul is coming up on May 9. Join to learn all about Advanced Search for Your Legacy Application Using Elasticsearch. David at 11:30 AM. United Kingdom We just announced the meetup around London with our expert ! Wed, May 6, 8pm. — Livia Froelicher (@LivFroe) United States - East United States - West On Thurs, join & for the 1st ever meetup in Salt Lake C","locales":"","title":"This Week in Elastic: Costin Leau discusses Elasticsearch in the Apache Hadoop ecosystem"}
{"index":{}}
{"author":"Andrew Montalenti","category":"","publish_date":"2015-04-10T00:00:00.000Z","url":"/blog/pythonic-analytics-with-elasticsearch","seo_title":"","content":" works with top media companies across the web to deliver a real-time analytics dashboard for digital storytellers. We also run an API platform for integrating analytics and content recommendations into websites like NewYorker.com and Arstechnica.com. Python is central to everything we do at the company. So, when we evaluate open source technologies, strong Python support is one of the first things we have in mind. Last year, we built a time series engine called which uses Python to its core. To make this time series engine work at massive scale, we had to take advantage of technologies outside of the Python community. Systems like Apache Storm and Apache Kafka caught our eye, but their Python support was lacking. So, we released our own open source modules like (for Storm) and (for Kafka). I am presenting our work on streamparse this year. This work allowed us to continue using Python, even while our real-time data processing backend spread load among hundreds of cores and many nodes. We maintained a design centered around immutability, reliability, and performance, with message volumes exceeding hundreds of thousands per second. Elasticsearch’s killer features for analytics We first came across Elasticsearch about three years ago, because our team was very experienced with Lucene and Solr. We tracked its development over time, but got very excited about Elasticsearch’s direction in 2014. We recently wrote an in-depth post about Lucene internals called . When we first evaluated Elasticsearch, we considered it merely as an alternative platform for full-text search and “hosted” Lucene. But, that all changed last year. Two features caught our eye that made Elasticsearch particularly interesting to Parse.ly: and . When we dug into Elasticsearch some more, we also found an extremely Python-friendly platform for document storage and analytics. Aggregations Aggregations were , but have been actively developed over the last year, and now comprise one of the most powerful query engines available. What’s more, aggregations offer one of the most flexible ways to model time series data. For Parse.ly’s use cases, it met the need to arbitrarily filter time series data, while offering ways to group ( bucket), resample ( bucket), and calculate statistics (, metrics). Though learning the aggregation query API is an exercise in , we eventually built our own wrapper for this atop the excellent Python libraries officially available through Elastic, like and . Time-based indices Time-based indices are a data management technique where you group all records together by some time bucket, such as month, day, or hour. Then, you have some job expunge records that are older than a certain . In Elasticsearch, this is so convenient because dropping an old index is a cheap operation – essentially equivalent to unlinking a few files. In time series or log data use cases, you may only care to keep a trailing 30 days or 6 months of data. Time-based indices help you accomplish this. What’s great about using Elasticsearch’s time-based indices on a Python project is that the Elastic team maintains the module, which is written in Python, has an , and an for quick management operations. Scaling Elasticsearch in production Together with Elasticsearch’s built-in replication and sharding model, Python programmers have a scalable platform atop which to build Pythonic analytics applications. However, Elasticsearch is not a silver bullet for the challenges of analytics over huge data sets. Parse.ly operates a huge Elasticsearch cluster, and our backend processes billions of data points per day to store over 4 billion documents in Elasticsearch. We had to write our own tooling for index versioning, schema management, monitoring, and cluster expansion, along with our own query layer that took advantage of the above Elasticsearch features. Thankfully, our knowledge of Lucene internals also came in handy, as we tweaked things like","locales":"","title":"Pythonic Analytics with Elasticsearch"}
{"index":{}}
{"author":"Leslie Hawthorn","category":"","publish_date":"2015-04-09T00:00:00.000Z","url":"/blog/2015-04-09-this-week-in-elastic","seo_title":"","content":" Welcome to ! In this weekly round up post, we bring you news on all aspects of the ELK Stack, including updates on Elasticsearch, Logstash and Kibana development, tips and tricks and upcoming events. Kibana 4 talk by at Elasticsearch Seoul meetup. — Igor Motov (@imotov) Elastic News Useful blog posts, how tos, tutorials and articles from the wide world of all the ELK stack ecosystem. 3 new Official Images added to Hub in March: — Docker (@docker) Slides and Videos Where to find Us We'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking about the ELK stack or hosting a meetup, First Elasticsearch Meetup in Thailand on Wednesday, April 22, 2015 by Thai Programmer Network Register here ... — UP1 (@somkiat) Australia Join us for the April Sydney ELK Meetup on April 22 for some pretty amazing talks on Docker and ELK plus Elasticsearch + Cassandra + Kafka + Spark in a Windows Environment. to save your seat.  Belgium The EU edition of will be held this year in Brussels April 15-16. You can join , lead developer of Elasticsearch for Apache Hadoop, for his presentation Real Insights, Real-time: Bridging Batch and Real-time Systems for Anomaly Detection, on .  Canada Join the Elasticsearch Toronto User Group on April 21 to talk about Zero Downtime for Elasticsearch in Production. to save your seat. Denmark The Aarhus Ruby User Group will get together on April 13 to discuss log aggregation on AWS using the ELK stack. They're also on the look out for additional speakers. You can RSVP and get in touch with the organizers to volunteer to present on their . Germany Hungary The Craft Conference will be held April 22-24 in Budapest. If you're heading to the conference, make sure to catch talk . Japan Israel The Fullstack Developers User Group of Tel Aviv-Yafo will meet on April 13 to for a workshop on Elasticsearch Basics and Dev Tools. Doors open at 4 PM. to attend. Poland Join the enterprise search meetup in Warsaw tuesday next week (14th Apr) w/ & me about all things — Alexander Reelsen (@spinscale) The Enterprise Search Warsaw Meetup will welcome and  at their April 14 meeting. Please join them to hear about speed in Elasticsearch and Elasticsearch's Rails integration. to save your seat. Spain United Kingdom United States With talking in Seoul, Korea! — Nathan Zamecnik (@zagnut) Where to Find You  If you're a regular reader, you're thinking of skipping this section. You may even be thinking to yourself, yes of course I will drop a note on Twitter when I am giving a talk on all things ELK. That's awesome, because we'd like to showcase every meetup, conference presentation and workshop on Elasticsearch, Logstash, and Kibana happening worldwide. And now, we've made it even easier for you to get support for your meetup! Head on over to  ! (And we'll still totally send you swag if you're giving a talk on anything ELKy at a conference.) Oh yeah, we're also  . If you'd like us to find you for employment purposes, just . We care more about your skill set and passion for Elasticsearch, Kibana, and Logstash than where you rest your head. Trainings If you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elastic: Kibana 4 video tutorials are back"}
{"index":{}}
{"author":"Doug Turnbull","category":"User Stories","publish_date":"2015-04-08T00:00:00.000Z","url":"/blog/introducing-taming-search","seo_title":"","content":" It's easy to hit a wall when improving the quality of search results. Search relevance ranking can feel very mystical. Leveraging the features of the search engine to return relevant search results is a challenging art. Improving relevance means crafting relevance ranking for a particular kind of content, for a specific user audience with their own expectations, expertise, and vernacular. For this reason, relevance work is unlike any other technical task. Despite the difficulty, the practice of improving relevance is extremely important. Even search developers like us can lose track of how often we use search -- how quickly it has become the *entire* user experience. As users, we don't organize and browse anymore:  we search. We look up contacts and friends, find lawnmower parts on craigslist. We search for music, emails, or movies to watch. Doctors search for the latest techniques at the bedsides of sick patients. Patent examiners search existing patents to identify prior art. Search is eating the world -- sinking its fangs into every application. Given search's increasing ubiquity, improving the quality of answers given by the search engine is a keenly felt need.To address this need, we're writing to teach the practice of search relevance. Search is creeping into so many applications and workflows. The needs of each application -- the very definition of relevance -- changes per user experience, user audience, and content. Yet the practice of improving search relevance has common ground and Taming Search teaches it to you!Taming Search bridges two areas of wisdom. On the one hand, there's academic works like that teaches you the computer science, heuristics, and natural language processing behind building a generally relevant search engine. On the other hand, there are practical books like or that provide an overview of Lucene technologies and their features, but don't delve search relevance in any great depth.To improve the search results of everyday applications, we need to bridge the two areas of wisdom. How can you apply the lessons of Information Retrieval to today's search engines? What practical search lessons aren't captured in academic literature that nobody tells you? What practices do the experts use when improving relevance? How can external, enriching resources like ontologies or machine learning technologies be brought to bear on the problem? answers these questions and more!So if you struggle with poor search results, think relevance scoring is a mystical black box, or just want to gain a deeper appreciation of the seeming magic in the search engine, then Taming Search is the book you've been searching for! Check it out, and please be part of the conversation by participating in . Happy Searching! ","locales":"","title":"Introducing Taming Search"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2015-04-06T00:00:00.000Z","url":"/blog/2015-04-06-where-in-the-world-is-elastic","seo_title":"","content":" Welcome to We're kicking off April in full swing as we attend and host events worldwide. Check out what's coming up next and we hope to see you one of these shows!Upcoming EventsApril 8-12: - Honza Kral, Pier-Hugues Pellerin, and Peter Kim will give a  on Wednesday, April 8, 9 a.m - 10:30 a.m. Don't forget to stop by our Elastic booth as well from April 9-11 to get sweet swag and talk to our developers!April 12-14: - and  are giving an on Sunday, April 12, 1 p.m. - 5 p.m. April 8-10: - Join us for some ELK stack fun with   and  :-  Wednesday, April 8, 5:55 p.m. - 6:25 p.m.- around the ELK stack, Thursday, April 9, 7:30 p.m. - 8:30-  on Friday, April 10, 2:25 p.m.-5:25 p.m.April 11: - Jun Ohtani will be speaking about  at 6 p.m.Upcoming MeetupsApril 11: April 9: April 7: April 8: That's it for this week. Stay tuned for Elastic happenings next week - there's much to come! - The Elastic TeamP.S.:  if you're interested in hosting a meetup or are giving a talk about Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic: Pycon Montreal & Devoxx Paris"}
{"index":{}}
{"author":"Nathan Zamecnik","category":"Engineering","publish_date":"2015-04-07T00:00:00.000Z","url":"/blog/kibana-4-video-tutorials-part-3","seo_title":"","content":" After taking a brief hiatus in publishing the Kibana 4 video tutorials due to , , and then decompressing from all that fun, we are starting up again! Following up on the  and of the tutorials, check out this new video on how to create tile maps in Kibana 4.  If you're more into reading than watching movies, you'll also be happy to know that the   section of Kibana 4 docs has been significantly expanded coinciding with the last week, adding reference-style guides for every visualization type. ","locales":"","title":"Kibana 4 Video Tutorials, Part 3"}
{"index":{}}
{"author":"Rashid Khan","category":"Engineering","publish_date":"2015-04-01T00:00:00.000Z","url":"/blog/kibana_4_0_2","seo_title":"","content":" Small enhancements And a few fixes ","locales":"","title":"Kibana 4.0.2 Released"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2015-04-07T00:00:00.000Z","url":"/blog/2015-04-07-this-week-in-elasticsearch-and-apache-lucene","seo_title":"","content":" Welcome to ! With this new weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. With talking in Seoul, Korea! — Nathan Zamecnik (@zagnut) Elasticsearch Core Elasticsearch Plugin Releases Apache Lucene Watch This Space , where we'll share more news on the whole ELK ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This week in Elasticsearch and Apache Lucene: Lucene 5.1.0 has a release branch"}
{"index":{}}
{"author":"Alexander Reelsen","category":"","publish_date":"2015-04-01T00:00:00.000Z","url":"/blog/2015-04-01-this-week-in-elasticsearch-and-apache-lucene","seo_title":"","content":" Welcome to ! With this new weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Top News We'll be deprecating our Rivers feature for Elasticsearch. You can read the full details, including recommendations for replacing your own Rivers plugins, in . Attending ? Come join me and to for an workshop and rebuild search! — Martijn Laarman (@Mpdreamz) Elasticsearch Core Elasticsearch Lyon meetup. 1pb/s of data collected by large hadron collided. One does not simply store 1pb/s. — Pascal Cans ッ (@pcans) Elasticsearch Plugin Releases We had several plugin release in this past week. You can find more information about the updates in  And let's not forget the Mapper Attachment plugin, also released as . You can find full details in the  for each plugin as sent to the Elasticsearch User mailing list.  Apache Lucene sessions are out! My talk on how we use at Microsoft,including some details: — Pablo Castro (@pmc) Watch This Space , where we'll share more news on the whole ELK ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This week in Elasticsearch and Apache Lucene: Deprecating Rivers feature for Elasticsearch"}
{"index":{}}
{"author":"Daniel Palay","category":"User Stories","publish_date":"2015-03-31T00:00:00.000Z","url":"/blog/my-year-at-elastic","seo_title":"","content":" .textwithvideo {float:left:  width:60%:  padding-right:20px: } .videowithtext {float:right:  width:40%: } @media (max-width: 480px) { .textwithvideo {float:left:  width:100%: } .videowithtext {float:left:  width:100%: } } One year ago, I stepped into our Los Altos office with a metric ton worth of butterflies in my stomach. Here I was an outsider, with only of six years of political and non-profit work to my name, walking into one of the fastest growing startups in the world. Sure, I'd been around this industry for vast majority of my cognitive life. I grew up in Mountain View, my computer scientist dad worked at Silicon Graphics, then a few startups, and finally at Google. I tried to take after my dad - I even joined Computer Club in Seventh Grade (I'm pretty sure I failed at making a video game in Basic and that was the end of that). From that point forward, I couldn't get into computers or technical tools - instead I always looked to others for tech help: my dad, my three computer scientist roommates, and now my father- and brother-in-law. So what led me to join Elastic (neé Elasticsearch) on March 31, 2014?  It would be easy to say the people - I mean how could I argue with the personalities, compassion and backstories of the colleagues I have here. There are doctors (medical and academic), published authors, well-known computer programmers and developer relations advocates, legends in sales and operations, and heck, there is even that wicked awesome bald guy who only wears black v-neck t-shirts, Levi's jeans, and Nike shoes. But there were people (maybe not as awesome) in my political and non-profit past. I could say the technology - which is pretty killer. I mean, just check out all the info on our (and while you're there, download the newest versions of everything available). But if you remember, I'm not that strong of a technologist. So why was I here? I could honestly talk about our community-driven success stories until my fingers fell off and I went blue in the face. Each of them is unique and wonderful in their own right, so stay tuned to our blog as we feature more of these stories moving forward and share how Elastic technology is helping play a part along the way. But for now, here is just a quick sample of what I'm talking about: But really, it's the stories that fall under the overarching theme of making society a better place that really get me. There are the nonprofits that use our software ( and GuideStar come to mind):  there are the universities - both domestic (including the University of Georgia system) and foreign (KU Leuven for instance) making intranet search easier for their students:  and there are even hospitals, including Mayo Clinic and Oklahoma's Integris Health, making sure that their patient care is faster and more efficient. So a year in, why am I here? I've gotten a little more technical (I know how to close an HTML bracket and even know how to poke around Kibana 3 and 4), but more to the point, given how our new office is in Mountain View (actually across the street from where I lived growing up), I'm feeling a little nostalgic, so here it goes: Here's to many more years. ","locales":"","title":"My Year at Elastic"}
{"index":{}}
{"author":"Shay Banon","category":"Engineering","publish_date":"2015-03-31T00:00:00.000Z","url":"/blog/deprecating-rivers","seo_title":"","content":" When I added rivers way back in time in the early days of Elasticsearch, the idea was somewhat novel. One of the first tasks that users do when using Elasticsearch is to get data into Elasticsearch to make it searchable, so why not allow our community to write plugins that can be installed directly on an Elasticsearch cluster to pull data into Elasticsearch automatically.The first few rivers implementations were quite successful and very helpful. The CouchDB river was immensely simple and popular (thanks to CouchDB changes API). Others were popular as well, for example, the RabbitMQ one. They did start to slowly show the problematic nature of rivers as well.What was the problem we were witnessing? Cluster stability. You see, by their nature, rivers deal with external systems, and those external systems require external libraries to work with. Those are great to use, but they come with an overhead. Part of it is built in overhead, things like additional memory usage, more sockets, file descriptors and so on. Others, sadly, are bugs.Part of our efforts in the past couple of years has been to improve Elasticsearch resiliency, and we kept seeing, time and time again, that rivers are a big cause for cluster instability, due to their inherent notion of working with external systems and external libraries. When Found joined us a couple of months ago, we found that they see the same thing, with rivers plugins causing most of the cluster instabilities across the thousands of clusters under management.We knew it for some time, and in the spirit of helping users build more resilient systems, we decided to deprecate rivers and ask users to focus on getting data to Elasticsearch from \"outside\" the cluster. Rivers are deprecated from 1.5 moving forward. We will probably keep the infrastructure around in 2.0, and only remove them at a later version, to ease the migration.The ease of use in getting data into Elasticsearch is still important though, so where should you go from here?For more than a year, we've had official client libraries for Elasticsearch in most programming languages. It means that hooking into your application and getting data through an existing codebase should be relatively simple. This technique also allows to easily munge the data before it gets to Elasticsearch. A common example is an application that already used an ORM to map the domain model to a database, and hooking and indexing the domain model back to Elasticsearch tends to be simple to implement.Also, Logstash, or similar tools, can be used to ship data into Elasticsearch. For example, some of the rivers Elasticsearch came with are now implemented as Logstash plugins (like the ) in the forthcoming Logstash 1.5.I love the community and work that has gone into a vibrant set of river plugins, and the decision to deprecate rivers was not taken lightly. What should you do if you are a river plugin author?Since rivers are relatively self-sufficient, the code can be extracted into a common library that can be used to get data into Elasticsearch. Then, this library can be used in various different places. A simple \"main class\" can be written to allow to execute it as a standalone process.Another option is to move the plugin to be a Logstash input. Logstash inputs are very simple to write, and in 1.5 the Logstash team has of writing, maintaining, and discovering plugins super simple.It is a hard decision to take something away, especially with all the effort that has gone into writing those river plugins by the wonderful authors. I deeply apologize for it, and we would love to help out with any questions and ideas on how to move forward adapting them. The issue for it is . ","locales":"","title":"Deprecating Rivers"}
{"index":{}}
{"author":"Michael McCandless","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2015-04-14T00:00:00.000Z","url":"/blog/2015-04-14-this-week-in-elasticsearch-and-apache-lucene","seo_title":"","content":" Welcome to ! With this weekly series, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. Found by Elastic users, we have made version 1.5.1 available for provisioning. — Found by Elastic (@foundsays) Elasticsearch Core Live from , shows how uses & for analytics — elastic (@elastic) Apache Lucene Are you wondering what exactly an Apache Lucene codec is? Read all about it here: — elastic (@elastic) Watch This Space , where we'll share more news on the whole ELK ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This week in Elasticsearch and Apache Lucene: Lucene 5.1.0 RC2 vote has passed"}
{"index":{}}
{"author":"Clinton Gormley","category":"Engineering","publish_date":"2015-04-09T00:00:00.000Z","url":"/blog/elasticsearch-1-5-1-released","seo_title":"Elasticsearch 1.5.1 Released","content":" Today, we are pleased to announce the bugfix release of , based on . This is the latest stable version of Elasticsearch. You can . This release contains an important bug fix which affects the speed of allocating a shard to a new node. The first phase of shard recovery copies all the segment files from the source node to the target node. During this phase, any index, update, or delete requests are recorded in the transaction log, to be replayed on the target node at the end of recovery. If the shard is very large, this transaction log can accumulate many events which need to be replayed.  Previously, merges of new segments were disabled on the target node during recovery. A large transaction log would result in an explosion of small new segments, severely impacting the speed of recovery. Issue changes the behaviour to enable merges on the target shard during recovery. Other notable bug fixes include: Finally, in case you haven't seen it, . Please download , try it out, and let us know what you think on Twitter (). You can report any problems on the . ","locales":"","title":"Elasticsearch 1.5.1 Released"}
{"index":{}}
{"author":"Daniel Palay","category":"User Stories","publish_date":"2015-04-09T00:00:00.000Z","url":"/blog/elasticon-video-of-the-week-microsoft","seo_title":"","content":" Pablo Castro has been with Microsoft for 13 years and has envisioned, pitched, coded and built teams for projects like Azure Search, SQL Server, and the .NET Framework. We had the great pleasure of having Pablo speak at our first inaugural Elastic{ON} about how Microsoft is using Elasticsearch across several use cases ranging from powering 24,000 search requests per second on MSN.com, to indexing and analyzing all social signals within Microsoft Dynamics CRM, and offering search capabilities within Microsoft Azure. Here's Pablo's video and some of his reflections from Elastic{ON}.  To watch Pablo's full Elastic{ON} talk, or check out his slides below: You can share this or find on Twitter. ","locales":"","title":"Elastic{On} Video of the Week: Powering Real-Time Search at Microsoft"}
{"index":{}}
{"author":"Michael McCandless","category":"Engineering","publish_date":"2015-04-08T00:00:00.000Z","url":"/blog/what-is-an-apache-lucene-codec","seo_title":"","content":" You've likely heard that uses something called a codec to read and write index files. What does that really mean? The codec is a concrete instance of the abstract   class. Each codec has a unique string name, such as “Lucene410\", and implements methods to return a separate class for each part of Lucene's index. The codec's name is registered with Java's so you can easily get the instance at any time from just its name. Whenever Lucene needs to access the index, it does so entirely through the codec APIs. This is a vital core abstraction: it isolates all the other complex logic of searching and indexing from the low-level details of how data structures are stored on disk and in RAM. This was a big step forward for Lucene because it presents a much lower barrier to research and innovation in the index file formats than before when the bit-level encoding details were scattered throughout the code base. Each format exposed by the codec in turn provides reading APIs, used at search-time, and writing APIs, used during indexing. The codec is set per segment and every segment is free to use a different codec, though that's uncommon.  A codec defines these 9 formats: Testing a Codec When you create a codec you don't have to implement all nine of these formats! Rather, you would typically use pre-existing formats for all parts except the one you are testing, and even for that one, you would start with source code from an existing implementation and tweak from there. Often it's information retrieval researchers or Lucene developers who experiment with new codecs, using the pluggable infrastructure to try out new ideas, test performance tradeoffs, etc. Speaking of testing, by default Lucene's test framework randomly picks a codec and formats from what's available in the classpath. This is a very powerful way to ferret out any bugs in our default and experimental codecs since . You can use this for your own codecs too. Perhaps you think you've fully debugged your shiny new high performance postings format and you're ready to submit a patch to Lucene, but before you do that, try running all Lucene's tests with your new format. This is easy: run a top-level . You'll also have to ensure your codec's class files are on ant's classpath. If you see any exotic test failures that go away when you stop using your codec, roll your sleeves up and start debugging! Experimental Codecs and Backwards Compatibility For each Lucene release, the default codec (e.g. Lucene50 for the 5.0.0 release) is the only one that is guaranteed to have backwards compatibility through the next major Lucene version. All other codecs and formats are experimental, meaning their format is free to change in incompatible and even undocumented ways on every release. Not only are they free to change, but they frequently do! Lucene uses the codec API to implement backwards compatibility, by keeping all codecs for reading (but not writing!) old indices in the module. If you look in that module you'll see a number of codecs to handle reading each of the major format changes that took place during Lucene's 4.x development. Because experimental formats are inherently unstable, they are not exposed in Elasticsearch, and are instead used by Lucene developers to experiment with new ideas. One particularly exotic experimental codec is , . It encodes absolutely everything in plain text files that you can browse with any text editor. This is a powerful way to learn what Lucene actually stores in each part of the index: just index a few documents and then go open your index in any text editor! However please do not attempt to use this for anything but the most trivial test indices: it is obviously very space and time consuming, and uses inefficient yet approachable implementations. and store doc values and postings entirely in heap, uncompressed as flat native java arrays. These formats are exceptionally wasteful of RAM! also stores all doc values in heap, ","locales":"","title":"What is an Apache Lucene Codec?"}
{"index":{}}
{"author":"Aaron Mildenstein","category":"","publish_date":"2015-04-03T00:00:00.000Z","url":"/blog/2015-04-03-this-week-in-logstash","seo_title":"","content":" Welcome back to This Week in Logstash! In these posts, we'll share the latest happenings in the world of Logstash and its ecosystem. How To Use and To Centralize Logs in — Manuel de la Peña (@mdelapenya)    Logstash Core Our team is working to put the final touches on Logstash 1.5. Two weeks ago, we released a candidate for 1.5.0 -- thanks to many users who provided feedback and reported issues. We identified and are working to resolve them. I'd like to highlight these issues: Logstash and Kibana live demo at — Arno ℇrpenbeck (@javacgn)    Logstash Ecosystem Testing out pulling oracle db stats and a dashboard of active sessions and current waits. — krisrice (@krisrice) A in the wild. — ChristophB (@dalatangi) ","locales":"","title":"This week in Logstash: Final touches on Logstash 1.5"}
{"index":{}}
{"author":"Leslie Hawthorn","category":"","publish_date":"2015-04-02T00:00:00.000Z","url":"/blog/2015-04-02-this-week-in-elastic","seo_title":"","content":" Welcome to ! In this weekly round up post, we bring you news on all aspects of the ELK Stack, including updates on Elasticsearch, Logstash and Kibana development, tips and tricks and upcoming events. Elastic News Useful blog posts, how tos, tutorials and articles from the wide world of all the ELK stack ecosystem. Found customers, you may now provision 1.5.0. Inner hits, shadow replica, resiliency enhancements! — Found by Elastic (@foundsays) We've revamped the full text search for the Django docs based on , including browser search! — Jannis Leidel (@jezdez) Slides and Videos Where to find Us We'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking about the ELK stack or hosting a meetup, That's adorable! “While I was cleaning the flat today I took this photo of my ELK cluster chilling! \" — Hobbsee (@SarahKowalik) Australia Belgium The EU edition of will be held this year in Brussels April 15-16. You can join , lead developer of Elasticsearch for Apache Hadoop, for his presentation Real Insights, Real-time: Bridging Batch and Real-time Systems for Anomaly Detection, on .  Canada The Elastic team is proud to sponsor , and , , ,  and  look forward to seeing you at our booth on the show floor. We'll be sharing all things ELK Stack for Pythonistas, plus telling you all about how  can make life better for all you folks looking to integrate Elasticsearch as a Service in your Python applications.  Even better, Honza, PH and Peter will bring you a 1.5 hour tutorial on , including an introduction to our Python language clients. The tutorial takes place on April 8 from 9 - 10:30 AM, and PyCon runs April 8-16, including conference sessions and sprints, in Montreal. In Toronto instead of Montreal? We've got you covered! Join the Elasticsearch Toronto User Group on April 21 to talk about Zero Downtime for Elasticsearch in Production. to save your seat. Denmark The Aarhus Ruby User Group will get together on April 13 to discuss log aggregation on AWS using the ELK stack. They're also on the look out for additional speakers. You can RSVP and get in touch with the organizers to volunteer to present on their . France Germany Hungary The Craft Conference will be held April 22-24 in Budapest. If you're heading to the conference, make sure to catch talk . Japan Korea The Seoul community will hold their next Study Session on April 7, with presentations from and from Elastic. Check out the on the event from Jong Min Kim, the Seoul Study Session organizer. Jong Min's post includes details on registering for the Study Session. India The TechNxt user group in Pune will meet on April 4 to talk Application Containers and Measuring Systems Performance. You guessed it, the systems performance talk will feature details on the ELK stack. to attend. Israel The Fullstack Developers User Group of Tel Aviv-Yafo will meet on April 13 to for a workshop on Elasticsearch Basics and Dev Tools. Doors open at 4 PM. to attend. Poland The Enterprise Search Warsaw Meetup will welcome and  at their April 14 meeting. Please join them to hear about speed in Elasticsearch and Elasticsearch's Rails integration. to save your seat. Spain Sweden The first ever Elastic Meetup in Göteborg will convene on April 9 at 5 PM to discuss getting started with Elasticsearch and the ELK Stack.   to save your seat. United Kingdom United States Attending ? Come join me and to for an workshop and rebuild search! — Martijn Laarman (@Mpdreamz) Where to Find You  If you're a regular reader, you're thinking of skipping this section. You may even be thinking to yourself, yes of course I will drop a note on Twitter when I am giving a talk on all things ELK. That's awesome, because we'd like to showcase every meetup, conference presentation and workshop on Elasticsearch, Logstash, and Kibana happening worldwide. And now, we've made it","locales":"","title":"This Week in Elastic: Release of Kibana 4.0.2 & Deprecating Rivers"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2015-03-30T00:00:00.000Z","url":"/blog/where-in-the-world-is-elastic","seo_title":"","content":" Welcome to Did you miss this post? If so, very sorry for going quiet for a while! We were very busy with , our first ever user conference. Have you checked out the videos from the conference yet? The first wave is now available! We've posted the presentation slides from every session and the first batch of videos, including talks from , , , , , , , and .Here is what's coming up next this week in events land: Upcoming MeetupsMarch 31: April 2: March 31: April 4: April 4: That's it for this week. Stay tuned for Elastic happenings next week - there's much to come! - The Elastic TeamP.S.:  if you're interested in hosting a meetup or are giving a talk about Elasticsearch, Logstash or Kibana – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elastic: Introducing Elastic{ON}15"}
{"index":{}}
{"author":"Rashid Khan","category":"","publish_date":"2015-03-27T00:00:00.000Z","url":"/blog/kurrently-kibana-2015-03-26","seo_title":"","content":" Like the rest of the Elastic engineering team, the Kibana gang gathered for nearly a week of food, fun (a.k.a. beer) and, shockingly, a bit of work too. Let's be honest, not much code got written, and not many pulls were merged, but we did get some important face to face done. Planning for Kibana 4.2 Four dot you say? What happened to Kibana 4.1? It's in progress as we speak. We've been hard at work on Kibana 4.1 since before Kibana 4.0 GA dropped. If you're interested in Kibana 4.1's key features check out the . That said, we always have our eye on tomorrow to confirm the decisions we make today, so we took the opportunity to nail down the Kibana 4.2 feature set while we were all together in Mountain View, California. So whats up with Kibana 4.2? The big stuff includes: A startup page Kibana currently waits for Elasticsearch to startup before it starts up, however it doesn't make the Kibana web server available until the Elasticsearch startup process is done. We think the better bet is to show you what we're waiting on, and why, in the browser. So we're designing a startup status page for Kibana that will be available even when Elasticsearch isn't. Document CSV export We're taking export beyond the chart! Kibana 4.2 will allow for exporting documents from Elasticsearch in a columnized CSV perfect for importing into spread sheets or munging with with command line tools. Authentication We're building a lightweight authentication framework with session support. This will allow for functionality such as logout and timeout, even when paired with HTTP basic auth in Elasticsearch. We plan to build this in an expandable fashion to allow for authenticating against custom services. In the first phase this will be limited to authentication, we plan in the future to allow for permissions on individual dashboards and other stored resources. Log event context extraction When you've finally found that one log you need, you often need to know how it got there. Often you need to see the logs around that log, and Kibana 4.2 will enable that. And oh so much more Want to see it all? Every last thing we're working on for Kibana 4.2.0? Check out the Oh, also a Zero In addition to planning 4.2, we also planned 4.0.2, our next bug fix release. While there's no major bugs, we're always looking to fix the little things. Our prime method of this is the end-of-the week party we shamelessly cribbed from the Elasticsearch team called . Every Friday we siege the living day lights out of the easily squash-able bugs, small feature requests and well scoped pull requests so they don't get lost in the noise and get the attention they deserve.And more! What else have we been up to in the last few days? All of this jazz is in the master branch right now. You'll find it all in upcoming releases! Here's a few of the highlights: Bubble charts! This allows for sizing the dots in a line chart according to the result of a metric aggregation, effectively adding another visual dimension. We also enabled the disabling of the connecting lines._timestamp in tables Until a few days ago, _timestamp was available for aggregations but not for viewing. Thats fixed now, along with a bug that could cause changes to the metaFields parameter to be ignored. ACE editor in the object editor We actually intended to have this, but it was excluded from the build. This allows for nicely formatted JSON in the stored object editor. Highlighting for multi-field mappings (e.g., .raw) In previous versions highlighting was not supported for fields that don't actually exist in _source. That's no longer the case, we make a run at highlighting everything possible now, even logstash's .raw fields.Decimals in the range aggregation In some browsers type=number was being treated as only whole numbers. We fixed that. More Next WeekAs you may have seen, we're now publishing weekly updates on Elasticsearch and Apache Lucene, Logstash and Kibana. We'll bring you the latest and","locales":"","title":"Kurrently in Kibana: Planning for Kibana 4.2"}
{"index":{}}
{"author":"Ryan Ernst","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2015-03-25T00:00:00.000Z","url":"/blog/30-mar-2015-this-week-in-elasticsearch-and-apache-lucene","seo_title":"","content":" Welcome to ! With this new weekly post, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. We're experimenting with this new format, and we welcome your feedback. Give us a to let us know what you think! Developer All Hands Meeting Last weekend, the Elasticsearch developer team met for an all hands meeting following . This was the first time since June of last year that the entire engineering team came together in one place. We're an incredibly distributed team that's doubled in size since then, so coming together to get on the same page, in the same place was, in a word, fantastic. (P.S. .) We took the time to learn about our new friends at , discuss the  and long term future, and just talk shop about the .  But we didn't stop there: we discussed our culture, our communication styles, and of course, we also got to know each other better in a way one can only do in person: over a beer or two. We even spent a night hacking together which produced some . Top News In case you missed it, we released Elasticsearch 1.5.0 on Monday! This is the latest stable version of Elasticsearch. It contains a number of important resiliency enhancements and bug fixes, and we advise all users to upgrade. This release also includes two new experimental features, inner hits and shadow replicas. Read all about the release and these new features in Clinton Gormley's . Elasticsearch: Level Up We'll share the wider world of all Elasticsearch news with you weekly on Wednesdays in our This Week in Elastic post. But we'll always bring you at least one awesome learning resource here weekly too. From our new Found family members: Designing for your , including tips and tricks for scaling up and out. Have a recommended article, blog post or video you think we should feature in this section? ! Elasticsearch CoreElasticsearch Plugin Releases Elastic and Apache Lucene Watch This Space Stay tuned to the blog, where we'll share more news on the whole ELK ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene - March 25, 2015"}
{"index":{}}
{"author":"Jilles van Gurp","category":"User Stories","publish_date":"2015-03-26T00:00:00.000Z","url":"/blog/using-elasticsearch-to-build-crm","seo_title":"","content":" At , we are creating a mobile first CRM that heavily uses Elasticsearch to do all the data heavy lifting and analytics. Mobile first in this case means the primary user experience is running in an environment that is typically not controlled by enterprise IT departments: the user's smartphone, which is the same device that sales people use to communicate with their customers and consequently the ideal place to gather data for CRM purposes.Being on the phone and close to where data is generated enables Inbot to automate CRM data gathering and relieves the user from redundant and tedious data entry tasks. As data enters our system, contextual reports about the sales activity adapt to new incoming data right away. This seamless experience effectively turns what used to be a monthly or quarterly chore for sales representatives into actionable insights generated straight from their sales activity in real-time.Why Elasticsearch?Elasticsearch is a perfect fit for our use case since it provides real-time analytics through aggregations. Aggregations are a game changing feature in our industry and an obviously useful tool for the type of reports that are needed in a CRM product. Few other databases provide this feature and most products that do tend to be specialized data mining products or specialist tools such as Apache Hadoop that are used by data scientists. Elasticsearch aggregations make it possible to get real-time answers to complex questions such as how many of my leads converted in the last quarter, what was the average time from prospect to sale, who sells the most in my team, etc. Additionally, Elasticsearch can store massive amounts of data, and provides very flexible ways of querying and ranking information.Elasticsearch as databaseOver the course of last year, we re-built the Inbot backend on top of Elasticsearch. Prior to this, we had a prototype backend based on MySQL that was struggling in a several areas. When I joined Inbot - early 2014 through acquisition of my location based search company Localstream - the plan initially was to complement the Inbot data platform with a new analytics engine based on Elasticsearch aggregations. However, in May 2014 we decided to re-build the complete backend on top of Elasticsearch and use it as a database as well. From past experience with Localstream, where we used Elasticsearch as a database, and Nokia, where we (ab)used Lucene as a key value store, I already knew that Elasticsearch could easily handle hundreds of millions of small JSON objects, even on very modest servers. At the time, using Elasticsearch as a database was still a somewhat controversial thing to do and the recommended practice was to have a separate datastore and then pump the data from there to Elasticsearch to expose it for search. The reasoning for this was that in case of data loss, you can always simply rebuild your indices from your primary store and everything will be fine. This was also before the infamous was published that brought up some resiliency related issues in Elasticsearch. The developer team has responded brilliantly to this and as of the latest 1.5.x release most of the issues have been addressed. The upcoming 2.0 release promises to deliver additional resilience related enhancements.The reasons we decided to focus on Elasticsearch as a storage layer were very simple: The renewed Inbot platform that was launched November 2014 stores documents that represent contacts, comments, people, teams, customer accounts, deals, conversions, and other business objects in the CRM domain. A typical user will have thousands of contacts, tens or even hundreds of thousands of activities, and hundreds of customers and leads that each have a deal history. All of this data is stored in Elasticsearch in a way that facilitates our reporting and querying needs. Using Elasticsearch for CRMA lot of features in the Inbot app are powered directly by Elasticsearch. A key feature of the","locales":"ko-kr","title":"Using Elasticsearch to build a CRM"}
{"index":{}}
{"author":"Leslie Hawthorn","category":"","publish_date":"2015-03-25T00:00:00.000Z","url":"/blog/2015-03-25-this-week-in-elastic","seo_title":"","content":" Welcome to ! In this weekly round up post, we bring you news on all aspects of the ELK Stack, including updates on Elasticsearch, Logstash and Kibana development, tips and tricks and upcoming events. What Do You Think? We're experimenting with a new format for our blog, publishing more granular updates on each project weekly. We'll then share a wider overview of the full ELK ecosystem each Wednesday. We'd love to hear what you think, and even better if you have a story to suggest that we share with the community. We welcome your feedback and submissions, so Elastic News Useful blog posts, how tos, tutorials and articles from the wide world of all the ELK stack ecosystem. module 0.9.3 released! Get your Puppetizing fix at — Richard pijnenburg (@Richardp82) Elasticsearch を使った位置情報検索 — クックパッド開発者ブログ (@cookpad_tech) Slides and Videos This week, we're delighted to bring you all things Elastic{ON}, our first ever user conference. You can check out all of the slides and videos we currently have online in our , but here are a few of our favorites. Where to find Us We'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking about the ELK stack or hosting a meetup, Australia  will visit the Australian Computer Society (ACS) meeting on April 8 to discussing Corralling Logs with ELK. You can  for this session, which is free of charges for ACS Members. Belgium The EU edition of will be held this year in Brussels April 15-16. You can join , lead developer of Elasticsearch for Apache Hadoop, for his presentation Real Insights, Real-time: Bridging Batch and Real-time Systems for Anomaly Detection, on .  Canada The Elastic team is proud to sponsor , and , , ,  and  look forward to seeing you at our booth on the show floor. We'll be sharing all things ELK Stack for Pythonistas, plus telling you all about how  can make life better for all you folks looking to integrate Elasticsearch as a Service in your Python applications.  Even better, Honza, PH and Peter will bring you a 1.5 hour tutorial on , including an introduction to our Python language clients. The tutorial takes place on April 8 from 9 - 10:30 AM, and PyCon runs April 8-16, including conference sessions and sprints, in Montreal. Denmark The Aarhus Ruby User Group will get together on April 13 to discuss log aggregation on AWS using the ELK stack. They're also on the look out for additional speakers. You can RSVP and get in touch with the organizers to volunteer to present on their . France Germany Japan Korea The Seoul community will hold their next Study Session on April 7, with presentations from and from Elastic. Check out the on the event from Jong Min Kim, the Seoul Study Session organizer. Jong Min's post includes details on registering for the Study Session. India The TechNxt user group in Pune will meet on April 4 to talk Application Containers and Measuring Systems Performance. You guessed it, the systems performance talk will feature details on the ELK stack. to attend. Italy  will take the stage at Code Motion Rome to discuss . David speaks at 3 PM on March 28, and the conference runs the 25-28. Spain The next Elasticsearch Barcelona Meetup will take place on April 13, covering Elasticsearch Applied for Music Monitoring and Elasticsearch: More than Just Search for your applications. to save your seat, as this meetup is filling up fast! Sweden The first ever Elastic Meetup in Göteborg will convene on April 9 at 5 PM to discuss getting started with Elasticsearch and the ELK Stack.   to save your seat. United States Ever have a need to search, analyze, or visualize your data? Join Rick Haffey to learn how can help. — LCNUG (@LCNUG) Where to Find You  If you're a regular reader, you're thinking of skipping this section. You may even be thinking to yourself, yes of course I will drop a note on ","locales":"","title":"This Week in Elastic: A new format for our blog"}
{"index":{}}
{"author":"Jay Modi","category":"News","publish_date":"2015-03-24T00:00:00.000Z","url":"/blog/shield-1-1-and-1-2-released","seo_title":"Shield 1.1 and 1.2 Released","content":" We're happy to announce Shield 1.0.2, Shield 1.1 and Shield 1.2, download it ! Shield 1.0.2 is a bugfix release, please see the for more information. Shield 1.1 and Shield 1.2 are the first feature releases since the introduction of Shield in late January. We have received important feedback from our users and the new Shield releases address some of the most requested features and improves overall performance. Without further ado, here are the highlights: elasticsearch 1.5 support Shield 1.1 and 1.2 are identical feature wise:  the only difference is the compatible versions of Elasticsearch. Shield 1.1 is the last feature release that will be compatible with Elasticsearch 1.4.2 and newer versions of 1.4.x:  Shield 1.2 requires Elasticsearch 1.5.0 or higher. Additionally, the plugin download service has been enhanced so that installing the latest version of Shield will actually download the appropriate version of Shield based on your Elasticsearch version. ldap user search Shield now supports LDAP user search, which connects to the LDAP server as a specific user with search privileges to find users and groups rather than requiring all users to have LDAP search privileges. LDAP user search provides improved flexibility, better performance when authenticating users in a complex directory structure, and is necessary to work in environments that restrict directory search operations. For more information, please see the . anonymous access Shield now supports anonymous access. Anonymous access (requests without user credentials) in Shield can be mapped to specific roles allowing for fine grained control of what actions are permitted for anonymous users. For example, anonymous searches against a specific index can be allowed while still restricting the modification of data in that index to users with proper authorization. Audit logging for anonymous access is also supported. Anonymous access is disabled by default, read for more information. dynamic ip filtering Shield now allows for IP Filtering settings to be configured dynamically via the . You can dynamically disable or enable ip filtering in addition to updating the allowed and denied hosts. This will be very helpful in expanding a locked down cluster as a new node's IP address can be added to the allowed IP addresses without the need to restart nodes in the cluster. A few examples are provided . mapping ldap users to roles Mapping ldap users to roles is now supported in addition to mapping ldap groups to roles. Mapping users to roles is helpful in environments where maintaining specific LDAP groups for elasticsearch access would cause too much overhead. An example mapping of both users and groups to roles can be found in the section. settings filtering Shield now filters out sensitive settings, such as SSL configuration and passwords, by default and provides a mechanism to specify other settings to filter. These settings will no longer appear in the output of the node settings API. Further information can be found in the section. For a complete changelist, please refer to the . upgrading Please refer to the of the Shield documentation. ","locales":"","title":"Shield 1.1 and 1.2 Released"}
{"index":{}}
{"author":"Ryan Ernst","category":"","publish_date":"2015-03-25T00:00:00.000Z","url":"/blog/25-mar-2015-this-week-in-elasticsearch-and-apache-lucene","seo_title":"","content":" Welcome to ! With this new weekly post, we're bringing you an update on all things Elasticsearch and Apache Lucene at Elastic, including the latest on commits, releases and other learning resources. We're experimenting with this new format, and we welcome your feedback. Give us a to let us know what you think! Developer All Hands Meeting Last weekend, the Elasticsearch developer team met for an all hands meeting following . This was the first time since June of last year that the entire engineering team came together in one place. We're an incredibly distributed team that's doubled in size since then, so coming together to get on the same page, in the same place was, in a word, fantastic. (P.S. .) We took the time to learn about our new friends at , discuss the  and long term future, and just talk shop about the .  But we didn't stop there: we discussed our culture, our communication styles, and of course, we also got to know each other better in a way one can only do in person: over a beer or two. We even spent a night hacking together which produced some . Top News In case you missed it, we released Elasticsearch 1.5.0 on Monday! This is the latest stable version of Elasticsearch. It contains a number of important resiliency enhancements and bug fixes, and we advise all users to upgrade. This release also includes two new experimental features, inner hits and shadow replicas. Read all about the release and these new features in Clinton Gormley's . Elasticsearch: Level Up We'll share the wider world of all Elasticsearch news with you weekly on Wednesdays in our This Week in Elastic post. But we'll always bring you at least one awesome learning resource here weekly too. From our new Found family members: Designing for your , including tips and tricks for scaling up and out. Have a recommended article, blog post or video you think we should feature in this section? ! Elasticsearch Core Elasticsearch Plugin Releases Elastic and Apache Lucene Watch This Space Stay tuned to the blog, where we'll share more news on the whole ELK ecosystem including news, learning resources and cool use cases! ","locales":"","title":"This Week in Elasticsearch and Apache Lucene: Developer All Hands meeting"}
{"index":{}}
{"author":"Clinton Gormley","category":"Engineering","publish_date":"2015-03-23T00:00:00.000Z","url":"/blog/elasticsearch-1-5-0-released","seo_title":"Elasticsearch 1.5.0 Released","content":" start body ","locales":"","title":"Elasticsearch 1.5.0 Released"}
{"index":{}}
{"author":"Shay Banon","category":"News","publish_date":"2015-03-24T00:00:00.000Z","url":"/blog/elasticon-sessions-are-online","seo_title":"","content":" With a little more than a week since our inaugural Elastic{ON} conference, I'm extremely excited to release today, which includes the presentation slides from every session and a handful of videos, including talks from , , , , the , , , , and . Elastic{ON} was all about sharing knowledge, from the sessions during it, to the Birds of Feather informal gatherings, and the hugely popular \"Ask Me Anything\" booth that was staffed during the whole conference with developers and employees. Sadly, while all the slides are available, we do not have audio for the first day main stage content due to technical problems, including our keynote. It's devastating to lose it, and we are thinking on how to fix it, all the way to kung fu movie style dubbing the videos. We hope you enjoy . I am especially excited with our opportunity to show some of the amazing use cases of our Elastic products out there, and happy we can share it with all of you today and in the coming days as we process the rest of them. ","locales":"","title":"Elastic{ON} Sessions Are Online"}
{"index":{}}
{"author":"Suyog Rao","category":"","publish_date":"2015-03-23T00:00:00.000Z","url":"/blog/2015-03-23-this-week-in-logstash","seo_title":"","content":" Welcome to our new blog series ! In these posts, we'll share the latest happenings in the world of Logstash and its ecosystem. Let's kick off the series with a special this week — The Engineering All Hands Edition. After wrapping up our 2 weeks ago, all of our engineers convened in our brand new office in Mountain View for a week of discussions and fun activities. We had a packed agenda to discuss development activities across products, engineering culture, community engagement, and unconference style break-out sessions for discussing each project in detail. We even had an engineering wide hackathon! hackathon tonight - harnessing the brainpower of 50+ engineers and building cool stuff — Kurt Hurtado (@kurtado) At the Logstash break-out session, we discussed the following things: I'd like to provide notes from few of these discussions here: Plugins ecosystem One of the major goals of Logstash 1.5 was to separate plugins to individual entities outside of the core. Plugins today exist as separate repositories in the GitHub organization. It is our hope that these changes will allow for more community involvement. To this end, we discussed the details of how we would make it easier for developers to create more plugins and contribute to existing ones. We are looking into providing per-repository commit rights to original authors and contributors. We would love for developers to submit their plugins to logstash-plugins — we discussed all the infrastructure we can provide to support this move. Things like continuous integration, auto generation of documentation, and discoverability are some of the benefits of having your plugins hosted in repository. Watch for a blog post coming soon to cover all these details. Pipeline semantics In this session, we discussed providing clean pipeline semantics for plugin developers. With plugins separated from the main core of Logstash, we can provide a clean, documented API to the different pipeline stages that is easy to understand and develop for. Our intention is to make it easy to write plugins in any language (like Java, Clojure, Scala, etc.) that runs on the JVM. We want to standardize the pipeline behavior on errors and non-recoverable conditions across the different stages. We discussed providing an abstraction layer to pipeline internals like threads, intra-stage queues, and dead-letter queues which plugin developers can safely use. We brainstormed the idea of rewriting the LogStash::Event object in Java to allow for efficient serialization across JVM languages. Testing infrastructure At Elastic, providing quality software is in the DNA of our engineers. We invest a lot of effort in testing infrastructure across our projects. We discussed how we can bolster Logstash testing to support plugins and add more integration testing. We discussed the use of Docker to bring up external services like Redis and Elasticsearch. This would make setup and teardown easy during integration testing. We discussed RSpec best practices, randomized testing, adding code coverage reports on our Jenkins runs, and so on. We are already tracking throughput performance of Logstash (across releases) using the ELK stack and we would like to expose that to our community. Long term, we would like to expose infrastructure for developers to test their patch against the performance test suite to make sure they don't introduce regressions! Whiteboarding our testing improvements:  Dev workflow In this session we discussed our development workflow — our intention is to document a lightweight process for everyone to use. We want a consistent GitHub experience across all our open source projects, so we are changing Logstash issue and pull request labeling to match Elasticsearch and Kibana labeling, with all the same colors too :). We want to make it easier to navigate through our GitHub issues and pull requests. All our work at Elastic is peer reviewed, and Pull Requests (PR) are no exc","locales":"","title":"This Week in Logstash: Notes from the Engineering All-Hands"}
{"index":{}}
{"author":"Aaron Mildenstein","category":"Engineering","publish_date":"2015-03-20T00:00:00.000Z","url":"/blog/curator-3-0-released","seo_title":"","content":" Curator 3.0 is a terrific update! It may not look like much has changed if you have been using a 2.x version, but there have been many improvements! I am pleased to present these changes to you! Why change it?Is Curator 2 working just fine for you? I'm so glad to hear that! You can, of course, continue to use Curator 2. Because I was dissatisfied with the performance of some operations, I decided to fix it. For example, the Elasticsearch API does not require you to delete indices one at a time, but Curator 2 did. I was dissatisfied with the index selection parameters. While I did have some unit testing, Curator was sorely lacking command-line integration testing. Some bugs might have been easily detected with command-line-level integration testing. All of these, and more still, led me to write Curator 3. Installation and Upgrading Install the latest version of Curator with: Curator can be upgraded to the latest version by running: : This is a major release update. Command-lines from Curator version 2 with Curator 3. With some small modifications, though, they should be good to go. Learn more about alternative methods of installation at . What's new? I am so glad you asked! Let's take a brief look at each of these. Updated API In Curator 2, the Curator API did index (or snapshot) selection in many of the methods. This required a staggering amount of command-line options. It also required fairly complex methods. Now with Curator 3, index selection and filtering is handled before any command method--close, delete, optimize, etc. --is called. The API methods expect to have the list of indices to act on sent to them as a parameter. As of Curator 3.0.1, a build_filter method has been added to assist in building index filters, as with the command-line. Learn more at . Command-line based on Click is an amazing tool to help build sophisticated command-line interfaces. Using Click also allows me to do full integration testing of the software as though I were issuing commands at a command-line. The big change for the command-line in Curator 3 is the additional sub-commands and , for index and snapshot selection, respectively. To see how this works, you can use the flag at each sub-command: curator --help curator show --help curator show indices --help Learn more in the wiki, in and . Pipelined index and snapshot selection To continue where the last section left off, the new index selection parameters can work together in ways that Curator 2 could not touch. You can now specify a line like: curator delete indices --older-than 30 --newer-than 60 --time-unit days \\ --timestring '%Y.%m.%d' --prefix logs --suffix prod \\ --exclude logs-2015.02.01-prod --exclude 2015.01.31 \\ --index logs-2015.02.01-dev This line will delete all indices older than 30 days, but newer than 60 days, with logs as a prefix and prod as a suffix, exclude the logs indicated by the patterns given, and force-include . Simply put, Curator 2 could not achieve this level of customizability. Curator 3 also comes with an option to allow you to select all indices. However, if you select the delete command with , the Kibana indices of .kibana, kibana-int, and .kibana-marvel will be auto-pruned. If you wish for Curator to delete one or more of these indices, you would have to manually include them with the flag. The flag bypasses all other filtering to add the specified index to the list to be operated on. In fact, the flag, if used by itself, will allow you to act on specified indices directly: curator delete indices --index index1 --index index2 --index index3 This allows for operations on individual indices, which previously required the use of curl or direct API calls. The best part of this change is that you can use Curator on non-time-series indices now! Curator is no longer restricted to operating on time-series indices! Learn more in the wiki, in and . Simultaneous operations (where allowed) Curator 2 would only do one action at a time. This could make some opera","locales":"","title":"Curator 3.0 Released"}
{"index":{}}
{"author":"John Boere","category":"User Stories","publish_date":"2015-03-19T00:00:00.000Z","url":"/blog/using-elk-to-keep-the-lights-on","seo_title":"","content":" Keeping the lights on is literally what we mean. We're talking about the world's power grid or more generally, electricity. Electricity, like air and water, is something that most of us take for granted. It should always be there, right? And we're OK when it is. Until it isn't, which somehow always happens at the worst time, like during a storm. How am I going to update my Facebook status now? Seriously though, as an electrical engineer it's my job to make sure we always have an abundance of reliable and high quality power. Without it the world would be a very different place. Most people don't think about it like this, but compare it to water. With shortages (droughts) or pollution, we and our food cannot prosper. It's the same with power, low quality will lead to outages and shorten the lifespan of devices. Interestingly enough without power most of us wouldn't even have access to water, as none of the pumps to deliver it would work. However, reliable power is easier said than done, and engineers have been working on improving this technology for over a 100 years now and still continue to do so. And so are we at . While we don't operate a power plant, we help our customers, utility companies, with preventative maintenance and reducing outage restoration times, among other things. Today, the [power]grid is transitioning from being 'dumb' to getting smarter. This smart grid uses technology to detect and react to local changes in usage, using computer-based remote control and automation. A large number of smart devices and sensors capture data points from the grid. Data is being collected from a variety of systems such as Supervisory Control and Data Acquisition (SCADA), Outage Management Systems (OMS), Smart Meters (AMI) just to name a few. Capturing data from this vast amount of equipment results in a lot of data points. And a lot of this data is also recorded over time intervals, allowing for historical and trend analysis. So what we have is a high volume of data that is being captured at (somewhat) high velocity. Ideally, this data would have to be analyzed in real time. But as it stands now, we have a long way to go before we get to that point. In one instance, data from smart meters doesn't work with the outage management system —from the same vendor! The root of the problem is that all this data is in its own data silos, making collaboration much more difficult, both from a technical and organizational perspective. The end result is increased outage times and lower customer service. Instead, what we need is an effective mechanism to search and analyze across all these incompatible datasets, preferably in real time. Hello ELK stack. At Cliffhanger Solutions, we index data in real time from various sources using Elasticsearch and Logstash. Sources include GPS location data from maintenance trucks or from tablets running our app, readings from smart meters and facility data from GIS (geographical information systems). To visualize, search and analyze this data we build a mapping application (called ATLAS web and mobile) in which we also embed Kibana dashboards. Think of it like Google Maps on steroids. Operators can now quickly get answers to questions like: \"Can I safely close this switch and restore power to these 1500 customers?\" or \"A storm is coming in from the South, how fast can I get my bucket trucks to the area where the storm will hit?\". As for preventative maintenance, engineers can seek answers to questions like: \"Transformers from vendor X have a higher than average MTBF (mean time between failures). Find all of them and sort them by installation date, then send them to the work order system for inspection or replacement\". While it might not sound like a big deal, this is actually pretty incredible and this wasn't possible until now without heavy investments in consultants or getting locked in with the few \"one stop shop\" large vendors that offer a 'total solution'. Anecdote: One company noti","locales":"","title":"Using ELK to Keep the Lights On"}
{"index":{}}
{"author":"Leslie Hawthorn","category":"","publish_date":"2015-03-18T00:00:00.000Z","url":"/blog/2015-03-18-this-week-in-elastic","seo_title":"","content":" Welcome to This Week in Elastic! Our company has a new name, a new logo and some new additions to our team: Welcome ! Starting next week, we'll be bringing you all the news you can use about all things ELK Stack, with more regular updates. You can still look forward to our weekly round up on Wednesdays. Hey dude, where's my commits? After an incredible week last week at , our dev team spent the rest of the week in an all hands meeting figuring out better ways for us to collaborate with the community, how to better improve our projects and enjoying each others' company. We'll return to our regularly scheduled update on all commits next week. Elastic{ON} Last week, we wrapped our first ever user conference in San Francisco. We had more than 1300 attendees, over 40 talks, standing room only Birds of a Feather sessions and a really good time! We'll be bringing you slides and videos from the conference in the next few days, but in the interim we figured you'd enjoy these photos from the event. The only thing better than great talks is to have them in an amazing location. Pier 27 provided more than a few gorgeous views to our attendees throughout the conference. During our opening reception and throughout the rest of the conference, members of our technical staff were around to answer attendee questions. Krijn Mossel and Lee Wright on deck for our Ask Me Anything sessions. During the opening keynotes, we announced our new branding and that the Found team has joined Elastic. Steven Schuurman and Shay Banon welcome the team from Found to the Elastic family. Our Birds of a Feather sessions were so popular, we ended up migrating many of them outside to accommodate more participants. Zachary Tong lead the Practical Time Series Analysis BoF Elastic News Useful blog posts, how tos and articles from the wide world of all the ELK stack ecosystem. If you're interested in seeing your tutorials featured in this section, ! ~100 humans have responded so far, more needed! MT Help us define future of , take our user survey: — Leslie Hawthorn (@lhawthorn) Blogged: Introduction to Analysing #ODI Runtime Data Through #Elasticsearch and #Kibana http://ritt.md/odi-kibana — Robin Moffatt (@rmoff) Slides and Videos Normally we try to put a few slides and videos here for you to enjoy, but this one was so good that we didn't want to distract you with anything else. Logs! Link! Zelda! DevOps! ELK! Many <3s! Where to find Us We'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking or hosting a meetup, let our Director of Developer Relations, , know! Australia will visit the Australian Computer Society (ACS) meeting on April 8 to discussing Corralling Logs with ELK. You can for this session, which is free of charges for ACS Members. France Germany Italy will take the stage at Code Motion Rome to discuss . David speaks at 3 PM on March 28, and the conference runs the 25-28. The Netherlands For folks in or near Utrecht, you can for the GOTO Night on Elasticsearch. Attendees will hear from Anne Veling and Jettro Coenradie, including a recap of the Elastic{ON} conference. The GOTO Night is scheduled for March 25 at bol.com's offices. Sweden United Kingdom The March London Elasticsearch Meetup is on for March 24, featuring speakers from Couchbase, OpenTable, and Postcode Anywhere. to save your seat. United States Where to Find You If you're a regular reader, you're thinking of skipping this section. You may even be thinking to yourself, yes of course I will drop a note on Twitter when I am giving a talk on all things ELK. That's awesome, because we'd like to showcase every meetup, conference presentation and workshop on Elasticsearch, Logstash, and Kibana happening worldwide. And now, we've made it even easier for you to get support for your meetup! Head on over to ! (And we'll still totally send you swag if you're giving a talk o","locales":"","title":"This Week in Elastic: A new name, logo, and welcome Found!"}
{"index":{}}
{"author":"Konrad Beiske","category":"","publish_date":"2015-03-17T00:00:00.000Z","url":"/blog/found-multi-tenancy","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud. There are many buzzwords that may be applied to Elasticsearch. Multi-tenancy is one. Getting started with a multi-tenant use case can be deceptively easy - there are some pitfalls that will require a little careful design. ","locales":"","title":"Discovering the Need for an Indexing Strategy in Multi-Tenant Applications"}
{"index":{}}
{"author":"Steven Schuurman","category":"News","publish_date":"2015-03-10T00:00:00.000Z","url":"/blog/elastic-you-know-for-more-than-search","seo_title":"","content":" When Shay, Uri, Simon and I founded Elasticsearch in 2012, we always knew we were signing up to provide more than \"just search\" to the market. That it was going to be successful at this scale, we could have never dreamt up.Three years, five more products, 20 million downloads, 170 employees, and over 17,000 Meetup members later, the future is looking very bright. If I just look at the we have at our first user conference, Elastic{ON}, I feel humbled. There's the Yale Department of Laboratory Medicine talking about how they use Elasticsearch to identify causes and potential cures for cancer, the U.S. Geological Survey uses the ELK Stack to supplement their seismographic data with social media data in order to better assess and react to earthquakes, FICO will show how it uses Elasticsearch to calculate more accurate credit scores, and we've even entered the final frontier, with NASA's Jet Propulsion Lab taking the mainstage on Wednesday morning to tell us how Elasticsearch monitors metrics from the Mars Curiosity rover.All of us at Elasticsearch are blown away each and every day with how developers around the world are putting our products to use. Today, we're about much more than just search. We're obsessed with data, and getting insights out of it. We want our users to query their data, drive analytics and make correlations that couldn't be extracted before, and we've built our products to easily scale along as your data volume grows. We believe we have put a suite of products in place that allows users to extract insights out of their data, that they weren't able to extract before. That alone is more rewarding to us, that many of you will ever know.Mostly driven by the fact that we are now servicing so many more use cases that span way beyond search, we have felt an increasing urge for some time now to rethink our company brand. Elasticsearch is, and will continue to be, the core engine of our product stack, but the company is building and maintaining a suite of solutions that deliver additional functionality beyond Elasticsearch. Because of this, we believe it is time to embrace a new company brand that's as versatile as our products.And on that note, I am extremely excited to be able to introduce you to our very new brand:As a growing number of people, including myself, have been referring to us as \"Elastic\" since the inception of the company anyway, we believe this is actually a very fitting brand name for our us.But there's more.As users have become more familiar with the versatility of the ELK stack, we have seen many organizations deploy multiple ELK clusters supporting as many different use cases. Whereas this is a wonderful evolutionary process, it does create challenges we haven't experienced before. We always felt that there was a good chance that this would happen – one company running many ELK clusters – and have therefore always envisioned ourselves creating a revolutionary product in the form of a centralized management infrastructure. An infrastructural layer with the objective to put operations people in control of their ELK landscape across their business.In order to accelerate this next phase of our company beyond search, we've also announced today that we acquired Elasticsearch SaaS provider Found (), whose technology will make it possible to manage multiple clusters: through the SaaS solution, and in the near future, also on premise. Shay gives more details around our teams joining forces in .I'd like to personally thank everyone that's been with us on our journey so far. It's been quite a ride, and it's far from over. For those that want to join us on our adventure, follow us on our new Twitter handle and our brand new website – we've got a lot more exciting things in store for 2015 and beyond! ","locales":"","title":"Elastic: For - You Know, More Than Search"}
{"index":{}}
{"author":"Shay Banon","category":"News","publish_date":"2015-03-10T00:00:00.000Z","url":"/blog/welcome-found","seo_title":"","content":" Since the early days of Elasticsearch, we knew search was a versatile platform that caters to many different use cases. Since its early start, and even more so since a company was founded around it, we've been investing in making this a reality. We didn't stop with just Elasticsearch. Over the past couple years, we developed language clients for many popular programming languages. We embraced and developed Kibana to bring us closer to our users when it comes to visualizing and exploring data. Logstash allows users to reach into to the vast amount of time series data they have, process it, and centralize it in Elasticsearch. Elasticsearch for Apache Hadoop allows users to integrate Elasticsearch natively with Hadoop and its vast ecosystem. As you can see, we've been busy. But, we never lost sight of one of our biggest ambitions when we started it all. See, if our belief was true and search enables so many different use cases, it's inevitable that organizations will start using our products across their entire organization, either on premise, or in the cloud. Once this adoption curve hits an organization, we wanted to provide the option to run something we called \"CloudES,\" providing \"Elasticsearch as a Service\" functionality in a single, simple product, both on premise and in the cloud. Yet, there is another reason for this \"CloudES\" product that started to manifest over the past few years. People were not just running multiple Elasticsearch clusters and looking for a way to consolidate them, they were also, at the same time, trying to solve their use case by providing our technology as a service to their entire internal organization. Logging as a Service is the prime example here, as our technology is so advanced to be the only one in the market to really be able to tackle such a problem. We realized that \"CloudES\" would be instrumental for helping companies achieve this ambitious goal as well. While we were busy, one company caught our eyes for some time now, Found ( ). For the past few years, Found has been busy building the best \"Elasticsearch as a Service\" out there. They have built a service that is extremely simple to use, with a very strong technical foundation. Moreover, they have been great educators about Elasticsearch with their Foundation community space and . When we met and started to deep dive into the technical architecture of Found, I was immediately impressed. Found has been providing Elasticsearch as a Service the right way, where each user has their own cluster. But one can't just stop there, and Found has been using Linux containers for some time now to make sure clusters can be easily be packed on beefy machines – critical when using AWS. But that's not where the technical challenges stop for such a service, and Found has addressed each and every one. The service has a scalable proxy architecture to properly direct traffic of users to their respective clusters, a smart manager that can allocate clusters and instances across a vast array of host machines, all while providing top notch security and privacy. There are many more features implemented in Found, including periodic snapshots, cluster upgrades, and support for multiple Elasticsearch versions. We all thought it was pretty impressive. Once we started to talk about our vision for such a service was when everything just clicked. Found has been thinking and experimenting with taking all that they have built, packaging it up, and allowing users to install it on premise, enabling them to provide Elasticsearch as a Service. This was just spot on when it came to our original vision around CloudES. Couple that with the fact the team is a perfect match to how we work at Elastic, and this opportunity to join forces was just too good to miss out on. I am proud to announce that Elastic has acquired Found, with the whole Found team joining us. We are now light years ahead in bringing our original vision to fruition, and you can see how this is just another good reason for our . about th","locales":"","title":"Welcome Found"}
{"index":{}}
{"author":"Morgan Goeller","category":"Engineering","publish_date":"2015-03-06T00:00:00.000Z","url":"/blog/kibana-4-video-tutorials-part-2","seo_title":"","content":" Following up on the sweetness in , we are back for more videos on how to make the most of Kibana. Of course, if you like text you can use the for Kibana 4. However, if you like videos, we've got 'em. The first video goes through Data Discovery, a new addition to Kibana 4:   Next, we dig into the nitty-gritty details of the Bar Chart:   Finally, we look at the Line Chart:   Enjoy! ","locales":"","title":"Kibana 4 Video Tutorials, Part 2"}
{"index":{}}
{"author":"Suyog Rao","category":"Engineering","publish_date":"2015-03-06T00:00:00.000Z","url":"/blog/logstash-1-5-0-rc2-released","seo_title":"","content":" We are pleased to announce a bug fix update to . Yesterday we and soon discovered a few issues which warranted a quick bug fix release. You can jump to the directly or check the for updates. Bug Fixes Elasticsearch output Logstash would not start when used with the “node” and “transport” protocol option of the elasticsearch-output plugin. The “http” protocol was not affected by this bug (). Lumberjack input When used with Lumberjack input, Logstash would not start because it was using an old version of the Lumberjack gem. We fixed the issue by publishing a new version (0.0.22) of the Lumberjack gem and updated Logstash to package it (). Again, thanks to our users for reporting these issues. We are constantly evolving our continuous integration platform and we will make necessary additions to catch these issues, so they don’t happen in the future. Please and let us know what you think! You can use our twitter handle () or head to our repository if you find any issues. ","locales":"","title":"Logstash 1.5.0 RC2 Released"}
{"index":{}}
{"author":"Jordan Sissel","category":"Engineering","publish_date":"2015-03-05T00:00:00.000Z","url":"/blog/logstash-forwarder-0-4-0-released","seo_title":"","content":" I am happy to announce that is now available! This is mostly a maintenance release, but before we talk about that, I want to first address some of the history and problems this project has had.It's been too long since the last release of this project, and for that, I am sorry. We didn't adequately focus on improvements to the forwarder. This neglect got bad, and even lead me to delete what I thought was a broken package repository only to find it was in use by many folks. Sorry about that. As humans, we sometimes make mistakes. Do things, mess up, try to do better next time!In the meantime, we want to provide a stable and happy experience with the logstash-forwarder, while we . Logstash-forwarder 0.4.0 is one such stability release, so what's new in it?Features new since the last release: Bug fixes: Note: This release works well with and has fixes that work with lumberjack input.You can see the detailed here.Please logstash-forwarder 0.4.0 and let us know what you think on . If you'd like to see what we have in store for the future of logstash-forwarder, visit our . ","locales":"","title":"Logstash-Forwarder 0.4.0 Released"}
{"index":{}}
{"author":"Suyog Rao","category":"Engineering","publish_date":"2015-03-05T00:00:00.000Z","url":"/blog/announcing-logstash-1-5-0-release-candidate","seo_title":"","content":" We are announcing the first candidate for the release of Logstash 1.5.0. You can download Logstash and review the associated . As we mentioned in the release of , the main areas of focus for 1.5.0 were , performance improvements, and integration. Thanks to all our users who reported their experiences with the beta1 release. We made a number of fixes and improvements since then:  we would like to highlight few of them here. [UPDATE] Known Issues If you are using the “node\" or “transport\" protocol with the Elasticsearch output, follow these steps to set up your output with Logstash 1.5.0 RC1 (): <code>% bin/plugin uninstall logstash-output-elasticsearch ... % bin/plugin install logstash-output-elasticsearch ... CouchDB Input We added a brand new plugin to stream events from the API of CouchDB. With this input, any future changes will automatically be streamed as well, making it easy to synchronize your CouchDB data to any target destination. Check out the reference for more information! We will be packaging CouchDB input in the Logstash release package, so it'll be ready to use as soon as you upgrade! Retries in Elasticsearch Output We made the Elasticsearch output more resilient to transient errors in Elasticsearch. Previously, partial failures from the bulk indexing functionality were not handled properly. With this fix, we added the ability to capture failed requests from Elasticsearch and retry them. Error codes like 429 (too many requests) will now be retried by default for 3 times. The number of retries and the interval between consecutive retries can be configured. () Heartbeat Input Plugin We created a new input plugin for generating heartbeat messages at periodic intervals. Use this to monitor Logstash — you can measure the latency of the pipeline using these heartbeat events, and also check for availability. For more information and configuration, check the . Improved S3 Input and Output We made a number of important fixes to the S3 input and output plugin. Among them: Windows Support We have made great improvements for running Logstash and plugin related infrastructure on Windows, which was degraded since the 1.4.2 release. Users can now install, upgrade, and remove individual plugins on Windows at any time. We also resolved issues related to initial setup, upgrade and file input plugin: Other fixes Documentation Updates We continue to enhance our reference documentation. Since beta1, we added a comprehensive guide for writing and publishing plugins. Check this to add an input plugin:  you can also find resources for developing other plugins in the same location. Update from beta1 Since releasing beta1 we made internal changes to the way we install and load plugin gems using the Bundler framework. Unfortunately with this change, we cannot upgrade plugins installed during beta1 to the RC1 version. It is strongly recommended to go with a clean install of LS 1.5.0 RC1. Moving forward, you should be able to upgrade the RC1 package to GA release without any issues. It is worth emphasizing that any Logstash configuration created in beta1 and RC1 can be moved safely to the final 1.5.0 release. Check it out! We are closing on the final few issues before we can release Logstash 1.5.0. As always, we would love your feedback on this release. Please it and let us know what you think. You can open and provide feedback on twitter (). Beyond 1.5 We already started planning beyond 1.5, and to make our efforts easier to follow, we published the with the themes of the next major release. We would love for you to help us in the release planning process by taking the ! ","locales":"","title":"Announcing Logstash 1.5.0 RC1"}
{"index":{}}
{"author":"Rashid Khan","category":"Engineering","publish_date":"2015-03-04T00:00:00.000Z","url":"/blog/kibana-4-0-1-released","seo_title":"","content":" Today we make available the first update to Kibana 4 incorporating a number of important usability and stability fixes.Grab the new release here: For more information on Kibana 4 see the original release blog post: Upgrades and Polling for ElasticsearchKibana 4.0.1 improves on the upgrade process in 2 ways in an attempt to make both upgrades to Kibana and to Elasticsearch more seamless. bug fixesIn addition to the two big items above we also fixed a variety of other issues, including: ","locales":"","title":"Kibana 4.0.1 Released"}
{"index":{}}
{"author":"Boaz Leskes","category":"Engineering","publish_date":"2015-03-05T00:00:00.000Z","url":"/blog/marvel-1-3-1-released","seo_title":"","content":" Today, we are happy to announce the release of . This is a bug fix release focusing on improving agent resiliency and fixing minor issues discovered since the release of , our security product. Although this release doesn’t add many new features, we recommend upgrading at the first opportunity. To upgrade, you must install the latest Marvel plugin on all of your Elasticsearch nodes. As with any other Java plugin, you will need to restart each node (one by one) in order for the newer version of Marvel to become active. The upgrade process is described in more detail in the .Here is a complete list of all the goodness that went into this release:Agent Monitoring UI Sense As always, we welcome all feedback so we can continue improving Elasticsearch Marvel. Please send questions, praise, or pains to the or find us on . ","locales":"","title":"Marvel 1.3.1 Released"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2015-03-04T00:00:00.000Z","url":"/blog/2015-03-04-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. just restarting elasticsearch over and over as an excuse to google marvel characters — Michael Dewar (@mikedewar) Elasticsearch core Geohashing Barcelona checkins during using and via — Outliers Collective (@outliers_es) In apache lucene this past week Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around the ELK stack – that's Elasticsearch plus Logstash and Kibana – including plugin and driver releases. Oh the places I'll go. MT : The roadmap is now easier to view, comment, & contribute to! — logstash (@logstash) Slides & VideosAn introduction to Elasticsearch and the ELK Stack. Lots of material, but well worth a gander for users who are just getting started.For lovers of Django and Elasticsearch, this recent presentation from the Berlin Django User Group is for you!Excellent introduction to application logging using LogstashBetter Decisions Through Data Using the ELK Stack (auf Deutsch) Building unlimited scalable ELK stack on AWS by Asaf Yigal at Elasticsearch Boston meetup — Igor Motov (@imotov) Where to find UsWe’d love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you’re speaking or hosting a meetup, let our Director of Developer Relations, , know!France Germany India The next Delhi Elasticsearch Meetup will take place on March 14, covering what's new in the ELK stack. to save your place. Italy will take the stage at Code Motion Rome to discuss . David speaks at 3 PM on March 28, and the conference runs the 25-28. The Netherlands For folks in or near Utrecht, you can for the GOTO Night on Elasticsearch. Attendees will hear from Anne Veling and Jettro Coenradie, including a recap of the Elastic{ON} conference. The GOTO Night is scheduled for March 25 at bol.com's offices. The track is complete feat 's Nik Everett & Peter Vulgaris! — GOTO Amsterdam (@GOTOamst) South Africa The inaugural Capetown Elasticsearch Meetup will convene on March 5 to talk shop and plan for the future of the group. to let the organizers know you plan to attend. Sweden The 8th Elasticsearch Stockholm Meetup is coming up on March 25. will cover data analysis using Kibana 4. We're still searching for a second speaker, so please do get in touch if you're interested in presenting. You can also to save your seat. United Kingdom Elasticsearch will be out in force at QCon London, which returns to the Queen Elizabeth II Conference Center this year. You can visit us at Booth 12 on the show floor. QCon runs March 2-6. And for folks looking for ELK stack goodness in Bristol, you can join the local Java Meetup on March 10 to hear about using Elasticsearch & Kibana alongside Apache Storm. to save your seat. Not planning to come to QCon London, but hoping to hear more about the ELK stack? The March London Elasticsearch Meetup is on for March 24, featuring speakers from Couchbase, OpenTable, and Postcode Anywhere. to save your seat. United States If you're heading to , you can visit our team in Booth 334 to hear all about , , and more! — Leslie Hawthorn (@lhawthorn) Where to Find You If you're a regular reader of This Week in Elasticsearch, a.k.a TWIES, you're thinking of skipping this section. You may even be thinking to yourself, yes of course I will drop a note on Twitter when I am giving a talk on all things ELK. That's awesome, because we'd like to showcase every meetup, conference presentation and workshop on Elasticsearch, Logstash, and Kibana happening worldwide. And now, we've made it even easier for you to get support for your meetup! Head on over to ! (And we'll sti","locales":"","title":"This Week in Elasticsearch - March 04, 2015"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2015-03-02T00:00:00.000Z","url":"/blog/where-in-the-world-is-elasticsearch-11","seo_title":"","content":" It's hard to believe, but the 1 week countdown has started! We are kicking off our first ever user conference, , next week, starting on March 9 in San Francisco! Feel free to share your excitement with us on Twitter using hashtag #elasticon. Hopefully we'll see you there! Until then we have a few more things going on in event land: Upcoming Events March 4-6: – Don't miss our guest session about how to monitor trends in society before they become issues. The guest session will be held on Tuesday, March 4 at 10:20 AM in the Rutherford room on the 4th floor. We will also hang out at Booth 12, so come say hi and grab some Elasticsearch sweets from our sweet stall! Upcoming MeetupsNorth America March 2: Europe March 3: March 3: March 4: Africa March 5: That's it for this week. Stay tuned for an Elastic{ON} special next week! - The Elasticsearch Team P.S.: if you're interested in hosting a meetup or are giving a talk about Elasticsearch – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elasticsearch? - March 02, 2015"}
{"index":{}}
{"author":"Daniel Palay","category":"News","publish_date":"2015-02-27T00:00:00.000Z","url":"/blog/behind-the-antlers-life-lessons-from-the-elasticon-cfp","seo_title":"","content":" Thanks for making my job difficult. No, seriously – I mean it. Thank you for making my job nearly impossible. Sounds strange to hear, no? Let me tell you why I really am grateful for all of you fine humans. When I was handed the job of curating all the responses to our call for papers, I really didn't know what to expect. I was 4 months into my new role here heading up our case study and community engagement programs, so I knew we had some amazing stories out there. I'd heard the whispers out of Rochester, MN, where Elasticsearch has been helping some of the world's best doctors revolutionize how data could help save lives. I'd talked with financial executives who gushed about how their work with the ELK stack had allowed their entire team to form a more complete understanding of their customers, driving overall engagement and increasing company revenue. And, I had even seen how our software was front and center during some recent emergency events, helping to let the public know when and where it was safe to travel. I could go on for more than three sentences, but why spoil all of our upcoming case studies? I knew that there were some great stories out there, but I was certainly not prepared for what was going to happen when November came around and we opened up the application process for the ELK in the Wild track at Elastic{ON}15. From that point until the moment we closed submissions on January 18, we received 142 proposals. These submissions came from 5 continents, countless industry verticals, and covered so many different use cases that I could hardly do them all justice by listing them out here. Suffice it to say, when I sat down on January 19, I finally figured out what it was like to hate your job, but love it all at the same time. I mean, how could I figure out which 11 of these proposed sessions merited selection over the rest. I read and re-read every single submission. I tried to find the special points in all of them (Did it have a positive effect on society? Was it a big name that would draw people to the session? Or did I just nerd out when I read the abstract?). There were submissions from companies whose services I use every day and some that I had never heard of. There were simple search use cases, and some use cases that even the most seasoned Elasticsearch employee hadn't thought of. You folks made me dread getting up in the morning, because I had no idea how I could choose. How could I say that one person's story was better than the next? Well, thanks to several internal meetings, a multitude of confectionary therapy sessions, and our hard working events team finding more space for us to hold a few more talks, we found ourselves with our final agenda – which you can see here. We have a and one that I'm proud to stand by. That said, I couldn't find a spot for everyone and that realization truly made my job terrible for a while. But then I remembered, the best part of our community is that whether it's at our first Elastic{ON} or on a webinar or at a meetup down the road, there will always be people – especially me (seriously, I'll be there with bells on) – wanting to hear the story of how you are changing the world one cluster at a time. Thank you for making my life difficult – the pleasure was all mine. ","locales":"","title":"Behind the Antlers: Life Lessons from the Elastic{ON} CFP"}
{"index":{}}
{"author":"Tanya Bragin","category":"News","publish_date":"2015-03-03T00:00:00.000Z","url":"/blog/high-level-logstash-roadmap-is-published","seo_title":"","content":" At Elasticsearch, we are committed to doing things the open source way. To us, “open\" does not only mean open source software development, but also open and transparent communication about the roadmap. We have always used GitHub to develop our open source projects, and each one of these projects already tracks a number of roadmap features as GitHub issues (see the , , , and issues for more information). While GitHub is great for sharing the details of our work, it can be difficult to get an overview of the immediate roadmap from a long issues list. To make it easier for everyone to consume, we decided to distill the Logstash roadmap issues into major themes and added these to the . We also included pointers to additional resources, if you want to get involved. We started with the because we are particularly excited to share with you what we are planning for Logstash 2.0. However, this is just the beginning! If you like where this is headed, our intent is to do the same for Elasticsearch, Kibana, and ES-Hadoop projects. We invite you to comment on the roadmap page on the and let us know if this information is helpful or if there is anything you'd like us to add or amend. As always, we welcome feature requests, bug reports, and pull requests on . ","locales":"","title":"High-Level Logstash Roadmap is Published"}
{"index":{}}
{"author":"Robyn Bergeron","category":"News","publish_date":"2015-02-25T00:00:00.000Z","url":"/blog/highlights-from-scale","seo_title":"","content":" On the road again! I spent this past weekend at the , also known as SCALE. This was the 13th year of this conference, and my 5th year of attending. SCALE is the largest community-organized and operated conference in the United States, with approximately 3,000 people in attendance this year. While I suspect a large portion of that audience is from the southern California area, there were attendees and speakers present from all over the world for 4 days of talks, trainings, learning, and fun. Lucky Number Booth 13! Come meet and from at — Garick Chan (@ilovegarick) As with most conferences, there was an expo hall, which is where , , , and spent the majority of our time talking to folks either coming by the booth, or visiting other open source projects at their booths. Kurt and Suyog had a demo going on our table, capturing the Twitter stream for the conference (#SCALE13x), which caught many folks' eyes. (Including those of , who is in charge of social media for the SCALE conference:  not surprisingly, he was the most prolific tweeter at the event.) Of course, as a conference with its roots in the Linux community, there were many Linux enthusiasts present, as well as many folks who have careers in systems administration. For many of these attendees, there was familiarity with Logstash, but not necessarily with the rest of the ELK stack. But in seeing the demo we had going, many were curious about other ways Elasticsearch and Kibana could be combined with their existing Logstash instances for more sysadmin-focused purposes. Kurt and Suyog pulled together a demo for an Apache web server logging case using the , which I think definitely was more relatable for many of the attendees. Demoing the cool, new Kibana 4 at booth /w — Suyog Rao (@suyograo) Another highlight of the conference for us was the Elasticsearch Birds of a Feather (BoF) session, in the prime-time slot of 7pm on Friday night. With 60+ people in attendance over the course of the hour, Suyog and Kurt gave an overview of the ELK stack, handled questions, and encouraged sharing of stories like pros! Wrapping up the evening we were fortunate enough to have a ticket to to give away, and with some attendees hailing from the northern part of California and thus easily able to attend, we drew names out of a hat for the lucky winner. Thanks to everyone who came to the BoF at ! — elasticsearch (@elasticsearch) Speaking of Elastic{On} — this will be my next major event to which I'm travelling. This is our first user conference, coming up soon on March 9 – 11 in San Francisco, California. I'm looking forward to meeting many of you there! ","locales":"","title":"Highlights from Southern California Linux Expo"}
{"index":{}}
{"author":"Tanya Bragin","category":"Engineering","publish_date":"2015-02-27T00:00:00.000Z","url":"/blog/kibana-4-video-tutorials-part-1","seo_title":"","content":" With the much-anticipated release of Kibana 4, we were thinking about how to help users get up to speed quickly with the new feature set. As a first step, we wrote up comprehensive for Kibana 4, which is a great reference. However, we felt that the power and ease-of-use of the new interface was best conveyed in a series of short video tutorials, ranging from a high-level overview to panel-by-panel recommendations on how to migrate visualizations from Kibana 3 to Kibana 4. Today, we publish the first four of these videos, with more to come! The first video introduces you to Kibana 4 navigation at a very high-level. Whether you are coming over from Kibana 3 or just starting with Kibana, this one is worth while to check out.   The second video walks you through how to build a specific visualization in Kibana 4, in this case a pie chart.   The third video shows how you can re-create Kibana 3 \"hits\" panels in Kibana 4.   The fourth video shows how you can re-create Kibana 3 \"terms\" panels in Kibana 4.   And we're always on the lookout for a good Kibana story. Send us yours to . ","locales":"","title":"Kibana 4 Video Tutorials, Part 1"}
{"index":{}}
{"author":"Luca Cavanna","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2015-02-25T00:00:00.000Z","url":"/blog/2015-02-25-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project.Elasticsearch core Using to visualize and analyze our costs. CFO is happy, prefers it even to — Alexander Leschinsky (@glsystemhaus) In apache lucene this past week Versions 1.4.3 and 1.4.4 released for the client. Now with special support for Grails 2.x! — Chris Earle (@pickypg) Elasticsearch EcosystemHere’s some more information about what is happening in the ecosystem we are maintaining around the ELK stack – that’s Elasticsearch plus Logstash and Kibana – including plugin and driver releases. Excellent introduction to Elasticsearch: what it does, fundamental concepts, and how to make it do useful things for you out of the box An introduction to Logstash and its ecosystem. Pablo Figue’s presentation from last week’s Elasticsearch Berlin User Group meeting From the January London Elasticsearch Meetup ","locales":"","title":"This Week in Elasticsearch - February 25, 2015"}
{"index":{}}
{"author":"Peter Kim","category":"Engineering","publish_date":"2015-02-23T00:00:00.000Z","url":"/blog/kibana-4-and-civic-hacking-investigating-campaign-contributions","seo_title":"","content":" It's a great time to be a civic hacker. We're seeing increased transparency from local and national governments through more data sets released to the public every day around subjects such as traffic accidents, adverse drug reactions, financial aid applications for higher education loans, restaurant inspections and even public restroom locations. Now, anyone can access this data, analyze it and build apps that promote the common good. Yay for civic hacking! The United States Federal Election Commission publishes campaign contributions data to its website (www.fec.gov), covering elections for President, Senate and House of Representatives. As stated on fec.gov: Providing this information to the public is critical to ensuring the integrity of the election process. So now that the FEC has provided us with the raw data, what can we do with it? If you don't consider yourself a Data Scientist who knows how to analyze data in R or create pretty D3.js-based visualizations like the New York Times, you probably feel stuck at this point. Fortunately, the ELK stack makes it possible to perform rich, visual, interactive data analysis with very little coding. I'll describe data loading steps in a separate post but for now, I'll provide a glimpse at some of the data visualizations possible with Kibana 4. Discover In Kibana 4, you typically start on the Discover tab. This is where you get a high-level view of the data set, immediately seeing the distribution of data over time, a list of the fields that structure your documents, and the contents of some documents in your index. In the screenshot above, we're looking at almost 2.1 million records representing campaign contributions from individuals to political committees during the 2013-2014 election cycle. We can see a clear trend in the number of contributions increasing over the election cycle, with a few interesting spikes at seemingly random points. The column on the left side lists all of the fields contained in the data set. This is extremely helpful in giving us context for the questions we might want to ask of the data. For example, since we now know our data set potentially contains fields such as “name\", “city\", “state\", “transactionAmount\" and “transactionDate\", we can start building a list of questions we'll want to ask of the data: The list of fields can also be helpful in identifying holes in the data set which may prevent you from getting answers to the questions you want to ask. For example, the file representing these individual contributions does not contain clear information about the committee receiving contributions or the candidate associated with the committee (technically, individual contributions go to a committee associated with a candidate). The raw data file simply contains the cryptic IDs of the committees and associated candidates. This makes it difficult for me to ask a question like “What are the names of the top 10 committees receiving contributions?\" Identifying these gaps using the Discover interface can lead us to decide we need to load additional data to make this application more useful. Visualize Once we've identified some questions we might want to ask, we can start building visualizations based on some of the attributes in the data set. Let's take one of the questions we brainstormed above as an example. Here's a pie chart representing the top 10 states from which individual campaign contributions are made, measured by the sum value of contributions: Not a lot of surprises here, as the pie chart shows California, New York, Texas, Florida and Illinois (the five most populous states in the US) amongst the top sources of contributions. DC's position at the #3 spot is interesting and worth investigating — even though DC would be the third least populous state if it was a state, perhaps its role as the seat of the federal government naturally draws residents w","locales":"","title":"Kibana 4 & Civic Hacking: Investigating Campaign Contributions"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2015-02-23T00:00:00.000Z","url":"/blog/where-in-the-world-is-elasticsearch-10","seo_title":"","content":" A bit over 2 weeks until we are off to our first ever user conference in San Francisco, March 9-11- ! Feel free to share your excitement with us on twitter using #elasticon. Hopefully we'll see you there!We are all pretty busy getting the last things organised and ready until then, however, we've got a few meetups on the list for this week nevertheless.Upcoming MeetupsEuropeFebruary 24: February 24: North AmericaFebruary 24: AsiaFebruary 24: February 25: That's it for this week. Stay tuned for Elasticsearch happenings next week – there's much to come! - The Elasticsearch TeamP.S.: if you're interested in hosting a meetup or are giving a talk about Elasticsearch – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elasticsearch? - February 23, 2015"}
{"index":{}}
{"author":"Leslie Hawthorn","category":"User Stories","publish_date":"2015-02-20T00:00:00.000Z","url":"/blog/how-elasticsearch-made-us-faster-literally","seo_title":"","content":" At , our work focuses on e-procurement. In a nutshell, e-procurement is online shopping for employees on behalf of their company, plus integrating this purchasing process with various business systems. Of course, we're using Elasticsearch extensively in this setup, e.g. when searching for products, but also for analytics. But some of us at Meplato have another passion: racing. For many years, we've toured race tracks and attended various track days and trainings. So, finally, some of us decided to get our official licenses and take part in a professional race series, the . The VLN is a series of 10 races over 4-6 hours, all taking place at the , a 21 km race track in Germany. At the VLN, you typically see about 200 cars on the track, ranging from Porsche 911s to Ferrari 458s and Opel OPCs, all racing at the same time and at (very) different speeds. Obviously, overtaking is not the exception, but the rule here. So what does this all have to do with Elasticsearch? Let me explain. In our first races, we found that we were slower than our competitors in similar cars. So what do good engineers do? First, they gather data. Second, they analyze it. And that's exactly what we did, using Elasticsearch. What we obviously needed was data about location and speed. While you can get a ton of data from a race car (worth a separate article), we found that getting data from only our car didn't solve the problem. After all, we only have data from one car, and we need data from the competitors as well. So we came up with a different solution. Virtually every team in the VLN uses an iOS application called to track the location of their car:  remember, it's a 21 km track. I've even heard that the Race Control Center uses it. (And kudos to the team at for such an awesome app!) The app tracks the exact location of every car on the track and streams it over the internet. While GPS-over-IP is not as exact as GPS data from the cars themselves, it's available for every car and, as it turns out, good enough for us to solve our problem. So, we wrote a proxy to record data on the wire and stored it in Elasticsearch. Looking at the data, we found that not only is the latitude and longitude of every car provided, but also its speed, steering wheel position, and some other metrics. Nice. Next, we hacked up a program to analyze the recorded data. We used to prepare a list of sectors for the track. Then, we used the geo-queries and geo-filters of Elasticsearch to find entry and exit times for each car in every sector. Given enough laps, that gave us a good estimate of where we lost time. We even tracked time loss down to individual corners of the track. Then, we generated a nice PDF of the results. One evening after the race, we figured that with all this data at hand, we could just replay the whole race. So, some of us sat down and, with the help of , came up with this: While we extracted the data for this replay, we could have easily done this in real-time with Elasticsearch. All in all, we had a lot of fun at the Nürburgring. Not only did we learn a lot about racing and cars but also about how technology, and Elasticsearch in particular, can help us to solve problems. And we finally made it to the podium two times. Happy racing! ","locales":"","title":"How Elasticsearch Made Us Faster – Literally"}
{"index":{}}
{"author":"Rashid Khan","category":"Engineering","publish_date":"2015-02-20T00:00:00.000Z","url":"/blog/kibana-aggregation-execution-order-and-you","seo_title":"","content":" By now you may have spotted those crafty little arrow buttons in the screen of Kibana 4 and said “Hey, what are you doing over there? And what are you up to?\". Well, those buttons control the . This concept defines how Elasticsearch goes about analyzing your data, and how Kibana displays the result. Let's define a common scenario: . Easy enough right? Well yes, but your demand is ambiguous and your goals are unclear. What defines the “most active users\"? Let's set more parameters: . Now we're getting closer, but there are two ways we can interpret all that: Top 5 users in each week, for a year In this screenshot, we run the date histogram first, followed by asking for the top 5 users. This creates a bucket for each week of the year. Within each of those weeks, we find the top 5 users. Because that top 5 could be, and is in this case, different from week-to-week, you see far more than 5 users in the legend. Further, if we look at the aggregation request in the shaded section, we can see the date histogram is requested first, with the terms aggregation within the date histogram. The result is that we see weeks in which some user has an outsized activity, even if they haven't been active in any other portion of the year. This lets us spot outliers in any given week. Top 5 users for the year, and their weekly activity Now, we click the up arrow to move the terms aggregation above the date histogram. We now calculate the top 5 users over the entire year, then create a date histogram for each. This results in just 5 legend values. However, we now see the users that are consistently very active instead of the spikey outliers. Go forth and aggregate So there you go: those arrows matter. Aggregation execution order applies to almost every chart in Kibana and significantly influences both what you see on the chart, as well as the conclusions you can draw from the data. And finally, if you think you have a good Kibana success story, we'd love to hear it. Give us a shout at or and we'll help share your successes with the world! ","locales":"","title":"Kibana, Aggregation Execution Order, and You"}
{"index":{}}
{"author":"Rashid Khan","category":"News","publish_date":"2015-02-19T00:00:00.000Z","url":"/blog/kibana-4-literally","seo_title":"","content":" Kibana 4 is now, literally, figuratively, conceptually, spiritually, and deliciously production ready. Ok, it was ready a week ago, but we wanted to make absolutely sure that we were totally happy with it. And we are, and we want to share the happiness that is Kibana 4.0.0 GA with you. Gratuitous screenshots and backstory below. If you're just too excited for all of that, we've devised a two-step plan: If you haven't already, you will need to upgrade your cluster to If you're upgrading from Kibana 4 RC1, you'll need to migrate your config. The back story Kibana has always been a tool for solving problems. Why am I getting paged at 2am every night? When did that code get pushed to production? Did it break something as a result? Well, we solved all of those. Globally, for years, not a single person has been paged at 2am. I know, right? As the answers get easier, the questions get harder. The easy wins were easy. Now, let's solve the hard problems, the problems three layers deep. Let's solve the problems that require analyzing multiple dimensions, multiple fields, and multiple data sources. Kibana 4 is us, working together, to solve the hardest problems in the least amount of time, and with the least hassle. We took everything we learned from Kibana 3 and applied it to Kibana 4. Why settle for one thousand points on a map, when we could have one billion? Why settle for one field on a chart? Or one chart in a panel? Why just one index on a dashboard? Let's generate 5 scenarios, comparing data across 2 fields and put them all on a dashboard with data from 3 indexes. Yeah. Let's do that, then let's go get ice cream. With sprinkles. The plot Like ice cream, problems come in many flavors. To that end, we've divided Kibana up neapolitan style, except we left out that flavor you don't like. If you're a long-term Kibana user, you'll feel right at home on the first tab . This allows you to quickly iterate on searches, find records, and keep solving the easy problems that are quickly corrected by finding that one line that tells the whole story. When things get more complex than simple search can account for, it's time to make magic with charts and graphs. Dive into the tab to break down data with the power of Elasticsearch aggregations. exposes the multi-dimensional nature of data and lets you build charts, tables and maps that quickly answer the kind of questions you never would have known to ask before. The question you might have asked first was “Why was the site slow last week?\", but the question revealed by the data is “Why did the average file size of requests made from Tokyo spike on Christmas day?\" Finally, bring it all together on the . Put it on the big screen and say: “There's your answer and here's a link for later. Also, I embedded it in the wiki, mailed you a csv export of the data, ate some ice cream and wrote the first chapter of my autobiography. Now get me more ice cream, I earned it.\" For a more in-depth look at each tab, check out the blog post. To be continued… It's probably time for a nap right? Nope. Kibana 4.1 is already in the works and we have big plans for the future. Much effort went into making the underpinnings of Kibana 4 stable and sensible, giving us a platform to build the future of Elasticsearch applications upon. The entire thing was designed to be extended. For example, visualizations were built to be, well, built upon. Open source is more than a GitHub account to us, it's our commitment to creating a structure that enables everyone to build new amazing things. Watch space for articles on building your own Kibana visualizations and creating your own applications that work with Elasticsearch. Want a sneak peek? Check out Spencer Alger's talk at . We wouldn't be here without you, and we're not going anywhere without your help, so please, ping us on with issues, suggestions, and contributions. Or, if you love IRC like we do, join us in #kibana on Freenode. Extra credit Want the whole Kibana 4 story as i","locales":"ko-kr","title":"Kibana 4. Literally."}
{"index":{}}
{"author":"Kevin Kluge","category":"Engineering","publish_date":"2015-02-19T00:00:00.000Z","url":"/blog/elasticsearch-1-4-4-and-1-3-9-released","seo_title":"","content":" Today, we released Elasticsearch 1.4.4 and 1.3.9. This is a small bug fix release, mainly including a packaging fix for our RPM and DEB packages if you are using Lucene expression scripts . You can . fixes The 1.4.3 RPM and DEB packages were for Antlr and ASM. These dependencies are required to use Lucene expression scripts with Elasticsearch. Since the for Groovy, we expect a lot more people will be using expressions scripts, so we quickly released 1.4.4. The release also fixes a few bugs around cluster pending tasks. And it fixes a bug where a negative interval on date histograms could cause an . The as well as the list out all the changes. feedback As always, we would love to hear from you. Please let us know what you think on Twitter via or by filing an . ","locales":"","title":"Elasticsearch 1.4.4 and 1.3.9 Released"}
{"index":{}}
{"author":"Alexander Reelsen","category":"","publish_date":"2015-02-18T00:00:00.000Z","url":"/blog/2015-02-18-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project.It's ready! 4 RC1 is freshly baked. Get the skinny on all the features or just jump right in. — elasticsearch (@elasticsearch) Elasticsearch core In Apache Lucene this Past Week log analysis in and . 30 minutes from concept to dashboard:  great tools to work with. — Robin Moffatt (@rmoff) Elasticsearch EcosystemHere’s some more information about what is happening in the ecosystem we are maintaining around the ELK stack – that’s Elasticsearch plus Logstash and Kibana – including plugin and driver releases. Slides & VideosJoe Jasinski shows how he uses Django, Elasticsearch, and Haystack for searching the contents of his website to the Chicago Python User Group.The MongoDB meetup group in Delhi recently heard from Bharvi Dixit about some use cases for MongoDB with Elasticsearch.The Elasticsearch meetup in London had some fantastic presentations at their last gathering.Where to Find UsWe'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking or hosting a meetup, let our Director of Developer Relations, , know!AustriaThe Vienna Elasticsearch Users Group will get together on March 4 to talk Elasticsearch at Cloud Foundry and more. to let the organizers know you’re attending.GermanyWant to know more and happen to be in Berlin next 24/02, join me () for an all night on logs! — Pere Urbón-Bayes (@purbon) IndiaThe Configuration Management Magic Meetup will convene on February 21 in Bangalore. Among the many talks on offer, you can hear all about Log Analysis using Elasticsearch, Kibana and Fluentd. to save your seat.IsraelThe Tel Aviv-Yafo ELK meetup group will be talking How to Use ELK to Analyze Logs from a Large Production AWS Environment on February 24. You can to attend.Speaker Update: Peter Vulgaris from speaks in the track at : — GOTO Amsterdam (@GOTOamst) South AfricaThe inaugural Capetown Elasticsearch Meetup will convene on March 5 to talk shop and plan for the future of the group. to let the organizers know you plan to attend.TaiwanThe Agile Code Camp team are convening a developer and designer hack day on February 23, and attendees will be developing with Elasticsearch. to attend the full day event.United KingdomElasticsearch will be out in force at QCon London, which returns to the Queen Elizabeth II Conference Center this year. You can visit us at our booth on the show floor, plus we’ll be having one of our engineers take the stage for the main program. Those details are in the works, but in the meantime you can take a look at . He’ll be sharing the story of how Elasticsearch and other technologies are powering the Norwegian Roads Authority’s brand new system to provide real-time traffic information to travelers throughout Norway.United StatesIf you’re at Strata in San Jose this week, don’t forget to join , creator of Elasticsearch for Apache Hadoop, at the conference. He’ll present on on Friday, Feb 20 at 11:30 AM.Coming up in San Francisco next month, you can join the SF MySQL Meetup to hear all about using MySQL and the ELK stack for audit logging. The user group will get together on March 11, but the event is filling up quickly. Register now to save your seat.And for our friends beyond Silicon Valley: It's ready! 4 RC1 is freshly baked. Get the skinny on all the features or just jump right in. — elasticsearch (@elasticsearch) Where to Find You If you’re a regular reader of This Week in Elasticsearch, a.k.a TWIES, you’re thinking of skipping this section. You may even be thinking to yourself, yes of course I will drop a note on Twitter when I am giving a talk on all things ELK. That’s awesome, because w","locales":"","title":"This Week in Elasticsearch - February 18, 2015"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2015-02-16T00:00:00.000Z","url":"/blog/where-in-the-world-is-elasticsearch-9","seo_title":"","content":" As promised last week, we are back with a bunch of events in our US home base, California! Check out the events we are at this week, plus meetups happening around Elasticsearch and the ELK stack. Even more exciting, the count down is {ON}, with less than a month to go to our first ever user conference in San Francisco March 9-11 – ! Upcoming EventsNorth America February 17-20: – Don't miss our tutorial given by Logstash creator, , and team member Tal Levy on Wednesday, February 18, 1:30 p.m.-5:00 p.m. , or talk: , Friday, February 20, 11:30 a.m.-12:10 p.m. February 19-22: – Developer Relations team member will give a talk titled on Saturday, February 21, 4:30 p.m.-5:30 p.m. in Room Los Angeles B February 19-20: – Make sure you say hi to who will be walking around in the hallway ready to talk all things #ELKstack with you! Upcoming MeetupsNorth America February 18: February 19: Europe February 18: Asia February 18: February 21: That's it for this week. Stay tuned for Elasticsearch happenings next week – there's much to come! - The Elasticsearch Team P.S.: if you're interested in hosting a meetup or are giving a talk about Elasticsearch – we can offer support and send you swag! ","locales":"","title":"Where in the World is Elasticsearch? - February 16, 2015"}
{"index":{}}
{"author":"Alex Brasetvik","category":"","publish_date":"2015-02-15T00:00:00.000Z","url":"/blog/found-uses-of-elasticsearch","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud.Elasticsearch is used for a lot of different use cases: \"classical\" full text search, analytics store, auto completer, spell checker, alerting engine, and as a general purpose document store. This article gives a brief overview of different common uses and important things to consider, with pointers to where you can learn more about them. ","locales":"","title":"Uses of Elasticsearch, and Things to Learn"}
{"index":{}}
{"author":"Robyn Bergeron","category":"News","publish_date":"2015-02-13T00:00:00.000Z","url":"/blog/highlights-cfgmgmtcamp-ansiblefest","seo_title":"","content":" As a Developer Advocate for Elasticsearch, I'm incredibly fortunate to be able to travel the world and speak with members of the Elasticsearch, Logstash, and Kibana communities. And whether I'm on the road or talking to folks in my hometown, the versatility of the ELK stack means that I get to see and hear about the myriad ways people are putting it to work. One of the synergies I consistently see lies with the ELK stack and DevOps-practicing organizations. While I'm a firm believer in the notion that DevOps is not “achieved\" simply by adopting a particular set of tools, and instead is rooted deeply in organizational culture — I do think that the ELK stack, in part or in whole, enables IT departments to deliver increasing value to their organizations. Enabling the ability to explore and correlate data, whether you're in marketing or keeping the website up and running, drives the continuous improvement that is embraced by innovative organizations. More importantly, I often hear that the adoption of the ELK stack is occurring on both the Dev and the Ops sides of organizations — often with Elasticsearch and Kibana for developers, and Logstash for operations teams — and that the shared love is a common ground that brings people together. Our stack helps initiate a crucial part of the DevOps journey: having conversations. My latest journey was one that had me off to Europe for a handful of events, including and , both events that are largely targeted at folks on the Ops side of the house, but cater to the developer audience as well. Config Management Camp This was my second year attending this event, which is held right after FOSDEM in Ghent, a lovely city 30 minutes train ride from Brussels. Despite the name, the audience and speakers for this event are definitely interested in the bigger picture of the tech world, and this theme came out often as speakers presented on several topics “beyond\" configuration management. The event was divided between a main track, which included keynote speakers each day, and separate tracks dedicated to various project communities, including Ansible, Chef, Puppet, and SaltStack. I was delighted to see Jez Humble keynote, as a talk he gave several years ago became my “a-ha!\" moment in understanding DevOps. Closing out the event was , the VP of Engineering at Kickstarter, who gave a talk about the . Best of all, when asked about his recommendations for a monitoring tool, James said – you guessed it – the ELK stack was one of his key recommended pieces to the puzzle. AnsibleFest Two days and one pleasant train ride later, I was in London for AnsibleFest, where Elasticsearch was a sponsor. AnsibleFest is largely focused on users of Ansible sharing their advice, stories, and successes, with just the right blend of technical content and inspirational “big picture\" coming through.Having attended an Ansible meetup in the past, where I met numerous folks who also used the ELK stack, I suspected that there would be a similar alignment at this event in London. The steady stream of people coming to our table confirmed that I was correct. The discussions often touched on the “bringing people together\" theme:  both Ansible and the ELK stack were useful to both developers and operators — and more pertinently, both were simple enough to learn and use, enabling users to start getting results quickly. Not surprisingly, people like to just get things done, and I'm glad to see that we're helping to make that happen. And if that wasn't enough to put a smile on my face (though it was!) — I was delighted to see Elasticsearch and the ELK stack mentioned in a number of presentations at the event. Seeing presenters from companies like Rackspace mentioning the ELK stack as part of their toolchain, and BigStep talking about how Ansible automates the deployment of their Elasticsearch application, certainly validated for me that the ELK stack communiti","locales":"","title":"Highlights from Configuration Management Camp & AnsibleFest London"}
{"index":{}}
{"author":"Adrien Grand","category":"Engineering","publish_date":"2015-02-17T00:00:00.000Z","url":"/blog/frame-of-reference-and-roaring-bitmaps","seo_title":"","content":" Postings listsWhile it may surprise you if you are new to search engine internals, one of the most important building blocks of a search engine is the ability to efficiently compress and quickly decode sorted lists of integers. Why is this useful? As you may know, Elasticsearch shards, , split the data that they store into segments which are regularly merged together. Inside each segment, documents are given an identifier between 0 and the number of documents in the segment (up to 2-1). This is conceptually like an index in an array: it is stored nowhere but is enough to identity an item. Segments store data about documents sequentially, and a doc ID is the index of a document in a segment. So the first document in a segment would have a doc ID of 0, the second 1, etc. until the last document, which has a doc ID equal to the total number of documents in the segment minus one.Why are these doc IDs useful? An inverted index needs to map terms to the list of documents that contain this term, called a , and these doc IDs that we just discussed are a perfect fit since they can be compressed efficiently.Frame Of ReferenceIn order to be able to compute intersections and unions efficiently, we require that these postings lists are sorted. A nice side-effect of this decision is that postings lists can be compressed with delta-encoding.For instance, if your postings list is , the list of deltas would be . What is interesting to note here is that all deltas are between 0 and 255, so you only need one byte per value. This is the technique that Lucene is using in order to encode your inverted index on disk: postings lists are split into blocks of 256 doc IDs and then each block is compressed separately using delta-encoding and bit packing: Lucene computes the maximum number of bits required to store deltas in a block, adds this information to the block header, and then encodes all deltas of the block using this number of bits. This encoding technique is known as (FOR) in the literature and has been used .Here is an example with a block size of 3 (instead of 256 in practice):The same abstraction is used at search time: queries and filters return a sorted iterator over the list of documents that they match. In the case of term queries and filters, implementation is very simple, we just need to return an iterator over a postings list from the inverted index. Other queries are more sophisticated. For instance, a disjunction would need to merge postings lists for and on the fly. But in the end, it is still using the same abstraction.Roaring bitmapsThis leads us to a second place where Lucene needs to encode sorted lists of integers: the filter cache. Filter caching is a popular technique which can speed up the execution of frequently-used filters. It is a simple cache that maps (filter, segment) pairs to the list of doc IDs that they match. But constraints are different from the inverted index: For these reasons, the best encoding techniques are not necessarily the same for an inverted index and for cached filters.So what should we use here? Clearly the most important requirement is to have something fast: if your cached filter is slower than executing the filter again, it is not only consuming memory but also making your queries slower. The more sophisticated an encoding is, the more likely it is to slow down encoding and decoding because of the increased CPU usage, so let's look at the simple options that we have to encode a sorted list of integers:Option 1: integer arrayProbably the simplest option: doc IDs are stored in an array. This makes iteration very simple, however compression is really bad. This encoding technique requires 4 bytes per entry, which makes dense filters very memory-consuming. If you have a segment that contains 100M documents, and a filter which matches most documents, caching a single filter on this segment requires roughly 400MB of memory. Ideally we should have something more memory","locales":"","title":"Frame of Reference and Roaring Bitmaps"}
{"index":{}}
{"author":"Kevin Kluge","category":"Engineering","publish_date":"2015-02-13T00:00:00.000Z","url":"/blog/shield-1-0-1-released","seo_title":"","content":" We're happy to announce the release of Shield 1.0.1!This is a bug fix release. We recommend that all users of Shield 1.0.0 upgrade to Shield 1.0.1. This version of Shield is certified to work with Elasticsearch 1.4.2 and 1.4.3.FixesMost importantly, this release fixes an issue where Shield 1.0.0 was incompatible with Elasticsearch 1.4.3. Even if you're currently using Elasticsearch 1.4.2, we recommend upgrading to Shield 1.0.1. This will protect you from encountering this incompatibility when you upgrade to Elasticsearch 1.4.3 or later in the future.We also fixed a bug with parsing the roles to users mapping. In some cases, Shield would not allow the user to receive privileges from all the roles it was a member of.EnhancementsKibana 4 has been evolving, and you may have seen yesterday(!). We have updated the default roles available with Shield to work with new features added to Kibana 4 RC1.We have also added a changelog to the documentation.UpgradingTo upgrade, just uninstall the current Shield and install Shield 1.0.1. Your configuration will be preserved. You can do this with a rolling upgrade of Elasticsearch.For each node, after you have stopped it, runbin/plugin -r shield bin/plugin -i elasticsearch/shield/latestThen restart the node. Larger deployments should follow the steps in the in order to ensure the recovery is as quick as possible.On upgrade, your current config files will keep their names and content. The config files provided by Shield 1.0.1 will be added with a “.new\" extension. If you are a Kibana 4 user or might become one, you'll want to copy the role additions for the Kibana user, found in roles.yml.new, to roles.yml. ","locales":"","title":"Shield 1.0.1 Released"}
{"index":{}}
{"author":"Rashid Khan","category":"Engineering","publish_date":"2015-02-12T00:00:00.000Z","url":"/blog/kibana-4-rc1-is-now-available","seo_title":"","content":" The first release candidate of Kibana 4 is ready to colorize, stackify, bar-a-tize, and pie-u-late. You may notice the second letter of the greek alphabet is conspicuously absent in the title. Yep, this isn’t a beta. What does that mean for you? It means we’ve sanded down the rough edges and shined up the whole shebang! It also means improved stability, performance, and yes, some fun new features too. The good stuff is below, but if you want to jump right in then upgrade to Elasticsearch 1.4.3 and grab the new build over on the right away. A couple tips Multi series charts Kibana 4 now supports multiple metric aggregations on each chart. For example, displaying the minimum, maximum and average of a field, or multiple totally unrelated fields, on a single plot. We’ve also added the much requested percentiles aggregation as well as a view of standard deviation. Partial bucket indication You may have noticed in many analytics engines everything always seems to be trending down at the last point. This behavior is usually due to the last bar not being “full”. For example, a daily bar chart, in which today isn’t over yet. Kibana now shows you how much time is left in the day by subtly shading the chart to indicate that more data may be forthcoming when working with time based series. Document tables on the dashboard In addition to saved visualizations, Kibana can now show saved searches on the dashboard. Add them in the same way you would add a visualization, but take note of that “Searches” tab. Kibana will load your saved search, along with its columns and sort order into a table on the dashboard. Markdown widget and table filtering Tired of answering the question: “What does this line here mean?” The markdown widget allows you to add helpful informational panels to complex dashboards. Plus, the data table visualization now supports the same click-to-filter functionality that every other panel supports. Filtering on Scripted fields Beta 3 disallowed filtering on scripts. RC1 enables filtering on scripts via completely transparent support for Elasticsearch’s script filter. Click to filter on scripted field values just like you would anywhere else. Kibana 4 RC1 also switches over to from Groovy due to the release of Elasticsearch 1.4.3. While Lucene expressions only support numeric values and functions for now, we’re working hard to bring in support for strings, dates, and more. Auto-refresh Auto refresh is back! And it uses the same federated request system for simultaneous panel updates that Kibana uses everywhere else. And, of course, it works everywhere, including Discover, Visualize, and Dashboard. NodeJS backend We’ve switched away from the Java (aka jRuby) based backend to a new, faster, more compatible, NodeJS-based backend. No need to worry, we package up the right version of NodeJS with the Kibana distribution and with no more Java dependency setup is even easier. The start process is still exactly the same: ./bin/kibana, except startup is nearly instant! The flip side is that you will need to download the right package for your operation system. As an entirely unrelated compensation for the operating system specific distribution, we’ve dropped in SSL support totally gratis, both from the browser and to Elasticsearch. And more Order now and we’ll throw in a set of steak knives. No, not really, but we will throw in formatted CSV exports, improved negative number handling, and a slick new style! Who knows what else we’ve hidden in there? Something? Nothing? The only way you’ll find out is by downloading it:  so go, go now! Go before someone else downloads it all and there’s nothing left for you! As always, we love hearing from you on IRC (#kibana on irc.freenode.net), , or with issues, suggestions, and contributions. ","locales":"","title":"Kibana 4 RC1 is Freshly Baked"}
{"index":{}}
{"author":"Alexander Reelsen","category":"","publish_date":"2015-02-11T00:00:00.000Z","url":"/blog/2015-02-11-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. Groovy scripting vulnerability. See the blog and download Elasticsearch v1.4.3 / v1.3.8 — elasticsearch (@elasticsearch) Elasticsearch Core wow! “: is now powering our data analytics platform. Welcome to Mars, … \" — Shay Banon (@kimchy) In Apache Lucene This Past Week 90-170% indexing throughput boost with new Elasticsearch-php core. Informal test, but encouraging results so far! — Zachary Tong (@ZacharyTong) Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases. Slides & Videos on the how and why of building Elasticsearch's API Spotify's engineering team share their Elasticsearch Use Cases at the recent Stockholm Meetup Alexander Reelsen's quick introduction to Elasticsearch's percolator, showcasing the potential of performing document enrichment before indexing Where to Find Us We'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking or hosting a meetup, let our Director of Developer Relations, , know! Austria Germany Want to know more and happen to be in Berlin next 24/02, join me () for an all night on logs! — Pere Urbón-Bayes (@purbon) India The Configuration Management Magic Meetup will convene on February 21 in Bangalore. Among the many talks on offer, you can hear all about Log Analysis using Elasticsearch, Kibana and Fluentd. to save your seat. Israel The Tel Aviv-Yafo ELK meetup group will be talking How to Use ELK to Analyze Logs from a Large Production AWS Environment on February 24. You can to attend. Japan Registration is already full for the next Elasticsearch Japan Study Session in Tokyo, but you can . The user group will get together on February 13 at 7:30 PM. Speaker Update: Peter Vulgaris from speaks in the track at : — GOTO Amsterdam (@GOTOamst) Norway The NDC Meetup Group in Oslo will get together on Feb 18 to talk Data Exploration with Elasticsearch. to save your place. South Africa The inaugural Capetown Elasticsearch Meetup will convene on March 5 to talk shop and plan for the future of the group. to let the organizers know you plan to attend. Taiwan The Agile Code Camp team are convening a developer and designer hack day on February 23, and attendees will be developing with Elasticsearch. to attend the full day event. United Kingdom Elasticsearch will be out in force at QCon London, which returns to the Queen Elizabeth II Conference Center this year. You can visit us at our booth on the show floor, plus we'll be having one of our engineers take the stage for the main program. Those details are in the works, but in the meantime you can take a look at . He'll be sharing the story of how Elasticsearch and other technologies are powering the Norwegian Roads Authority's brand new system to provide real-time traffic information to travelers throughout Norway. United States Heading to in San Jose? We've got several activites planned for around the conference. Check them out - you don't have to be attending Strata to enjoy some of the fun! Coming up in San Francisco next month, you can join the SF MySQL Meetup to hear all about using MySQL and the ELK stack for audit logging. The user group will get together on March 11, but the event is filling up quickly. to save your seat. And for our friends beyond Silicon Valley: mentors attendees at the recent Django Girls Brno workshop Where to Find You If you're a regular reader of This Week in Elasticsearch, a.k.a TWIES, you're thinking of skipping this section. You may even be thinking to yourself, yes of cou","locales":"","title":"This Week in Elasticsearch - February 11, 2015"}
{"index":{}}
{"author":"Clinton Gormley","category":"Engineering","publish_date":"2015-02-11T00:00:00.000Z","url":"/blog/elasticsearch-1-4-3-and-1-3-8-released","seo_title":"","content":" Today, we have released the and bug fix release of , based on , and . You can download them and read the full changes list here: For blog posts about past releases see: You can read about all of the changes that have been made in the and the , but the security issue is explained below:Groovy scripting vulnerability foundElasticsearch versions 1.3.0-1.3.7 and 1.4.0-1.4.2 have a vulnerability in the Groovy scripting engine. The vulnerability allows an attacker to construct Groovy scripts that escape the sandbox and execute shell commands as the user running the Elasticsearch Java VM.We have been assigned CVE-2015-1427 for this issue.Versions 1.3.8 and 1.4.3 disable sandboxing for Groovy by default. As a consequence, .If you are running a vulnerable version, you should either upgrade to v1.3.8 or v1.4.3, or disable dynamic Groovy scripts by adding this setting to all nodes in the cluster:script.groovy.sandbox.enabled: falseThis will turn off the Groovy sandbox, thus preventing dynamic Groovy scripts from being accepted inline as part of a request or from being retrieved from the special index.In the meantime, you can still use Groovy scripts by saving them as files in the directory on every data node. See for more information about how to do this. ","locales":"","title":"Elasticsearch 1.4.3 and 1.3.8 Released"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2015-02-09T00:00:00.000Z","url":"/blog/where-in-the-world-is-elasticsearch-8","seo_title":"","content":" It's another week full of meetups all around the world, and we wouldn't have it any other way. We are taking a short break this week from conferences and other events to get ready for what's coming thereafter: Strata+Hadoop World San Jose, SCALE13x, and SXSW15. Upcoming MeetupsNorth America February 10: February 11: Europe February 11: February 12: Asia February 11: February 13: That's it for this week. Stay tuned for Elasticsearch happenings next week - there's much to come! - The Elasticsearch Team P.S.: if you're interested in hosting a meetup or are giving a talk about Elasticsearch - we can offer support and send you swag! ","locales":"","title":"Where in the World is Elasticsearch? - February 09, 2015"}
{"index":{}}
{"author":"Chris Earle","category":"Engineering","publish_date":"2015-02-11T00:00:00.000Z","url":"/blog/running-groovy-scripts-without-dynamic-scripting","seo_title":"","content":" ","locales":"","title":"Running Groovy Scripts without Dynamic Scripting"}
{"index":{}}
{"author":"Mark Harwood","category":"","publish_date":"2015-02-06T00:00:00.000Z","url":"/blog/spotting-bad-actors-what-your-logs-can-tell-you-about-protecting-your-business","seo_title":"","content":" In this blog post, we'll use Elasticsearch's aggregations to analyze web server log files, with the goal of discovering how to block unwelcome visitors to a site. This is a responsibility every webmaster has, and the fundamental choice is to either: It may be more convenient for a webmaster to block a whole range of IP addresses grouped under a single subnet.However, this may be being over-zealous as there may be a wealth of well-behaved visitors who are now blocked along with the bad guys. How do webmasters understand the mix of good vs bad traffic at each level to make a good business decision? Elasticsearch to the rescue! We'll explore how the runs Elasticsearch queries to find sources of bad behavior and uses a \"Sankey\" flow diagram (see below) to illustrate the size and concentrations of risk at various points in a site's traffic flow. Setting up the example To follow along with this demonstration, you will first need to install the netrisk plugin running the following command in your elasticsearch (1.4.0 or later) home directory: All being well, the plugin should be installed. Before we can use it, however, we must provide some data with the appropriate configuration. We'll examine the details of the required index mapping later, but, for now, you can index some anonymized test data by running the shell script in this directory: This will create an index called \"mylogs\" with some data from real log records which contain an anonymized IP address and an HTTP response code. Finally you can launch the plugin using the URL Running the example The example data has a limited set of attributes we can use to identify risky traffic. We only have HTTP response status codes, but these are sufficient to find some bad behaviour in this data. The 200/300 range of HTTP response codes represents typical site traffic, whereas the 400/500 ranges represent failures, e.g. attempts to access non-existent pages. The query to single out those requests in our data is as follows: The netrisk plugin uses the standard Lucene query parser (the same one used by Kibana), so your query could use ORs to look for other features in your data that might suggest risky traffic, e.g. requests missing a UserAgent. Our query does not need to be to be too certain in determining what is \"bad\" - we just need to suggest a sense of what have a bad smell about it and then the aggregations framework will do the rest in finding sources with high concentrations of this type of content, undiluted by anything else. If we run the above search, the Sankey diagram should appear showing trails of the riskier traffic flowing into our website. Various pieces of information are summarized in the diagram: You'll notice that the lines in the diagram tend to change from red to green as they move from left to right through various stages of subnet. This is because on the left hand side of the diagram each node is typically representing smaller numbers of users who are dedicated to bad behavior. Each stage to the right represents a subnet covering a larger number of users who will typically dilute the good/bad mix with added volumes of well-behaved users who make up the normal access patterns. However, some subnets may represent entire countries where the mix of traffic stays in the red because your site may not have any relevance to that region, and the only people interested in visiting your site from there are miscreants. How does it work? Preparing the data This analysis relies on having statistics about the frequencies of both IP addresses and subnets encoded in the index. To do this analysis, we can't just index each IP address as a single string:  we must break it up into multiple tokens (e.g. the IPv4 address 186.28.25.186 is indexed as tokens 186.28.25.186, 186.28.25, 186.28 and 186). This is done using the following mapping definition: curl -XPOST \"http://localhost:9200/","locales":"","title":"Spotting Bad Actors: What Your Logs Can Tell You about Protecting Your Business"}
{"index":{}}
{"author":"Luca Cavanna","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2015-02-04T00:00:00.000Z","url":"/blog/2015-02-04-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project.Elasticsearch Core Check out the Q&A w/ Britta Weber at Fyber's Elasticsearch UG () meetup. — Fyber (@Fyber) In Apache Lucene This Past Week Realtime updates from PostgreSQL to Elasticsearch - Atlassian Developers — Found (@foundsays) Elasticsearch EcosystemHere's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases. Blogged: One Tip a Day: Using Dynamic Templates To Avoid Rigorous Mappings — Itamar Syn-Hershko (@synhershko) Slides & VideosAll the talks from last Friday's p Talks, including Q&A with (Hint: Elasticsearch & Robots!)Capacity Planning and Custom Setups Suitable for Large Elasticsearch DeploymentsBetter Decisions Through Better Data (auf Deutsch)OH: In the last 12 hours, we have taken in 12GB of sensor data in — Chris Matthieu (@chrismatthieu) Where to Find UsWe'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking or hosting a meetup, let our Director of Developer Relations, , know!AustriaThe Vienna Ruby Users Group will get convene on Feb 12, with talks on Logstash, Jekyll and Octopress. You can to save your seat.Germany IsraelThe Tel Aviv-Yafo ELK meetup group will be talking How to Use ELK to Analyze Logs from a Large Production AWS Environment on February 24. These folks are looking for space to meet, so you can host get in touch with the organizers. You can now while they're finding a location.JapanRegistration is already full for the next Elasticsearch Japan Study Session in Tokyo, but you can . The user group will get together on February 13 at 7:30 PM.NorwayThe NDC Meetup Group in Oslo will get together on Feb 18 to talk Data Exploration with Elasticsearch. to save your place.TaiwanThe Agile Code Camp team are convening a developer and designer hack day on February 23, and attendees will be developing with Elasticsearch. to attend the full day event. United KingdomIf you're a star in the Ansible Galaxy, you're no doubt attending AnsibleFest London on Feb 5. Stop by and say hello to and at our table in the exhibits area! Our Developer Advocate, , will also be hanging out in the hallway track if you'd like to say hello!United States Heading to in San Jose? We've got several activites planned for around the conference. Check them out - you don't have to be attending Strata to enjoy some of the fun! And for our friends beyond Silicon Valley: Where to Find You If you're a regular reader of This Week in Elasticsearch, a.k.a TWIES, you're thinking of skipping this section. You may even be thinking to yourself, yes of course I will drop a note on Twitter when I am giving a talk on all things ELK. That's awesome, because we'd like to showcase every meetup, conference presentation and workshop on Elasticsearch, Logstash, and Kibana happening worldwide. And now, we've made it even easier for you to get support for your meetup!Head on over to ! (And we'll still totally send you swag if you're giving a talk on anything ELKy at a conference.)Oh yeah, we're also . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticsearch, Kibana, and Logstash than where you rest your head.TrainingsIf you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elasticsearch - February 04, 2015"}
{"index":{}}
{"author":"Richard Pijnenburg","category":"Engineering","publish_date":"2015-02-02T00:00:00.000Z","url":"/blog/elasticsearch-puppet-module-0-9-0-released","seo_title":"","content":" I'm very happy to announce the latest release of our Elasticsearch Puppet module, version 0.9.0Since its creation two years ago, the Elasticsearch Puppet module has grown to be one of the most widely used modules and was included in the inaugural class of modules.New FeaturesAs with every release, we have a lot of new and exciting things in the module. One of these is support for out of the box for defining instances and plugins.We also added different custom facts to expose interesting information about the running Elasticsearch instances. These custom facts can be used together with other applications like Mcollective.An example output of these custom facts:ChangesWe added support for OpenSuse 13.x and enabled Puppet 3.7 testing. Along the way, we added several internal changes which will help improve the stability of the project. One of these changes is using the official module to manage the java installation. This replaces our custom class with a wonderfully, well-tested module from Puppet Labs.FixesThanks to the feedback from both the community and customers, we fixed several bugs. It is worth mentioning few important fixes here: RoadmapThe Elasticsearch puppet module will continue to evolve in the future, with different needs and requirements. With this release, we are paving the road for future changes without breaking expectations for your environments.In the next major release, we are planning on using . We are also working to extend support for different Operating systems such as SLES, Windows and many more.As always, if you find any bugs or have questions, enhancements, please reach out to us on the Github page, or on our page. ","locales":"","title":"Elasticsearch Puppet Module 0.9.0 Released"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2015-02-02T00:00:00.000Z","url":"/blog/where-in-the-world-is-elasticsearch-7","seo_title":"","content":" Did you enjoy any of the Elasticsearch events last week? We hope so! We are continuing our event and meetup journey this week and heading over to Belgium, London, San Francisco, Cupertino (CA), Stockholm, Munich and Taipei. Check out all the details so you don't miss out. Upcoming EventsEurope February 2 - 3: - & will see you in the hallway track between talks, plus don't miss Robyn's panel on Provisioning Infrastructure as Code. February 5: - and will be hanging around our booth in the exhibit area, so please go say hi! Our developer advocate Robyn Bergeron will also be around. Pssst: you will also get a chance to win one of our limited edition (signed) Elasticsearch books. Upcoming MeetupsNorth America February 4: February 5: Europe February 2: February 5: Asia February 4: That's it for this week. Stay tuned for Elasticsearch happenings next week - there's much to come! - The Elasticsearch Team P.S.: if you're interested in hosting a meetup or are giving a talk about Elasticsearch - we can offer support and send you swag! ","locales":"","title":"Where in the World Is Elasticsearch? - February 02, 2015"}
{"index":{}}
{"author":"Michael McCandless","category":"Engineering","publish_date":"2015-01-30T00:00:00.000Z","url":"/blog/lucenes-handling-of-deleted-documents","seo_title":"","content":" When a document is deleted or updated (= delete + add), simply marks a bit in a per-segment bitset to record that the document is deleted. All subsequent searches simply skip any deleted documents. It is not until that the bytes consumed by deleted documents are reclaimed. Likewise, any terms that occur only in deleted documents (ghost terms) are not removed until merge. This approach is necessary because it would otherwise be far too costly to update Lucene's write-once index data structures and aggregate statistics for every document deletion, but it has some implications: Merging Reclaims Deleted Documents Lucene's default merge policy, , already prefers merges that would reclaim more deleted documents, other factors being equal. Over time this means segments with more deletions will be targeted for merging. While it does have a tunable setting () to control how aggressively it targets deletions, it is dangerous to increase this too much otherwise it could select poor (costly) merge choices, dwarfing any gains from slightly fewer deleted documents. I was curious how effective its defaults are in practice, so I ran a simple worst-case indexing test. First, I built an initial index with 100 M added documents (no deletions) derived from English export. Then I updated that index by forever randomly replacing an existing document (never adding a new document), so that every add also incurs a deletion. There was no pattern to the updates, such as favoring replacing older or newer documents. This is unrealistic, but it is a good worst case test because the deletes accumulate uniformly, in proportion to each segment's size. In real usage, certain segments (old or new) would accumulate deletions at a faster rate and thus be more quickly selected for merging. I measured the percentage of deleted (but not yet merged away) documents over time, computed as (where is constant at 100 M in my test). The graph below shows an initial startup transient, when the percentage quickly rise from 0% to 45% at which point a couple of large merges complete and bring it back down. After that the deletions percentage hovers between 35% and 60%, with a sawtooth shape showing a sudden drop whenever varying sized merges finish. It looks somewhat like the stock market! A maximum sized segment (default: 5 GB) will only be eligible for merging once it accumulates 50% deletions. If this is too slow for your usage, try decreasing that maximum (): this will result in a somewhat larger segment count, but the reclaiming should happen more quickly, especially when there is a pattern to the deletions. How Do Deleted Documents Affect Search Performance? Because deleted documents remain in the index, they must still be decoded from the postings lists and then skipped during searching, so there is added search cost. To test how much, I ran a search performance test for varying queries using the 100 M document index with no deletions as the baseline, and the same index with 50% deleted documents (i.e., 150 M documents with 50M deleted). Both indices were single-segment. Here are the results: The bad news is there is clearly a non-trivial performance cost to deleted documents, and this is something we can work to reduce over time (patches welcome!). The good news is the cost is typically quite a bit lower than the percentage deletes (50% in this test) because these documents are filtered out at a low level before any of the costly query matchers and scorers see them. The more costly queries (Phrase, Span) tend to see the lowest impact, which is also good because it is the slow queries that determine node capacity for most applications. How About Expunge Deletes? Elasticsearch's , which in turn calls Lucene's method. While this will forcefully reclaim space from deleted documents, this operation is very costly: under the hood, it forces merging of any segments that have more than 10% (by default) deletions. Use it sparingly: it is b","locales":"","title":"Lucene's Handling of Deleted Documents"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2015-01-28T00:00:00.000Z","url":"/blog/2015-01-28this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project.Elasticsearch Core In Apache Lucene this Past Week . presenting at Montreal meetup! — Colin Surprenant (@colinsurprenant) Elasticsearch EcosystemHere's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases. Slides & VideosTaylor Lovett shares how Elasticsearch can make WordPress search sing introduces the ELK stackPour nos amis francophone par David PilatoBritta Weber speaks about , and at . — Fyber (@Fyber) Where to Find UsWe'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking or hosting a meetup, let our Director of Developer Relations, , know!Belgium Germany IsraelThe Tel Aviv-Yafo ELK meetup group will be talking How to Use ELK to Analyze Logs from a Large Production AWS Environment on February 24. These folks are looking for space to meet, so you can host get in touch with the organizers. You can now while they're finding a location.JapanRegistration is already full for the next Elasticsearch Japan Study Session in Tokyo, but you can . The user group will get together on February 13 at 7:30 PM.NorwayJoin the NDC Meetup in Oslo on Feb 18 and hear talk about Data Exploration with — Found (@foundsays) SwedenJust announced: Stockolm Meetup for on 2 Feb. Many thanks to for hosting & speaking! — Leslie Hawthorn (@lhawthorn) Taiwan United Kingdom United States Where to Find You If you're a regular reader of This Week in Elasticsearch, a.k.a TWIES, you're thinking of skipping this section. You may even be thinking to yourself, yes of course I will drop a note on Twitter when I am giving a talk on all things ELK. That's awesome, because we'd like to showcase every meetup, conference presentation and workshop on Elasticsearch, Logstash, and Kibana happening worldwide. And now, we've made it even easier for you to get support for your meetup!Head on over to ! (And we'll still totally send you swag if you're giving a talk on anything ELKy at a conference.)Oh yeah, we're also . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticsearch, Kibana, and Logstash than where you rest your head.TrainingsIf you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elasticsearch - January 28, 2015"}
{"index":{}}
{"author":"Colin Goodheart-Smithe","category":"","publish_date":"2015-01-29T00:00:00.000Z","url":"/blog/numeric-aggregations-an-exploration-of-uk-housing-data","seo_title":"","content":" The Elasticsearch aggregations series continues! In the previous blog posts, we discovered , and . Today, we will explore some of the aggregations available for numeric data. Since this is a developer-focused series, we suggest you follow along at /blog/numeric-aggregations-an-exploration-of-uk-housing-datahome, so that you can play with the data and the various code examples that we show. For this article, we are going to use . This dataset includes details of every house sale in the UK for 2014. To follow along with this blog you will need Elasticsearch version 1.3.0 or later. To follow along, you'll need to restore a Snapshot into your local cluster. The snapshot is about 200MB, and it may take some time depending on your connection: # Register the land registry Repository PUT /_snapshot/demo_uk_landregistry_data { \"type\": \"url\", \"settings\": { \"url\": \"http://data.elasticsearch.org/blogs/data/snapshots/demo_uk_landregistry_data/demo_uk_landregistry_data\" } } # (Optional) Inspect the repository to view available snapshots GET /_snapshot/demo_uk_landregistry_data/_all # Restore the snapshot into your cluster POST /_snapshot/demo_uk_landregistry_data/demo_uk_landregistry_data/_restore # Watch the download progress. GET /housesales/_recovery Once your cluster has finished restoring the Snapshot, let's perform a simple search to see what the data holds: GET housesales/_search { \"_shards\": {...}, \"hits\": { \"total\": 646386, \"max_score\": 1, \"hits\": [ { \"_index\": \"housesales\", \"_type\": \"housesale\", \"_id\": \"AUsHjsE4-ULRehTiURN7\", \"_score\": 1, \"_source\": { \"town\": \"DUNSTABLE\", \"status\": \"A\", \"location\": { \"lat\": 51.882769709, \"lon\": -0.513041822 }, \"district\": \"CENTRAL BEDFORDSHIRE\", \"locality\": \"\", \"price\": 140000, \"housetype\": \"Terraced\", \"oldnew\": \"N\", \"county\": \"CENTRAL BEDFORDSHIRE\", \"duration\": \"Freehold\", \"street\": \"GREAT NORTHERN ROAD\", \"postcode\": \"LU54BN\", \"date\": \"2014-06-19 00:00\", \"paon\": \"43\", \"saon\": \"\" } }, ... ] } } Each document in our index represents an individual property sale in the UK and contains various metadata about the sale such as the price paid, the date of the sale, whether the sale was freehold or leasehold, and various information indicating the location of the property. There are around 650,000 documents in the index so there should be plenty of data to use to identify interesting trends. Let's start our investigation by looking at the dataset as a whole: GET housesales/_search?search_type=count { \"aggs\": { \"house_price_stats\": { \"stats\": { \"field\": \"price\" } } } } Here, we are simply asking for some basic statistics about the price of sales as a whole. This gives us the following response: { ... \"hits\": { \"total\": 646386, \"max_score\": 0, \"hits\": [] }, \"aggregations\": { \"house_price_stats\": { \"count\": 646386, \"min\": 7000, \"max\": 50000000, \"avg\": 257876.91169053785, \"sum\": 166688025440 } } } So, Elasticsearch has reported back the count (the number of sales) and the minimum, maximum, average (mean), and sum of the sales. We could do the same thing by using the min, max, avg, sum, and value_count aggregations, but the stats aggregation provides a convenient way to return all this information together. We can see from the above response that property sales in the UK in 2014 totalled over £166 billion (over $250 billion), with a mean average sale of around £260,000 ($390,000). But this aggregation doesn't tell us much about the distribution of house prices. How much are the few very expensive houses pulling up the mean average value. To answer this we can use the percentiles aggregation. GET housesales/_search?search_type=count { \"aggs\": { \"house_price_stats\": { \"stats\": { \"field\": \"price\" } }, \"house_price_percentiles\": { \"percentiles\": { \"field\": \"price\", \"percents\": [ 50 ] } } } } So now we have added a new aggregation called 'house_price_percentiles' to our previous request and we are using it to ask for the maximum value of a sale tha","locales":"","title":"Numeric Aggregations: An Exploration of UK Housing Data"}
{"index":{}}
{"author":"Martijn Laarman","category":"Engineering","publish_date":"2015-01-28T00:00:00.000Z","url":"/blog/nest-1-4-released","seo_title":"","content":" The .NET team within Elasticsearch is happy to announce that NEST and Elasticsearch.NET 1.4 The big theme for this release was parity. After we released , we went through all the PR’s that made it into and created tickets for them in our own GitHib issues. Within 2 weeks, all of them were closed! This means that you can now use the new , and aggregations, or call the . Check out the for the full list. Stability fixes Rather than releasing before Christmas, we focused on closing as many of the lingering open tickets and stability fixes as possible. We also heard from many of our users that our default connect timeout of was a little too stringent on cloud platforms. Our default timeout is now and when using . Better shield support NEST 1.3 fully supports all the moving parts of Shield, but we found a bug in our that would reintroduce newly found nodes as and not . If you are planning to use shield and , upgrading to is highly recommended. We also added a greater default connect timeout when using , which is now set to . We found that connecting locally over is typically fast enough, but when using remote nodes you need some more leeway built in. Configuring ids in code An , long ignored, has finally made it in! NEST by default infers ‘s on objects when they are not passed explicitly by looking at the property of your object. Previously the only way to override this behavior was through the attribute: [ElasticType(IdProperty=\"AlternateId\"] This change introduced an alternative way to configure id properties, via code: var settings = new ConnectionSettings() .MapIdPropertyFor(o => o.AlternateId):  Observe on backups and restores Much like , that coordinates several long running/sequential calls and exposes them as an , we now offer a similar experience for backup and restore: var interval = TimeSpan.FromMilliseconds(100):  var restoreObservable = this.Client.RestoreObservable(interval, r => r .Repository(_repositoryName) .Snapshot(_snapshotName) .RenamePattern(d + \"_(.+)\") .RenameReplacement(d + \"_restored_$1\") .Index(_indexName) .IgnoreUnavailable(true)):  In this example we will return an that you can subscribe allowing you to monitor restore progress and be notified when it completes (or fails). Many many thanks to who volunteered to implement this new feature. .NET 4.5 builds in our NuGet package. We included specific builds of our DLLs in our NuGet package so that you can now use and on applications that target the desktop CLR (). Support for the Core CLR () is high on our wishlist but since it might mean some breaking changes it won’t be available until we release . Our builds now also have enabled with the help from . See for details on how to set it up. Community feedback Every release blog post we touch on how much we appreciate your contributions, but we really really do! We are pleasantly surprised at the number of you who are on the bleeding edge builds and help us spot errors before they are even released. deserves a special mention here for helping us catch an edge case aggregation parser bug in our bleeding edge builds by providing one of the best , allowing us to fix the bug within hours. As always many of our have been provided by the community, thank you! If you are developing a .NET application with NEST and you need a fix that has not been released on NuGet yet, remember you can always get our bleeding edge builds from . Keep in mind, these builds have been unit tested but have not gone through the rigorous integration testing that we do for our NuGet releases against Elasticsearch release. What’s next? We are going to focus on our branch making it the default branch on GitHub as well. will go into maintenance mode, keeping track with Elasticsearch updates and releases. Our current status-quo means we have 5 different unit/integration tests projects, we have been spiking a unique approach to unify all our testing and documentation needs using Roslyn with something we’ve dubbed . Much of our efforts in 2.0","locales":"","title":"NEST 1.4 Released"}
{"index":{}}
{"author":"Steven Schuurman","category":"News","publish_date":"2015-01-27T00:00:00.000Z","url":"/blog/shield-redefining-can-elk","seo_title":"","content":" When we originally built Elasticsearch, we architected it to serve as a powerful search engine that could easily scale across a distributed environment and be used to both search and analyze any type of data — essentially serve as the backbone for a business to centralize, explore, and extract insights out of any data they want. When we released the software four years ago, however, using a search engine to also perform operational and even business analytics was a completely foreign, somewhat extreme idea — so we let our users make this discovery on their own. Fast forward to nearly 20 million downloads later and 4x customer growth in 2014, businesses worldwide continue to implement Elasticsearch, Logstash, and Kibana for an endless variety of use cases. Adoption started with end user-facing sites like , , and implementing us for search, then moved to businesses like , , and TomTom utilizing Elasticsearch for log analysis. Over the past year, we've started to see ELK stack usage align with our initial vision of powering custom analytics use cases — many that have far exceeded our wildest dreams. built its own analytics tool on top of Elasticsearch to make sure it publishes and presents the right news content on its site at the best time of day, not only leveraging data to make sure its content gets maximum exposure but also to guide its editorial strategy by evaluating which content its audience reads. uses our software to analyze patient data. Financial services firms analyze trade data with Elasticsearch to help advise their clients on investment decisions. Criminal investigative units put forensics data in Elasticsearch to help solve crimes. The list goes on …. Needless to say, businesses continue to put more and more data into the ELK stack to extract insights. In order to meet customer demand to utilize ELK across even more of their business, we're extremely excited to introduce , a security and administrative plugin to the ELK stack that makes it easier for our customers to do even more things with their most valuable asset: their data. Shield's introduction today follows a successful beta with a handful of our users and customers. Some of the new capabilities Shield provides that let businesses do even more with the ELK stack include: Shield is available starting today as part of our Development, Gold, and Platinum subscriptions (which means it's *free* for existing customers!). Those interested in checking it out can download a free 30-day trial at . We couldn't be more excited about the next evolution of both our software stack as well as our business, and hope our users are, too. ","locales":"","title":"Shield: Redefining What You Can Do with ELK"}
{"index":{}}
{"author":"Uri Boness","category":"Engineering","publish_date":"2015-01-27T00:00:00.000Z","url":"/blog/you-know-for-security-shield-goes-ga","seo_title":"","content":" Today we are pleased to announce — the first release of our security plugin for Elasticsearch. While we announced the coming of Shield back in November, today is when the security functionality for Elasticsearch completes the transition that started with a general wish, moved to concrete ideas and execution plan, and is now a reality. While it has always been possible to secure Elasticsearch clusters by deploying them within well-secured environments, we continuously received requests from customers and users to have a more integrated solution. We started exploring what such a product would look like, spending a lot of time making sure we truly understood the security needs of our customers and users. The result is Shield — a commercial Elasticsearch plugin that enables securing Elasticsearch clusters. And we are pleased to include it as part of all our Development, Gold, and Platinum subscriptions at no additional cost. This first release is focused on infrastructure and foundational functionality. We went to great lengths preparing Elasticsearch itself for security, not just on its extensibility side, but also carefully rethinking how the data flows in it. We’ve built a foundation that not only delivers immediate tangible value when it comes to securing Elasticsearch clusters, but also enables us to extend its functionality incrementally and rapidly over time. Features Shield 1.0 focuses on the following five aspects: Authentication Security at large is all about identity (e.g., Who called this API?, What service connected to our system?, etc.). At any given time in the lifetime of a service, one could associate a subject (a.k.a ) with any of the currently running sub-/processes. Having this association mandates that the users will be identified just before any of the sub-/processes start running. The process of identifying the users is called Authentication and it is triggered for every API call in Elasticsearch. There are many different authentication methods, each requires the user to provide different types of credentials by which they’ll be identified (a.k.a. ). In Shield 1.0 we kept it simple and require the authentication token to be in the form of a . (That said, Shield’s authentication infrastructure is built to easily extend this and support other authentication tokens in the future.) Receiving the user credentials is not enough, next we need to verify and authenticate them. In Shield, this is the responsibility of realms. A realm can be seen as an authentication provider/service that can either resolve and verify the relevant user, or reject the authentication token due to incorrect credentials or simply because the user is unknown. Shield’s authentication mechanism enables you to configure multiple realms and chain them together where one realm can serve as a fallback to another. In Shield 1.0 we support three realm types: The realm in the elasticsearch.yml configuration file in the following manner: shield.authc realms: esuser: type: esusers order: 0 ldap: type: ldap order: 1 url: ldaps://url/to/ldap1/server ldap_fallback: type: ldap order: 2 url: ldaps://url/to/ldap2/server As mentioned above, the realms are consulted one at a time in a chain. The per-realm order setting determines the order in which they will be consulted. NOTE: Shield comes with a command-line tool to manage the users stored in the esusers files. Authorization Authorization is the process of granting or denying a user access to a protected resource. Modern systems use the role-based access control (a.k.a RBAC) model to determine user permissions. In this model, each user is associated with a set of roles, where each role defines a set of permissions. This enables sophisticated configuration where permissions can be shared across functional groups. For example, we may define the following roles: Having that, from the finance department may have both employee and finance roles, granting her access to co","locales":"","title":"You Know, for Security: Shield Goes GA"}
{"index":{}}
{"author":"Zachary Tong","category":"Engineering","publish_date":"2015-01-22T00:00:00.000Z","url":"/blog/intro-to-aggregations-pt-2-sub-aggregations","seo_title":"","content":" ","locales":"","title":"Intro to Aggregations pt. 2: Sub-Aggregations"}
{"index":{}}
{"author":"Konrad Beiske","category":"","publish_date":"2015-01-21T00:00:00.000Z","url":"/blog/found-understanding-memory-pressure-indicator","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud.We all know memory is critical to Elasticsearch, but when should you add more? In the console we've included a memory pressure indicator so that you can easily check where your cluster is at in terms of memory usage and capacity. For those familiar with how the JVM garbage collector works: \"The indicator uses the fill percentage of the old generation pool\". In this article I will cover the basics of the old pool and why we chose that as the indicator. ","locales":"","title":"Understanding the Memory Pressure Indicator"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2015-01-26T00:00:00.000Z","url":"/blog/where-in-the-world-is-elasticsearch-6","seo_title":"","content":" After a full month of meetup activity around the world, we're excited to end January with a few more opportunities to talk the ELK stack with our community members. Here's what's on the itinerary in short: Belgium, Germany, Taiwan, United Arab Emirates, the United Kingdom and the United States. We hope to see you at one of these wonderful places! Upcoming EventsEurope January 26-30: - Join for his talk on Thursday, 29, 11:00 a.m. - 11:45 a.m. Alex will discuss . January 29-30: - will be attending talks and would love to talk all things Elasticsearch & the ELK stack with you in the hallway track while sipping a coffee (sponsored by Elasticsearch :)). Make sure to say hello! January 31 - February 1: - Visit us in the on Saturday, January 31, which will include a talk from on . Also, don't miss out on excellent presentation , Sunday, February 1, 3:30 p.m. - 4:15 p.m. Last but not least, you can visit , , and the rest of the Elasticsearch crew at our table in the exhibits hall, Building K, Level 2. Stop by for some awesome knowledge sharing and swag! Upcoming MeetupsNorth America January 29: Europe January 27: January 27: January 29: January 29: January 30: - Pre-FOSDEM Special Edition Asia January 27: January 29: That's it for this week. Stay tuned for Elasticsearch happenings next week - there's much to come! - The Elasticsearch Team P.S.: if you're interested in hosting a meetup or are giving a talk about Elasticsearch - we can offer support and send you swag! ","locales":"","title":"Where in the World Is Elasticsearch? - January 26, 2015"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2015-01-21T00:00:00.000Z","url":"/blog/2015-01-21-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. Elasticsearch core In Apache Lucene this Past Week Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases. \"Cloud years are functionally equivalent to dog years\": ICYMI How we built ElasticSearch cluster on — Mingle (@thatsmingle) I have to say does a lot of things right! Moving NEST over was a pretty smooth experience — Martijn Laarman (@Mpdreamz) Slides & Videos introduces the ELK stack at the recent linux.conf.au conference Phil Wills from TheGuardian.com on how they've scaled to 1M unique browser per month using Elasticsearch & Scala Levi Reich from Thomson Reuters on their Elasticsearch use case, and our very own introduces the ELK stack Where to find Us We'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking or hosting a meetup, let our Director of Developer Relations, , know! Q&A with speakers and team at meetup! — Ryan Roemer (@ryan_roemer) Australia and will be visiting the Melbourne Search Users Group tomorrow. You'll hear from them on an Introduction to the ELK stack and how to contribute to the ELK community. Mark Wallis from Lexer will also be presenting, discussing Search Interfaces and User Experience. You can still for this Jan 21 meetup. Belgium France The 12th Elasticsearch Paris Meetup is on in Paris tomorrow night, but we're currently sold out. You can still . We'll let folks know if spaces open up tomorrow. Germany United Arab Emirates The Hadoop User Group is gathering in Dubai on January 27 for a meetup. Elasticsearch's own will be joining via video conference to give a presentation titled \"Make Sense of your (BIG) data!\" today to save your spot. United Kingdom United States on and right now at — elasticsearch Vienna (@elasticvienna) Where to Find You If you're a regular reader of This Week in Elasticsearch, a.k.a TWIES, you're thinking of skipping this section. You may even be thinking to yourself, yes of course I will drop a note on Twitter when I am giving a talk on all things ELK. That's awesome, because we'd like to showcase every meetup, conference presentation and workshop on Elasticsearch, Logstash, and Kibana happening worldwide. And now, we've made it even easier for you to get support for your meetup! Head on over to ! (And we'll still totally send you swag if you're giving a talk on anything ELKy at a conference.) Oh yeah, we're also . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticsearch, Kibana, and Logstash than where you rest your head. Trainings If you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elasticsearch - January 21, 2015"}
{"index":{}}
{"author":"Leslie Hawthorn","category":"User Stories","publish_date":"2015-01-20T00:00:00.000Z","url":"/blog/scaling-trackjs-with-elasticsearch-for-fun-and-profit","seo_title":"","content":" JavaScript errors are not great, and rarely do they provide you with enough information to identify what went wrong. TrackJS, a JavaScript error reporting service for modern web applications, helps solve this problem. Similar to an airplane's black box, TrackJS captures events from the application, user, and network leading up to the error, so you can recreate and fix the problem. If you're building a JavaScript app, this is an essential tool. In the spirit of knowledge sharing, we wanted to tell our Elasticsearch story. Like many others out there, TrackJS is a small company scaling quickly, and open source solutions can play a critical role in growing your organization without breaking the bank. TrackJS uses Elasticsearch to provide real-time JavaScript error reporting analytics to our customers. It underpins our backend and allows us to slice and dice client-side errors. We've encountered a number of scaling issues as we've grown, and have gained hard-won knowledge in the process. We don't have an unlimited budget — the answer for us is not always “add a bigger box.\" This post is a raw look at how our business has grown to process over 200 million error events per month on a limited budget — and still provide a great user experience. But before you dive in, we invite you to get an in-depth look at how we use Elasticsearch in an . We'll give you the inside scoop on what happened when we hit #1 on HackerNews. (And if you'd like to get even more out of the session, we invite you to grab a free trial of of our software at .) In the Beginning TrackJS started simple, with an application server and database server. The database was a traditional RDBMS with a normalized schema and proper referential integrity. Our application performs many aggregate counts. To do aggregates in SQL inevitably means a GROUP BY, which often equates to a table scan. This negatively impacts performance, and once we hit a few million errors stored, we couldn't afford a large enough box to process things in any reasonable timeframe. We tried removing foreign keys, tweaking the indexes, denormalizing everything, and in the end, we concluded it was the wrong tool for the job. We wanted near real-time analytics, and SQL wasn't going to give it to us. The Right Tool for the Job We auditioned several NoSQL databases to replace our relational database. The test was simple: perform several aggregate counts over one million errors on a single core VM with 1.5GB of RAM, and see who does it best. It was meager hardware, but we were curious to see how various tools would perform. We will not name names here, but many challengers came up short. (One even experienced catastrophic data loss.) When using only the defaults on Windows machines, Elasticsearch was able to handle the challenge without breaking a sweat. Running on Azure TrackJS is primarily hosted on Microsoft Azure. We host our Elasticsearch cluster on a few virtual machines in a single Azure Cloud Service. Azure gives us load balancing out of the box. It does not support multicast, so we explicitly turn it off in our Elasticsearch config and instead rely on the unicast hosts list. Originally, our web tier was not on the same virtual network as our Elasticsearch cluster. Azure exposed a public IP/hostname so it wasn't a big deal (or so we thought). We started experiencing random connectivity issues, but only when connecting to Elasticsearch from our web tier. After lots of diagnostics, we believe it's related to the load balancer/NAT our web tier went through. Today, we connect directly between internal IPs and the system is much more stable. Index Per Customer Next, we had to figure out how to index our error data. Elasticsearch gives you lots of options, which can be overwhelming. We really didn't know what we were doing, or know how to organize documents, so we went with the simplest thing we could think of: assign one index per cust","locales":"","title":"Scaling Trackjs with Elasticsearch for Fun and Profit"}
{"index":{}}
{"author":"Florian Gilcher","category":"","publish_date":"2014-12-24T00:00:00.000Z","url":"/blog/found-full-text-search-is-an-input-oriented-task","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud.Full-text search is an interesting field for backend developers: it requires raw technical knowledge, knowledge about the input languages and lots of feedback gathering on the usability side. In this article, I'd like to give you a rough overview of the challenges in each area and inform you about basic solutions for them. ","locales":"","title":"Full-Text Search is an Input-Oriented Task"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2015-01-14T00:00:00.000Z","url":"/blog/2015-01-14-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project.Elasticsearch core In Apache Lucene this past week analyzing my ~14k emails of the last 8 years with and Kibana — Florian Purchess (@florianpurchess) Elasticsearch EcosystemHere's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases. Slides & VideosGaspar Muñoz and Santiago Mola spoke at the Madrid Elasticsearch meetup on December 15. The video from their talk -- On-the-fly ETL con EFK: ElasticSearch, Flume y Kibana -- is now available, en español!At one the December gathering of the Elasticsearch London MeetUp group, - CTO & Co-founder at Orchestrate.io - shared some great real-world lessons in his talk Where to find UsWe'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking or hosting a meetup, let our Director of Developer Relations, , know!AustriaThe Vienna Elasticsearch user group will be doing a joint meetup with the Vienna DB meetup group on Tuesday, January 20 (in Vienna, of course!). Karel Minarik and Martijn van Groningen of Elasticsearch will both be presenting. Save your spot and !BelgiumIf you're headed out to (Europe's largest gathering of open source developers!) in Brussels at the end of January, be sure to squeeze time into your schedule for the BeLux (Belgium / Luxembourg) Elasticsearch user group on Friday, January 30. Can't make it? Catch us at the Elasticsearch booth, or visit the Open Source Search devroom track at FOSDEM. GermanyOur partner Intrafind is organizing an \"Elasticsearch Expert Talk\" on January 28 in Munich starting at 1pm. You can register using . The Search Meetup Karlsruhe group will be gathering on January 29 for a presentation from Nico Heid on the ELK stack. Check out their for more information and to register for the event.New Zealand and will both be speaking at (also known as LCA) in Auckland. This fantastic event is happening now, and runs through January 16, and is definitely worth attending!The NetherlandsCome to the CRI Service Kennissessie in Houten on January 15 to learn about enterprise logging with Elasticsearch, rsyslog, Docker, and Kibana. Save your place by today.United Arab EmiratesThe Hadoop User Group is gathering in Dubai on January 27 for a meetup. Elasticsearch's own will be joining via video conference to give a presentation titled \"Make Sense of your (BIG) data!\" today to save your spot.United KingdomIf you couldn't decide between the Bath Ruby Battlebot event and the upcoming Elasticsearch meetup, we've got great news: The Elasticsearch meetup has changed dates, so now you can attend both! for this meetup in Bath, now on January 22. There is still a remaining speaking slot, too, so be sure to let the organizer know if you're interested. United StatesThe South Shore .NET user group in Plymouth, Massachusetts, is getting together on January 29 for a presentation on Elasticsearch for data mining. This meetup will be covering Elasticsearch basics and a few other areas, and have you ready to add Elasticsearch to your data analysis toolkit. to save your spot!Tons of folks in SysAdmin Mini Conf to hear on The : Corralling Your Logs — Leslie Hawthorn (@lhawthorn) Where to Find You If you're a regular reader of This Week in Elasticsearch, a.k.a TWIES, you're thinking of skipping this section. You may even be thinking to yourself, yes of course I will drop a note on Twitter when I am giving a talk on all things ELK. That's awesome, because we'd like to showcase every meetup, conference presentation and workshop on ","locales":"","title":"This Week in Elasticsearch - January 14, 2015"}
{"index":{}}
{"author":"Zachary Tong","category":"Engineering","publish_date":"2015-01-13T00:00:00.000Z","url":"/blog/intro-to-aggregations","seo_title":"","content":" global wrapper ","locales":"","title":"Intro to Aggregations"}
{"index":{}}
{"author":"Njal Karevoll","category":"","publish_date":"2015-01-14T00:00:00.000Z","url":"/blog/found-interfacing-elasticsearch-picking-client","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud. When starting using Elasticsearch, it's easy to get confused about all the different ways to connect to Elasticsearch and why one of them should be preferred over the other. In this article we'll provide an overview of the different client types available and give some pointers on when one should be chosen over another. ","locales":"","title":"Interfacing with Elasticsearch: Picking a Client"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2015-01-12T00:00:00.000Z","url":"/blog/where-in-the-world-is-elasticsearch-4","seo_title":"","content":" The new year is still very young but we are already busy again with lots of meetups and events going on. Check them out! Upcoming EventsAustralia January 12-16: - we'll have 2 sessions at this conference: I. will talk about on Tuesday, January 13 from 4:10-4:35 p.m. II. will drop some knowledge about on Wednesday, January 14 from 2:15-3:00 p.m. Upcoming MeetupsNorth America January 14: January 14: January 14: January 15: Europe January 12: January 14: January 15: January 15: Australia January 12: Asia January 13: - The Elasticsearch Team P.S.: if you're interested in hosting a meetup or are giving a talk about Elasticsearch - we can offer support and send you swag! ","locales":"","title":"Where in the World is Elasticsearch? - January 12, 2015"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2015-01-05T00:00:00.000Z","url":"/blog/where-in-the-world-is-elasticsearch-3","seo_title":"","content":" We're excited to kick off 2015 by announcing our winners of the #ElksInTheWild holiday photo contest! Congrats to both of our winners, and thanks for participating. You can expect an Elasticsearch surprise in the next few weeks! Awesome holidays with my lovely . It really went wild : ) — Bharvi Dixit (@d_bharvi) are so fast these days! — Simon Bahuchet (@UncleGarf) Here's what's going on in the Elasticsearch event-land this week. We hope to meet you at one of these places, and watch for more happenings in your area later this month! Upcoming Meetups North America January 8: South America January 7: Europe January 8: Asia January 10: - The Elasticsearch Team P.S.: if you're interested in hosting a meetup or are giving a talk about Elasticsearch - we can offer support and send you swag! ","locales":"","title":"Where in the World is Elasticsearch? - January 05, 2015"}
{"index":{}}
{"author":"Shay Banon","category":"News","publish_date":"2014-12-18T00:00:00.000Z","url":"/blog/its-time-elasticon15-registration-is-finally-open","seo_title":"","content":" It's official: Registration for . Almost a month ago, I gave you an early look at what to expect from our first Elasticsearch user conference happening March 9-11 at Pier 27 in San Francisco, CA. Since then, we've seen a huge, positive response to Elasticsearch hosting this exciting event. The whole company has come together to plan a truly inspiring couple of days, and I'm happy to share even more details with you today about what's to come. First, I can imagine everyone wants to know who is speaking and what topics we will cover. We have put together a preliminary , which outlines a variety of sessions that fall into three main tracks: ELK in the Wild, Developer, and Demo Theater. is entirely dedicated to hearing from you, the user. Many of you out there submitted to speak during this track and share how you're leveraging Elasticsearch, Logstash, and Kibana to do great things. As we've been reviewing the abstracts, we have to say, we'll never get tired of hearing about the amazing work our users are doing. is your opportunity to hear from the people behind the projects you know and love. They will cover a number of themes and give you insight into what you can expect to see in the future. I am super excited about our speaker lineup: Clinton Gormley, Isabel Drost-Fromm, Jordan Sissel, Rashid Khan, Costin Leau, Robert Muir, Boaz Leskes, Spencer Alger, and the list goes on! is our time to work together and learn together. Throughout this track, members of the Elasticsearch team will walk you through live demos of our products. Leslie Hawthorn will be opening this track with a talk about the community. Need I say more? I also want to point out our - and they want to hear from you! So when we aren't on stage speaking, we'll be there to hang out between sessions. You can find us wandering the halls, at our Ask Me Anything stations, as well as hanging out in the developer lounge where we'll host conversations around specific topics and have lightning talks. The is on the Elastic{ON} website. We'll be updating and adding to it in the weeks to come…maybe your name will be on it!? It's been mesmerizing to watch Elasticsearch grow. Elastic{ON} definitely marks a milestone for the company and the products, and we'd love for you to be part of the continuing journey this coming March. (Also, the events team kindly reminded me that space is limited and is sure to fill up fast, so on the Elastic{ON} website ASAP!) ","locales":"","title":"It’s Time! Elastic{ON}15 Registration Is Finally Open!"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-12-17T00:00:00.000Z","url":"/blog/2014-12-17-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. Elasticsearch core Looks like my North American friends are all hanging out together. I wonder what mischief is afoot — logstash (@logstash) Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases. Slides & Videos on DIY Aggregations for Elasticsearch From the December Search Meetup Munich: provides an introduction to Elasticsearch and explores the gotchas of Distributed Systems From the December London Elasticsearch Meetup: The Guardian on how they use Elasticsearch for 100s of searches per second How Logmatic.io uses Elasticsearch in their SaaS log analysis tool (en français) Where to find Us We'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking or hosting a meetup, let our Director of Developer Relations, , know! Germany For folks in or around Bonn on December 18, the Bonn ELK Meetup group will be holding their inaugural meetup. to save your place. Israel The first-ever in Tel Aviv-Yafo will be happening on Tuesday, January 13, and they're kicking off with a great topic: an overview of the Kibana 4 Beta and real-life use cases. Get today! New Zealand and will both be speaking at (also known as LCA) in Auckland. This fantastic event runs January 12 - 16, and is definitely worth attending! And accompanying , we'll be having our first-ever in Auckland on January 12. The schedule is still being finalized, but  to save your spot. Spain Our own  will be joining the Barcelona Meetup on January 8 to share his presentation, . to save your place. The Netherlands Come to the CRI Service Kennissessie in Houten on January 15 to learn about enterprise logging with Elasticsearch, rsyslog, Docker, and Kibana. Save your place by today. United States The Codemash conference in Sandusky, Ohio, will have Itamar Syn-Hershko speaking on making distributed search and analytics on big data . Register to join this session on January 8! and just ready for (flume, ES, Kibana) — Stratio (@StratioBD) Where to Find You If you're a regular reader of This Week in Elasticsearch, a.k.a TWIES, you're thinking of skipping this section. You may even be thinking to yourself, yes of course I will drop a note on Twitter when I am giving a talk on all things ELK. That's awesome, because we'd like to showcase every meetup, conference presentation and workshop on Elasticsearch, Logstash, and Kibana happening worldwide. And now, we've made it even easier for you to get support for your meetup! Head on over to ! (And we'll still totally send you swag if you're giving a talk on anything ELKy at a conference.) Oh yeah, we're also  . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticsearch, Kibana, and Logstash than where you rest your head. Trainings If you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elasticsearch - December 17, 2014"}
{"index":{}}
{"author":"Rashid Khan","category":"Engineering","publish_date":"2014-12-16T00:00:00.000Z","url":"/blog/kibana-4-beta-3-now-more-filtery","seo_title":"","content":" Kibana 4 Beta 3 is out! Along with the usual smattering of small fixes come a few knockout headliners. Once again, we offer you the choice to dive right in by grabbing it here: . However, we suggest you check out the remainder of this post for the feature-by-feature breakdown. Ooo, break it down now! Dance! Interactive Charts and Dashboards Filters are back on the dashboard, and now available in single visualizations too! Bars, points, and pie slices are all clickable and create toggleable filters. We also added some functions to operate on all filters, so you can toggle back and forth between sets with a single click. Scripted Fields Kibana now includes support for Elasticsearch scripting! Not only can you write scripts, you can name them and access them like fields anywhere in the application. Create a scripted field and it becomes part of the documents you view in Kibana as if it was always there. The only catch is that since the script isn't technically part of the Elasticsearch index, you can not search scripted fields. You can, however, use scripts to combine several fields, or perform math on number fields, and then drop the result into a visualization. To help get you started, we've added a handy link in the scripted fields screen titled “Create a few examples from your date fields.\" Find it by heading to the Settings tab's “Index\" section. Select or create an index pattern and click the “Scripted Fields\" tab. Once you've done that, you'll find yourself with a few new numeric fields available for use in aggregations. For example, we can look at the 24 hours that make up the day, and get the total hits for all of them across 30 days: Highlighting and a New Format for _source JSON is great. We all love JSON. Who doesn't love JSON? XML, that's who, but that's beside the point. JSON can be a bit of a jumble to look at full time, so we've taken to nicely formatting it. The raw JSON for the event is, of course, always available by expanding the record's row and clicking over to the JSON tab. Oh, and while we were at it, we threw in highlighting, too. Kibana will now automatically highlight matching fields, and even march them to the front of the line: Hit Links Maybe you noticed that little in the screenshot above? You might not need to share a visualization or a search, you simply need someone to see just one important hit. Now, it's easy! Metric Visualization Sometimes you don't need a chart a document! You just need that , on a dashboard, right now. And here it is: Ok, there it is! Enjoy! As always, ping us on with issues, suggestions and contributions. Or, if you love IRC like we do, join us in #kibana on Freenode. ","locales":"fr-fr","title":"Kibana 4 Beta 3: Now More Filtery"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2015-01-19T00:00:00.000Z","url":"/blog/where-in-the-world-is-elasticsearch-5","seo_title":"","content":" Hello Elasticsearch Community! We have got a full host of meetups ahead of us this week. Check out where we are! Upcoming MeetupsNorth America January 19: January 20: Europe January 20: January 22: January 22: Australia January 22: There's more to come next week, plus we are heading to FOSDEM! - The Elasticsearch Team P.S.: if you're interested in hosting a meetup or are giving a talk about Elasticsearch - we can offer support and send you swag! ","locales":"","title":"Where in the World Is Elasticsearch? - January 19, 2015"}
{"index":{}}
{"author":"David Pilato","category":"News","publish_date":"2015-01-12T00:00:00.000Z","url":"/blog/google-cloud-platform-delivers-real-time-search-with-click-to-deploy-elasticsearch","seo_title":"","content":" From the beginning, one of the goals of Elasticsearch was to We've achieved another milestone on that mission with that Elasticsearch (plus Marvel!) is available as a click-to-deploy install on Google Compute Engine (GCE). This means that Elasticsearch can be automatically provisioned on GCE in minutes, giving businesses powerful search and immediate insights into the workloads they're running on Google's infrastructure — from full-text application search to analytics for operational insights. To give it a spin, head on over to .It's been a big year for us in terms of being selected by a variety of key infrastructure players as the search and analytics engine they recommend to their customers. Over the summer, our became certified on , making Elasticsearch compatible across all Apache-based Hadoop distributions thanks to previous partnerships with and . as the underlying full-text search engine for its newly released Azure Search, and we also announced a to provide real-time search and analytics to its Unified Computing System customers. We couldn't be more thrilled, and humbled, that industry leaders like these, along with thousand of businesses worldwide (like Target, Mayo Clinic, Facebook, GitHub, Netflix, Verizon, and PayPal), continue to benefit from our software for mission-critical use cases. We look forward to continuing to help them mine and extract value from their most valuable asset: their data.P.S. We also love a good story. Are you using Elasticsearch on GCE? Tell us about it via or on . ","locales":"","title":"Google Cloud Platform Delivers Real-Time Search with Click-to-Deploy Elasticsearch "}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2015-01-07T00:00:00.000Z","url":"/blog/2015-01-07-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. Elasticsearch core are so fast these days! — Simon Bahuchet (@UncleGarf) Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases. Slides & Videos Bhaskar Karambelkar of Verizon shared his tips on scaling Elasticsearch for production-scale data at the Washington, D.C. meetup on December 11. From the Elasticsearch meetup in Madrid, Spain on December 15: On-the-fly ETL con EFK: Elasticsearch, Flume, Kibana (en español), by Gaspar Muñoz and Santiago Mola. Where to find Us We'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking or hosting a meetup, let our Director of Developer Relations, , know! Belgium If you're headed out to (Europe's largest gathering of open source developers!) in Brussels at the end of January, be sure to squeeze time into your schedule for the BeLux (Belgium / Luxembourg) Elasticsearch user group on Friday, January 30. Can't make it? Catch us at the Elasticsearch booth, or visit the Open Source Search devroom track at FOSDEM. Germany will talk about systems on January 13 in Berlin. Very limited space, so please register. Israel The first-ever in Tel Aviv-Yafo will be happening on Tuesday, January 13, and they're kicking off with a great topic: an overview of the Kibana 4 Beta and real-life use cases. Get today! New Zealand and will both be speaking at (also known as LCA) in Auckland. This fantastic event runs January 12 - 16, and is definitely worth attending! And accompanying , we'll be having our first-ever in Auckland on January 12. The schedule is still being finalized, but  to save your spot. Spain Just a few days away - our own  will be joining the Barcelona Meetup on January 8 to share his presentation, . to save your place - it's filling up quickly! The Netherlands Come to the CRI Service Kennissessie in Houten on January 15 to learn about enterprise logging with Elasticsearch, rsyslog, Docker, and Kibana. Save your place by today. United Arab Emirates The Hadoop User Group is gathering in Dubai on January 27 for a meetup. Elasticsearch's own will be joining via video conference to give a presentation titled \"Make Sense of your (BIG) data!\" today to save your spot. United States The Codemash conference in Sandusky, Ohio, will have Itamar Syn-Hershko speaking on making distributed search and analytics on big data . Register to join this session on January 8! If you're in Arizona, come to the Elasticsearch meetup at the GoDaddy offices in Scottsdale on Wednesday, January 14 for a great lineup of talks, including a Q & A session with Elasticsearch CTO Shay Banon. Get registered for what is sure to be a great meetup! The South Shore .NET user group in Plymouth, Massachusetts, is getting together on January 29 for a presentation on Elasticsearch for data mining. This meetup will be covering Elasticsearch basics and a few other areas, and have you ready to add Elasticsearch to your data analysis toolkit. to save your spot! Finally, the Washington, D.C. Elasticsearch meetup group will have an \"Unconference\" meetup on January 14. Not sure what an unconference is? In short, it's a conference format where rather than having a structured schedule of talks decided in advance, the schedule is decided onsite, by the attendees themselves - which ensures that the content presented is of value and interest to the people who show up. Intrigued? for this specially planned meetup - we're sure you'll have a great time! Awesome hol","locales":"","title":"This Week in Elasticsearch - January 07, 2015"}
{"index":{}}
{"author":"Boaz Leskes","category":"News","publish_date":"2014-12-17T00:00:00.000Z","url":"/blog/marvel-1-3-0-released","seo_title":"","content":" Today, we are happy to announce the release of . A lot has happened in Elasticsearch (including the release of Elasticsearch 1.4) since the previous release of Marvel. This version adds monitoring for these new features, e.g. query cache and the new circuit breakers. On top of that, Sense's knowledge base was extended to include the latest API. We also added HTTPs support, preparing Marvel for the up-and-coming release of , our security product. To upgrade, you must install the latest Marvel plugin on all of your Elasticsearch nodes. As with any other Java plugin, you will need to restart each node (one by one) in order for the newer version of Marvel to become active. The upgrade process is described in more detail in the . To conclude, here is complete list of all the goodness that went into this release: Agent Monitoring UI Sense As always, we welcome all feedback so we can continue improving Elasticsearch Marvel. Please send questions, praise, or pains to the or find us on . ","locales":"","title":"Marvel 1.3.0 Released"}
{"index":{}}
{"author":"Clinton Gormley","category":"Engineering","publish_date":"2014-12-16T00:00:00.000Z","url":"/blog/elasticsearch-1-4-2-released","seo_title":"","content":" Today, we are happy to announce the bug fix release of  , based on , and the bug fix release of . You can download them and read the full changes list here: For blog posts about past releases see: You can read about all of the changes that have been made in the and the , but we highlight some of the more important changes below: ","locales":"","title":"Elasticsearch 1.4.2 and 1.3.7 Released - December 16, 2014"}
{"index":{}}
{"author":"Jordan Sissel","category":"Engineering","publish_date":"2014-12-12T00:00:00.000Z","url":"/blog/plugin-ecosystem-changes","seo_title":"","content":" With the release of Logstash 1.5.0 Beta 1 (), we are changing the way plugins are installed, maintained, and published. We have taken the feedback from our loverly users and community over time and our goal is to make it easier to use and develop plugins. This project is only the beginning — we will be iterating on this idea to provide a one-stop solution to discover and share a community of plugins! In this blog we would like to explain our reasons behind this decision, take you through the new workflow and future roadmap we have in mind. There’s a plugin for that! Logstash has a rich collection of plugins (inputs, filters, outputs, and codecs) which are both developed by Elasticsearch and contributed to by the community. One of the core strengths of Logstash is the availability of these plugins and ease of adding new ones to extend behavior. Today, there are over 165 plugins in the ecosystem which are split across two projects: New Plugin Ecosystem Changes the plugins will now be separated out from Logstash core into their own self-contained packages using rubygems. We chose rubygems because of its powerful and standard way to package and distribute libraries with dependencies. Plus, we will be leveraging the platform for publishing and discovery. We have also added infrastructure to easily install, update, and remove plugins on top of Logstash. The project will slowly cease to exist — all plugins will have a single home. Reasons to move to the Plugin Ecosystem Having a large set of plugins exposes unique challenges in distribution and publishing. Some of the pain points which inspired us to make these change were: The Details: Source Code Location Logstash source code will continue to exist in its current GitHub but will not have any code or tests source related to plugins. Having this separation allows us to focus on core as well as to iterate quickly on individual plugins, which will improve the overall quality of Logstash project. The source code for all plugins will be located in a new GitHub organization and each plugin will exist as individual repositories under it. At first blush, this may seem hard to maintain, but it provides clear isolation for tests, issues and dependencies. Our aim is to automate testing, documentation, and gem publishing and provide additional tooling to ease this move. However, it is not necessary for developers to host their plugin source under this organization — it is only necessary to publish it to to make it available to our community. Workflow Below we describe the interaction/workflow with the new plugin ecosystem from various perspectives: Logstash Users: Users will download the Logstash binary similar to previous releases. Logstash 1.5.0 will ship with the same set of plugins packaged with 1.4.2 to ease the migration to the new system. Additionally, users will be able to install and upgrade any Logstash plugin after the initial deployment. script will be used for all plugin lifecycle interaction Most of the plugins will have their gems uploaded to . For example, if the user has to install the Apache Kafka plugin - , using a file location: Documentation Even though the plugins are separated, documentation for all plugins will be in one central Logstash Plugin Developers: Plugin developers and authors will be able to publish plugins for the Logstash ecosystem. Plugins will be able to declare external dependencies on gems and/or java libraries. More importantly, developers will be able to release improvements to plugins outside of the release cadence of Logstash. Rubygems technology was chosen for the packaging system, dependency management, and hosting facility. Developers familiar with publishing regular Ruby gems will be able to easily publish Logstash plugins. Elasticsearch provides and maintains the tooling to aid developers with these functions. Developing and testing locally JRuby is the only prerequisites for developing the plugin. Providing a pa","locales":"","title":"Exciting Logstash Plugin Ecosystem Changes"}
{"index":{}}
{"author":"Suyog Rao","category":"Engineering","publish_date":"2014-12-11T00:00:00.000Z","url":"/blog/logstash-1-5-0-beta1-released","seo_title":"","content":" global wrapper ","locales":"","title":"Logstash 1.5.0.Beta1 Released"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-12-10T00:00:00.000Z","url":"/blog/2014-12-10-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project.Elasticsearch Core Ever tried running multiple instances of Elasticsearch on a single host? explains how we do it. — Etsy Engineering (@codeascraft) Elasticsearch EcosystemHere's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases. // <!--[CDATA[<br /--> <span id=\"mce_marker\" data-mce-type=\"bookmark\"></span><span id=\"__caret\">_</span><br> // ]]&gt:  Slides & Videos at last week's Elasticsearch meetup in Chicago, presenting \"Extending your logs: Why not make your logs do the legwork?\"Greg DeKoenigsberg, VP of Community at Ansible, was also caught on video at last week's meetup in Chicago, walking the audience through the steps for how he set up Elasticsearch + Logstash with Ansible for his very own analytics purposes.Where to Find UsWe'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking or hosting a meetup, let our Director of Developer Relations, , know!FranceThe Elasticsearch France Meetup will return to Paris for Meetup #11 on December 17. This meetup always fills quickly, so to save your place.GermanyNext meetup at munich office will have talks about Elasticsearch & search quality at 15th Dec — Alexander Reelsen (@spinscale) And, for folks in or around Bonn on December 18, the Bonn ELK Meetup group will be holding their inaugural meetup. Register now to save your place.Spain15/12/14: On-the-fly ETL con EFK: , Flume y | — Adesis Netlife (@adesis) Additionally, our own  will be joining the Barcelona Meetup on January 8 to share his presentation titled, \"Make sense of your data with Elasticsearch.\" to save your place.United States Where to Find You If you're a regular reader of This Week in Elasticsearch, a.k.a TWIES, you're thinking of skipping this section. You may even be thinking to yourself, yes of course I will drop a note on Twitter when I am giving a talk on all things ELK. That's awesome, because we'd like to showcase every meetup, conference presentation and workshop on Elasticsearch, Logstash, and Kibana happening worldwide. And now, we've made it even easier for you to get support for your meetup!Head on over to ! (And we'll still totally send you swag if you're giving a talk on anything ELKy at a conference.)Packed room for the first meetup in Lyon! — Tanguy Leroux (@tlrx) Oh yeah, we're also . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticsearch, Kibana, and Logstash than where you rest your head.TrainingsIf you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elasticsearch - December 10, 2014"}
{"index":{}}
{"author":"Greg Marzouka","category":"Engineering","publish_date":"2014-12-09T00:00:00.000Z","url":"/blog/nest-and-elasticsearch-net-1-3","seo_title":"","content":" Last week, we released NEST and Elasticsearch.NET 1.3, while not one of our most action-packed releases, during its postmortem we realized there were quite a few important features and fixes that warranted a blog post! New Security Features: You may have heard that Elasticsearch is coming out with a new security product, . Shield offers a rich set of features for securing your Elasticsearch cluster, and we took the initiative to make sure NEST and Elasticsearch.NET will have full support for Shield when it’s officially released. Prior to 1.3, basic authentication credentials could only be specified on the URI like so: var uri = new Uri(\"http://username:password@localhost:9200\"):  var settings = new ConnectionSettings(uri):  This isn’t the best way to manage credentials in your application, and more importantly it doesn’t allow you to specify different credentials per request. Now you can specify basic authentication credentials for all requests at the global level as follows: var uri = new Uri(\"http://localhost:9200\"):  var settings = new ConnectionSettings(uri):  .SetBasicAuthentication(\"user\", \"password\"):  Or per request, overriding any credentials set at the global level: var response = client.Search<MyClass>(s => s .MatchAll() .RequestConfiguration(rc => rc .BasicAuthentication(\"anotheruser\", \"password\") ) ):  The client is thread-safe, so you can use a single client, in which case, passing a per request configuration is the only way to pass state local to the request. Instantiating a client each time is also supported. In this case each client instance could hold a different object with their own set of basic authorization credentials. Do note that if you new a client each time (or your IoC does), they all should use the same instance. Now that an is a valid response from Elasticsearch, we had to make some modifications to our connection pooling retry logic in order to fail as quickly as possible on responses, and not retry the request on other nodes. New Property Name Mapping API: Prior to 1.3, expressions (e.g ) could only be controlled using the attribute, or the shotgun which happens too late in the pipeline to know which property on what type it actually is. Therefore, we introduced a third approach to override what a strongly typed expression resolves to: settings.MapPropertiesFor<MyClass>(props => props .Rename(p => p.Foo, \"bar\") ):  This allows you to refer to the property using the expression in any of the API calls, but resolving the property name to before being sent to Elasticsearch. Additionally, before 1.3, the only way to exclude properties from being mapped was to use or Json.NET’s attribute on the property. Now with this new API, you can also ignore properties in code: settings.MapPropertiesFor<MyClass>(props => props .Rename(p => p.Foo, \"bar\") .Ignore(p => p.Bar) ):  Other New Features: We received an influx of pull requests from our community, so a special thank you to everyone who contributed to this release! Just some of the new features added in 1.3… See the for the complete list of changes. what’s next? brings a ton of new APIs and aggregations. We plan on mapping all of them in our 1.4 release of NEST and Elasticsearch.NET. For a list of all the features that are currently planned, just filter by the on our GitHub issues page. This list is constantly growing, so please feel free to add to it, or let us know if we’re missing anything. It’s an exciting time to be a .NET developer! If you’ve been keeping up with the latest news, then you know that .NET is now open source and Microsoft is cooking up some awesomeness with , an exciting alternative, leaner stack to develop .NET applications on. Starting with NEST 1.4 we will include a proper .NET 4.5 build that works on and up. We are also exploring adding support for the new cross platform core CLR () in , though support for this is not a blocker for a 1.4 release of NEST. That’s all from the .NET corner at Elast","locales":"","title":"Nest and Elasticsearch.Net 1.3"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2014-12-08T00:00:00.000Z","url":"/blog/where-in-the-world-is-elasticsearch-elksinthewild-holiday-special","seo_title":"","content":" You may remember our <#ElksInTheWild> summer competition that we did right before the summer break. We had so much fun seeing all these <#ElksInTheWild>, we thought we should do it again! Instructions are outlined below - and remember - the best picture wins! Here's what's going on in the Elasticsearch event land before the holidays. We hope to meet you at one of these places. Upcoming Meetups North America December 9: December 9: December 9: December 11: Europe December 9: December 10: December 15: December 15: December 17: December 18: Asia December 13: That's it for now and until we meet again in 2015. Stay tuned for a lot more to come in the new year. We'll be starting our world travels again and discovering new places, too! - The Elasticsearch Team P.S.: if you're interested in hosting a meetup or are giving a talk about Elasticsearch - we can offer support and send you swag! ","locales":"","title":"Where in the World is Elasticsearch? #elksinthewild Holiday Special!"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-12-03T00:00:00.000Z","url":"/blog/2014-12-03-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. Elasticsearch core Pre-registration for is closing soon. Get your name {ON} the list before it's too late. See you in March! — elasticsearch (@elasticsearch) Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases. We just released NEST 1.3.0 .net client to ! See whats new here: — Martijn Laarman (@Mpdreamz) How is enabling scientists worldwide to better share & collaborate on the next frontier of discoveries? — elasticsearch (@elasticsearch) Slides & Videos on Lucene 5 at our latest Washington, DC Elasticsearch Meetup Short and sweet: Demo of Weave to tie together Elasticsearch, Docker, and Apache Spark From the latest London Elasticsearch Meetup From this week's Women Who Code Austin sessions on Item Based Search with Elasticsearch (en français) Where to find Us We'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking or hosting a meetup, let our Director of Developer Relations, , know! France Germany Next meetup at munich office will have talks about Elasticsearch & search quality at 15th Dec — Alexander Reelsen (@spinscale) And, for folks in or around Bonn on December 18, the Bonn ELK Meetup group will be holding their inaugural meetup. to save your place. The Netherlands Elasticsearch NL meetup schedule for Dec 9th: Sinterklaas/X-Mas/H­­anukkah drinks RSVP on — Boaz Leskes (@bleskes) Spain No te pierdas On-the-fly ETL con EFK: , Flume y [15/12/2014] | Telefónica Flagship Store — Adesis Netlife (@adesis) United Kingdom The London Elasticsearch Meetup will convene on December 10 at 6:30 PM. This one always fills up quickly, so to save your place. United States In NYC? will Intro you to the stack, using local car traffic data, at Dec 9th meetup — Leslie Hawthorn (@lhawthorn) PDX folks join & to learn about , 's SIEM platform & Shield on Dec 9th — Leslie Hawthorn (@lhawthorn) Where to Find You If you're a regular reader of This Week in Elasticsearch, a.k.a TWIES, you're thinking of skipping this section. You may even be thinking to yourself, yes of course I will drop a note on Twitter when I am giving a talk on all things ELK. That's awesome, because we'd like to showcase every meetup, conference presentation and workshop on Elasticsearch, Logstash, and Kibana happening worldwide. And now, we've made it even easier for you to get support for your meetup! Head on over to ! (And we'll still totally send you swag if you're giving a talk on anything ELKy at a conference.) \" speed is inverted indexing\" talk cc — Women Who Code ATX (@wwcodeatx) Oh yeah, we're also . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticsearch, Kibana, and Logstash than where you rest your head. Training If you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elasticsearch - December 03, 2014"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2014-12-01T00:00:00.000Z","url":"/blog/where-in-the-world-is-elasticsearch-2","seo_title":"","content":" It's December and Christmas is getting closer. But there are still a few things happening in the world of Elasticsearch. Check them out! Here's just a sneak peek from last weeks where talked about the company culture at Elasticsearch. Fire side chat with at TLV about company culture — Boaz Leskes (@bleskes) Upcoming EventsEurope November 30-December 1: (Israel) - Join talking about TODAY 11:00 a.m. - 11:40 a.m. Upcoming MeetupsNorth America December 2: December 2: December 4: Europe December 2: December 3: December 4: That's it for this week. Stay tuned for a Christmas special next week! ...and just a few final words from from last weeks : Admin guide for by — Filip Zrůst (@frzng) P.S.: if you're interested in hosting a meetup or are giving a talk about Elasticsearch - we can offer support and send you swag! ","locales":"","title":"Where in the World Is Elasticsearch? - December 01, 2014"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-11-26T00:00:00.000Z","url":"/blog/2014-11-26-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project.Elasticsearch Core ICYMI: - our 1st user conference - is coming to SF March 9-11, 2015. Save your spot today. — elasticsearch (@elasticsearch) Elasticsearch EcosystemHere's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases. How does manage, monitor, & innovate a video content website w/500k visitors? See how the helps… — elasticsearch (@elasticsearch) Slides & VideosThe slides from my capacity planning talk @ the Elasticsearch Meetup in Tel Aviv are online on .Until the next time… — Boaz Leskes (@bleskes) Slides for my talk at about are out! Have fun :) — David Pilato (@dadoonet) Slides from our last Elasticsearch NL meetup are online. ING: , RIPE: — Boaz Leskes (@bleskes) Where to Find UsWe'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking or hosting a meetup, let our Director of Developer Relations, , know!Czech RepublicOur very own and will speak at the next Elasticsearch CZ Meetup, taking place in Prague tomorrow, November 27 from 7-10 PM. to save your place. France Germany:, Karlsruhe, Nov. 26-28: Patrick Peschlow on .Next tuesday I'll talk about performance (OS/JVM/Application) with Elasticsearch at the Software Performance Meetup — Alexander Reelsen (@spinscale) : And for folks in or around Bonn on December 18, the Bonn ELK Meetup group will be holding their inaugural meetup. to save your place.IndiaAttention Delhi! We have our 1st meetup coming up this week! 29 Nov, 2pm . Sign up now! — Livia Froelicher (@LivFroe) ItalyJoin at CodeMotion Milan, November 26-29. David will teach you all about .IsraelFresh from his performance at CodeMotion Milan, the intrepid will be reprising his staring role in at CodeMotion TelAviv on Monday, December 1.United Kingdom United States Don't miss the next Chicago meetup hosted here at Vodori on 12/2 from 6-8pm — vodori, inc. (@vodori) Where to Find You If you're a regular reader of This Week in Elasticsearch, a.k.a TWIES, you're thinking of skipping this section. You may even be thinking to yourself, yes of course I will drop a note on Twitter when I am giving a talk on all things ELK. That's awesome, because we'd like to showcase every meetup, conference presentation and workshop on Elasticsearch, Logstash, and Kibana happening worldwide. And now, we've made it even easier for you to get support for your meetup!Head on over to ! (And we'll still totally send you swag if you're giving a talk on anything ELKy at a conference.)Oh yeah, we're also . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticsearch, Kibana, and Logstash than where you rest your head.TrainingsIf you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elasticsearch - November 26, 2014"}
{"index":{}}
{"author":"Konrad Beiske","category":"","publish_date":"2014-11-26T00:00:00.000Z","url":"/blog/found-elasticsearch-upgrade-guide","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud.Upgrading a system is one thing. Usually it involves taking a backup, doing the upgrade and then verifying everything is OK. Upgrading a system that your software depends on can be quite a different experience. In particular when the task is long overdue. ","locales":"","title":"Elasticsearch Upgrade Guide as Seen by the Client"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2014-11-24T00:00:00.000Z","url":"/blog/where-in-the-world-is-elasticsearch","seo_title":"","content":" After a great couple of days at in Barcelona where we gave away a limited edition of the first Elasticsearch book with and the authors - & - signatures, we are now ready for another week full of various Elasticsearch events! THE book signed by & the authors! RT this tweet & you are in for a win at BCN! — elasticsearch (@elasticsearch) Upcoming Events Europe November 26-28: (Germany) - will be there to show some shiny Kibana demos and Patrick Peschlow will give a talk on . November 28-29: (Italy) - will teach you all about . Friday, November 28, 2:00 p.m. Upcoming Meetups North America November 29: Europe November 24: November 24: November 27: Asia November 29: That's it for this week. Stay tuned for Elasticsearch happenings next week! It may calm down a bit during Christmas time and especially in the US we wish you a very Happy Thanksgiving! - The Elasticsearch Team P.S.: if you're interested in hosting a meetup or are giving a talk about Elasticsearch - we can offer support and send you swag! ","locales":"","title":"Where in the World Is Elasticsearch? - November 24, 2014"}
{"index":{}}
{"author":"Clinton Gormley","category":"Engineering","publish_date":"2014-11-26T00:00:00.000Z","url":"/blog/elasticsearch-1-4-1-released","seo_title":"","content":" Today, we are happy to announce the bug fix release of , based on , and the bug fix release of . You can download them and read the full changes list here: For blog posts about past releases see: You can read about all of the changes that have been made in the and the , but we highlight some of the more important changes below: ","locales":"","title":"Elasticsearch 1.4.1 and 1.3.6 Released"}
{"index":{}}
{"author":"Shay Banon","category":"News","publish_date":"2014-11-20T00:00:00.000Z","url":"/blog/its-on-announcing-our-first-user-conference-elasticon15","seo_title":"","content":" It's been a little over two years since we formed a company around Elasticsearch, and the engagement with our community, users, and customers has taken on a life of its own. There are now around the globe, hundreds of conferences featuring our products, and a growing list of events where our own developers engage audiences in the Elasticsearch story. It's clear we hit a nerve. Over and over, we kept hearing one question: “When will Elasticsearch get a conference of its own?\" We listened, and I am happy to announce the first Elasticsearch conference, , is happening March 9 through 11, 2015 in San Francisco, California. The conference details are unfolding as we speak, but there are a few things we already have planned that I want to share with you. First, Elastic{ON} will be centered around and the ecosystem of products surrounding it, including Apache Lucene, , , the various , , , and . Part of what makes Elasticsearch tick is the close communication we have with our users. To that extent, we're doing a few things to make sure the conference is run the same way. What does this mean for you? It means that *all* the developers at our company (that's right, every single one of them) will be attending the conference — and they want to hear from you. Elastic{ON} will feature a dedicated track that gives you a unique opportunity to talk with our engineers about all the work they currently do and plan to do. Afterwards, we're coordinating an Elasticsearch dev all hands meeting where we'll discuss your feedback and apply it to future products and events. The second aspect of the conference is hearing you, the user, speak about how you use our platform. I am lucky enough to be able to travel the world and talk to users and customers frequently, and am continuously amazed by how our products are being put to use. We plan to create a platform for our users, customers, and contributors in the community to talk about their use cases and successes. Elastic{ON} will be a great way to meet and talk with other users in your space and share knowledge. Please, if you're interested, don't hesitate to at the conference. We will also have a hands-on track with our developers, who will go through some high-level overviews and technical deep dives of our various products. Or you can drop by our \"Agents of Elasticsearch\" station to ask any questions that are on your mind. Bottom line: Elastic{ON} is all about you! And obviously, we plan to have a lot of fun while we're together. I am super excited about the conference, and I hope you are as well. I would love to personally welcome each and every one of you to join us:  it's going to be great. (And make sure to to save your spot – my events team keeps reminding me that registration will fill up fast!) ","locales":"","title":"It’s {ON}: Announcing Our First User Conference - Elastic{ON}15"}
{"index":{}}
{"author":"Chris Earle","category":"Engineering","publish_date":"2014-11-26T00:00:00.000Z","url":"/blog/making-elasticsearch-groovy-er","seo_title":"","content":" The Groovy community has been asking for us to ensure it’s an officially supported language once again, and we’ve been listening. With the recent release of , we are also excited to announce the release of the official Elasticsearch Groovy client (1.4.1), which is fully compatible with Elasticsearch 1.4.1. Starting today, the Groovy client becomes an officially supported client hosted on . The old, long-defunct Groovy client has been completely rewritten for this release in order to guarantee 100% Java client compatiblity. Unlike before, any feature that exists in the Java client is now guaranteed to exist in the Groovy client. New Features The More You Know The new and improved Groovy client does away with the old, Groovy-friendly variants of Java client objects in favor of using . Groovy extension modules provide the excellent ability to add new methods to existing classes and the Groovy client takes full advantage of them. Thanks to the use of the extension modules, Java client examples are 100% compatible with the Groovy client, which means that any Groovy project can transition to using the Groovy client where when it is convenient without having to make hard decisions about features. Naturally, any new Groovy code can be written to take full advantage of the new client immediately. What does 100% compatibility mean? Having 100% compatibility means that you can will use the same code as the Java client, often with added Groovy-isms as opposed to custom-written Groovy equivalents (ye olde will be missed … but hopefully forgotten). From development of the Groovy client’s perspective, this means that there is less code to write and test, and superior usability. And, from your perspective, it hopefully means less new code to learn and no worrying about missing any functionality as future versions are released. // No Groovy imports! import org.elasticsearch.action.search.SearchResponse import org.elasticsearch.client.Client import static org.elasticsearch.node.NodeBuilder.nodeBuilder // Create a client node using a mix of the Java NodeBuilder and Groovy extensions Client client = nodeBuilder().client(true).settings { cluster { name = \"my-cluster-name\" } arbitrary { setting = \"arbitraryValue\" } }.node().client // Perform a search on your cluster using a Closure! SearchResponse response = client.search { indices \"index1\", \"index2\" types \"type1\", \"type2\" source { query { match_all { } } } }.actionGet() Making Life Groovy-er As with earlier incarnations of the Groovy client, support for treating a Groovy as data — instead of as a block of code — is a core concept. This allows you to do things that are sometimes verbose when using the Java client by using a Groovy . In your Java code, you might see something akin to: import static org.elasticsearch.common.xcontent.XContentFactory.*:  XContentBuilder builder = jsonBuilder() .startObject() .field(\"user\", \"kimchy\") .field(\"postDate\", new Date()) .field(\"message\", \"trying out Elasticsearch\") .endObject() String json = builder.string() However, if you start to use the Groovy client, then you could replace this with an equivalent : String json = { user = \"kimchy\" postDate = new Date() message = \"trying out Elasticsearch\" }.asJsonString() No code . Just Groovy magic! And thanks to this feature, the Groovy client makes full use of passing a into your Elasticsearch requests as well: import org.elasticsearch.action.ListenableActionFuture def username = \"kimchy\" ListenableActionFuture<IndexResponse> responseFuture = client.index { index \"my-index\" type \"my-type\" id \"my-id\" source { user = username postDate = new Date() message = \"Trying out Elasticsearch Groovy\" nested { nested_object { some_int = 123 some_double = 4.56 some_object_list = [{ key = \"Closures\" }, { key = \"Beats\" }, { key = \"Bears\" }] } favorites = Integer.MAX_VALUE } } } This enables very powerful flows for submitting requests to your Elasticsearch cluster(s) by allowing you to use a instead of a o","locales":"","title":"Making Elasticsearch Groovy-er"}
{"index":{}}
{"author":"Leslie Hawthorn","category":"Engineering","publish_date":"2014-11-25T00:00:00.000Z","url":"/blog/white-paper-testing-automation-for-distributed-applications","seo_title":"","content":" Over the last few weeks, published a series of blog posts about how we go about testing and quality assurance (QA) for Elasticsearch. From continuous integration to the various levels of testing to performing randomized test runs, the importance of testing and QA for Elasticsearch cannot be understated. In addition to these great articles, Isabel has also authored a for those who'd like to go even more in-depth. Get . Get testing. And enjoy! ","locales":"","title":"White Paper: Testing Automation for Distributed Applications"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-11-19T00:00:00.000Z","url":"/blog/2014-11-1-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. Elasticsearch core Thurs. Barcelona meetup: reg now for Q&A w/ — Leslie Hawthorn (@lhawthorn) Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases. Congratulations to the Chocolatey Team on their successful Kickstarter! We're proud to have contributed. We are celebrating .NET going OSS by backing — Martijn Laarman (@Mpdreamz) Elastically Searching w00t! 1.2 terabytes of event data 1.7 billion event documents — David Pilato (@dadoonet) Slides & Videos Our very own , creator of Logstash, brings you an intro to ELK Christine Flood on Shenandoah and how the project uses Elasticsearch Our very own David Pilato on Elasticsearch and Drupal (en français) アップしました。 \"niconicoの検索を支えるElasticsearch\" — shoito (@shoito) Where to find Us We'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking or hosting a meetup, let our Director of Developer Relations, , know! Canada I'll be talking about , and at -- ELK != Plain 'Ol Syslog - More info: — Jason Kendall (@coolacid) Czech Republic Our very own and will speak at the next Elasticsearch CZ Meetup, taking place in Prague on November 27. to save your place. France Germany We have a number of upcoming events in Germany: India The first ever Elasticsearch Delhi meetup is coming up on November 29. to save your place and to hear about several real world Elasticsearch use cases. The organizers are looking for additional speakers, so don't hesitate to contact them to volunteer! Israel and will both take the stage at DevOps Days Tel Aviv, which takes place November 23-24. Shay will hold a fireside chat on Dev, Ops and Culture, and Boaz will lead a workshop on the ELK stack. Not heading to DevOps Days? No worries! We've got a meetup planned for November 23. You can to attend. Italy Join at CodeMotion Milan, November 26-29. David will teach you all about . Spain Heading to Strata Barcelona? Great, so are we! You can hear from Shay Banon - - and Costin Leau - - plus visit us on the show floor. We are going to have special gifts on hand too, so make sure to stop by to say hello. Even if you're not planning to attend Strata, we have other great talks on offer in Barcelona on November 20. You can for our Special Strata Meetup. And, just because we can't have enough awesome things happening in Barcelona in one week, you can also hear from at the NoSQL Matters Conference on November 22. Clinton will deep dive into ! Call Madrid home instead of Barca? Awesome, we've got you covered! David Pilato will teach you all about at Codemotion Madrid on Friday, November 21 at 12:15 PM. Not attending Codemotion Madrid? There's also a meetup scheduled for November 24, where you can hear all about your fellow users' experiences with Elasticsearch. to save your place. United Kingdom United States On tonight: join the Gainesville LUG to hear all about the . Doors open at 6 PM. Reg now: — Leslie Hawthorn (@lhawthorn) Don't miss the next Chicago meetup hosted here at Vodori on 12/2 from 6-8pm — vodori, inc. (@vodori) Where to Find You If you're a regular reader of This Week in Elasticsearch, a.k.a TWIES, you're thinking of skipping this section. You may even be thinking to yourself, yes of course I will drop a note on Twitter when I am giving a talk on all things ELK. That's awesome, because we'd like to showcase every meetup, conference presentation and workshop on Elasticsearch, Logstash, and Kiban","locales":"","title":"This Week in Elasticsearch - November 19, 2014"}
{"index":{}}
{"author":"Marty Messer","category":"Engineering","publish_date":"2014-11-17T00:00:00.000Z","url":"/blog/people-behind-projects-more-than-a-bug","seo_title":"","content":" — When I join a company, it's typically at the stage of growth where the vision is so focused, the work pressure is so intense, and the product release schedule is so tight that the welcome note from the dev team has always been one of relief: “Thank god, you're here!\" Developers typically don't like dealing with support or the operational issues of integration. They like product development. However, at Elasticsearch, the situation was different. The developers loved customer interaction, and enjoyed dealing with support, and I had to gently hug it away from them. Enterprise software is a complicated and multidisciplinary field. In the Customer Care department at Elasticsearch, we have teams of support-focused engineers working alongside the developers of the product, and, boy, will our developers do anything for a customer. The precedents for this value system have been role-modeled and emulated since the founding of this company. Being involved in Customer Care also makes developers more aware of the operational aspects of building the platform. This customer-centricity brings us closer, and this fact is apparent in our customer surveys. Our customer survey response rates are 30 percent, three times the industry standards. People want to tell us how they are feeling. And this is a testament not only to the effectiveness of our Customer Care process, but also to the excellence of our recruiting. It's about the people we are bringing in who make all the difference. Earlier this year, we started working with an EdTech company with a 100 million users. They use Elasticsearch to let their users search across more than 2 billion terms to find the most relevant learning material. They were having some operational performance issues, and so we started having a conversation with them. As we started peeling off the layers, we realized they were using an operating system that no other customer was using, and this selection was causing the operational performance issues (and this is with Elasticsearch, which has been downloaded over 10 million times!). When you start calling people's choice of operating systems into question, they can get prickly. Selecting an operating system is like a religious choice. It's like saying, “Your baby's ugly.\" But we knew we needed to have a conversation, and so we laid out a list of scientific reasons for why the operating system was a reason for concern. But instead of a formal presentation that could have come across as condescending, we ended up having a simple conversation. And the tech lead at the company told me, “You know, we have never really liked this system, and we have sort of been looking for a reason to migrate. This is going to be it.\" So then we were able to offer a more optimal set of operating systems as options, and have a more consultative conversation about the next steps regarding the choice of architecture and analytics. At the end of that conversation, I felt good that I had succeeded in setting his expectations higher than what he had himself thought possible. We talked about building standalone clusters. We then went on to build a parallel infrastructure to AB test and to demonstrate the effectiveness of our recommended operating system. The conversation went beyond what our client team expected of us. For me to confidently set expectations higher than what the customer anticipated is so rare in the world of support. It's way more than just dealing with a bug here. The act of seeking support poses a burden to one's self-image. Complicated technical issues aren't always fun. People come to us because something isn't working. But our team practices a form of intellectual honesty, of being aware that we may know Elasticsearch best, but the customer team knows their business and their technical domain inside out. At the end, we share the same goal, and the goal is to have the best, the most stable Elasticsearch installation possib","locales":"","title":"The People Behind the Projects: More than a Bug"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2014-11-17T00:00:00.000Z","url":"/blog/world-elasticsearch-25","seo_title":"","content":" Just a small highlight from last week before we start the deep dive into this week's happenings: Over 100 Elasticsearch fans came to the 10th edition of our meetup in Paris. It is just heart warming to have such a great crowd in one room. Thanks to for hosting tonights meetup in Paris witch . It was a truly great crowd! — Livia Froelicher (@LivFroe) This week we'll be visiting various cities in Canada, Germany, Hungary, Israel, Spain, The Netherlands, UK and the US. Hopefully we'll see you there! Upcoming Events North America November 18: , Broomfield (CO) - Stop by our booth for some swag, and check out talk about on Tuesday, November 18 from 5:15 p.m. - 5:30 p.m. November 22: , Toronto - will give a talk titled “Elasticsearch/Logstash/Kibana: Not plain ol syslog\" from 4:30 p.m.- 5:00 p.m. Europe November 17-21: (Hungary) - will be giving two talks : Contributing to Apache Projects in a Nutshell – November 17 at 2:40 p.m. : how to find out whether the search box you offer your users is helping – November 19 at 10:40 a.m. November 18-20: (Germany) - on . Thursday, November 20, 9:30 a.m. - 10:30 a.m. November 19-21: (Spain) - Hear from about , Friday, November 21 at 11:50 a.m. – and talking about , Thursday, November 20 at 5:45 p.m. Plus visit us on the show floor at booth Nr. P5. November 20: (Germany) - on . November 21-22: (Spain) - David Pilato will teach you all about , Friday, November 21 at 12:15 p.m. November 21: (UK) - will deep dive into the \"Ins and Outs of the ELK Stack\" at 3:10 p.m. November 22: (Spain) - will speak about at 11:45 a.m. November 23-24 - (Israel) - Shay Banon will hold a fireside chat on DevOps and Culture on Sunday, November 23 at 10:30 a.m. and will lead a workshop on the ELK stack on Sunday, November 23 at 3:30 p.m. Upcoming Meetups North America November 20: Europe November 17: November 20: That's it for this week. Stay tuned for Elasticsearch happenings next week - there's much to come! - The Elasticsearch Team P.S.: if you're interested in hosting a meetup or are giving a talk about Elasticsearch - we can offer support and send you swag! ","locales":"","title":"Where in the World Is Elasticsearch? - November 17, 2014"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-11-12T00:00:00.000Z","url":"/blog/2014-11-12-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project.Working on Search at ... Our total searches in the cluster is > 14,000,000,000. Yes, that's BILLION. Holy crap. — David Haney (@haneycodes) Elasticsearch core NEST 1.2.3 now available . Compatibility issues with ES 1.4 have been addressed. — Greg Marzouka (@gregmarzouka) Elasticsearch EcosystemHere's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases.Introducing Raigad - An Elasticsearch Sidecar — NetflixOSS (@NetflixOSS) Our very own has written an article in NDC magazine about — Aleksander Stensby (@astensby) Slides & VideosPresented by at last week's Øredev 2014 ConferencePresented by at last week's San Francisco Cloud Mafia MeetupPresented by at last week's JMahgreb 2014 ConferenceWhere to find UsWe'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking or hosting a meetup, let our Director of Developer Relations, , know!Headed to in DC? Join the team for a meetup tomorrow to hear from & others — elasticsearch (@elasticsearch) CanadaI'll be talking about , and at -- ELK != Plain 'Ol Syslog - More info: — Jason Kendall (@coolacid) Czech RepublicOur very own and will speak at the next Elasticsearch CZ Meetup, taking place in Prague on November 27. to save your place. Denmark will keynote the in Copenhagen. Stay tuned for more schedule details, but Simon will cover Randomized Testing during his presentation, a.k.a. \"evil tests.\" The conference takes place on November 14, 2014.France GermanyWe have a number of upcoming events in Germany: Hungary will be giving two talks at ApacheCon Europe: ApacheCon Europe is on November 17-21, 2014 in Budapest.Israel and will both take the stage at DevOps Days Tel Aviv, which takes place November 23-24. Shay will hold a fireside chat on Dev, Ops and Culture, and Boaz will lead a workshop on the ELK stack.Not heading to DevOps Days? No worries! We've got a meetup planned for November 23. You can to attend.ItalyJoin at CodeMotion Milan, November 26-29. David will teach you all about . Japan The NetherlandsNext Elasticsearch NL meetup is scheduled. Mark the date: November 17. Talks by ING and RIPE. See you there! RSVP: — Boaz Leskes (@bleskes) RomaniaNew meetup on *tonight*! Bucharest Big Data user group will talk Analytics using Hadoop & — Leslie Hawthorn (@lhawthorn) SpainHeading to Strata Barcelona? Great, so are we! You can hear from Shay Banon - - and Costin Leau - - plus visit us on the show floor. We are going to have special gifts on hand too, so make sure to stop by to say hello. Even if you're not planning to attend Strata, we have other great talks on offer in Barcelona on November 20. You can for our Special Strata Meetup.And, just because we can't have enough awesome things happening in Barcelona in one week, you can also hear from at the NoSQL Matters Conference on November 22. Clinton will deep dive into ! Call Madrid home instead of Barca? Awesome, we've got you covered!David Pilato will teach you all about at Codemotion Madrid on Friday, November 21 at 12:15 PM. Not attending Codemotion Madrid? There's also a meetup scheduled for November 24, where you can hear all about your fellow users' experiences with Elasticsearch. to save your place.United Kingdom United States Meetup November in Philadelphia: Building an educational content search application using — jamestyack (@jamestyack) PDX friends, will demo 4 at the November 13 meetup + what's new in the ELK stack! — Leslie Hawthorn (@lhawthorn) Where ","locales":"","title":"This Week in Elasticsearch - November 12, 2014"}
{"index":{}}
{"author":"Costin Leau","category":"Engineering","publish_date":"2014-11-17T00:00:00.000Z","url":"/blog/elasticsearch-yarn-and-ssl","seo_title":"","content":" I am happy to announce Elasticsearch for Apache Hadoop 2.1.Beta3 has just been . We are introducing two new features: SSL connectivity and enhanced HTTP authentication and dedicated support for running Elasticsearch on YARN. Elasticsearch on YARN With 2.1.Beta3, we introduce the (aka es-yarn) project for running an Elasticsearch cluster within a YARN environment. Similar to the plugin, es-yarn is distributed as part of the Elasticsearch for Apache Hadoop (aka es-hadoop) project, but is and has no dependencies outside YARN itself. With es-yarn, one can now provision, start and stop Elasticsearch directly on a YARN cluster. In YARN lingo, es-yarn bootstraps a client that deploys a dedicated in YARN which, on its behalf, creates one container for each Elasticsearch node required. For the user, es-yarn is a straight-forward CLI (Command-Line Interface) for deploying and managing the life cycle of the Elasticsearch cluster within YARN. To wit, simply and run: $ hadoop jar elasticsearch-yarn-2.1.Beta3.jar No command specified Usage: -download-es : Downloads Elasticsearch.zip -install : Installs/Provisions Elasticsearch-YARN into HDFS -install-es : Installs/Provisions Elasticsearch into HDFS -start : Starts provisioned Elasticsearch in YARN -status : Reports status of Elasticsearch in YARN -stop : Stops Elasticsearch in YARN -help : Prints this help Configuration options can be specified _after_ each command:  see the documentation for more information. Each command should be self-explanatory. Typically one would: Download Elasticsearch You can do this yourself. However, out of the box, es-yarn can do this for you: $ hadoop jar elasticsearch-yarn-2.1.Beta3.jar -download-es Downloading Elasticsearch 1.4.0 Downloading ...........................................................................DONE Provision Elasticsearch and Elasticsearch on YARN in HDFS To start a YARN application, YARN needs to get access to the needed artifacts from HDFS. es-yarn can provision HDFS on your behalf: $ hadoop jar elasticsearch-yarn-2.1.Beta3.jar -install-es Uploaded /opt/es-yarn/downloads/elasticsearch-2.1.Beta3.zip to HDFS at hdfs://127.0.0.1:50463/apps/elasticsearch/elasticsearch-2.1.Beta3.zip $ hadoop jar elasticsearch-yarn-2.1.Beta3.jar -install Uploaded opt/es-yarn/elasticsearch-yarn-2.1.Beta3.jar to HDFS at hdfs://127.0.0.1:50463/apps/elasticsearch/elasticsearch-yarn-2.1.Beta3.jar Start Elasticsearch on YARN $ hadoop jar elasticsearch-yarn-2.1.Beta3.jar -start Launched a 1 node Elasticsearch-YARN cluster [application_1415921358606_0001@http://hadoop:8088/proxy/application_1415921358606_0001/] at Fri Nov 14 21:11:39 EET 2014 and voila: Want to run multiple nodes? Just tell es-yarn so: $ hadoop jar elasticsearch-yarn-2.1.Beta3.jar -start containers=2 Launched a 2 node Elasticsearch-YARN cluster [application_1415921359403_0001@http://hadoop:8088/proxy/application_1415921359403_0001/] at Fri Nov 14 21:24:53 EET 2014 Stop the cluster When you are done, shutdown the cluster like this: $ hadoop jar elasticsearch-yarn-2.1.Beta3.jar -stop Stopped Elasticsearch-YARN cluster with id application_1415921358606_0001 It’s that simple! SSL and HTTP authentication es-hadoop uses REST over HTTP to communicate with Elastisearch. Release 2.1.Beta3 introduces official support for basic HTTP authentication allowing Hadoop jobs running against a restricted Elasticsearch cluster to identify themselves accordingly. While es-hadoop has supported authentication through its options, with 2.1.Beta3 it is graduated to an individual component and thus can be used within or outside the context of a proxy configuration. Further more, the new 2.1 Beta release introduces support for cryptographic connections between Elasticsearch and your Hadoop cluster. Thus data-sensitive environments can encrypt the data at transport level to prevent snooping and preserve data confidentiality. Note that while self-signed certificates are supported (","locales":"","title":"Elasticsearch on YARN and SSL Support in Elasticsearch Hadoop"}
{"index":{}}
{"author":"Njal Karevoll","category":"","publish_date":"2014-11-12T00:00:00.000Z","url":"/blog/found-document-processing","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud.An OverviewRaw documents from the source database or server may need some extra processing before being indexed in Elasticsearch. In this article, we consider a few different options for this processing. ","locales":"","title":"Document Processing and Elasticsearch"}
{"index":{}}
{"author":"","category":"","publish_date":"2014-11-10T00:00:00.000Z","url":"/blog/elk-cisco-ucs-turn-massive-data-massive-insights","seo_title":"","content":" We announced a partnership today - - that we’re all really excited about at Elasticsearch. This is an important partnership to us because Cisco is a leader in data center infrastructure, and we are joining forces to bring businesses that leverage Cisco’s Unified Computing System (UCS) Platform the ability to gain real-time search and analytics, and in turn, insights, out of their data with the Elasticsearch ELK stack. Elasticsearch is in the business of providing real-time insight into data - whether structured or unstructured, human- or machine-generated:  we bring a search-based architecture to data analytics. By combining the ELK stack with Cisco UCS, organizations benefit by having a turnkey underlying infrastructure solution that provides them with real-time search and analytics for a variety of applications, from log analysis, to structured, semi-structured, or unstructured searches, as well as as a web-backend for custom applications that use search-based analytics as a core functionality. Together, Elasticsearch and Cisco provide access to a flexible and powerful infrastructure for scalable ELK deployments that increase business and IT agility, reduce total cost of ownership, and deliver exceptional ROI. We’ve already been doing some major deployments with Cisco in the area of financial service, telecom, and retail industries. And, the Cisco Open Security Operations Center (OpenSOC) project uses Elasticsearch and UCS for data-driven security discoveries, providing a unified platform for ingesting, storing and analyzing security logs. Today, we are releasing our joint reference architecture to deploy optimized Elasticsearch and UCS deployments. The reference architecture allows one to identify the right UCS configuration based on use case (like log analytics), retention policies (how long you want to keep the data), etc. Stay tuned for further developments between Elasticsearch and Cisco, but in the meantime: ","locales":"","title":"ELK + Cisco UCS turn massive data into massive insights"}
{"index":{}}
{"author":"Clinton Gormley","category":"Engineering","publish_date":"2014-11-05T00:00:00.000Z","url":"/blog/elasticsearch-1-4-0-released","seo_title":"","content":" Today, we are happy to announce the release of , based on , and of bug fix release . You can download them and read the full changes list here: For blog posts about past releases in the 1.3 branch, see: , , , , . As we said with the Beta1 release, the major theme of 1.4.0 is : making Elasticsearch more stable and reliable than ever before, with better memory management, improved discovery algorithms, and better detection of corrupted data. Some highlights from the Beta1 release include: Please read the for more details. You can read about all of the changes that have gone into 1.4.0 since Beta1 in the , but there are two major changes which deserve to be highlighted: ","locales":"","title":"Elasticsearch 1.4.0 And 1.3.5 Released"}
{"index":{}}
{"author":"Rashid Khan","category":"Engineering","publish_date":"2014-11-11T00:00:00.000Z","url":"/blog/kibana-4-beta-2-get-now","seo_title":"","content":" Eeeeeeeee! It's here! Kibana 4 Beta 2 is ready to be poked, prodded, and otherwise fiddled with by you, your friends, and your exceptionally intelligent pets. We look forward to your feedback. If you just can't wait to get started, , otherwise read on for the good stuff. Along with dozens of small tweaks and fixes, here are some of the fun new things to look out for in this release of Kibana: Map Support Maps are back and they are more powerful than ever! The new tile map visualization makes use of Elasticsearch's awesomely powerful to display geographic data, such as visualizing relative response times: Visualization Options In Beta 1, bar charts were always stacked. In Kibana 4 Beta 2, we've added options that allow you to modify how visualizations display their data. For example, by grouping bars: Or displaying bars as percentages: Area Charts Beta 2 brings back area charts, both stacked and unstacked: Advanced Parameters We aim to support as many Elasticsearch features as possible, but sometimes we just haven't gotten to that one aggregation option you really need right now. For that option, we've introduced a JSON input which allows you to specify additional aggregation parameters to send with your request. For example, maybe you want to pass in a agg, or increase the in a cardinality agg. In this case, we pass a script as an advanced parameter, taking the log of the of the field and placing it on the X-axis: Data Tables Sometimes you want a flashy chart, and sometimes you just want the numbers. The data table visualization makes that desire a reality: Hey! Where Did My Dashboards Go? The internal Kibana index changed from to . We recommend you move your documents (e.g., dashboards, settings, visualizations) from the old index to the new one. However you can also simply set in kibana.yml. What Are We Working on Now? Check out our path to Kibana 4 GA on our . As always, we welcome feedback/bugs/fixes on ","locales":"ja-jp","title":"Kibana 4 Beta 2: Get It Now"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2014-11-10T00:00:00.000Z","url":"/blog/world-elasticsearch-24","seo_title":"","content":" Last week we were at where talked about all the nice things you can do with your logs using the ELK stack and held the closing keynote where she made the audience laugh with her talk about \"The Human Element in Development\". great talk by on how easy it is to do powerful log queries using — Courtney Hemphill (@chemphill) Full room at the final keynote by ! Come and join us now at Hall 1! — Livia Froelicher (@LivFroe) This week we'll be visiting various cities in Belgium, Denmark, France, Germany, Japan, the UK and the US. Hopefully we'll see you there! Upcoming Events Europe November 10-12: - will be onsite with our partner codecentric to present some great Kibana 4 demos. November 10-14: - will host an . Monday, November 10 at 9:30 a.m. November 13: - Come say hi at our booth Number 15 and we'll show you all the great things you can do with the ELK stack. We'll also have a guest speaker, Dan Leong from Daily Mail, who will be talking about . 10:40 a.m. - 10:55 a.m. November 14: - will be giving a keynote on \"Randomized Testing - Maximise Probability for Reliable Software\". The time is still not online, but make sure to check the website the day before! November 14: - David Pilato will be presenting on \"Elasticsearch and Drupal\" at 2 p.m. Asia November 15: - will be presenting and talking about including some demos. Upcoming Meetups North America November 12: November 13: November 13: Europe November 13: That's it for this week. Stay tuned for Elasticsearch happenings next week - there's much to come! - The Elasticsearch Team P.S.: if you're interested in hosting a meetup or are giving a talk about Elasticsearch - we can offer support and send you swag! ","locales":"","title":"Where in the World Is Elasticsearch? - November 10, 2014"}
{"index":{}}
{"author":"Shay Banon","category":"Engineering","publish_date":"2014-11-04T00:00:00.000Z","url":"/blog/shield-know-security-coming-soon","seo_title":"","content":" Since the early days of Elasticsearch, one could secure Elasticsearch using external systems. For example, since Elasticsearch APIs are exposed through REST over HTTP, users secure it using or firewalls. With the explosive adoption of Elasticsearch and the ELK stack, our customers and users started to ask us for a more integrated security solution with advanced features that are not implemented using external systems. After spending a lot of time with our customers, some of them with very demanding security requirements, we started to work on a security product within Elasticsearch, which we named Shield. (Any Marvel fans out there sensing a theme? :) ) Over the last few months, I have been traveling all over the world talking to customers and users at meetups, and every time I mentioned Shield, people got very excited. For this reason, we thought it made sense to publicly announce that we are working on a security product, explain what it is, and that we are expecting to release it by the end of the year. Shield, in the same spirit of , is built on top of Elasticsearch public extensions points, and is easily installed as a plugin to add security features to any existing Elasticsearch installation. It does not require a different distribution of Elasticsearch, and relies heavily on the open public APIs Elasticsearch already exposes. Shield itself provides four main feature themes: Role-Based Access Control Set granular cluster, index, and alias-level permissions for each user of your Elasticsearch cluster. For example, allow the marketing department to freely search and analyze social media data with read-only permissions, while preventing access to sensitive financial data. Authentication System Support Shield integrates with LDAP-based authentication systems as well as Active Directory, so your users don't need to remember yet another password. We also provide a native authentication system, for those who want to manage all access within Elasticsearch. Encrypted Communications Node-to-node encryption protects your data from intruders. With certificate-based SSL/TLS encryption and secure client communications with HTTPS, Shield keeps data traveling over the wire protected. Audit Logging Ensure compliance and keep a pulse on security-related activity happening in your Elasticsearch deployment:  record login failures and attempts to access unauthorized information. Recently, we successfully launched a beta with a select group of customers who are putting each aspect of Shield to the test. We are excited to take what we learn and fold that valuable knowledge back into the finished product for everyone to use. Finally, I am very happy to announce that Shield will be free for existing and future . Our customers already enjoy the relationship they built with our developers when it comes to supporting them through their development and production deployments, as well as the huge investment we make in developing the products themselves, and we are thrilled to provide them, at no additional cost, the option to use Shield. We think Shield will develop into a one-stop shop when it comes to securing the ELK stack, satisfying even the most demanding security requirements. We are very excited to make this available to our customers, and I hope you are as well. as the release date draws near.... ","locales":"","title":"Shield: You Know, For Security"}
{"index":{}}
{"author":"Aaron Mildenstein","category":"Engineering","publish_date":"2014-11-10T00:00:00.000Z","url":"/blog/people-behind-projects-swiss-army-knife","seo_title":"","content":" — I encountered my first computer in elementary school in Provo, Utah. It was the early 1980s, and the future was in the school library. I learned a tiny bit of BASIC and loaded games from a cassette drive onto a Commodore PET computer. A few years later, it was the Commodore VIC-20. When I started the sixth grade, I transferred to a school funded by the World Institute for Computer Aided Training (WICAT) where various subjects were taught or were enhanced by software running on computer terminals connected to a mini-computer. It was there that I received my first programming lessons in UCSD Pascal. Though I did not yet know it, by then, the die was cast. It wasn’t all by design, though. I wanted stuff and we weren’t well-off. So I would go to thrift stores, find broken electronics, take them apart, and fix them. I fixed my first television at the age of twelve. The repairman had examined the TV and said, “Well, X, Y, and Z have to be replaced.” So I looked up the parts, went to an electronics store to buy them, replaced a couple of parts, soldered some new parts, turned on the television, and it came on. I learned to gain understanding and knowledge by taking things apart and, where possible, repairing and reassembling them. My dad taught me to do the same with cars. When we needed a car, he would go buy a car that was in disrepair and we would fix it so that we could have a car for less money. My own journey with computer science continued on to college, but I didn’t complete a degree. I met the girl who became my wife, and we got married soon after. I started working and never ended up going back to school. After saving up some money, I quit my job and became immersed in Linux for three months. That’s all I did—all day, everyday. From there, I branched out to employment as a software tester and then moved to systems administration. That’s when I started scripting and learning Perl and Python. A few years ago, I was working at Alcatel-Lucent where we were trying to create a centralized logging platform to better monitor the conditions of our servers and applications. We investigated Splunk, but the sticker shock was supreme, because we were generating an enormous volume of logs per day. We tried to make our own centralized logging system, and that was too painful, slow, and buggy. We gave up on that project, and our logging needs went unmet for a few more years. While vacationing in the summer of 2011, I heard mention of Logstash in a user-group email, and within weeks, I was testing Logstash and Elasticsearch at work. I loved how helpful everyone was in the IRC chat rooms. The company gave me the go-ahead to architect a four-node cluster of Elasticsearch, Logstash, and version one of Kibana, making me one of the first enterprise users of the ELK stack who worked on tracking problems with logs. During those days, I wrote a script called Logstash Index Cleaner to help delete old indexes. This script has evolved through many iterations and is now the Elasticsearch Curator project, for managing time-series indices. When Logstash was formally incorporated into Elasticsearch, I also joined the company. It’s exhilarating to be in an environment where somebody who didn’t graduate from college but who has the passion to learn this information can debate and contribute to the development of the product alongside team members with PhDs. The focus here is a desire for the technology to be furthered, a desire to make open source as a model succeed. Recently, I have been supporting one of the leading telecom companies in their development of a log management system that captures and summarizes over a terabyte of logs a day. They have over 45 nodes in production for a Security Event and Threat Analysis (SETA) reporting tool that tracks the security of all customer-facing devices. The tool ensures that the data that the company’s customers share remain confident","locales":"","title":"The People Behind the Projects: The Swiss Army Knife"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-11-05T00:00:00.000Z","url":"/blog/2014-11-05-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project.Elasticsearch core Elasticsearch EcosystemHere's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases. Slides & VideosWhere to find UsWe'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking or hosting a meetup, let our Director of Developer Relations, , know!Belgium will host a at Devoxx Belgium 2014. David's workshop kicks off on Monday, November 10th at 9:30 AM, and the conference runs from November 10-14 in Antwerp.CanadaI'll be talking about , and at -- ELK != Plain 'Ol Syslog - More info: — Jason Kendall (@coolacid) Denmark will keynote the in Copenhagen. Stay tuned for more schedule details, but Simon will cover Randomized Testing during his presentation, a.k.a. \"evil tests.\" The conference takes place on November 14, 2014.France GermanyFor folks in Berlin, we'll have a booth at the GOTO Berlin conference, plus two awesome talks from Elasticsearchers: Join us at the Kosmos Event Center on November 6th and 7th for !And, we have a few of other great events coming up in Germany: Hungary will be giving two talks at ApacheCon Europe: ApacheCon Europe is on November 17-21, 2014 in Budapest.IrelandOur CEO, , will be speaking at the Web Summit conference. You can hear from Steven on Friday, November 6th on . Web Summit is on in Dublin November 4-6th.Israel and will both take the stage at DevOps Days Tel Aviv, which takes place November 23-24th. Shay will hold a fireside chat on Dev, Ops and Culture, and Boaz will lead a workshop on the ELK stack.Not heading to DevOps Days? No worries! We've got a meetup planned for November 23rd. You can to attend.ItalyJoin at CodeMotion Milan, November 26-29th. David will teach you all about . Japan The NetherlandsNext Elasticsearch NL meetup is scheduled. Mark the date: 17//11. Talks by ING and RIPE. See you there! RSVP: — Boaz Leskes (@bleskes) RomaniaNew meetup on *tonight*! Bucharest Big Data user group will talk Analytics using Hadoop & — Leslie Hawthorn (@lhawthorn) SpainHeading to Strata Barcelona? Great, so are we! You can hear from Shay Banon - - and Costin Leau - - plus visit us on the show floor. We are going to have special gifts on hand too, so make sure to stop by to say hello. Even if you're not planning to attend Strata, we have other great talks on offer in Barcelona on November 20th. You can for our Special Strata Meetup.And, just because we can't have enough awesome things happening in Barcelona in one week, you can also hear from at the NoSQL Matters Conference on November 22nd. Clinton will deep dive into ! Call Madrid home instead of Barca? Awesome, we've got you covered!David Pilato will teach you all about at Codemotion Madrid on Friday, November 21st at 12:15 PM. Not attending Codemotion Madrid? There's also a meetup scheduled for November 24th, where you can hear all about your fellow users' experiences with Elasticsearch. to save your place.SwedenHonza Kral will be speaking at the Øredev conference in Malmö on . The conference runs Nov. 4-7th, and Honza takes the stage on Nov. 6th at 5:40 PM. While Honza is visiting Sweden, we figured it would be a great time to host a meetup in Malmö. Please join us Friday, November 7th to hear from Honza on How to Make Sense of Your Big Data using Elasticsearch. to save your place.United KingdomThe Elasticsearch team will be at the conference in London on November 13th and we are honored to have the UK's own present how .United StatesDC friends, go see at Enterprise ","locales":"","title":"This Week in Elasticsearch - November 05, 2014"}
{"index":{}}
{"author":"Andrew Selden","category":"Engineering","publish_date":"2014-11-03T00:00:00.000Z","url":"/blog/people-behind-projects-commons","seo_title":"","content":" -- I meandered some in the early years. I had no idea what I wanted to do in college, but my parents always told my brother and I that we needed to get a Liberal Arts education. They actively encouraged us from being practical. I dabbled in philosophy, history, and political science before settling on Russian Language and Literature as a major. After college, I started working at the Bioinformatics Lab at UPenn. This was in the early days of what came to be called “big data\". There was no Hadoop. No Elasticsearch. None of the great tools we have today to manage massive data sets. We had to cobble together large Linux clusters using Perl, NFS, and a smattering of other technologies. We'd get these large grants from the National Science Foundation to perform analytics on various genomes. Today you would just rent space in the cloud, but 12 years ago there was no cloud. It's so interesting to see how the state of the industry has evolved. A lot of the computational techniques we were struggling to build a decade ago are now mainstream in just about every tech startup. I simultaneously got my Masters in Computer Science taking courses part-time alongside work. After all, who wants to get out of school with student debt? After finishing graduate school, I moved to the Bay Area and ended up as a search engineer for a large media surveillance company. We crawled the web, pulling in content in over 30 different languages to analyze and create competitive intelligence for our clients. We were very successful and built a great product, but I always felt that the software tools were too hard to use. Then I discovered Elasticsearch. The technology probably wasn't even a year old when we introduced Elasticsearch into our company. But even in its earliest days it was so clearly superior to everything else. Clustering just worked. Search just worked. It was a breath of fresh air to have a tool I didn't have to fight to get working. It's been amazing to watch the evolution in how customers use Elasticsearch, from a purely search-oriented technology into a full-fledged analytics engine. Elasticsearch comes at the problem from the perspective of a search engine. Most big data products come at the problem from the perspective of putting files on disks, scanning them from start to end. That approach is very linear, whereas Elasticsearch wants to find that relevant needle in the haystack in milliseconds. Easy to get data in. Easy to get data out. For instance, you give the data to Hadoop. It will take that data and store it on disk. Later, if you want to analyze the data, it will allow you to launch a number of parallel processes, each of which scan a part of the file. They take apart and scan the file from start to finish. It's linear within chunks. You give it data and it puts the bits on disks. With Elasticsearch, the moment you give us data, we don't just take it and store it on disk. We do deep analysis on it. We construct data structures and put those on disk. The data structures are organized in such a way that you can do extremely fast lookups. If I want to search for the word “Dostoevsky\" in a set of billions of documents, I don't have to sit idly while the system scans everything for hours. I just have to lookup “Dostoevsky.\" It's one hop. The preprocessing was completed at the time of ingestion. Search engines and information retrieval algorithms are a well-understood problem,whereas parallel processing data analytics is still new arena -- and the ELK stack is ahead of the curve here. For example, say you're Walmart with thousands of retail outlets, and you're selling five types of electronics across thousands of stores. You want the min-maxes and averages per store for every electronic. You also want to find the most effective salespeople across your network. Those questions were traditionally answered over the course of weeks or months by using SQL on an Oracle analytics engine. With Elastics","locales":"","title":"The People Behind the Projects: for the Commons"}
{"index":{}}
{"author":"Suyog Rao","category":"Engineering","publish_date":"2014-10-27T00:00:00.000Z","url":"/blog/people-behind-projects-dna","seo_title":"","content":" I had been exposed to the world of engineering at a very early age. My father was a mechanical engineer and always had a methodical approach to solving issues. If a toilet at home wasn’t working, he would try to evaluate the root cause of the problem rather than searching for a quick fix. He always had a calm and logical response: “This doesn’t work, let’s figure out why.” As a twelve-year-old, I found this practical approach pretty impressive. At university, I worked on a bioinformatics research project where we tried to figure out the causal relationship between certain patterns of DNA and diseases in humans. If we clustered those DNA patterns together, we could identify which diseases each pattern correlated with. It was in this lab that my appreciation for computer science and its application to solving problems in the real world began. I was first introduced to the ELK stack when I was at Loggly. When I started exploring the different versions of the Elasticsearch code, I realized that it hasn’t changed a lot since 2010 when Shay Banon wrote the first version. Unlike other software systems, there hasn’t been a drastic architectural change and things aren’t constantly being written and rewritten. Elasticsearch had this foresight: “Data will grow exponentially. Machines can be rented. Therefore, let us build a search mechanism that can scale.” The elastic—that is, the scale—part of Elasticsearch is incredible. Many people are able to create search algorithms, but the elasticity of the company gives it a major competitive advantage. If such foresight is part of the DNA of the company, I wanted to be there. So here I am. One of my favorite features of Elasticsearch is called “allocation awareness.” We created this when we realized that “not all data are created equal.” It allows you to assign a higher-end machine to certain indices that need more computing power, and older, less frequented data to a low-end machine. We still have access to the old data, but it’s of fading relevance. One of my recent memories of helping a customer was from a mobile analytics company that uses Elasticsearch to gather data about in-app experience, user session information, and API usage. They were migrating from using a NoSQL product to Elasticsearch for providing these real-time analytics and had gone live the previous week. When I received the support call, one of the nodes in their cluster was experiencing high CPU load and the memory usage was very high. Using Marvel, we narrowed the problem to a special use case of theirs where they were doing frequent updates to documents using scripts, which was adding high load on the primary shards. Once I realized this was the root cause of the problem, I wrote a script to balance the primary shards equally across their four machines. So, instead of allowing 80 percent of one CPU to get loaded, we could have CPUs on other machines sharing the workload. The new script allowed us to distribute jobs evenly across all the machines. And now that their urgent request has been addressed, we are taking corrective measures to make sure that this code is scripted in for other similar use cases. I love the quick feedback loop from customers to products, which benefits everyone. It’s all very gratifying. ","locales":"","title":"The People Behind the Projects: DNA"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2014-11-03T00:00:00.000Z","url":"/blog/world-elasticsearch-23","seo_title":"","content":" Last week we were at to demo some Kibana 4 dashboards and give out USB ELKs to attendees. At Ghent? Meet our booth crew w' & and learn all things ! — elasticsearch (@elasticsearch) has the cutest swag! Thank you :) — Krzysztof Wilczynski (@kwilczynski) This week we'll be visiting various cities in Denmark, Germany, Ireland, Morocco, Sweden and in the US. Hopefully we'll see you there! Upcoming Events North America November 5-7: - will be part of a panel discussion about on Thursday, November 6 from 10:45 a.m. - 11:45 a.m. Online registration is now closed but you can still register onsite by completing . Europe November 4-7: - Join in his session on . Thursday, November 6 from 5:40 p.m. - 6:20 p.m. November 5-7: - Check out talk about , Thursday, November 6 from 10:20 a.m. - 11:10 a.m. And don't forget to attend the closing keynote by awesome : , Friday, November 7 from 5:00 p.m. - 5:50 p.m. November 5: - Honza Kral will present “Analyzing and Searching Data using Elasticsearch\" and the Registration is free of charge, but you'll need to . Africa November 4-6: - Don't miss . Tuesday, November 4 from 10:00 a.m. - 12:00 p.m. Upcoming Meetups North America November 5: November 6: Europe November 5: November 6: November 7: That's it for this week, and stay tuned for Elasticsearch happenings next week. There's much to come! - The Elasticsearch Team P.S.: if you're interested in hosting a meetup or are giving a talk about Elasticsearch - we can offer support and send you swag! ","locales":"","title":"Where in the World Is Elasticsearch? - November 03, 2014"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-10-29T00:00:00.000Z","url":"/blog/2014-10-29-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project.Elasticsearch Core Revealing the Uncommonly Common with Elasticsearch Webcast Oct 30 9am PT/ 12pm ET — O'Reilly Strata (@strataconf) Elasticsearch EcosystemHere's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases. . talking about Kibana 3, 4 & at Tempe meetup. All things ELK-y. — robyn bergeron (@robynbergeron) Slides & VideosThanks SO much all who saw the meetup on replication tonight! Slides here (see comments): — sunnygleason (@sunnygleason) And from the oldies but goodies files, you Kevin Kluge and Steve Mayzak's talk from OSCON 2014, where they show you how to monitor data from a drone using ELK. You'll also learn how to literally crash your demo, plus fun stuff for all fans of the Internet of Things. Check it out!Where to Find UsWe'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking or hosting a meetup, let our Director of Developer Relations, , know!Belgium will host a at Devoxx Belgium 2014. David's workshop kicks off on Monday, November 10th at 9:30 AM, and the conference runs from November 10-14th in Antwerp.CanadaI'll be talking about , and at -- ELK != Plain 'Ol Syslog - More info: — Jason Kendall (@coolacid) Denmark France GermanyFor folks in Berlin, we'll have a booth at the GOTO Berlin conference, plus two awesome talks from Elasticsearchers: Join us at the Kosmos Event Center on November 6th and 7th for !And, we have a few of other great events coming up in Germany: Hungary will be giving two talks at ApacheCon Europe: ApacheCon Europe is on November 17-21, 2014 in Budapest.IrelandOur CEO, , will be speaking at the Web Summit conference. You can hear from Steven on Friday, November 6th on . Web Summit is on in Dublin November 4-6th.ItalyTomorrow, October 30th, we'll be working with - the official Zimbra event in Italy (organized by Seacom) - to help show the ELK stack as a Zimbra log analyzer. You may also with to join us that evening at 5:30 PM for the second , including talks and networking.Japan The NetherlandsING on IT tour. Proud to host Meetup Events. Elastic Search next. Mon 17/11 press center AMP Amsterdam — Rob van Elburg (@RAVanElburg) Poland will speak at PolyConf 2014 on Elasticsearch's language clients, Bridging the gap: . Honza takes the stage at 5:30 PM tomorrow night, October 30th, and the conference runs the 30th-31st in Poznan.SwedenHonza Kral will be speaking at the Øredev conference in Malmö on . The conference runs Nov. 4-7th, and Honza takes the stage on Nov. 6th at 5:40 PM. While Honza is visiting Sweden, we figured it would be a great time to host a meetup in Malmö. Please join us Friday, November 7th to hear from Honza on How to Make Sense of Your Big Data using Elasticsearch. to save your place, and, even better, if you're interested in sharing your story at the meetup! United KingdomThe Elasticsearch team will be at the conference in London on November 13th and we are honored to have the UK's own present how .United States Where to Find YouOur Director of Developer Relations, Leslie Hawthorn, is hard at work to help folks create more Elasticsearch meetup groups and to help meetup organizers find more speakers. If you are interested in either effort, .Oh yeah, we're also . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticsearch, Kibana, and Logstash than where you rest your head.TrainingsIf you are interested in Elasticsearch training we have","locales":"","title":"This Week in Elasticsearch - October 29, 2014"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2014-10-27T00:00:00.000Z","url":"/blog/world-elasticsearch-22","seo_title":"","content":" While our ELKs are starting to conquer the world, we are doing the same with conferences and meetups. :) This week you can find us in Florence (IT), Ghent (BE), Los Angeles (US), Poznan (PL) and San Francisco (US). Come join us for a session to learn even more about Elasticsearch. I've a new friend watching my logs! Tx ! /Cc — Xavier Mertens (@xme) Upcoming Events Europe October 27-28: . Stop by our booth to see live demos, talk to our Solutions Architect, and learn about all things Elasticsearch! October 30-31: . Join for his session on . Thursday, October 30 5:30 p.m. - 6:00 p.m. (CET) October 30: . Our partner Seacom will be talking about Elasticsearch, 11:30 a.m. - 12:30 p.m. (CET). Upcoming Meetups North America October 29: October 30: Europe October 28: October 30: That's it for this week, and stay tuned for Elasticsearch happenings next week. There's much to come! - The Elasticsearch Team P.S.: if you're interested in hosting a meetup or are giving a talk about Elasticsearch - we can offer support and send you swag! ","locales":"","title":"Where in the World Is Elasticsearch? - October 27, 2014"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-10-22T00:00:00.000Z","url":"/blog/2014-10-22-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. All new to Elasticsearch? Dive right in with Dr. Myagmar's overview. Elasticsearch core \": Interactive analysis of 14 years of crime data using , , . \" — Will Button (@wfbutton) Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases. I'll be a hosting an Oct 30th webcast featuring : Anomaly Detection with — Ben Lorica (@bigdata) Slides & Videos ElasticSearch入門 [embedded in ] — ymizushi (@ymizushi) Slides for my talk at are available at :) — David Pilato (@dadoonet) Yann Cluchey, the stalwart organizer of the Elasticsearch London Meetup, deep dives on GfK's architecture and their ELK stack use case Need a quick review of our latest webinar? Look no further! ","locales":"","title":"This Week in Elasticsearch - October 22, 2014"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2014-10-20T00:00:00.000Z","url":"/blog/world-elasticsearch-21","seo_title":"","content":" This week you'll find us in Beijing, Geneva, Michigan, Milan, Minneapolis, Paris, Phoenix, Raleigh and Seattle. Before we tell you all about this week's events, check out this Kibana dashboard from last week's Leeds DevOps Meetup - you can analyze everything you want, even “The Walking Dead\" tweets. :) The Walking Dead tweet analysis for tonight, I think Carol was a popular term... — Steve Elliott (@Tegud) We also had a great time this week in New York at , educating attendees how they can provide real-time insight into their Hadoop data. The team is ready for Day 2 at . Come see us @ Booth P1 & get your on — elasticsearch (@elasticsearch) Upcoming Events North America October 22-23: - don't miss talking about Wednesday, October 22 at 2:15 p.m. (ET) Europe October 20: IBM's event, - our partner Seacom will be educating folks about Elasticsearch on IBM Power8 systems. Event starts at 9:30 a.m. (CET). To learn more and sign up, click . October 23-24: . will give a . Thursday 23, 9:30 a.m. - 1:30 p.m. (CET) October 23-24: . Hear David Pilato talk about . Friday 24, 2:45 p.m. - 3:30 p.m. (CET) Upcoming Meetups North America October 21: October 22: October 23: October 23: Asia October 25: That's it for this week, and stay tuned for Elasticsearch happenings next week. There's much to come! - The Elasticsearch Team P.S.: if you're interested in hosting a meetup or are giving a talk about Elasticsearch - we can offer support and send you swag! ","locales":"","title":"Where in the World Is Elasticsearch? - October 20, 2014"}
{"index":{}}
{"author":"Antonio Bonuccelli","category":"Engineering","publish_date":"2014-10-21T00:00:00.000Z","url":"/blog/use-elk-display-security-datasources-iptables-kippo-honeypot","seo_title":"","content":" Among the countless possible use cases where ELK can help save the day, displaying security-relevant data is certainly a very interesting one. In this blog post, using a virtual machine sitting on the cloud, we're going to show how to quickly set up a clustered instance of Elasticsearch to visualise firewall and honeypot datasources, namely IPtables and KippoSSH, focusing on the ELK-relevant configuration bits. KippoSSH is a medium interaction honeypot capable of recording plenty of information about the attacker, including interactive TTY sessions recordings:  for the purpose of this blog post, we'll leave that latter piece of info aside, and focus on making sense of some brute force data. Starting from the live raw data, we have logs containing: These look like: #iptables elk@debianVM:~$ tail -3 /var/log/kern.log Sep 24 13:52:06 debianVM kernel: iptables denied: IN=eth0 OUT= MAC=f2:3c:91:73:6c:71:84:78:ac:0d:8f:41:08:00 SRC=184.95.32.162 DST=85.159.211.137 LEN=48 TOS=0x00 PREC=0x00 TTL=110 ID=6886 PROTO=TCP SPT=37377 DPT=3306 Sep 24 13:52:44 debianVM kernel: iptables denied: IN=eth0 OUT= MAC=f2:3c:91:73:6c:71:84:78:ac:0d:8f:41:08:00 SRC=122.143.247.8 DST=85.159.211.137 LEN=40 TOS=0x00 PREC=0x00 TTL=107 ID=54661 PROTO=TCP SPT=6000 DPT=1433 Sep 24 13:55:22 debianVM kernel: iptables denied: IN=eth0 OUT= MAC=f2:3c:91:73:6c:71:84:78:ac:0d:8f:41:08:00 SRC=50.63.43.162 DST=85.159.211.137 LEN=44 TOS=0x00 PREC=0x00 TTL=57 ID=55765 PROTO=TCP SPT=80 DPT=5568 #kippo SSH elk@debianVM:~$ egrep 'New connection' -A10 /opt/kippo/kippo-master/log/kippo.log | tail -11 2014-09-24 15:00:51+0100 [kippo.core.honeypot.HoneyPotSSHFactory] New connection: 221.146.74.146:49609 (85.159.211.137:2222) [session: 660] 2014-09-24 15:00:51+0100 [HoneyPotTransport,660,221.146.74.146] Remote SSH version: SSH-2.0-libssh2_1.4.1 2014-09-24 15:00:51+0100 [HoneyPotTransport,660,221.146.74.146] kex alg, key alg: diffie-hellman-group1-sha1 ssh-rsa 2014-09-24 15:00:51+0100 [HoneyPotTransport,660,221.146.74.146] outgoing: aes128-ctr hmac-sha1 none 2014-09-24 15:00:51+0100 [HoneyPotTransport,660,221.146.74.146] incoming: aes128-ctr hmac-sha1 none 2014-09-24 15:00:52+0100 [HoneyPotTransport,660,221.146.74.146] NEW KEYS 2014-09-24 15:00:52+0100 [HoneyPotTransport,660,221.146.74.146] starting service ssh-userauth 2014-09-24 15:00:53+0100 [SSHService ssh-userauth on HoneyPotTransport,660,221.146.74.146] root trying auth password 2014-09-24 15:00:53+0100 [SSHService ssh-userauth on HoneyPotTransport,660,221.146.74.146] login attempt [root/123456] failed 2014-09-24 15:00:54+0100 [-] root failed auth password 2014-09-24 15:00:54+0100 [-] unauthorized login: Lots of interesting information that can be extracted from the above data. Event collection and processing: Logstash Logstash is an awesome piece of software and the first layer of the ELK stack, where the journey of an event begins. Starting from the raw data, we want to be able to: So we want to extract fields of interest like source IP, destination port, target usernames and passwords to name the obvious ones. For sake of brevity, we will go specifically after only two log entries here, one from IPTables: Sep 24 15:42:03 debianVM kernel: iptables denied: IN=eth0 OUT= MAC=f2:3c:91:73:6c:71:84:78:ac:0d:8f:41:08:00 SRC=184.95.32.162 DST=85.159.211.137 LEN=48 TOS=0x00 PREC=0x00 TTL=110 ID=6886 PROTO=TCP SPT=37377 DPT=3306 and one from KippoSSH: 2014-09-24 15:00:53+0100 [SSHService ssh-userauth on HoneyPotTransport,660,221.146.74.146] login attempt [root/123456] failed We'll need to tell Logstash where the logs are located and what to do with them, below is our logstash config file (). Notice the three different sections : #input section, what data do we want to collect input { file { type => \"linux-syslog\" path => \"/var/log/kern.log\" } file { type => \"honey-kippo\" path => \"/opt/kippo/kippo-master/log/kippo.log\" } } #filter section, wha","locales":"","title":"Use ELK to Visualise Security Data: IPTables and KippoSSH Honeypot"}
{"index":{}}
{"author":"Isabel Drost-Fromm","category":"Engineering","publish_date":"2014-10-17T00:00:00.000Z","url":"/blog/elasticsearch-testing-qa-increasing-coverage-randomizing-test-runs","seo_title":"","content":" When writing tests, developers tend to have a certain picture of how the code is supposed to work in their mind. Based on that picture they craft tests, ideally checking not only the but also boundary conditions and error cases. Often, though, our perception of what the code should be doing is not in line with what is actually going on. Here's one famous example code snippet that shows unexpected behavior: In Java, there is no integer value defined that would be large enough to represent the absolute value of Integer.MIN_VALUE:  as a result, the . Apart from such \"surprising\" behavior, developer inexperience with the problem domain can lead to overlooking otherwise obvious test cases. For example, engineers inexperienced with geographic coordinates might overlook double checking whether their code works at the North/South pole or at the date line leading to problems in production systems. One approach to find issues in programs used by the security industry is to feed the program under test with all sorts of expected and in particular unexpected or even invalid random input. This particular technique is known as or fuzzing. The term was initially coined by Richard Hamlet in 1994 as one type of black box testing. In the recent past, though, it's become increasingly popular to use the same concept for white box testing by replacing hard coded test values with some automated way of generating valid input data: When using pseudo-random input - based on pre-defined constraints - checking the test result for correctness can be done in multiple ways: Several years ago the Lucene community added support for to their unit test suite. In contrast to other approaches, the idea here is to not only use random input into the program under test, but also create randomized runtime environments. When initializing each test run with a new set of randomly chosen input parameters, it also makes sense to re-run one particular test multiple times, each time with a new set of input parameters. When running tests on a developer's workstation, the number of re-runs should be limited to decrease testing time: Overly long running test suites often aren't executed by the developers after making changes. When running on a continuous integration server, though, the number of iterations can easily be increased to 50 or 100 to cover more ground, especially if the test itself is cheap in terms of runtime. A simple randomization example that runs 100 iterations and is based on a list of length 10 to 100, filled with random short numbers would look like this: 01 @Test 02 @Repeat (iterations = 100) 03 public void testSorting { 04 int length = randomIntBetween(10, 100):  05 short[] list = new short[length]:  06 for (int i = 0:  i < length:  i++) { 07 list[i] = randomShort():  08 } 09 short[] result = Arrays.sort(list):  10 assertTrue(isSorted(result)):  11 } Line 02 defines the number of iterations to run. Each run is initialized with a different test seed leading to different values being generated in the test. On failure, this test seed is provided to the user to allow for reproducing (and fixing) what went wrong deterministically. Line 04 defines the length of our array to be a random value between a maximum and minimum boundary. The max and min values can be omitted to generate just any integer value. Line 07 uses this notation to generate short values. This example only shows a very limited - but already powerful - subset of the functionality provided by the carrotsearch randomized testing framework that is the basis for randomized testing in both Apache Lucene and Elasticsearch. Other types of input parameters that can be generated include, but are not limited to, random String generation (limited to Unicode or ASCII characters only) and input data of every primitive data type available in Java. The framework also checks for threads lingering around after","locales":"","title":"Elasticsearch Testing and QA: Increasing Coverage by Randomizing Test Runs"}
{"index":{}}
{"author":"Aaron Mildenstein","category":"Engineering","publish_date":"2014-10-14T00:00:00.000Z","url":"/blog/little-logstash-lessons-part-using-grok-mutate-type-data","seo_title":"","content":" ","locales":"de-de","title":"Little Logstash Lessons - Part I: Using grok and mutate to type your data"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2014-10-13T00:00:00.000Z","url":"/blog/world-elasticsearch-20","seo_title":"","content":" Last week ended with a great conference in Hamburg, . gave an introduction to the ELK stack with 250 attendees. We also had speaking about “When Search boxes don't work – How to best ensure the quality of your search application\". Upcoming EventsNorth America Oct. 15th-17th: - Stop by our booth (P1) to see live demos, talk to our Solutions Architects, and learn about how Elasticsearch works with Hadoop. Europe Oct. 13th-15th: - Join in the panel discussion about . Wednesday, 2:30-3:20pm. Oct. 16th-19th: - Listen to talking about Polyglot Persistence. You can catch Honza's keynote address this Thursday at 7:00 PM. Oct. 17th: - Don't miss out on talk on which is scheduled for 2:00pm-2:50pm. Upcoming MeetupsNorth America Oct. 14th: : You'll be treated to three use case talks: on Elasticsearch in Production, Real-Time Geo-Replication and Custom Tokenization. Oct. 15th: at Twitter. Hear talks from our very own , and speakers from Concurrent, and Found AS. Seats are all spoken for, but you can still sign up to be added to the waitlist. Oct. 16th: : Join to hear about the full ELK stack, plus a talk on migrating Elasticsearch production clusters. Europe Oct. 14th: ELK use case presentation at the Oct. 16th: “Elasticsearch Performance in a Nutshell\" at the Asia Oct. 18th: Elasticsearch Workshop at the That's it for this week, and stay tuned for Elasticsearch happenings next week. There's much to come! - The Elasticsearch Team P.S.: if you're interested in hosting a meetup or are giving a talk about Elasticsearch - we can offer support and send you swag! ","locales":"","title":"Where in the World Is Elasticsearch? - October 13, 2014"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-10-15T00:00:00.000Z","url":"/blog/2014-10-15-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project.Elasticsearch Core I'll be a hosting an Oct 30th webcast featuring : Anomaly Detection with — Ben Lorica (@bigdata) Elasticsearch EcosystemHere's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases. Slides & VideosHere are the slides for my \"Application Logging in the 21st century\" talk I just gave at today: — Tim Bunce (@timbunce) on the Elasticsearch Language ClientsMy slides about data visualisation and D3.js from the EUROPEN 2014 conference are here: — Karel Minařík (@karmiq) Where to Find UsWe'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking or hosting a meetup, let our Director of Developer Relations, , know!BelgiumWe're a proud sponsor of the 5th anniversary of , held in its original hometown of Ghent. Elasticsearch will have a booth at the conference, so stop by and say hello to and between sessions. We'll be at DevOps Days October 27-28th. Canada will regale all with tales of the ELK stack at the next Montreal Elasticsearch Meetup. You can join Colin this Thursday, Oct. 16th at 6:30 PM. to save your spot.ChinaZeng Yong, aka Medcl, is one of our most active community members in China. He has an upcoming talk on all things Elasticsearch: at QCon Shanghai on October 17, 2014.Medcl is also busy convening the 3rd annual Elasticsearch Users Conference in China for October 25th. You can find out .France GermanyIf you happen to be in Karlsruhe on October 16th, you can catch Patrick Peschlow on Elasticsearch Performance in a Nutshell at the Search Meetup Karlsruhe. , and the meetup will begin at 7:15 PM.For folks in Berlin, we'll have a booth at the GOTO Berlin conference, plus two awesome talks from Elasticsearchers: Join us at the Kosmos Event Center on November 6th and 7th for !IndiaThe Hyderabad Scalability Meetup group is hosting a getting started with Elasticsearch hackathon on October 18th. to save your place, looks like space is limited!Italy Poland SwedenHonza Kral will be speaking at the Øredev conference in Malmö on . The conference runs Nov. 4-7th, and Honza takes the stage on Nov. 6th at 5:40 PM. Switzerland will return to Soft-Shake in Geneva once again this year. He will speak on Advanced Search for Your Legacy Application in the Big Data Track. Soft-Shake takes place on October 23-24th, and David will speak in Slot 5 on Thursday.United Kingdom United States Where to Find YouOur Director of Developer Relations, Leslie Hawthorn, is hard at work to help folks create more Elasticsearch meetup groups and to help meetup organizers find more speakers. If you are interested in either effort, .Oh yeah, we're also . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticsearch, Kibana, and Logstash than where you rest your head.TrainingsIf you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elasticsearch - October 15, 2014"}
{"index":{}}
{"author":"Rashid Khan","category":"Engineering","publish_date":"2014-10-10T00:00:00.000Z","url":"/blog/kibana-4-beta-1-1-pointy-needles-blunted","seo_title":"","content":" The response to Kibana 4 Beta 1 has been amazing! The overwhelmingly positive reactions have been incredible. The whole team is extremely thankful for the bevy of comments and contributions that have been pouring in. We've found some particularly pointy needles in the haystack that we decided should be addressed right away, thus we bring you Kibana 4 Beta 1.1 . This is software. It we do our best to make things great, but this should not be used in production. You can get Kibana 4 Beta 1.1 . Get it now, enjoy it, and be sure to give us your . ","locales":"","title":"Kibana 4 Beta 1.1: Pointy Needles Blunted"}
{"index":{}}
{"author":"Alex Brasetvik","category":"","publish_date":"2014-10-15T00:00:00.000Z","url":"/blog/found-elasticsearch-top-down","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud. The previous article in this series, Elasticsearch from the Bottom Up, covered essential data structures within a single shard. In this article, we will look at the distributed nature of Elasticsearch. ","locales":"","title":"Elasticsearch from the Top Down"}
{"index":{}}
{"author":"Isabel Drost-Fromm","category":"Engineering","publish_date":"2014-10-09T00:00:00.000Z","url":"/blog/elasticsearch-testing-qa-testing-levels-elasticsearch","seo_title":"","content":" \"Works on my machine\" is a phrase that became famous for software projects lacking automated testing infrastructure. Even today, when most checks are done automatically on an integration test server, it's still crucial to be able to reproduce bugs and test features on your local box. Ideally, those should be the same tests that are being run by the CI environment (or some stripped down version thereof). Testing Layers Elasticsearch tests check the code base from multiple perspectives. Traditional unit tests are the usual check that core algorithm implementations are correct and all methods behave the way they should. One level up, integration tests run against a locally running cluster, making sure all pieces of the application work nicely together and can be interacted with through the Java Client API. REST tests make sure all REST endpoints work according to their specification. Backwards compatibility tests are a special case that were introduced recently. Instead of running some test against a cluster containing nodes of only one Elasticsearch version, a previous release can be downloaded, installed and started. Tests then run against a mixed node cluster making sure that everything works as expected and is compatible between releases. Testing the Elasticsearch Java Layer Elasticsearch attacks testing from a bunch of different angles. Java code is tested on more than on the unit test level:  the Java Client API is also checked by integration tests that pull up complete Elasticsearch clusters to run requests against. Essentially, the goal for integration tests (based on the class ElasticsearchIntegrationTest) is to make sure Java API calls work against a full running cluster. It is cheap to pull up an example Elasticsearch cluster in terms of CPU power and memory needed, even on an ordinary laptop. When extending the above test class, it also becomes simple in terms of development overhead. The cluster is pulled up for you and reused between tests unless you specify something else in the test's ClusterScope annotation. Looking at an example integration test, let's walk through the most important annotations and features that makes writing Elasticsearch integration tests so trivial: 01 // make sure all tests in the test suite run on a separate test cluster as we will modify the 02 // cluster configuration 03 @ElasticsearchIntegrationTest.ClusterScope(scope = ElasticsearchIntegrationTest.Scope.SUITE) 04 public class TemplateQueryTest extends ElasticsearchIntegrationTest { 05 06 @Before 07 public void setup() throws IOException { 08 // create an index, make sure it is ready for indexing and add documents to it 09 createIndex(\"test\"):  10 ensureGreen(\"test\"):  11 12 index(\"test\", \"testtype\", \"1\", jsonBuilder().startObject().field(\"text\", \"value1\").endObject()):  13 index(\"test\", \"testtype\", \"2\", jsonBuilder().startObject().field(\"text\", \"value2\").endObject()):  14 refresh():  15 } 16 17 // for our test we want to make sure the config path of the cluster actually points 18 // to the test resources that we provide - this is the cluster modification referred 19 // to earlier 20 @Override 21 public Settings nodeSettings(int nodeOrdinal) { 22 return settingsBuilder().put(super.nodeSettings(nodeOrdinal)) 23 .put(\"path.conf\", this.getResource(\"config\").getPath()).build():  24 } 25 26 @Test 27 public void testTemplateInBody() throws IOException { 28 Map<String, Object> vars = new HashMap<>():  29 vars.put(\"template\", \"all\"):  30 31 TemplateQueryBuilder builder = new TemplateQueryBuilder( 32 \"{\\\"match_{{template}}\\\": {}}\\\"\", vars):  33 34 // the search client to use in the test comes pre-configured as part of the 35 // integration test 36 SearchResponse sr = client().prepareSearch().setQuery(builder) 37 .execute().actionGet():  38 39 // specific assertions make checks very simple 40 assertHitCount(sr, 2):  41 } In our example, line 03 defines the cluster scope o","locales":"","title":"Elasticsearch Testing & QA: Testing Levels of Elasticsearch"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-10-08T00:00:00.000Z","url":"/blog/2014-10-08-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project.Elasticsearch core So happy to be tweeting 4 screenshots. are awesome — Rashid Khan (@rashidkpc) Elasticsearch EcosystemHere's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases. Happy to announce that for Apache Hadoop 2.1 is now certified on Cloudera 5, including support for Apache Spark! — Cloudera Connect (@ClouderaConnect) Slides & VideosThis week, we're bringing you a whole whack of Elasticsearch love from last week's DrupalCon Amsterdam! Not a Drupalista? No problem - these videos are great for all lovers of things PHP and folks interested in learning how to scale the ELK stack for centralized logging and monitoring. Where to find UsWe'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking or hosting a meetup, let our Director of Developer Relations, , know!BelgiumWe're a proud sponsor of the 5th anniversary of , held in its original hometown of Ghent. Elasticsearch will have a booth at the conference, so stop by and say hello to and between sessions. We'll be at DevOps Days October 27-28th. ChinaZeng Yong, aka Medcl, is one of our most active community members in China. He has two upcoming talks on all things Elasticsearch: Medcl is also busy convening the 3rd annual Elasticsearch Users Conference in China for October 25th. You can find out .France GermanyThe code.talks conference is back in Hamburg this year on October 9th and 10th, though with a new name. (We had a great time there last year when this conference was known simply as Developer Conference EU.) will be speaking once again, along with . Here's all the Elasticsearch and ELK stack information on offer at : The week after code.talks, will be heading LinuxCon Europe in Dusseldorf, where she'll speak on the panel Empowering Your Corporate Open Source Software Developers. The panel takes place on Wednesday, October 15th at 2:30 PM, and the conference runs October 13-15th at Congress Centre Dusseldorf.And, if you happen to be in Karlsruhe on October 16th, you can catch Patrick Peschlow on Elasticsearch Performance in a Nutshell at the Search Meetup Karlsruhe. , and the meetup will begin at 7:15 PM.IndiaThe Hyderabad Scalability Meetup group is hosting a getting started with Elasticsearch hackathon on October 18th. to save your place, looks like space is limited!Poland SwedenHonza Kral will be speaking at the Øredev conference in Malmö on . The conference runs Nov. 4-7th, and Honza takes the stage on Nov. 6th at 5:40 PM. Switzerland will return to Soft-Shake in Geneva once again this year. He will speak on Advanced Search for Your Legacy Application in the Big Data Track. Soft-Shake takes place on October 23-24th, and David will speak in Slot 5 on Thursday.United KingdomThe Leeds DevOps meetup group will convene on Oct. 14th for their one year anniversary meeting! Along with the celebration, you can also enjoy a use case talk from the fine folks at LateRooms on how they use the ELK stack to grapple with millions of lines of log data. You can still !United States Where to Find YouOur Director of Developer Relations, Leslie Hawthorn, is hard at work to help folks create more Elasticsearch meetup groups and to help meetup organizers find more speakers. If you are interested in either effort, .Oh yeah, we're also . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticsearch, Kibana, and Logstash than where you rest your head.TrainingIf yo","locales":"","title":"This Week in Elasticsearch - October 08, 2014"}
{"index":{}}
{"author":"Costin Leau","category":"News","publish_date":"2014-10-08T00:00:00.000Z","url":"/blog/elasticsearch-hadoop-2-0-2-and-2-1-beta2","seo_title":"","content":" I am pleased to announce Elasticsearch for Apache Hadoop releases and . (If you haven't been following our story so far, is our connector that serves up real-time search & analytics for your Hadoop deployments.) 2.0.2 is the latest release containing several bug fixes and is recommended upgrade for all existing users. 2.1.Beta2 is the second from the development branch bringing a number of new features and improvements besides the typical bug fixes, adding and support. Spark SQL Support 2.1 Beta2 extends our Spark support through integration. One can save s to Elasticsearch or materialize them based on indices or queries (effectively creating ). For example, finding out the “Smith\"s is a one liner: import org.apache.spark.sql.SQLContext import org.elasticsearch.spark.sql._ ... val people = sqlContext.esRDD(\"spark/people\",\"?q=Smith\") // check the associated schema println(people.schema) // root // |-- name: string (nullable = true) // |-- surname: string (nullable = true) // |-- age: long (nullable = true) The data and its associated schema are loaded through the returned and through Spark SQL, and can be further interrogated through SQL. Writing to Elasticsearch looks strikingly similar, as any can be indexed. For this example, let's use the Java support: import org.apache.spark.sql.api.java.*:  import org.elasticsearch.spark.sql.java.api.JavaEsSparkSQL:  JavaSchemaRDD people = JavaSQLContext.parquetFile(\"people.dat\") // filter data using SQL people.registerTempTable(\"people\"):  JavaSchemRDD teenagers = sqlContext.sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\") // index it to Elastic JavaEsSparkSQL.saveToEs(teenagers, \"spark/teens\"):  Again, it's just a one liner to save the data to Elasticsearch. In addition to the Spark SQL support, the Spark module has had several improvements: the existing s have been enhanced to s and the code base has been upgraded to Spark 1.1 while with Spark 1.0. CDH 5.1 Certified Happy to announce that for Apache Hadoop 2.1 is now certified on Cloudera 5, including support for Apache Spark! — Cloudera Connect (@ClouderaConnect) Speaking of Spark, we are glad to report that es-hadoop is now officially certified for CDH 5.1 (in addition to ) this time the category. We are tracking our releases to Hadoop's releases to make sure our product evolves in step with its ecosystem, giving our users peace of mind knowing that es-hadoop will simply work out of the box. Apache Storm Integration 2.1 Beta2 makes a first class citizen. (And, by the way, congrats to the Storm team for to a top level project) in the Apache incubator. es-hadoop brings real-time search and analytics to Storm's stream data processing platform through dedicated native and implementation to ingest data and fan-out queries from and to Storm topologies. To index data to Elasticsearch simply use : TopologyBuilder builder = new TopologyBuilder():  builder.setBolt(\"esBolt\", new EsBolt(\"twitter/tweets\")):  Executing queries in Elasticsearch for Storm is yet another one-liner: TopologyBuilder builder = new TopologyBuilder():  builder.setSpout(\"es-spout\", new EsSpout(\"twitter/tweets\", \"?q=nfl*), 5):  builder.setBolt(\"bolt\", new PrinterBolt()).shuffleGrouping(\"es-spout\"):  That's it! Under the covers, es-hadoop uses its parallelized infrastructure to the and instances across the index shards for what we call . Low-latency/high-performance patterns like and are supported to provide excellent through-put out of the box and closely integrate the real-time capabilities of Storm and Elasticsearch. Elasticsearch 1.4 Repository Support Elasticsearch 1.4 Beta 1 was released bringing significant enhancements especially in resilience area. Among them, the snapshot and restore infrastructure has been , with the new version supported by 2.1 Beta 2. (For Elasticsearch 1.0 – 1.3 please use es-hadoop 2.0.x.) Elasticsearch Comes to NYC If you happen to be in NYC next week and are interested in Elasticsearch","locales":"","title":"Elasticsearch Hadoop 2.0.2 and 2.1.Beta2 Released"}
{"index":{}}
{"author":"Karel Minařík","category":"Engineering","publish_date":"2014-10-07T00:00:00.000Z","url":"/blog/playing-http-tricks-nginx","seo_title":"","content":" One of the defining features of Elasticsearch is that it’s exposed as a (loosely) RESTful service over HTTP.The benefits are easy to spell out, of course: the API is familiar and predictable to all web developers. It’s easy to use with “bare hands”� via the command, or in the browser. It’s easy to write API wrappers in various programming languages.Nevertheless, the importance of the HTTP-based nature of Elasticsearch is rooted deeper: in the way it fits into the existing paradigm of software development and architecture.HTTP As an Architectural ParadigmThe typical modern software or information system is quite frequently a collection of loosely coupled services, communicating over network: typically, via HTTP. Design-wise, the important aspect of this approach is that you can always “rip apart” the chain of services, and insert another component, which adds or changes functionality, into the “stack�.” In the old days, this has been traditionally called “middleware,” but it resurfaced in context of RESTful web services, for example as , used notably in the .HTTP is particularly well suited for such architectures, because its perceived shortcomings (lack of state, text-based representation, URI-centric semantics, …) turn into an advantage: neither “middleware”� has to accommodate for something specific in the “chain”, and just passes along status codes, headers, and bodies. In this sense, HTTP is – it doesn’t matter, for example, if you fetch an image from the original web server or a cache on a different continent. It’s still the same “resource�.” is a prime example of this aspect of HTTP, presented already in Roy Fielding’s on RESTful architectures. (For a thorough information on the subject, see Ryan Tomayko’s and Mark Nottingham’s .)Technically, the cache operates as a here – it “stands for” some other component in the stack.But proxies can do so much more. A good example is authentication and authorization: a proxy can intercept requests to a service, perform authentication and/or authorization routines, and either allow or deny access to the client.This type of proxy is usually called a . The name makes sense when you consider that a traditional proxy “forwards” traffic from a network to the network (the internet), which is reversed here, because the “reverse” proxy forwards requests from the internet to a “local” backend. Such a proxy could be implemented in a programming language like Java or Go, or with a framework like Node.js. Alternatively, we could use a configurable webserver like Nginx.Nginx is an open source web server, originally writen by Igor Sysoev, focused on high performance, and low memory footprint. (For a detailed technical overview, see the relevant chapter of the book.)Nginx has been designed with a proxy role in mind from the start, and supports many related configuration directives an options. It is fairly common to run Nginx as a load balancer in front of Ruby on Rails of Django applications. Many large PHP applications even running to accelerate serving static content and scale the application. Most parts of this article assume a , but the advanced parts rely on the .To run Nginx as a “100% transparent”� proxy for Elasticsearch, we need a very minimal configuration:http { server { listen 8080:  location / { proxy_pass http://localhost:9200:  } } } When we execute a request to , we’ll get a response from Elasticsearch running on port 9200.This proxy is of course quite useless — it just hands over data between the client and Elasticsearch:  though astute readers might have guessed that it adds something to the “stack,” namely the logging of every request.Use CasesIn this article, we’ll go through some of the more interesting use cases for Nginx as a reverse proxy for Elasticsearch.Persistent HTTP ConnectionsLet’s start with a very simple example: using Nginx as a proxy which keeps persist","locales":"de-de","title":"Playing HTTP Tricks with Nginx"}
{"index":{}}
{"author":"Rashid Khan","category":"News","publish_date":"2014-10-06T00:00:00.000Z","url":"/blog/kibana-4-beta-1-released","seo_title":"","content":" We’re pretty darn happy, to share the future of Kibana, and the first beta release of Kibana 4 with you today. This is software. It we do our best to make things great, but this should not be used in production. I want it now! Gimmie! Get it , see the for the new (easier!) installation procedure. That said, you really should read the rest, there’s some great tips down there. Welcome to Kibana 4 We’re taking the long road with Kibana 4: You can expect to see several beta releases, each with new features, visualizations and enhancements. We combed over feedback, mailing lists, IRC and the Github issues to compile the features that made it into Beta 1, and we think we hit a lot of the biggies. We’re already hard at work on Beta 2 and we’re happy to share our roadmap with you, simply checkout the tagged issues in Github. As always, your feedback is crucial in making sure we get it right. In addition to your feedback we took a step back to consider how people look at data, and further, how they solve real problems. We found that one question will lead to others and those questions will lead to yet more. If you attended Monitorama, or any of a handful of Elasticsearch meet ups, you may have already seen the Kibana 4 proof of concept demo that allowed you to progressively create ever more complex charts. Kibana 4 takes that PoC and expands it to dozens of new features that allow you to compose questions, get answers, and solve problems like never before. That level of composability can be found throughout Kibana 4 in the way aggregations, searching, visualizations and dashboards fit together. To simplify and streamline composition we’ve broken out Kibana 4 into three distinct interfaces, all working together, each adept at answering a unique set of questions. A familiar face If you’re a long term Kibana user you will recognize and feel right at home in the Discover tab. Discover functions much like a traditional search interface with a list of documents and a timeline of events. Type in a search, hit enter and let Kibana dig through your Elasticsearch index. Speaking of indices, a quick drop down allows you to quickly switch between indices while you search. If you want to switch back, click your browser’s back button and you’re there. Don’t like your new search terms? The old ones are a back button click away, or available in the history of the search field. And speaking of searches, feel free to type either Lucene Query String syntax or, an oft requested feature, into that search box. We know JSON can be tricky to type out, so whether you use Lucene Query Strings, or JSON, we’ll validate the syntax for you before shipping it off to Elasticsearch. And that holds true no matter where you type a query in Kibana 4. Those queries can also be saved for later. Importantly, queries are no longer bound to the dashboard, they can be recalled in Discover, or even tied to a visualization which is later put on a dashboard. Plus, the , no matter what screen you’re on, so linking to queries is super easy. I’m here for the charts The Visualize tab of Kibana 4 is the culmination of that long-in-the-tooth proof of concept I talked about earlier. Kibana 4 brings the power of Elasticsearch’s nested aggregations to the click of a mouse. Maybe I want to know what countries are hitting my site, when they’re doing so and whether or not they are authenticated. I can ask that question, and see how the answers relate to each other all within a single request on a single canvas: While Kibana 3 could only show time on the histogram panel, and terms on bar chart, Kibana 4 can make use of multiple . These include both bucket and metric aggregations, including the much anticipated (aka unique count) aggregation, and support for more is on the way. We had to build an entirely new visualization framework to deal with the complexity of aggregations. Right now there are three supported types: Bar charts, line charts and sunburst charts. Fe","locales":"","title":"Kibana 4 Beta 1 Released"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-10-01T00:00:00.000Z","url":"/blog/2014-10-01-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project.Elasticsearch Core Full house at last night's DevOps Amsterdam Meetup: DrupalCon & ELK Stack StyleElasticsearch EcosystemHere's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases. . and talking about at for meetup. — David Pilato (@dadoonet) Slides & VideosA great lessons learned story from Etsy, including how they're using Elasticsearch & Logstash in their new architectureAn incredibly in-depth use case shared at last week's Elasticsearch Netherlands Meetup. Thanks again for hosting us, Bol.com!An end to end tutorial on the ELK stack, including you can use as you watch Elasticsearch Aggregations Webineri Tamamlandı (Video) — kodcu.com (@kodcucom) Where to Find UsWe'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking or hosting a meetup, let our Director of Developer Relations, , know!AustriaThe Vienna Elasticsearch User Group will convene tomorrow on October 2nd at 7 PM. This next meetup will focus on Elasticsearch use cases. You can to attend.BelgiumWe're a proud sponsor of the 5th anniversary of , held in its original hometown of Ghent. Elasticsearch will have a booth at the conference, so stop by and say hello to between sessions. We'll be at DevOps Days October 27-28th. ChinaZeng Yong, aka Medcl, is one of our most active community members in China. He has two upcoming talks on all things Elasticsearch: France GermanyThe code.talks conference is back in Hamburg this year on October 9th and 10th, though with a new name. (We had a great time there last year when this conference was known simply as Developer Conference EU.) will be speaking once again, along with . Here's all the Elasticsearch and ELK stack information on offer at code.talks: The week after code.talks, will be heading LinuxCon Europe in Dusseldorf, where she'll speak on the panel . The panel takes place on Wednesday, October 15th at 2:30 PM, and the conference runs October 13-15th at Congress Centre Dusseldorf.And, if you happen to be in Karlsruhe on October 16th, you can catch Patrick Peschlow on Elasticsearch Performance in a Nutshell at the Search Meetup Karlsruhe. , and the meetup will begin at 7:15 PM.Japan: Jun Ohtani will attend the in Tokyo. Jun will present on the ELK stack and why he works for Elasticsearch. Global Hack Day takes place October 3rd-5th. The NetherlandsNikolay Ignatov and Welin Welchev from Propeople will present on for Drupal. Join them on Thursday at 10:45 AM to learn all about the Elasticsearch Connector module!NorwayThe next Elasticsearch Oslo Meetup is on for October 7th at 6 PM. to save your seat. We'll get back to you with more details as soon as the meeting agenda is finalized.Poland SpainThe first ever Elasticsearch Meetup in Madrid has been scheduled! The meetup will take place on October 7th from 7:00 - 8:30 PM. Our very own will cover all things Elasticsearch. You can to save your place.SwedenTwo of our developers, and , will be visiting Sweden on October 7th. Starting at 7:00 PM, we'll convene our sixth Elasticsearch Stockholm Meetup, featuring a demo of our new high-level Python library elasticsearch-dsl and an overview of circuit breaker. You can to save your place.Switzerland will return to Soft-Shake in Geneva once again this year. He will speak on Advanced Search for Your Legacy Application in the Big Data Track. Soft-Shake takes place on October 23-24th, and David will speak in Slot 5 on Thursday.New meetup: Minneapolis on Oct. 22nd, 6:30 PM. Join us t","locales":"","title":"This Week in Elasticsearch - October 01, 2014"}
{"index":{}}
{"author":"Aaron Mildenstein","category":"Engineering","publish_date":"2014-09-26T00:00:00.000Z","url":"/blog/write-logstash-input-plugin","seo_title":"","content":" Logstash is an event processing pipeline, which features a rich ecosystem of plugins, allowing users to push data in, manipulate it, and then send it to various backends. While there is a multitude of plugins currently available for Logstash, perhaps, the one that fits your exact needs has yet to be created. That’s where you come in…To that end, this tutorial is to help walk you through the process of building your own. We’re going to use the source code from the plugin to go through the process of creating an input plugin for Logstash. This tutorial expects a certain amount of Ruby knowledge, but, hopefully, you will find it fairly easy to follow. Anatomy of an input plugin For this example, we’ll be using the . While this is a basic plugin, the design principles and requirements apply to any input plugin. This document will refer to the source code by line (with links to the relevant block) and will show inline examples. Clicking the links will take you to highlighted parts of the code. It seems like a small thing, but please don’t omit adding to the top of your input plugin. Logstash depends on things being in UTF-8, so we put this here to tell the Ruby interpreter that we’re going to be using the UTF-8 encoding. A Logstash input plugin requires some parent classes that can be referenced through the indicated statements. The following statements are mandatory: You may also need the require statement if your plugin is going to obtain the local hostname by way of a call Of course, the plugin you build may depend on other code, or even gems. Just put them here along with these Logstash dependencies. Here, we’ll cover many subsections one by one. : Where the action is How do I make my own plugin from this? Since you now have a good overview of how a plugin is built, and what the flow looks like, you should be able to envision your path to a plugin that does what you want. Most of what you will want to do will be in the method, or, at least, will be accessed from the method. You can write other methods, include other required gems or code, and basically get your plugin to do anything you want so long as you: Testing Write unit and integration tests to ensure that your plugin behaves as expected. Tests for existing plugins are in the Logstash code in the path in the input, codec, filter, and output directories. These files provide excellent examples from which to derive your own tests. If you would like to submit your plugin to the greater Logstash community, please be sure to include tests! A few examples of input plugin tests (ranging from simple to more complex) are: Summary This is a simple example of how you could write your own Logstash input plugin. Your final product can be as simple or as complex as your needs require. Once you find how easy it can be to write your own input plugin, you are empowered to make Logstash work for you in new and exciting ways! And once you get your new plugins working, we’d love to hear about them? Just drop us a line on so we can share in your awesomeness. Happy Logstashing! ","locales":"","title":"How to write a Logstash input plugin"}
{"index":{}}
{"author":"Martijn Laarman","category":"Engineering","publish_date":"2014-09-16T00:00:00.000Z","url":"/blog/nest-elasticsearch-net-1-1-released","seo_title":"","content":" It's been a while since we released the of the NEST & .NET clients for Elasticsearch. In the meantime, we've done two releases, as most of our efforts have focused on getting out the door.And, here you have it - all that's new in NEST and Elasticsearch.NET.Map all the APIs!We are very pleased to announce that we now support the API's that are exposed in Elasticsearch 1.3.2! And NEST now also supports of the .The only APIs still missing from NEST are the Search Template related APIs. While we are pretty far in spiking a nice way to support mustache templates using C#, we decided to let it bake a while longer instead of rushing it into this 1.1 release.We've also exposed the internal on the NEST and Elasticsearch.NET clients so that you can hit any URL while still taking advantage of the cluster failover and built-in response handling. This new feature will come in handy when calling Elasticsearch plugin APIs.Map all the aggregations!We've added support for the following aggregations: Loads of miscellaneous improvementsWe've also improved our API coverage for existing APIs in tons of places and fixed several bugs. Please have a look at our complete changelog .We have to give a special shoutout to who sent a total of 8 pull requests improved our multifield mapping API in a backwards compatible fashion. Thanks to Patric's work, the multifield mapping API more closely resembles the new way of mapping properties.Future plansWhile we are very pleased to release 1.1, we can not sit still while our colleagues working on Elasticsearch are moving at such an incredible pace. The list of will be our main focus for the 1.2 release. We'll also be going over all the existing APIs in a very rigorous manner to see if there are features that have been updated that we've missed.As always please don't hesitate to submit your questions/suggestions/issues to either or . ","locales":"","title":"NEST and Elasticsearch.NET 1.1 released!"}
{"index":{}}
{"author":"Isabel Drost-Fromm","category":"Engineering","publish_date":"2014-10-02T00:00:00.000Z","url":"/blog/elasticsearch-testing-qa-elasticsearch-continuous-integration","seo_title":"","content":" Writing tests is a crucial part of developing new features for and fixing bugs in Elasticsearch. In order to make creating these tests as easy as possible, we use a couple of techniques to help developers. Many of these are available to our downstream users as well. Elasticsearch CI Elasticsearch regularly goes through quite a few test cycles, including after each new commit and triggered on a continuous basis. After check in, the Elasticsearch code base goes through a set of automated tests (to a large extent inspired by what is run for Apache Lucene itself). On each commit, a smoke test verifies the system can be built at all, and whether basic functionality (connecting to the resulting search server, installing plugins, etc.) are possible. Builds that pass this initial sanity check are examined further. The Java Unit and Integration test suites are executed, and afterwards backwards compatibility (where applicable) is verified. Finally, the REST tests are checked. In addition to these checks, we run all Java level checks on a continuous basis on several hardware configurations and operating systems. All jobs are Jenkins managed. The key to reducing manual configuration overhead in our setting is an aggressive automation strategy. For the cloud in particular, it pays quickly to have dedicated scripts for initializing all instance types needed for testing. In our case this includes being able to spin up clusters of any topology to test various ingestion scenarios. In the remaining two blog posts, we will first walk you through the various types of tests that we run to ensure Elasticsearch works as expected. Later in the series, we'll show how we use test randomization to increase test coverage not only at the Java level but also to optimize time and hardware resources needed to check our code base on a large set of deployment configurations including but not limited to testing against various JDK versions.\" ","locales":"","title":"Elasticsearch Testing & QA: Elasticsearch Continuous Integration"}
{"index":{}}
{"author":"Konrad Beiske","category":"","publish_date":"2014-09-30T00:00:00.000Z","url":"/blog/found-crash-elasticsearch","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud. As much as we love Elasticsearch, at Found we've seen customers crash their clusters in numerous ways. Mostly due to simple misunderstandings and usually the fixes are fairly straightforward. In our quest to enlighten new adopters and entertain the experienced users of Elasticsearch, we'd like to share this list of pitfalls. ","locales":"","title":"Six Ways to Crash Elasticsearch"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2014-09-29T00:00:00.000Z","url":"/blog/world-elasticsearch-18","seo_title":"","content":" We had a fantastic meetup premiere in Utrecht last week with 140 people attending. Thanks again to bol.com for hosting us and to R. Toma for giving a talk on “Scaling an ELK stack @bol.com\", it was a pleasure to be there! The are already available, too! Tonight's NL Meetup. Largest ever, first outside of Amsterdam and group up to 500 members. Woot! — Leslie Hawthorn (@lhawthorn) gave a talk last week on at PuppetConf 2014 where attendees were happy to conclude the outstanding conference with #hugops. Stay tuned to see the full recording, and thanks to our friends at Puppet for having us! Glad to see the final slide of the conf is . — Nick Lewis (@nick_lewis) This week we are once again covering the world with different events. From Aarhus to Stockholm via Bellevue and Tokyo to Vienna. Enjoy! Upcoming Events Europe Sept. 29th-30th: Goto Aarhus (Denmark) - join us for this interesting guest speaking session where Yann Cluchey is presenting how . Tuesday 30th September, 1:20pm-2:10pm. Tokyo Oct. 3rd: . Meet for a talk about the ELK stack. Upcoming Meetups North America Oct. 2nd: Europe Oct. 1st: Oct. 2nd: That's it for this week, and stay tuned for Elasticsearch happenings next week. There's lots to come! - The Elasticsearch Team P.S.: Contact us if you're interested in hosting a meetup or are giving a talk about Elasticsearch - we can offer support and send you swag! ","locales":"","title":"Where in the World Is Elasticsearch? - September 29, 2014"}
{"index":{}}
{"author":"Richard Pijnenburg","category":"User Stories","publish_date":"2014-09-23T00:00:00.000Z","url":"/blog/celebrating-success-news-puppetconf-2014","seo_title":"","content":" I first began developing the Elasticsearch Puppet module at a previous employer, where I was already a huge fan of both technologies. I loved working with Puppet because it was so easy to use:  its configuration language is very descriptive, making it simple to understand what you’re doing under the hood. I’d really enjoyed my experiments with Logstash and Elasticsearch, and started whipping up Puppet modules for both of them in my spare time, hoping to see both deployed at my workplace. These efforts led to me joining Elasticsearch to work on the Puppet modules full-time, and the rest, as they say, is history. Today, I have a great news to add to that story. If you’re attending PuppetConf 2014, you’ve probably heard about the brand new . I’m really extremely honored and excited to share that the Elasticsearch Puppet module was selected as one of the very first modules to be Puppet Approved! The team at Puppet Labs has created the Puppet Approved program to make it easier for customers to choose modules for specific automation tasks and quickly deploy them to production. Puppet Approved modules are recommended by Puppet Labs, and meet its standards for quality composition, reliable operation, and active development. In true open source fashion, the Elasticsearch Puppet module was a personal side project turned dream job. I’m thrilled to see that so many members of the Puppet community find the Elasticsearch Puppet module useful. Happy Puppetizing! ","locales":"","title":"Celebrating Our Success: News from PuppetConf 2014"}
{"index":{}}
{"author":"Njal Karevoll","category":"","publish_date":"2014-09-16T00:00:00.000Z","url":"/blog/found-scripting-elasticsearch","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud.Scripting is an important part of the toolbox of any Elasticsearch user, and enables evaluating custom expressions that may be used to synthesize fields or provide customized scoring. In this post we take a brief look at the upcoming changes to the scripting module and the different scripting languages available to use today. In a recent about scripting, Elasticsearch outlined some coming changes to its scripting support. The two important takeaways from that post is that the sandboxing story of scripts will be improved and that the default scripting language will be changed from MVEL to Groovy. Let’s take a look at the current official scripting languages. ","locales":"","title":"Scripting with Elasticsearch"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2014-10-06T00:00:00.000Z","url":"/blog/world-elasticsearch-19","seo_title":"","content":" Last week we attended Goto Aarhus where Yann Cluchey talked about the Elasticsearch use case at Cogenta. Great talk and a lot of interested folks. Check out the slides . A great turnout for Yann from talking about using — Gary Harvey (@Gshtrifork) Furthermore was on in Amsterdam - our 2nd home town. Here's a small flashback and twitter highlight of the week! How elastic is ? It's unbelievable elastic :) — Frontkom (@frontkom) So here's what's on the list for next week: Chicago - Paris - Madrid - Stockholm - Oslo - Hamburg. Read on for more! Upcoming Events North America Oct. 7th-8th: - stop by our table to pick up swag, and visit with our midwest territory manager , and Solutions Architect . Europe Oct. 7th: - will give a short talk with our partner Alterway about the and will later also present . will give an overview of the . Oct. 9th-10th: - don't miss out on our two Elasticsearch talks: 1. will give an , Thursday 9th October, 1pm. 2. will talk about , Friday 10th October, 3pm. Furthermore we have lovely and at our booth, so go say hello to them and grab a cool piece o' swag. Upcoming Meetups Europe Oct. 7th: Oct. 7th: Oct. 7th: That's it for this week, and stay tuned for Elasticsearch happenings next week. There's lots to come! - The Elasticsearch Team P.S.: if you're interested in hosting a meetup or are giving a talk about Elasticsearch - we can offer support and send you swag! ","locales":"","title":"Where in the World Is Elasticsearch? - October 06, 2014"}
{"index":{}}
{"author":"Clinton Gormley","category":"Engineering","publish_date":"2014-10-01T00:00:00.000Z","url":"/blog/elasticsearch-1-4-0-beta-released","seo_title":"","content":" <!--<br /> div.itemizedlist { margin-top: -15px}<br /> --> Today, we are happy to announce the release of , based on . You can download them and read the full changes list here: . The theme of 1.4.0 is : making Elasticsearch more stable and reliable than ever before. It is easy to be reliable when everything functions as it should. The difficult part comes when the unexpected happens: nodes run out of memory, their performance is degraded by slow garbage collections or heavy I/O, networks fail or transmit data erratically. This Beta release includes three major efforts to improve resiliency: Distributed systems are complex. We have an extensive test suite which creates random scenarios to try to simulate conditions that we could never imagine, but we recognise that there are an infinite number of edge cases. The changes in 1.4.0.Beta1 include all of the improvements that we have made thus far. We ask you to test these changes out in the real world and to . { \"@context\": \"http://schema.org\", \"@type\": \"Organization\", \"name\" : \"Elastic\", \"url\": \"https://www.elastic.co/\", \"logo\": \"https://www.elastic.co/static/images/elastic-logo-200.png\", \"sameAs\" : [ \"https://www.facebook.com/elastic.co\", \"https://twitter.com/elastic\", \"https://plus.google.com/105178019064686397293\", \"https://www.youtube.com/user/elasticsearch\", \"https://www.linkedin.com/company/elasticsearch\" ], \"potentialAction\": { \"@type\": \"SearchAction\", \"target\": \"https://www.elastic.co/search?q={query_string}\", \"query-input\": \"required name=query_string\" } } START Parse.ly Include: Standard (function(s, p, d) { var h=d.location.protocol, i=p+\"-\"+s, e=d.getElementById(i), r=d.getElementById(p+\"-root\"), u=h===\"https:\"?\"d1z2jf7jlzjs58.cloudfront.net\" :\"static.\"+p+\".com\":  if (e) return:  e = d.createElement(s):  e.id = i:  e.async = true:  e.src = h+\"//\"+u+\"/p.js\":  r.appendChild(e):  })(\"script\", \"parsely\", document):  END Parse.ly Include: Standard ","locales":"","title":"Elasticsearch 1.4.0.Beta1 released"}
{"index":{}}
{"author":"Clinton Gormley","category":"Engineering","publish_date":"2014-09-29T00:00:00.000Z","url":"/blog/elasticsearch-1-3-3-released","seo_title":"","content":" Today, we are happy to announce the bugfix release of , based on . You can download it and read the full changes list here: . Elasticsearch 1.3.3 is the latest stable release and we recommend that all users upgrade. Some of the highlights of this release include: { \"@context\": \"http://schema.org\", \"@type\": \"Organization\", \"name\" : \"Elastic\", \"url\": \"https://www.elastic.co/\", \"logo\": \"https://www.elastic.co/static/images/elastic-logo-200.png\", \"sameAs\" : [ \"https://www.facebook.com/elastic.co\", \"https://twitter.com/elastic\", \"https://plus.google.com/105178019064686397293\", \"https://www.youtube.com/user/elasticsearch\", \"https://www.linkedin.com/company/elasticsearch\" ], \"potentialAction\": { \"@type\": \"SearchAction\", \"target\": \"https://www.elastic.co/search?q={query_string}\", \"query-input\": \"required name=query_string\" } } START Parse.ly Include: Standard (function(s, p, d) { var h=d.location.protocol, i=p+\"-\"+s, e=d.getElementById(i), r=d.getElementById(p+\"-root\"), u=h===\"https:\"?\"d1z2jf7jlzjs58.cloudfront.net\" :\"static.\"+p+\".com\":  if (e) return:  e = d.createElement(s):  e.id = i:  e.async = true:  e.src = h+\"//\"+u+\"/p.js\":  r.appendChild(e):  })(\"script\", \"parsely\", document):  END Parse.ly Include: Standard ","locales":"","title":"Elasticsearch 1.3.3 released"}
{"index":{}}
{"author":"Ryan Ernst","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-09-24T00:00:00.000Z","url":"/blog/2014-09-24this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. Elasticsearch Core Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases. RT : Big news: is here & we're honored to be part of the inaugural module class — Puppet Labs (@puppetlabs) We celebrated our largest ever Elasticsearch Netherlands Meetup with Bol.com on Monday. Many thanks to them for hosting, and to Renzo for sharing their very detailed use case! ","locales":"","title":"This Week in Elasticsearch - September 24, 2014"}
{"index":{}}
{"author":"","category":"Engineering","publish_date":"2014-09-18T00:00:00.000Z","url":"/blog/top-hits-aggregation","seo_title":"","content":" ","locales":"","title":"The Top Hits Aggregation"}
{"index":{}}
{"author":"Konrad Beiske","category":"","publish_date":"2014-09-14T00:00:00.000Z","url":"/blog/found-zookeeper-king-of-coordination","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud.Let's explore Apache ZooKeeper, a distributed coordination service for distributed systems. Needless to say, there are plenty of use cases! At Found, for example, we use ZooKeeper extensively for discovery, resource allocation, leader election and high priority notifications. In this article, we'll introduce you to this King of Coordination and look closely at how we use ZooKeeper at Found. ","locales":"","title":"ZooKeeper - The King of Coordination"}
{"index":{}}
{"author":"Steve Mayzak","category":"News","publish_date":"2014-09-10T00:00:00.000Z","url":"/blog/bring-your-own-demo","seo_title":"","content":" There are so many great examples of community members using the powers of Elasticsearch, Logstash, and Kibana (the ELK stack) to get meaningful insights from data. From companies: The list goes on and on. At Elasticsearch, we're continually inspired and amazed at the creative ways companies are using our software stack to extract meaningful value from their data. It should come as no surprise that we also work hard on finding insights in many publicly available data sets in order to put our products to the test. Unfortunately, as of yet, we haven’t had a great collaborative way to share this work with the community and for you to share with each other. That changes now!Start Your EnginesWe are delighted to announce a new repository in GitHub under  where we will begin sharing easy-to-use demos of the ELK stack for everyone to enjoy, enhance, and contribute back with the goal of greater education. These demos will include all Logstash configs, plus Kibana dashboards and custom settings in Elasticsearch to make them a great, real-world starting point for those of you looking to kickstart an internal project. We aren’t going to limit this repo to just us, though. If you have come across a great public dataset and want to contribute a demo to the repo, please do. You can also post an issue in the repo with a link and description of a dataset you would like to see turned into a demo eventually. While we probably won’t be able to get to all of them, we promise to do our best!Under the HoodWith the goal of making your demo-playing experience as pain-free as possible, we have come up with a simple process and structure. We will adhere to this structure for all the demos we create and ask that you do the same. That said, if you have suggestions for improvement, we're ready and eager to listen via email at or .The FrameworkFirst, we wanted the 'getting started process' to be as simple and quick as possible. Second, we wanted the demos to run in a self-contained environment. With this in mind, we’ve decided to combine Vagrant, Virtual Box, and Puppet, with Snapshots (a feature we introduced in Elasticsearch 1.0) to help you download, install, configure, and run the full ELK stack. Every time we release a demo, we will accompany it with a blog post with instructions and details about the insights we gained from it. The instructions will also be part of the .: If you'd rather not setup a VM and understand how to restore a snapshot using our REST API, by all means, clone the repo and just restore the snapshot. In that case, there's no need for Vagrant or Virtual Box. If you decide to go this route, please do it in a fresh install of Elasticsearch rather than an existing one to keep things simple. We don't want to overwrite anything you've already been working on.As of now, we’ve tested this on a lot of Mac/Linux laptops and a bit on Windows, so if you have issues with your particular OS, please let us know and we will get things fixed or, if need be, make recommendations on alternatives. For Windows in particular, the first demo we publish will have specific instructions so keep an eye out for those.Feel Free to Take the wheelWe hope you enjoy this new project and please don't hesitate to contribute your own hard work for everyone else to benefit from and build upon. ","locales":"","title":"BYOD (Bring Your Own Demo)"}
{"index":{}}
{"author":"Clinton Gormley","category":"Engineering","publish_date":"2014-09-30T00:00:00.000Z","url":"/blog/elasticsearch-1-3-4-released","seo_title":"","content":" Today, we are happy to announce the bugfix release of , based on . You can download it and read the full changes list here: . Elasticsearch 1.3.4 is the latest stable release. Users of Elasticsearch 1.3.3 who have 100 or more shards per node should upgrade. For blog posts about past releases in the 1.3 branch, see: , , , . Elasticsearch 1.3.3 included a change to the management threadpool (used, amongst other things, to collect node and index statistics) to make it a fixed pool with a queue size limited to 100. The change was added to prevent a queue explosion in the case of slow statistics collection, which was experienced by some of our bigger users. Usually, collecting statistics is very fast, but a bug introduced in a previous release slowed down the file system check enough to cause a notable delay on nodes with many shards. Unfortunately, limiting the size of the management thread pool means that stats calls can fail on nodes with more than 100 shards, because the queue fills up. This release reverts the change to the management thread pool, and in a future release we will look at other ways of handling this scenario. Please , try it out, and let us know what you think on Twitter (). You can report any problems on the . ","locales":"","title":"Elasticsearch 1.3.4 released"}
{"index":{}}
{"author":"Aaron Mildenstein","category":"Engineering","publish_date":"2014-09-25T00:00:00.000Z","url":"/blog/elasticsearch-curator-version-2-0-released","seo_title":"","content":" I am pleased to announce the immediate availability of Curator 2.0! Curator is our tool that helps your curate, or manage, your time-series indices. What's changed? So many new features were being added to Curator that it's monolithic nature was getting in the way of its future progress. As a result, Curator 2.0 now separates the from the . Because there have been some changes in the command-line flags, be sure to test with the flag. The Elasticsearch Curator Python API As the API has been separated, it is now simple to write your own scripts using the same methods that Curator uses. These methods are documented thoroughly . The Curator Script In order to preserve reverse compatibility as much as possible, installing Curator in the recommended way () continues to install an that allows Curator to be invoked with the same command. will work exactly as it did before! New Features The changes to the Elasticsearch Curator Python API aren't the only new features! Snapshots Snapshot functionality has been completely reworked to allow multiple indices per snapshot, incremental snapshots, named snapshots, and the ability to capture indices in a snapshot. Accordingly, when you wish to delete snapshots older than a given time period, Curator will now use the time the snapshot was created (as stored in the snapshot metadata) to determine its age. Prefixes and suffixes and wildcards…Oh, my! In addition to prefixes, Curator now allows you to use suffixes and wildcards to determine the index pattern it should look for. Additionally, prefixes and suffixes can be empty, allowing your index name to be just a date. Curator now expects your indices to match a pattern of + + . Both and support wildcards, and can be empty. Delay after optimize I love getting feature requests and bug reports from the community. was to introduce a delay after an optimize. This delay allows your Elasticsearch cluster to quiesce before continuing with the next optimize call. Conclusion The new changes in Curator are awesome! I look forward to hearing about new ways you are using the Elasticsearch Curator API. If you run into trouble or find something we missed, please log an issue on our page. If you love Curator, please tell us about it! We love tweets with #elasticsearch in them! Curator is growing and maturing into a bigger, better project with each new release! Thanks for reading, and Happy Curating! ","locales":"","title":"Elasticsearch Curator Version 2.0 Released "}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2014-09-22T00:00:00.000Z","url":"/blog/world-elasticsearch-17","seo_title":"","content":" We're kicking off with a great event in our home town, San Francisco, and continue with some cool meetups on all things Elasticsearch all over Europe and in a completely new country. Curious? Read on for all the details. Upcoming Events North America Sept. 22nd: Puppet Conference San Francisco - come listen to who will be speaking on , on Wednesday from 5:10pm-6:00pm, located in Salon 7-9. If you're lucky you can still get a discount! PuppetConf is next week! WEEEE. 35% discount if you want it: — @jordansissel (@jordansissel) Upcoming Meetups Europe Sept. 22nd: Sept. 24th: Sept. 25th: Sept. 26th: Want to come hack on next week? Join us in Cambridge at free food, wifi & coffee! — Charlie Hull (@FlaxSearch) Asia Sept. 27th: That's it for this week, and stay tuned for Elasticsearch happenings next week. - The Elasticsearch Team P.S.: if you're interested in hosting a meetup or are giving a talk about Elasticsearch - we can offer support and send you swag. ","locales":"","title":"Where in the World Is Elasticsearch? - September 22, 2014"}
{"index":{}}
{"author":"Ryan Ernst","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-09-17T00:00:00.000Z","url":"/blog/2014-09-17-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project.Elasticsearch core Elasticsearch EcosystemHere's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases.Average UK house prices 2014. elasticsearch GeoHashGrid + Percentiles agg at work. — Mark Harwood (@elasticmark) Slides & Videos introduces the Elasticsearch .NET clientsAll things Laravel and ElasticsearchWhere to find UsWe'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking or hosting a meetup, let our Director of Developer Relations, , know! & on the ELK Stack at last week's API WorldAustriaThe Vienna Elasticsearch User Group will convene on October 2nd at 7 PM. This next meetup will focus on Elasticsearch use cases. You can .DenmarkYann Cluchey, the awesome human who organizes the Elasticsearch London User Group, will take the stage at GOTO Aarhus on Tuesday, September 30th to share his company's Elasticsearch use case. Join Yann at 1:20 PM to hear all about . GOTO Aarhus runs September 29-30th.France GermanyThe code.talks conference is back in Hamburg this year on October 9th and 10th, though with a new name. (We had a great time there last year when this conference was known simply as Developer Conference EU.) will be speaking once again, along with .Here's all the Elasticsearch and ELK stack information on offer at code.talks: The week after code.talks, Leslie Hawthorn will be heading LinuxCon Europe in Dusseldorf, where she'll speak on the panel . The panel takes place on Wednesday, October 15th at 2:30 PM, and the conference runs October 13-15th at Congress Centre Dusseldorf.The NetherlandsYou can join us next Monday, September 22nd for our first ever Elasticsearch Netherlands Meetup in Utrecht! The fine folks at Bol.com will share all about how they use the ELK stack and have scaled it out to easily process a daily volume of 300M log events and 185 GB of data. to attend.Can't join us in person? No problem. Thanks to our friends at Bol, the event will be livestreamed.Save the date & watch this space: next NL meetup will be livestreamed at (22 Sept, starting @ 19 CEST) — Leslie Hawthorn (@lhawthorn) We're also super excited that this year's will be in our hometown of Amsterdam on Sept. 29th - Oct. 3rd. will be out and about in the hallway track, so make sure to say hello to her! There are also some great talks on all things Elasticsearch and the ELK stack on offer from these Drupalistas: NorwayThe next Elasticsearch Oslo Meetup is on for October 7th at 6 PM. to save your seat. We'll get back to you with more details as soon as the meeting agenda is finalized.SerbiaPatrick Peschlow from our partner firm codecentric AG will take the stage at the 2nd Annual Coding Serbia Conference on September 25th. Join him at 8:00 PM to learn how you can go from .SpainThe first ever Elasticsearch Meetup in Madrid has been scheduled! The meetup will take place on October 7th from 7:00 - 8:30 PM. Our very own will cover all things Elasticsearch. You can to save your place.SwedenThe fifth Elasticsearch Stockholm Meetup is on for October 1st at 5:30 PM. The agenda will be posted shortly, but in the meantime to ensure there's a seat for you.United KingdomPlease join the folks from the Enterprise Search Cambridge group for a full day Elasticsearch hackathon on September 26th! You can to attend.United States Where to Find YouOur Community Manager, , is hard at work to help folks create more Elasticsearch meetup groups and to help meetup organizers find more speakers. If you are interes","locales":"","title":"This Week in Elasticsearch - September 17, 2014"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2014-09-15T00:00:00.000Z","url":"/blog/world-elasticsearch-16","seo_title":"","content":" You can guess yourself from the map. Not good at Geography? Well then, read on to find out what exciting places we'll be visiting... ...but before you do that, here's our favorite tweets from last week's events: 1. with on files, data structures and their usage in Elasticsearch: Once again, rocking another meetup at hosted by — George P. Stathis (@gstathis) 2. with and his live streaming session on : will now talk about how get your information actionable — Findwise (@Findwise) Upcoming EventsEurope Sept. 19th: JUG Summer Camp 2014, La Rochelle, France - come say hi to and listen to his talk on , 16:15-17:15. Upcoming MeetupsNorth America Sept. 15th: Sept. 18th: Asia Sept. 16th: That's it for this week, and stay tuned for Elasticsearch happenings next week. We've got big things coming up in San Francisco and we'll have a our first ever Dutch Elasticsearch meetup happening in Utrecht! - The Elasticsearch Team P.S.: if you're interested in hosting a meetup or are giving a talk about Elasticsearch - we can offer support and send you swag! ","locales":"","title":"Where in the World Is Elasticsearch? - September 15, 2014"}
{"index":{}}
{"author":"Dimitri Marx","category":"User Stories","publish_date":"2014-09-12T00:00:00.000Z","url":"/blog/byodemos-new-york-city-traffic-incidents","seo_title":"","content":" For the introduction of Elasticsearch #BYODemo environment, please check out our announcement .Background This status quo is unacceptable. The City of New York must no longer regard traffic crashes as mere ‘accidents,’ but rather as preventable incidents that can be systematically addressed. No level of fatality on city streets is inevitable or acceptable. This Vision Zero Action Plan is the City’s foundation for ending traffic deaths and injuries on our streets. The City will use every tool at its disposal to improve the safety of our streets.” From NYC.gov’s Before making major public policy decisions based on anecdotal information that may cost significant taxpayer money based on anecdotal information, are there tools available to analyze data to identify patterns and trends then drill down into details in motor vehicle collisions? Can we get answers to questions such as: In less than an hour, we can set up Logstash, Elasticsearch, and Kibana (ELK) to start asking these questions. With our search capabilities, powerful APIs, and visualizations, it’s possible to calculate totals, counts, averages and have a very interactive experience with your data. And if you don’t know what to ask, you can use aggregations to discover aspects and anomalies of the data.Getting startedAfter following , you should see the following dashboard in your browser.By clicking on the rows , several stats panels will be opened, displaying statistics for deaths and injuries at a glance.The row accommodates a hits panel displaying the varying reasons lives are lost in combination with an automobile accident.In the row you will find the same for injuries.We have setup a number of different panels to show the many different ways you can visualize and interact with the data in Kibana. We encourage you to expand each row and learn more about the data and how to visualize it in Kibana.Discovering the uncommonly commonBy clicking the row , a histogram for all accident types over the time will be displayed.There are several spikes indicating some unusual behavior in our data. But which events and accident types are responsible for these? Can we find the reason among our all-time top 5 accident types? Let us open the row and see if we can answer this question.By looking at the the top 5 accident types: , , , , in the histogram it becomes clear that these events are not necessarily the major factors for most of the spikes. How can we find out what the top contributing factors for these time periods are?Let us zoom into the time around March 1, 2013 on the histogram (just span a rectangle with the mouse around the spike).Now let us see the top accidents types for the selected time range. In doing so, we can refer to the terms panel in the row .There is a change of order. Suddenly, a new accident type is appearing among the top 5 – . Is this the accident type responsible for the spike in accidents reported? If yes, we would see some correlation of spikes between all events and this particular accident type. To confirm this, we are going to modify the histogram . By clicking on the cog symbol in the top right corner, a setting dialog will be displayed.Please open the tab called and activate by clicking on it. Now click on the Save button. The histogram should look like this.The correlation of all events and accidents of the type at the February 9, 2013 looks like this.Let’s have a look at this eye-catching behavior. After selecting the time frame around this spike and removing all events query, we get something like this.Since we are looking at the query for , we assume that the reason for this unusual event is due to some changed weather conditions.By doing some research, we discover that on February 9, 2013, the storm unofficially called or “Blizzard of 2013” developed strong activity over NYC and other parts of the East Coast. It was snowing, freezing cold, and streets got slippery, no wonder ther","locales":"","title":"#BYODemos: New York City Traffic Incidents"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-09-10T00:00:00.000Z","url":"/blog/2014-09-10-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. Elasticsearch core awesome presentation with real life use case demo by at — Colin Surprenant (@colinsurprenant) Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases. All set for the meetup tonight — Boaz Leskes (@bleskes) Slides & Videos At last night's Elasticsearch Boston Meetup, EverTrue shared their use case All things Elasticsearch & Python from last week's An Introduction to Elasticsearch and Kibana (日本語で) From last week's DevOps Ireland Meetup: Making sense of your data using the ELK stack Thursday, September 11th from 10:00 - 17:00 CEST ","locales":"","title":"This week in Elasticsearch - September 10, 2014"}
{"index":{}}
{"author":"Alexander Reelsen","category":"Engineering","publish_date":"2014-08-26T00:00:00.000Z","url":"/blog/using-percolator-geo-tagging","seo_title":"","content":" ","locales":"","title":"Using the Percolator for Geo Tagging"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2014-09-08T00:00:00.000Z","url":"/blog/world-elasticsearch-15","seo_title":"","content":" In short: Boston, Copenhagen, Florida, Oslo and San Francisco! :) Wanna know what's going on in all these cities? Read on for the full overview (but before that, we would love to share a few impressions from last week's happenings): 1. with and All things and by ! — Livia Froelicher (@LivFroe) Search terms related to \"gun\" - produced by significant terms + graphing aggs on entity-centric index. — Mark Harwood (@elasticmark) 2. with getting ready for the Irish crowd All set for the meetup tonight — Boaz Leskes (@bleskes) Upcoming Events North America Sept. 11th-13th: , join for his keynote session on , Saturday 13th, 17:20-18:30 . Also on Saturday at 15:40 is session, which is an . Europe Sept. 9th-11th: , attend workshop , Tuesday 9th, 13:30-17:30. Sept. 11th: , sign up and listen to talking about , Conference runs from 10:00-17:00. Upcoming Meetups North America Sept. 9th: Sept. 9th: Europe Sept. 8th: - Casual style That's it for this week, and stay tuned for Elasticsearch happenings next week. - The Elasticsearch Team P.S.: if you're interested in hosting a meetup or are giving a talk about Elasticsearch - we can offer support and send you swag! ","locales":"","title":"Where in the World Is Elasticsearch? - September 08, 2014"}
{"index":{}}
{"author":"Michael McCandless","category":"Engineering","publish_date":"2014-09-03T00:00:00.000Z","url":"/blog/performance-considerations-elasticsearch-indexing","seo_title":"","content":" Elasticsearch users have delightfully diverse use cases, ranging from appending tiny log-line documents to indexing Web-scale collections of large documents, and maximizing indexing throughput is often a common and important goal. While we try hard to set good general defaults for \"typical\" applications, you can quickly improve your indexing performance by following a few simple best practices, described here. To begin with, do not use a very large java heap if you can help it: set it only as large as is necessary (ideally no more than half of the machine's RAM) to hold the overall maximum working set size for your usage of Elasticsearch. This leaves the remaining (hopefully sizable) RAM for the OS to manage for IO caching. Make sure the OS is not . Upgrade to the most recent Elasticsearch release (): numerous indexing related issues have been fixed in recent releases. Before delving into the details, a caveat: remember that all the information here is up-to-date as of today (), but as Elasticsearch is a fast moving target, this information may no longer be accurate when you, future Googler, come across it. If you are unsure, just . is especially useful when tuning your cluster for indexing throughput: as you iterate on each setting described here you can easily visualize the impact of each change on your cluster's behavior. Client side Always use the , which indexes multiple documents in one request, and experiment with the right number of documents to send with each bulk request. The optimal size depends on many factors, but try to err in the direction of too few rather than too many documents. Use concurrent bulk requests with client-side threads or separate asynchronous requests. Before you conclude indexing is too slow, be sure you are really making full use of your cluster's hardware: use tools like , and to confirm you are saturating either CPU or IO across all nodes. If you are not then you need more concurrent requests, but if you hit from the java client, or HTTP response from REST requests, then you are sending too many concurrent requests. If you are using , you can see the rejection counts under the section of the . It is usually not a good idea to increase the bulk thread pool size (defaults to the number of cores) as that will likely decrease overall indexing throughput:  it is better to decrease client-side concurrency or add more nodes instead. Since the settings we discuss here are focused on maximizing indexing throughput for a single shard, it is best to first test just a single node, with a single shard and no replicas, to measure what a single Lucene index is capable of on your documents, and iterate on tuning that, before scaling it out to the entire cluster. This can also give you a baseline to roughly estimate how many nodes you will need in the full cluster to meet your indexing throughput requirements. Once you have a single shard working well, you can take full advantage of Elasticsearch's scalability and multiple nodes in your cluster by increasing the shard count and replica count. Before drawing any conclusions, be sure to measure performance of the full cluster over a fairly long time (say 60 minutes), so your test covers the full lifecycle including events like large merges, GC cycles, shard movements, exceeding the OS's IO cache, possibly unexpected swapping, etc. Storage devices Unsurprisingly, the storage devices that hold the index have a huge impact on indexing performance: Segments and merging Under the hood, newly indexed documents are first held in RAM by Lucene's . Periodically, when the RAM buffer is full, or when Elasticsearch triggers a flush or refresh, these documents are written to new on-disk segments. Eventually there are too many segments, and they are merged according to the . This process cascades: the merged segments produce a larger segment, and after enough small merges, those large","locales":"de-de,fr-fr,ja-jp,ko-kr","title":"Performance Considerations for Elasticsearch Indexing"}
{"index":{}}
{"author":"Justin Hoffman","category":"","publish_date":"2014-08-26T00:00:00.000Z","url":"/blog/elasticsearch-training-comes-to-india","seo_title":"","content":" We feel very lucky that thousands of businesses worldwide continue to adopt Elasticsearch, Logstash and Kibana for uses that start as simple as full-text search, all the way up to performing powerful, real-time analytics for log analysis, fraud detection, digital forensics among a plethora of other use cases. In order to set up their implementations for success and resolve known issues faster, we support our users by offering subscriptions to get direct access to our engineers for guidance on topics such as security considerations, proper architecting, sizing, performance optimization and integrations. But the first step in this process actually starts with training. Our training courses go over the ins and outs of our technology including the ELK stack, helping attendees understand the basics of distributed search application development and the various use cases our products can help with. Today, we’re thrilled to announce we will be bringing our training courses to India thanks to our new partnership with SpringPeople. SpringPeople is a leader in corporate training within India for high-end and emerging technologies. They currently work with market leaders like EMC, MuleSoft, Typesafe and VMware - and we’re thrilled to join the ranks. The first Core Elasticsearch course will be held on 24/25 September and ELK Workshop on 26 September. Secure your spot today - they go quick! - at . ","locales":"","title":"Elasticsearch Training Comes to India"}
{"index":{}}
{"author":"Alex Brasetvik","category":"","publish_date":"2014-08-19T00:00:00.000Z","url":"/blog/found-optimizing-elasticsearch-searches","seo_title":"Optimizing Elasticsearch Searches","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud. Simple Suggestions for Speedier Searches Elasticsearch can query, filter and aggregate in many ways. Often there are several ways to solve the same problem – and possibly with very different performance characteristics. This article will cover some important optimizations that can buy you a lot of performance. ","locales":"","title":"Optimizing Elasticsearch Searches"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-08-27T00:00:00.000Z","url":"/blog/2014-08-27-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project.Elasticsearch core In case you've missed the webinar, the recording is available here: Enjoy! — Costin Leau (@costinl) Elasticsearch EcosystemHere's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases. Definitive Guide goes to the printers Sept 1. Get 50% off now at — Clinton Gormley (@clintongormley) Slides & VideosNew to Elasticsearch? Rick Winfrey introduces you to its features, with great getting started code examples in Ruby. Thanks Rick! treats us to walk through of the options of determining search quality & how Elasticsearch can help you in your questRafael Lopes on all things Elasticsearch, AWS & High Availability at the recent Elasticsearch Brasil Meetup . But we can't claim credit for this one. ","locales":"","title":"This Week in Elasticsearch - August 27, 2014"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-08-20T00:00:00.000Z","url":"/blog/2014-08-20-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. Elasticsearch core Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases. Slides & Videos Jeffrey Sogolov on the ELK Stack (and how it can save you millions of dollars!) Our very own Alex Ksikes on multimedia search, matching procedures and a little bit of Apache Lucene on the side gives you all the news you can use on the latest features in Elasticsearch Where to find Us We'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking or hosting a meetup, let our Community Manager, , know! Germany If you're heading to on August 23rd & 24th, we're happy to bring you bouncy castle & ball pool love this year. And make sure to see on . United Kingdom Please join the folks from the Enterprise Search Cambridge group for a full day Elasticsearch hackathon! You can to attend. United States Where to Find You Our Community Manager, , is hard at work to help folks create more Elasticsearch meetup groups and to help meetup organizers find more speakers. If you are interested in either effort, . Oh yeah, we're also . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticsearch, Kibana, and Logstash than where you rest your head. Trainings If you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This week in Elasticsearch - August 20, 2014"}
{"index":{}}
{"author":"Konrad Beiske","category":"","publish_date":"2014-08-15T00:00:00.000Z","url":"/blog/found-getting-started-with-lire-and-elasticsearch","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud. Search for Images Using ImagesLIRE (Lucene Image REtrieval) is a plugin for Lucene to index and search images. A cool and quirky feature that sets it apart is that it does content based retrieval, a fancy word for saying that you use images in your search query and it retrieves similar images from the index. In order to use LIRE with Elasticsearch, we need to make Elasticsearch aware of the new data type and the query that is provided by LIRE. Luckily there is a plugin for Elasticsearch that does just that. ","locales":"","title":"Getting Started with LIRE and Elasticsearch"}
{"index":{}}
{"author":"Costin Leau","category":"Engineering","publish_date":"2014-08-14T00:00:00.000Z","url":"/blog/es-hadoop-2-0-1-and-2-1-beta1","seo_title":"","content":" I am happy to announce Elasticsearch for Apache Hadoop releases and 2.0.1 is the latest release, that fixes several bugs, improves compatibility across various Hadoop distributions and also tracks the latest library updates. 2.1.Beta1 is the first release from the development branch focused mainly on the , in the Hadoop ecosystem:  in particular Beta1 provides integration with Apache Spark support Elasticsearch for Apache Hadoop 2.0 added , through its Map/Reduce functionality. Beta1 goes that, providing a dedicated Spark (or Resilient Distributed Dataset) for Elasticsearch, for both Java and Scala. Thus, one can easily execute searches in Elasticsearch and transparently feed the results back to Spark for transformation. For example, using the artists example from the , counting the performers starting with “me” is a one-liner: import org.elasticsearch.spark._ val sc = new SparkContext(new SparkConf()) val number = sc.esRDD(\"radio/artists\", \"?me*\").count() Dedicated Java and Scala APIs In Beta1, with the introduction of the Spark module, Elasticsearch for Apache Hadoop has grown beyond Java and is also using as a language (as Apache Spark itself is written in Scala). As such, one will find the typical Scala patterns (like – gotta love the name) in place as well as support for and objects as returned by the . However, Java users are not forgotten, the Spark module provides a dedicated for Java which returns <code>java.util collections and proper JDK types – the equivalent of its Scala brethren, but for Java. Since the infrastructure is the same, one will get the same end result regardless of the which API is used. In fact, the two APIs are fully compatible and one can even use both in the same application, at the same time. To wit, here’s an example of the Java RDD leveraging Java 8 lambda expressions for conciseness, to filter some entries (please ignore the fact one and do the filtering directly through Elasticsearch): import org.elasticsearch.spark.java.api.JavaEsSpark:  JavaSparkContext jsc = ... JavaRDD<Map<String, Object>> esRDD = JavaEsSpark.esRDD(jsc, \"radio/artists\", \"?me*\"):  JavaRDD<Map<String, Object>> filtered = esRDD.filter( m -> m.values().stream().filter(v -> v.contains(\"mega\"))):  Notice how the returned collection is used as is, without any conversion of any sort. Index arbitrary s to Elasticsearch In a similar vein, through the Spark module can be saved to Elasticsearch as long as its structure maps to a document (one can easily transform the data or plug her own serializer if needed): val game = Map(\"media_type\"->\"game\",\"title\" -> \"FF VI\",\"year\" -> \"1994\") val book = Map(\"media_type\" -> \"book\",\"title\" -> \"A Clash of Kings\",\"year\" -> \"1999\") val cd = Map(\"media_type\" -> \"music\",\"title\" -> \"Surfing With The Alien\") val sc = new SparkContext(...) sc.makeRDD(Seq(game, book, cd)).saveToEs(\"my-collection/{media-type}\") The attentive user may have noticed the pattern usage in the index definition – the Spark module supports all the functionality of es-hadoop such as above, writing of , scripting or customizing the . In fact, all the options are supported as the underlying code-base is the same – whether you are using Java, Scala or the Map/Reduce layer. Modular design – pick only what you need While part of Elasticsearch for Apache Hadoop, the Spark module is self-contained and can be used either itself or alongside the , <code>Hive, and <code>Cascading integrations. One can use it in or against a YARN cluster. We cannot wait to get your on 2.1.Beta1 – you can find it in the and the new features explained in the . And by the way, if you are interested in data exploration, search, and identifying anomalies in real-time on your Hadoop data, you are warmly invited to our upcoming webinar hosted by yours truly, next week, on Wednesday, August 20th. The webinar will showcase some of the ways in which Elasticsearch for Apache Hadoop can help. Please . ","locales":"","title":"Elasticsearch Hadoop 2.0.1 and 2.1.Beta1 Released"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-09-03T00:00:00.000Z","url":"/blog/2014-09-03-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. Elasticsearch Core Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases. If you use Elasticsearch with Laravel, I've got a package (beta) that integrates ES types with Laravel models: — Adam Fairholm (@adamfairholm) L'usine logicielle de Voyages-SNCF. Avec du dedans :) — David Pilato (@dadoonet) Slides & Videos Great overview of the ELK stack for logging & data visualization For all you PaaS lovers out there, check out how to use the ELK stack on OpenShift Origin Great exploration of using the ELK stack and other tools to monitor your infrastructure ","locales":"","title":"This Week in Elasticsearch - September 03, 2014"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2014-08-25T00:00:00.000Z","url":"/blog/world-elasticsearch-14","seo_title":"","content":" Last week we came back from summer break and kicked it off with a great talk about “Elasticsearch Architectures in the Wild\" at the in Kansas. . dropping some serious knowledge about ELK at the meetup — DevOps Kansas City (@DevOpsKC) The next couple of weeks are calming down a bit so we are gearing up for what's to come in the next couple of months. Little teaser: The Nordics and Florida. :) Stay tuned and read on for this and next week's events, hopefully one is happening in a city near you! Upcoming Events Europe Sept. 4th: , listen to talking about . Upcoming Meetups Europe Aug. 26th: Sept. 3rd: That's it for this week, and stay tuned for Elasticsearch happenings in two weeks. - The Elasticsearch Team P.S.: if you're interested in hosting a meetup or are giving a talk about Elasticsearch - we can offer support and send you swag! ","locales":"","title":"Where in the World Is Elasticsearch? - August 25, 2014"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2014-08-18T00:00:00.000Z","url":"/blog/world-elasticsearch-13","seo_title":"","content":" Elasticsearchers are back, fully tanked up with sun and energy, and ready for some conferences and meetups this week. We hope you all had a lovely summer break, too! Before we get to the events, we would first like to congratulate our #ELKsintheWild competition winner, . For those who missed the competition, here's a short recap: everybody in possession of one of our cute plushy ELKs could take part by taking a picture during the summer holidays and sharing it via Twitter tagging #ELKsintheWild. The best picture wins an Elasticsearch surprise. Thank you Ricky for participating and sharing that very sweet Tweet with us. Expect an Elasticsearch package in your mailbox soon! :) Thank you ! My Elks arrived and have met the monkeys and are ready for adventures — Ricky Moorhouse (@rickymoorhouse) Now check out our events and meetups taking place next week, hopefully one is happening in a city near you. Upcoming Events North America Aug. 20th-22nd: , come by and listen to speaking on a panel about or sign up for her workshop on . Europe Aug. 23rd-24th: , meet at our small booth over lunch time and visit her talk about on Sunday 24th, 10am @ room HS1. Upcoming Meetups North America Aug. 20th: South America Aug. 19th: That's it for this week, and stay tuned for Elasticsearch happenings next week. - The Elasticsearch Team P.S.: if you're interested in hosting a meetup or are giving a talk about Elasticsearch - we can offer support and send you swag! ","locales":"","title":"Where in the World Is Elasticsearch? - August 18, 2014"}
{"index":{}}
{"author":"Clinton Gormley","category":"Engineering","publish_date":"2014-08-13T00:00:00.000Z","url":"/blog/elasticsearch-1-3-2-released","seo_title":"","content":" Today, we are happy to announce bugfix releases  and  . You can download them and read the full list of changes here: These two releases fix a rare bug in the compression of data transferred between nodes ().  We recommend that everyone upgrades.Communication between Elasticsearch nodes is compressed using LZF to reduce the amount of data transferred across the network. A small number of byte sequences can produce a hash collision which, when combined with a bug in the that we use, results in data corruption.We noticed this bug thanks to a recent checksum change in 1.3.0, which is part of the work we are doing to make Elasticsearch as resilient as possible. While we have always added checksums to files that we write, we did not check that files transferred across the network had the same checksum as the original. This added check resulted in an exception for one of our users when trying to recover a replica shard from the primary: one segment was being corrupted during transfer, which altered the checksum.We managed to create a test case, find the bug, and submit a pull request to the author of the compression library, who reacted very promptly and released a new version with the bugfix.The sequence of events needed to trigger this bug occurs rarely. It may be responsible for the occasional case of corruption that we have seen reported, but which has remained unexplained.  We advise users to upgrade, but in the meantime you can avoid this bug completely by disabling compression with the following request:curl -XPUT \"http://localhost:9200/_cluster/settings\" -d' { \"persistent\": { \"indices.recovery.compress\": false } }'Please download , try it out, and let us know what you think on Twitter (). You can report any problems on the . ","locales":"","title":"Elasticsearch 1.3.2 and 1.2.4 Released"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-07-30T00:00:00.000Z","url":"/blog/2014-07-30-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project.EC2! DDoS! Cloud! Elasticsearch!If you've been reading the news, you've been seeing those words together quite a bit for the past few days. Don't panic! (And bring your towel.) .tl: dr: Elasticsearch core logstash is just a way to get people hooked on using ElasticSearch for everything, I think. Well played, — Michael Pearson (@mipearson) Elasticsearch EcosystemHere's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases. Slides & VideosLearn how Elasticsearch powers the Building Performance Database, helping folks understand their building's energy footprint All about AOL's Network Forensics tool Moloch, powered by Elasticsearch Rotem Hermon, the organizer of the Elasticsearch Tel Aviv meetup, on serendip.me How Clairvoyant uses the ELK stack for log analysis ","locales":"","title":"This Week in Elasticsearch - July 30, 2014"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-08-06T00:00:00.000Z","url":"/blog/2014-08-06-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project.Elasticsearch core You too can start a six node ElasticSearch cluster in under 10 minutes! — Megabyte Mike (@megabytemike) Elasticsearch EcosystemHere's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases. Total plotted in /. US has 5x more than closest rival. pic2 by Top 10 Country Population. — Bhaskar Karambelkar (@bhaskar_vk) Slides & Videos Where to find UsWe'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking or hosting a meetup, let our Community Manager, , know!GermanyIf you're heading to on August 23rd & 24th, we're happy to bring you bouncy castle love this year. And make sure to see on .PolandThe Warsaw Java Users Group is holding a Microservices Hackathon on August 9th from 10 AM to 10 PM. You can spend those twelve hours hacking on the ELK stack and a variety of other tools. to attend.United StatesHeading to Chicago for LinuxCon and Cloud Open North America? Say hello to , and Empowering Your Corporate Open Source Software Developers.Where to Find YouOur Community Manager, , is hard at work to help folks create more Elasticsearch meetup groups and to help meetup organizers find more speakers. If you are interested in either effort, .Oh yeah, we're also . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticsearch, Kibana, and Logstash than where you rest your head.TrainingIf you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elasticsearch - August 06, 2014"}
{"index":{}}
{"author":"Clinton Gormley","category":"Engineering","publish_date":"2014-07-28T00:00:00.000Z","url":"/blog/elasticsearch-1-3-1-released","seo_title":"","content":" Today, we are happy to announce the bugfix release of , based on Lucene 4.9. You can download it and read the full changes list here: .This release fixes a backwards compatibility bug () in index recovery. This bug and upgrading to Elasticsearch 1.3.1 will fix the problem. It will affect any user who tries to upgrade to version 1.3.0, whose indices still include segments which were written to in any of these versions of Elasticsearch: The bug  prevents the recovery of replicas for these older indices.  An index that has segments from these versions, and has replicas enabled, will reach status yellow, but never green.  The exceptions in the logs will look like this: IllegalArgumentException[No enum constant org.apache.lucene.util.Version.x.x.x]Certain versions of Lucene are missing constants which represent minor Lucene versions, and incorrect version numbers have been recorded in some segments.   has been opened to address this issue in Lucene. This issue should have been caught by our backwards compatibility tests, but was missed because of the missing constant in Lucene.  The test suite will be improved to cover that eventuality in the future.This release also fixes a few minor aggregation bugs, which are listed in the .Please download , try it out, and let us know what you think . You can report any problems on the . ","locales":"","title":"Elasticsearch 1.3.1 Released"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-07-23T00:00:00.000Z","url":"/blog/2014-07-23-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. Alright, we are ready for tonight! Swing by booth P18 for some awesome demos! — cariG (@cariG) Elasticsearch Core Turns out 10 stickers is worth 1 bacon maple . — Kimberly Lembo (@kimlembo) Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases. Slides & Videos Where to Find Us We'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking or hosting a meetup, let our Community Manager, , know! Germany Israel Poland The Warsaw Java Users Group is holding a Microservices Hackathon on August 9th from 10 AM to 10 PM. You can spend those twelve hours hacking on the ELK stack and a variety of other tools. to attend. United States Where to Find You Our Community Manager, , is hard at work to help folks create more Elasticsearch meetup groups and to help meetup organizers find more speakers. If you are interested in either effort, . Oh yeah, we're also . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticsearch, Kibana, and Logstash than where you rest your head. Trainings If you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elasticsearch - July 23, 2014"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-08-13T00:00:00.000Z","url":"/blog/2014-08-13-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. Elasticsearch Core Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases. Slides & Videos Are you a Pythonista? Then you are really going to want to watch our resident Python expert in this presentation from the recent in Berlin. Did Honza's talk not quite satiate your weekly need for all things Python? Don't worry, we've got this special presentation from community member at EuroPython last month that will hopefully do the trick. A ToDo List: 1) Visit Booth 202:  2) Pick up cool leather notebook:  3) Watch demo — elasticsearch (@elasticsearch) Where to Find Us We'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking or hosting a meetup, let our Community Manager, , know! Germany If you're heading to on August 23rd & 24th, we're happy to bring you bouncy castle love this year. And make sure to see on . United States Heading to Chicago for LinuxCon and Cloud Open North America? Say hello to , and Empowering Your Corporate Open Source Software Developers. Where to Find You Our Community Manager, , is hard at work to help folks create more Elasticsearch meetup groups and to help meetup organizers find more speakers. If you are interested in either effort, . Oh yeah, we're also . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticsearch, Kibana, and Logstash than where you rest your head. Trainings If you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elasticsearch - August 13, 2014"}
{"index":{}}
{"author":"Lee Hinman","category":"Engineering","publish_date":"2014-08-07T00:00:00.000Z","url":"/blog/scripting","seo_title":"","content":" With the release of 1.3 and moving forward, we are making some big changes to the scripting infrastructure in Elasticsearch. In this blog post we’ll cover the major changes that we’re making, some of the upcoming changes to scripting, and some of the new ways to work with scripting in ES! Dynamic scripting disabled One of the first (and most noticable) changes that has impacted scripting is in the Elasticsearch 1.2 release. We recently released a blog post titled about the security implications of this. Please take a look for more information about how dynamic scripting was changed after the 1.2 release. Groovy and the sandbox Starting with version 1.3, we have decided to add a sandboxed version of Groovy to the Elasticsearch scripting languages, with plans to transition all scripts from MVEL to Groovy in the long run. The reason for this is threefold: The last point leads back to dynamic scripting. In 1.2.0 we disabled dynamic scripting for non-sandboxed languages. However, since Groovy can be sandboxed, we can still allow dynamic scripts to be sent with each request. What do we mean by sandboxing? First sandboxing does NOT address or prevent DOS (Denial Of Service) attack scripts, it is only intended to prevent scripts from accessing parts of the operating system or internals of Elasticsearch they are not intended to access. A malicious script can still be run with an infinite loop, exhausting system resources by sending it many times. If you would like to disable the sandbox (thus causing scripts sent dynamically as strings with requests to be denied), you can disable it by adding to your configuration in Elasticsearch 1.3 or later. Be sure to check out the different sandboxing configuration parameters in the , as well as more information about disabling the sandbox, or enabling dynamic scripting for Elasticsearch overall. In the 1.3 release of Elasticsearch, MVEL is still the default language. We are planning to transition away from MVEL to Groovy as the default language for the 1.4 release. You will be able to transition your scripts from MVEL to Groovy by specifying in the scripts, or changing the default scripting language for all scripts to Groovy by adding to elasticsearch.yml. You can then transition each MVEL script individually to Groovy. When upgrading to Elasticsearch 1.4, MVEL will be removed entirely as a scripting language. If you want to continue using MVEL as a scripting language you will need to install the plugin. While testing, however, we found that the Groovy and MVEL languages were so similar that MVEL scripts needed very minimal changes, if any, to work with Groovy. Why not Javascript? One question that we anticipate getting is why not use Javascript for the scripting language? After all, Javascript is becoming a very popular language. There are a few reasons we decided to go with Groovy instead of Javascript. First, Groovy was faster than Javascript (using Rhino) in our tests, and Nashorn has poor support for concurrent execution of scripts. Additionally, the syntactic difference between Groovy and Javascript is very small for simple scripts, so there should be little difficulty understanding scripts. With the release of 1.3 however, we do have an additional scripting language available for use: Lucene Expressions. Lucene Expressions Lucene Expressions provide a mechanism for dynamically evaluating a single Javascript numeric statement, per document. Its primary purpose is to provide easy scoring adjustment, without writing custom Java code, but the framework allows execution with any per document use case. Each expression is compiled to Java bytecode, to achieve “native code”-like performance. Integrating expressions as a new scripting language was an easy fit. The new “expression” lang for scripts can be used for virtually all current uses of query scripts in ES: , , sort scripts and numeric aggregation scripts. And did we mention they are fast? Initial benchmarks show speeds many times faster than Groo","locales":"","title":"All about Scripting"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2014-07-28T00:00:00.000Z","url":"/blog/world-elasticsearch-12","seo_title":"","content":" The summer heat is kicking in (even in normally chilly Amsterdam) and us Elasticsearchers need a little break. Hence, we decided to combine the next three weeks of events into one blog post and start-off again in mid August (the 18th to be exact). We'll also announce our winner(s) of the #ElksInTheWild holiday competition then - so be sure to take some holiday pictures with your plush ELK and share your photos with us on Twitter. What's this #ElksInTheWild thingy about? If you missed our last blog post here's the “how-to\" guide: In case you lovely humans are around, here is what's on the calendar for the next three weeks: Upcoming Events North America Jul 31st - Aug 1st: - say hi to Aug 7th - 10th: - say hi to Aug 11th - 14th: - stop by our booth (we're there all week!) and see session on Thursday at 9:25am Europe Aug 1st - 3rd: - say hallo to and Upcoming Meetups North America Jul 31st: Europe Jul 28th: Aug 4th: Aug 9th: That's it for now. Stay tuned for more Elasticsearch happenings after our summer break. P.S.: if you're interested in hosting a meetup or are giving a talk about Elasticsearch - we can offer support and send you swag! ","locales":"","title":"Where in the World Is Elasticsearch? - July 28, 2014"}
{"index":{}}
{"author":"Clinton Gormley","category":"Engineering","publish_date":"2014-07-23T00:00:00.000Z","url":"/blog/elasticsearch-1-3-0-released","seo_title":"","content":" Today, we are happy to announce the release of , based on, along with a bugfix release of . You can download them and read the full changes list here: Elasticsearch 1.3.0 is the latest stable release. It is chock full of new features, performance and stability improvements, and bugfixes.  We recommend upgrading, especially for users with high indexing or aggregation loads. The full change log is available in the , but we will highlight the most important changes below: Security Elasticsearch previously allowed any request to return its response in a format.  While very useful, this meant that any web page you view could send requests to any Elasticsearch instance which you have access to.   (BREAKING:) but can be enabled if you so choose (). Scripting In the 1.2 branch, we disabled dynamic scripting by default. This was a good decision from a security standpoint, but made it more difficult to use scripting with Elasticsearch.  This release adds a number of awesome scripting features which give you the best of both worlds: Aggregations Aggregations keep on getting better and better. We have added three new aggregations: Besides these new features, aggregation performance and memory usage have also received a lot of love: Indexing Elasticsearch is used for a range of very different use cases, such as large document search, centralised logging, and high performance analytics. Document size and indexing rate varies dramatically between these use cases, so it is important that Elasticsearch is able to adapt dynamically.  We are closer to an auto-regulating system than ever before, thanks to changes that have been made in recent releases and to the following: At Elasticsearch we like good defaults.  We don't want our users to have to twiddle many confusing knobs in the hope that one particular combination might deliver what they need:  It should just work. With the above changes, we have reduced the number of settings that you know about to just three: Suggesters We have a new in-house suggesters expert, so expect a number of improvements to suggesters in the near future. To start off with, we have added  the much requested ability to limit \"did-you-mean\" phrase suggestions to phrases that actually exist in the index (). Mapping The new  feature adds the ability to use scripting to transform the source document on-the-fly during indexing (). This doesn't change the field that is stored on disk, but it changes how the field is indexed. Mapping has also seen some significant performance improvements: Disk, files and I/O The slowest component of any modern server is the disk.  Small improvements to I/O and disk usage can make a big difference to the performance of the system overall.  With this in mind, we have made the following changes: Resiliency With a complex asynchronous distributed system like Elasticsearch, there are always complicated corner cases which can impact the stability of a system.  You can read about the many low-level improvements and bugfixes that have been added in this release . These resiliency improvements are part of an ongoing effort to make Elasticsearch rock solid. ","locales":"","title":"Elasticsearch 1.3.0 And 1.2.3 Released"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2014-07-21T00:00:00.000Z","url":"/blog/world-elasticsearch-11","seo_title":"","content":" Last week our three hackers , and were on a mission to discover the tech side of Japan. Igor and Honza also held a talk at their first ever Elasticsearch meetup in Tokyo. hacking, yet lost in translation, with and — Shay Banon (@kimchy) Tokyo meetup has started. has the floor — Igor Motov (@imotov) Little ELK didn't quite get to travel that far but the German folks at the Java Forum in Stuttgart loved him and even ranked him Number One among all conference freebies at the Java Forum Stuttgart. Sweet! Für mich das Giveaway des Tages : -) :-) — Achim L. (@achiml75) Now, check out what's on the Elasticsearch calendar for this week (teaser: we have one session where we'll teach you how to monitor your drone project with Elasticsearch!): Upcoming Events North America - , and are all speaking. Be sure to check out their sessions and stop by our booth (P18) to say hello. Europe - Honza Král speaking - Honza Král speaking Upcoming Meetups North America Europe Oceania If you are one of the lucky ones on holiday traveling somewhere around the world, here are 2 holiday tips: You have a plushy ELK? Take it with you, take a fun holiday picture and about it using the #ELKsIntheWild hashtag. Our top 3 will get some awesome swag sent over, so the competition is on! Bored on your sun chair? Just scan through our and have a look at one or the other cool video on there. :) That's it for this week, and stay tuned for Elasticsearch happenings next week. We hope to see you at one of our events. PS: if you're interested in hosting a meetup or are giving a talk about Elasticsearch - we can offer support and send you swag! ","locales":"","title":"Where in the World Is Elasticsearch - July 21, 2014"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-07-16T00:00:00.000Z","url":"/blog/2014-07-16-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. presents at last week's Elasticsearch Korea Study Session Photo credit: JongMin Kim Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases. amazing turnout to the elasticsearch at Tokyo — Shay Banon (@kimchy) Slides & Videos LivingSocial shares their use case Alexander Mols' slides from the recent Dutch PHP Conference Learn how Yieldbot uses Kafka together with new Elasticsearch features like doc values at Devoxx UK 2014 Where to find Us We'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking or hosting a meetup, let our Community Manager, , know! Germany Israel Italy The very first Elasticsearch Italy Meetup has been scheduled for July 16th! Please join us to hear from on . You'll also be treated to a case study talk on how uses Elasticsearch in its Threat Management System for Breach Detection, Intelligence & Response. Doors open at 6:00 PM, and . Japan 首都大で7/17午後 さんと さんにelasticsearch入門とクックパッドにおけるサービス開発についてトークしていただきます。参加費無料、申し込み不要ですので、お気軽にどうぞ！ — Mamoru Komachi (@mamoruk) New Zealand The Auckland JVM Users Group will rebooting their meetup series, with their newest offering focused on Elasticsearch. You can join them on Tuesday, July 22nd at 6:00 PM, and . Poland The Warsaw Java Users Group is holding a Microservices Hackathon on August 9th from 10 AM to 10 PM. You can spend those twelve hours hacking on the ELK stack and a variety of other tools. to attend. United Kingdom The London VoiP User Group will get together on July 22nd at 6:30 PM. These folks are in search of a venue, so if you're excited to hear about Eye-candy from CDRs with the ELK stack, and help these good folks find a place to meetup. United States Where to Find You Our Community Manager, , is hard at work to help folks create more Elasticsearch meetup groups and to help meetup organizers find more speakers. If you are interested in either effort, . Oh yeah, we're also  . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticsearch, Kibana, and Logstash than where you rest your head. Trainings If you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This week in Elasticsearch - July 16, 2014"}
{"index":{}}
{"author":"Aaron Mildenstein","category":"Engineering","publish_date":"2014-07-24T00:00:00.000Z","url":"/blog/curator-1-2-0-released","seo_title":"","content":" Greetings! Even though it has only been a few weeks since was released, we're rolling out another new version. Introducing Curator v1.2.0! New features These changes are documented thoroughly in the Updates Date patterns and In previous releases of Curator, the date was calculated by separating the elements of the index name using a separator character. This design decision was simple for use with the Logstash indices the program was originally designed to manage. Since then, however, Curator has matured into a time-series index manager, and that has meant different index naming schemas. There is still a need to do date math calculations by interval, and so the option remains, though now you can also specify as the time unit. The default options should still work out of the box as they did previously. They are as follows: What this means, is that if you specify as your time unit, and do not specify a , the default will be , which is \"Year.Month.Day.Hour\" expressed in . Similarly, if you were to specify as your time unit, and allowed Curator to provide the default it would be . Where this feature now provides value is with indices with no separator character between date elements. For example, if I had daily indices like you could for indices older than 2 days with a command like this: Note that the default time unit is in this example. Hourly indices—like could be managed in similar fashion: Replacing If you were using a custom separator character with a previous version of Curator, your change should be relatively simple. If your old command was for an index like , your command would have used . Now, your command will look like this: Just put your separator string in between the year (), month (), and day () identifiers! This also means that you can now do what was previously impossible in Curator: mixed separator characters. Now you could process indices like with a like . Learn more about in the . Feedback Many of these features came about because of user comments and requests. If you have a feature request or find a bug, ! We also love shout-outs on Twitter. Our twitter handle is @elasticsearch Happy Curating! ","locales":"","title":"Curator 1.2.0 Released"}
{"index":{}}
{"author":"Martijn Laarman","category":"Engineering","publish_date":"2014-07-18T00:00:00.000Z","url":"/blog/nest-elasticsearch-1-0-ga","seo_title":"","content":" Last week, we released the to the nuget.org unstable feed. Today, we are happy to announce the general availability of 1.0 in the nuget stable channel! No more separate *.Signed packages All the assemblies are strong name signed by default. While we personally feel strong naming is up there with the invention of null as big mistakes, we can't escape the fact that it's a reality for some of our customers. In , if you strong name your application or library, you can only use libraries that are also strongly named. If you do not, you are free to use signed and unsigned assemblies. The choice whether to strong name or not therefore comes down to servicing the most people. We already see that great libraries like that take a dependency on NEST are signed themselves. Breaking Changes So we promised no breaking changes between the RC and GA, but sharply observed that our mapping of the Update API and updates inside the Bulk API were inconsistent and together . This also has consequences for some of the other API's where you can pass an CLR Object to infer to id on. This used to be called but has been renamed to the more descriptive . To make up for the breaking changes between the RC and GA, we released a little bit earlier! Connection Pooling Since the RC, we have improved the exception messages it throws to be as verbose as possible. Pings and sniffs exceptions are clearly distinguishable. The addresses of all the nodes that failed are now also visible in the exception message itself. Exceptions For the RC, we wrote a ton of unit tests throwing exceptions in deliberate places to be sure that exceptions that bubble out of the client are consistent: when using connection pooling, and the actual exception when not regardless whether a synchronous or asynchronous method is called. We've added unit tests for these exceptions and wrote a ton of integration tests for timeouts and other connection related exceptions. Better GeoShape mapping Previously, NEST only allowed for a Coordinates object of type for shapes which only worked for a sub set of the GeoJSON objects that Elasticsearch supports.  Rather than one GeoShape descriptor for all shape types, NEST now exposes a separate descriptor for each, accepting the correct coordinates structure for that type. CLS-Compliant? We're proud to announce that NEST is now CLS-Compliant! Read more about what that means . What's Next? If you have any issues, comments, or ideas, please don't forget to report them on our . ","locales":"","title":"Elasticsearch.NET & NEST 1.0 GA Released"}
{"index":{}}
{"author":"Aaron Katz","category":"News","publish_date":"2014-07-17T00:00:00.000Z","url":"/blog/joined-elasticsearch-2","seo_title":"","content":" Twelve years ago, as the dust from the dot-com implosion settled, I joined a one-product, one-hundred person startup that was on a mission to reinvent the enterprise software industry. We accomplished this by developing products that were easy-to-use, easy-to-deploy, community-driven, and engineered around a simple yet fundamental principle that had somehow been lost by traditional software vendors during the melee of the dot-com frenzy: vendor success is a derivative of customer success. Focus on the latter and the former will follow. That company was and, after working there for more than a decade I have decided to close one incredible chapter and start the next. Here, at a glance, is why I joined Elasticsearch and why I believe it will emerge as the next great enterprise software company. First and foremost, extracting business value from data will be the number one priority for enterprises across all industries within the next five years. Right now people talk about big data, analytics, predictive computing, artificial intelligence and many other data-focused disciplines. I predict that a term will emerge to capture this sector of the technology industry in the same way that “cloud computing” captured software-as-a-service, utility computing, file sharing, storage, web services, application hosting, etc. The seemingly endless use cases the Elasticsearch ELK stack addresses uniquely positions the company to help define this rapidly evolving category. Secondly, open source is here to stay and, like social networking and mobility, still has so much room to mature and grow. The Elasticsearch ELK stack, and all of its core products - Elasticsearch, Logstash and Kibana - started independently as passion projects before a company was built around them. It is this evangelical, community-led approach to technology development that will write the future of enterprise software. Thirdly, platforms that deliver insight from unprecedented volumes of data in milliseconds will dominate the space. They scale massively, are simple to deploy, and can be implemented at a fraction of the cost of traditional methods. These platforms will emerge as the new standard. Elasticsearch offers this today, as evidenced by the 10M total downloads to date, and I have not seen this type of viral growth in enterprise software in the last decade. And lastly, I fundamentally believe, having spent a significant part of my career living or working in Asia Pacific, Europe, or Latin America, that the long-term winners in our industry are those who approach international and emerging markets with the same intensity as they do mature markets. Elasticsearch shares this belief more so than any other company I have encountered at such an early stage. If you want to be a part of this energy, growth, and innovation, head on over to . You can drop us a line at , or you can contact me directly at . ","locales":"","title":"Why I joined elasticsearch - July 17, 2014"}
{"index":{}}
{"author":"Richard Pijnenburg","category":"Engineering","publish_date":"2014-07-14T00:00:00.000Z","url":"/blog/streamlining-maintenance-puppet-modules","seo_title":"","content":" Since the beginning, we've tried to support all current and historic releases of Puppet. We're big fans of Puppet - just like many of our users - so we went broad in our version support. We wanted to ensure the best possible experience for as many folks as possible. At the time of writing this post, we're testing against 9 different versions of Puppet, both vanilla and Puppet Enterprise. We have a great testing infrastructure to do this work for us. However, long term maintenance to support all these versions, with even more great bits to come, is proving to be more challenging. Moving forward After pondering the challenges and tradeoffs, we've decided to shrink the versions we test against and officially support. Puppet is developed and released at quite a fast pace - one of the reasons we love it - so we'll make sure our modules always keep up with Puppet's latest releases. Specifically, we'll support the latest version of vanilla Puppet, plus the prior four releases. For Puppet Enterprise, we will support the latest two releases. What this means for you Moving forward, we will support the following: Puppet Open Source: 3.2.x to 3.6.x Puppet Enterprise: 3.1.x and 3.2.x If you're running an older version, our Puppet modules should still keep right on working for you. However, with this change we cannot guarantee that will always be the case. We'd recommend to ensure compatibility, and because why not get the latest bits? ","locales":"","title":"Streamlining Maintenance of Our Puppet Modules"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2014-07-14T00:00:00.000Z","url":"/blog/world-elasticsearch-10","seo_title":"","content":" Last week was awesome! We had some great presentations around the world by our own developers and happy Elasticsearch users like . Seoul meetup starting with talking about data structures used in elasticsearch. — Honza Král (@HonzaKral) Philly meetup, presents. Thanks for providing an awesome space. — jamestyack (@jamestyack) This week we are very excited to check out some brand new places - hello, Bath and Milan! - and accompany on his way to Tokyo for the 5th Elasticsearch Study Session! flying to Tokyo today for our first training, excited, first time at Tokyo + see first hand our amazing community — Shay Banon (@kimchy) Events & Conferences Berlin Are you on your way to EuroPython or perhaps you are already in Berlin and are looking for weekend plans? , happening in Berlin on July 19th and 20th, is your place to go if you are all passionate about documentation systems, tech writing theory, and information delivery. will be giving a talk about on Sunday July 20th at 2:30pm. We're happy to provide you the post-conference meetup drinks. And remember, we're . Portland If you are all in for , join us at in Portland from July 20th until 24th. We're looking forward to our visit to PDX, especially because we get to see all you lovely people. But since OSCON isn't for a few more days make sure to stay tuned here, as we'll get into all the juicy OSCON details (where to find us - Booth P18, when to hear us, what swag we'll have, etc.) in next week's post. Stuttgart is a one day conference covering all things Java. , will be attending along with . Make sure to say hello at booth number 20 between sessions. The conference takes place on July 17th at the Culture & Convention Centre Liederhalle. Tokyo As part of a public educational course at the Tokyo Metropolitan University, , will give an Introduction to Elasticsearch on July 17th. Want to join us? . Meetups Bath Neil Andrassy, CTO at , organizes our first in the South West of England on July 14th in Bath. Neil will kick this thing off by explaining a little more about how . In the area? Join and sign up . Berlin It is a busy tech week in Berlin and we are trying to make people not busier but happier with an ELK Hackathon. Led by Florian Gilcher, organizer of the , the Hackathon is happening on July 15th from 10am-6pm. There is limited space, so if you fancy some hacking before heading to the , please sign up . Cincinnati Feel like meeting up for an evening of fun with the user group? Their upcoming meetup on July 16th will cover some of the most popular 'Free and Open Source Software' (FOSS) tools used to monitor various aspects of your computer environment. Tools covered will include the ELK stack. Milan Our partner is hosting our first ever meetup in Milan on July 16th. Yay. , , and will be onsite. Luca will be talking about followed by a case study presentation from about . Paris The in France is gathering in Paris on July 15th and Olivier Dolbeau will give a short introduction to ELK. Doors open at 7:30pm. Spaces are limited so make sure you register. Tokyo After Seoul, Elasticsearch descends on Tokyo this week. And it's not 'just' your garden variety meetup in the wild. Our own crew with Shay Banon, Honza Kral, and Jun Ohtani are going to be there. So don't miss the chance to meet them all on July 14th and hear Igor and Honza talk about all things Elasticsearch. Sign up . ","locales":"","title":"Where in the World Is Elasticsearch - July 14, 2014"}
{"index":{}}
{"author":"Livia Froelicher","category":"News","publish_date":"2014-07-11T00:00:00.000Z","url":"/blog/summer-elasticsearch-berlin","seo_title":"","content":" The Elasticsearch community in Berlin has been busy with all sorts of events over the past few weeks - and there is much more to come! In case you haven’t been following the news at lately or you haven’t joined our - yet :) - here’s a brief summary of past and future activities. Elasticsearch Hackfest On May 28th, we held an Elasticsearch Hackfest, organised and coached by Asquera and Elasticsearch as part of Berlin Buzzwords’ Hackathon day. Around 60 people got together to learn more about Elasticsearch and work on cool projects. The teams worked on small, fun to build apps that sent users notifications using the Percolator, made movie recommendations based on other movies you like using Elasticsearch or helped German newbies to determine a word gender with Logstash. The Elasticsearch Hackfest was great for developers new to Elasticsearch, as they got to learn in a more hands-on environment. For more seasoned Elasticsearch users, it was a great chance to play around with all the newest bits and to chat with other members of the community and with the Elasticsearch team members. Most of the European Union-based employees of Elasticsearch journeyed to Berlin for Buzzwords and joined the hackathon - thanks for being so helpful and accessible! Today's hackathon. Thanks for all! — Leslie Hawthorn (@lhawthorn) Elasticsearch User Group The Berlin Elasticsearch User Group, organised by Asquera, takes place the last Tuesday of every month. While we changed our May meeting to a hackfest, we went back to our regular schedule at the end of June. And we’ll see you in July, as well! In June, we continued our tradition of hosting a beginners workshop and a more advanced talk. This time, Jilles Van Gurp told us about how they take advantage of the ELK stack (Elasticsearch + Logstash + Kibana) at his company, Linko.   ","locales":"","title":"The Summer of Elasticsearch in Berlin"}
{"index":{}}
{"author":"Martijn Laarman","category":"Engineering","publish_date":"2014-07-10T00:00:00.000Z","url":"/blog/elasticsearch-net-nest-1-0-release-candidate-now-available","seo_title":"","content":" After a very successful period, we are pleased to announce the availability of the release candidate for the Elasticsearch .NET clients. So what does release candidate mean for you exactly? Just like the beta, the RC is being released on the nuget unstable channel. However, we’ve now committed our public API as released in this RC. No breaking changes will be introduced between this RC release and the final 1.0 release forthcoming on the nuget stable feed. We’re pushing for a 1.0 stable release 10 days after this release candidate. We’ll spend these 10 days ramping up the documentation for NEST and Elasticsearch.NET and responding to bugs discovered in the RC. As with all of our .NET releases, we’ve tested this RC against version of Elasticsearch 1.0 and up, including the latest 1.2.2 release. In addition to our automated testing, we’ve also spent time upgrading some of our customers’ existing applications to the new . The firsthand experience and feedback on the state of the new client from these real world scenarios has been incredibly helpful in improving from . Wait, wait – 2 clients? Quick recap on our story so far… Starting with 1.0, we made a conscious decision to split out the bare metal low level moving parts from into . The low level client does not come with design choices around types, so you can inject your own routines for serialization, connection handling and connection pooling handling. This change makes a great choice for interfacing to small libraries such as logging adapters, as the low level client is completely dependency free. If you are writing more involved applications, we recommend since it is strongly typed around 90% of the Elasticsearch universe. If you are using and need to drop down to the low client, you can always do by simply calling into property. uses internally, allowing you to inject your own moving parts as you see fit. Breaking changes This list is by no means exhaustive, but in our work converting existing applications using the release we’ve found these to be the main breaking changes you might run into when upgrading to the : If you find others, please ! Whats new As always, we’d like to thank the community first and foremost. The amount of feedback coming in from GitHub issues and Stack Overflow has been absolutely stellar. All of this continuous feedback means we’ve been able to address many quirks and bugs that the beta1 release introduced, plus add new functionality for the deepest places where the Elasticsearch and .NET universes intersect. New hire I’m very pleased to be able announce the team at Elasticsearch Inc has doubled in size with the hiring of . Greg is based in Jersey City, New Jersey, so the team now spans two continents! Ok, so technically Greg is not a new feature or bug fix of the , but he’s been instrumental in getting the RC out the door. Welcome Greg! Object Initializer Syntax The big new feature of the RC is the object initializer syntax. When I first started to write NEST, I wrote it for me and the Elasticsearch projects I was doing at that time. I really like the fluent syntax, but not all NEST users feel the same way. With this release, we hope to get the fluent syntax haters back on board! So what does this mean in practice? Lets consider an example: var response = this._client.CreateIndex(c => c .Index(\"new-index-name\") .Settings(s => s .Add(\"index.settings\", \"value\") ) .AddMapping<object>(m => m .Type(\"my_root_object\") .Properties(p => p .String(sm => sm.Name(\"my_field\").Analyzer(\"default\")) ) ) ):  var request = new CreateIndexRequest(\"new-index-name\") { IndexSettings = new IndexSettings { Settings = new Dictionary<string, object> { {\"index.settings\", \"value\"} }, Mappings = new List<RootObjectMapping> { { new RootObjectMapping { Name = \"my_root_object\", Properties = new Dictionary<PropertyNameMarker, IElasticType> { {\"my_field\", new StringMapping() { Analyzer = \"defa","locales":"","title":"Elasticsearch.Net & Nest 1.0 Release Candidate"}
{"index":{}}
{"author":"Lee Hinman","category":"Engineering","publish_date":"2014-07-09T00:00:00.000Z","url":"/blog/scripting-security","seo_title":"","content":" ","locales":"","title":"Scripting and Security"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-07-09T00:00:00.000Z","url":"/blog/2014-07-09-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. Elasticsearch Core Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases. For all you Pythonistas out there ... I just released new version of client (1.1.0) - — Honza Král (@HonzaKral) Bonjour, powered by ES : ) — Jean-Yves Stervinou (@jy) The Magic Zapper team uses Elasticsearch to help find relevant TV content for you, all from your mobile phone \"remote control.\" Slides & Videos Heading to our EU hometown for DrupalCon Amsterdam? Check out on Elasticsearch and Drupal before you arrive! From the oldies but goodies files: Igor Motov on Ongoing Resiliency Improvements in Elasticsearch Love Scala and Elasticsearch? Hear from the folks at Sports195 about their custom Scala libraries that make Elasticsearch sing. Learn all about how PSA Peugeot Citroën uses the ELK Stack (en français) Where to find Us We'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking or hosting a meetup, let our Community Manager, , know! Australia \"Search: A Journey of Delivery on a Budget\" from & - July 10 — ⓜⓡ ⓜⓞⓝⓓⓞ (@s_mcleod) Germany Israel The Second Elasticsearch Tel Aviv Meetup will convene on July 28th, focusing on real world use cases. The agenda is now finalized and you can hear from three different companies on how they use Elasticsearch. You can . Italy The very first Elasticsearch Italy Meetup has been scheduled for July 16th! Please join us to hear from on What's New in Elasticsearch. You'll also be treated to a case study talk on how Lutech uses Elasticsearch in its Threat Management System for Breach Detection, Intelligence & Response. Doors open at 6:00 PM, and . Japan The will convene on July 14th at 6:30 PM. Register now to get a chance to hear from our core developers and . Even cooler, our CTO will be attending this meetup, so register now to save your place! Korea The will convene in Seoul on July 10th at 7 PM. Register now to get a chance to hear from our core developers and . New Zealand The Auckland JVM Users Group will rebooting their meetup series, with their newest offering focused on Elasticsearch. You can join them on Tuesday, July 22nd at 6:00 PM, and . Spain Clinton Gormley will be speaking on Scaling Real-Time Search and Analytics with Elasticsearch at on July 10th. Clint takes the stage at 9:15 AM. United Kingdom Meeting up in Portlandia, OSCON style Join us for an focused BOF session, July 23rd at 7pm at OSCON - details are here — stevemayzak (@smayzak) United States Where to Find You Our Community Manager, , is hard at work to help folks create more Elasticsearch meetup groups and to help meetup organizers find more speakers. If you are interested in either effort, . Oh yeah, we're also . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticearch, Kibana, and Logstash than where you rest your head. The price of support is dirt cheap compared to the value that their stack represents so I'll be a happy (paying) customer ... — Henrik Johansen (@HenrikJohansen) We think you're pretty nifty too, Henrik! If you're interested, here's where to . Trainings If you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elasticsearch - July 09, 2014"}
{"index":{}}
{"author":"Njal Karevoll","category":"","publish_date":"2014-07-09T00:00:00.000Z","url":"/blog/found-elasticsearch-plugin-types","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud. In order to write a great plugin, we need to have an overview of the different plugin types and scopes Elasticsearch has made available to us. This article gives a rundown of these and also some tips on how to write a 'well behaved' plugin. ","locales":"","title":"Elasticsearch Plugin Types"}
{"index":{}}
{"author":"Daniel Palay","category":"","publish_date":"2014-06-26T00:00:00.000Z","url":"/blog/world-elasticsearch-esallhands-edition","seo_title":"","content":" It's been quite the week at Elasticsearch, as all of our employees descend on our EU HQ for a week of hands on collaboration.We're just about to wrap up our meetings. But as the saying goes, the journey is more important than the final destination. To that end, we had our staff tweet their journey using #ESAllHands all week and collected many of them below. If you missed us around the world, don't worry, our merry band of Elasticsearchers will be back in a town near you soon. Until then, we you! // app:<br> // res:<br> window.s = window.s || {}: <br> s[\"api_url\"] = \"//private-api.storify.com/v1\": <br> s[\"base_url\"] = \"//storify.com\": <br> s[\"env\"] = \"production\": <br> s[\"git\"] = \"f450393f\": <br> s[\"namespace\"] = \"storifyapp\": <br> s[\"protocol\"] = \"https\":  <p>window.s = window.s || {}: <br> s.user = window.s.user || {}: <br> s.user[\"username\"] = \"Elasticsearch\": <br> s.user[\"canEdit\"] = true: </p> <p>window.s = window.s || {}: <br> s.story = window.s.story || {}: <br> s.story[\"sid\"] = \"53aaa3a17cced80a64000133\": <br> s.story[\"slug\"] = \"esallhands\": <br> s.story[\"permalink\"] = \"http://storify.com/Elasticsearch/esallhands\": <br> s.story[\"title\"] = \"#ESAllHands\": <br> s.story[\"date\"] = {\"created\":\"2014-06-25T10:25:37.942Z\", \"modified\":\"2014-06-26T11:28:02.049Z\", \"published\":\"2014-06-26T11:28:02.049Z\"}: <br> s.story[\"stats\"] = {\"popularity\":0, \"views\":26, \"likes\":0, \"comments\":0, \"elementComments\":0, \"embeds\":[{\"clicks\":0, \"views\":15, \"href\":\"https://storify.com/dpalay/esallhands\", \"domain\":\"storify.com\"}, {\"clicks\":0, \"views\":11, \"href\":\"http://www.elasticsearch.org/?p=79357&amp: preview=true\", \"domain\":\"elasticsearch.org\"}], \"elements\":{\"text\":11, \"quote\":4, \"image\":26, \"video\":0, \"link\":0, \"other\":0}, \"clicks\":0}: <br> s.story[\"canEdit\"] = true: <br> s.story[\"liked\"] = false: <br> s.story[\"not_indexed\"] = false: <br> s.story[\"author\"] = {\"username\":\"Elasticsearch\", \"name\":\"Elasticsearch\", \"options\":{\"infinite_scroll\":false, \"hide_stats\":false, \"allow_embedding\":true, \"comments\":true, \"related_stories\":true, \"ga\":false}, \"style\":{\"fonts\":{\"title\":\"Georgia, Times\", \"body\":\"Helvetica, Arial\"}, \"colors\":{\"text\":\"#666\", \"link\":\"#3676c4\", \"background\":\"#fff\"}, \"typekit\":{\"fonts\":[]}}, \"ga_tracker\":undefined, \"features_enabled\":{\"custom_embed_style\":false, \"private_stories\":false, \"html_for_seo\":false, \"no_advertising\":false, \"business_options\":false, \"headerless_embed\":false, \"pdf\":false, \"realtime_updates\":false, \"storylock\":false, \"maxEditors\":0}}: </p> ","locales":"","title":"Where in the World is Elasticsearch? #ESAllHands edition"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-06-26T00:00:00.000Z","url":"/blog/2014-06-24-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. Elasticsearch core Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases. : The Age of by via — Timo Batista (@TimoBatista) Slides & Videos Introduces Elasticsearch and Kibana gives a high level overview of using the ELK stack at ScaleConf 2014 Cédric Hourcade shares Daily Motion's use case on an Overview of Using the ELK stack (auf Deutsch) Mr. ( ) shares a moment with our travel buddy . Almost time for — Daniel Palay (@danielpalay) Where to find Us We'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking or hosting a meetup, let our Community Manager,, know! France The 8th Elasticsearch France Meetup will take place in Paris on July 7th at Coworkshop. Space is very much limited, so take a look and to save your place. Israel Japan The will convene on July 14th at 6:30 PM. Register now to get a chance to hear from our core developers and . Korea The will convene in Seoul on July 10th at 7 PM. Register now to get a chance to hear from our core developers and . The Netherlands Spain Clinton Gormley will be speaking on Scaling Real-Time Search and Analytics with Elasticsearch at on July 10th. Clint takes the stage at 9:15 AM. United Kingdom If you're in or around Bath on July 14th, the South-West Elasticsearch Community will convene to talk about how you're using Elasticsearch. You can , which will kick off at 7 PM. United States Where to Find You Our Community Manager, , is hard at work to help folks create more Elasticsearch meetup groups and to help meetup organizers find more speakers. If you are interested in either effort, . Oh yeah, we're also . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticearch, Kibana, and Logstash than where you rest your head. Trainings If you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This week in Elasticsearch - June 26, 2014"}
{"index":{}}
{"author":"Richard Pijnenburg","category":"Engineering","publish_date":"2014-06-18T00:00:00.000Z","url":"/blog/elasticsearch-puppet-module-0-4-0-released","seo_title":"","content":" ","locales":"","title":"Elasticsearch Puppet module 0.4.0 released"}
{"index":{}}
{"author":"Alexander Reelsen","category":"Engineering","publish_date":"2014-06-13T00:00:00.000Z","url":"/blog/elasticsearch-1-2-adding-context-suggestions","seo_title":"","content":" The need for speed If you have not yet read the introductory blog post about the completion suggester, why we built it, and what makes it so fast, you should do so ! Speed is nothing without control One of the most requested requirements for the suggester was the possibility to apply filters to the suggestions. As the query being executed for a completion suggest request is not a real search request the data structure being accessed differs, applying filters was not as simple as one would hope. We spent some time thinking about this problem, and came up with a solution we call context suggestions. The name describes the difference between this one and other suggesters: users want to get suggestions back, but in the scope of a context. What can a scope include? Like with many things in Elasticsearch, there are endless possibilities. You, the person using your data, will know best what the correct scope is. However, we wanted to show you a couple of examples of scoping a suggestion. Filtering by fields The first possibility is to filter by fields. One common use case is when you want to return suggestions only for a certain type. The mapping would look like this: DELETE /posts PUT /posts PUT /posts/article/_mapping { \"article\" : { \"properties\" : { \"suggest_field\": { \"type\": \"completion\", \"context\": { \"type\": { \"type\": \"category\", \"path\": \"_type\" } } } } } } PUT /posts/teaser/_mapping { \"teaser\" : { \"properties\" : { \"suggest_field\": { \"type\": \"completion\", \"context\": { \"type\": { \"type\": \"category\", \"path\": \"_type\" } } } } } } Now index two documents: PUT /posts/article/1 { \"suggest_field\" : { \"input\" : [ \"Medicine - Better than homeopathy?\" ] } } PUT posts/teaser/2 { \"suggest_field\" : { \"input\" : [ \"Music - can it help plants to grow?\" ] } } Now a suggestion needs to contain context information: POST /posts/_suggest?pretty' { \"suggest\" : { \"text\" : \"m\", \"completion\" : { \"field\" : \"suggest_field\", \"size\": 10, \"context\": { \"type\": \"article\" } } } } Because the suggester is using the type as its context, only the first suggestion (“Medicine – Better than homeopathy?”) will be returned by the suggester. The second suggestion is ignored because it is a different type than . A very common use case for this example would be an e-commerce shop:  imagine you have selected a category and want to return products which are inside of the selected product category. Using geo locations Another interesting use case is to take geo locations into account. Imagine you are retrieving suggestions for restaurants:  you probably want to suggest restaurants near the user. Ideally, we would filter the suggestions to include only those which are around 2km from the users’ location. This means that you need to supply location information on query and index time. Let us take care of the mapping first: DELETE /venues PUT /venues PUT /venues/poi/_mapping { \"poi\" : { \"properties\" : { \"suggest_field\": { \"type\": \"completion\", \"context\": { \"location\": { \"type\": \"geo\", \"precision\" : \"500m\" } } } } } } The next step is to index a document, which contains location information in the suggest field: PUT /venues/poi/1 { \"suggest_field\": { \"input\": [\"The Shed\", \"shed\"], \"context\": { \"location\": { \"lat\": 51.9481442, \"lon\": -5.1817516 } } } } And now, all of a sudden, you can get suggestions back which only apply to a certain area: POST /venues/_suggest { \"suggest\" : { \"text\" : \"s\", \"completion\" : { \"field\" : \"suggest_field\", \"context\": { \"location\": { \"value\": { \"lat\": 51.938119, \"lon\": -5.174051 } } } } } } Combining several suggesters So after understanding this principle, your next step would be to answer the question: . Even though this seems more tricky, the great part about the context suggester is the possibility of using several suggesters sequentially. You can create a completion field mapping, which needs a field and a geo location in order to return suggestions. So, let us create a new index with a ne","locales":"","title":"Elasticsearch 1.2: Adding Context to Suggestions"}
{"index":{}}
{"author":"Florian Hopf","category":"","publish_date":"2014-06-24T00:00:00.000Z","url":"/blog/found-java-clients-for-elasticsearch","seo_title":"","content":" One of the important aspects of Elasticsearch is that it is programming language independent. All of the APIs for indexing, searching and monitoring can be accessed using HTTP and JSON so it can be integrated in any language that has those capabilities. Nevertheless Java, the language Elasticsearch and Lucene are implemented in, is very dominant. In this post I would like to show you some of the options for integrating Elasticsearch with a Java application. ","locales":"","title":"Java Clients for Elasticsearch"}
{"index":{}}
{"author":"Costin Leau","category":"News","publish_date":"2014-06-19T00:00:00.000Z","url":"/blog/elasticseach-hadoop-certified-cloudera-cdh5","seo_title":"","content":" I am happy to announce that has been . Since the beginning, (es-hadoop) has been tested against the popular CDH distribution and, during the few last months, we have been working closely with Cloudera to complete the process. With this certification in place, we can offer users complete peace of mind that Elasticsearch is thoroughly tested and validated against the CDH environment. Adding to our existing partnerships with and , es-hadoop users can rest assured that whatever they use, we at Elasticsearch are fully committed to supporting it. We are delighted to see es-hadoop used in production by businesses in a variety of fields, from social recommendations to financial services, multi-national media companies to global telecoms. Organizations in each of these industries leverage es-hadoop to gather insight and perform analytics on massive volumes of data, and we’re proud we can help them achieve their analysis goals in real-time. We’ll continue to expand on es-hadoop’s feature set to bring Elasticsearch’s rich experience to users of CDH 5 and all other flavors of Hadoop. We know your business is hungry for data, and we’re proud to serve up the best possible search and analytics experience. But don’t take our word for it - and let us know what you think! If you are interested in data exploration, search and identifying anomalies in real-time on your Hadoop data, you are warmly invited to an upcoming webinar by yours truly on Wednesday August 20th, which will showcase the above through Elasticsearch for Apache Hadoop. ","locales":"","title":"Elasticsearch Hadoop certified for Cloudera CDH5"}
{"index":{}}
{"author":"Daniel Palay","category":"","publish_date":"2014-06-16T00:00:00.000Z","url":"/blog/world-elasticsearch-8","seo_title":"","content":" Quick answer: In Amsterdam, London and Paris. But wait, you actually want the details...keep reading and we’ll drop all location knowledge you want (and maybe even more : -)). Amsterdam We’ve got a busy week in our hometown of Amsterdam with sessions at two fantastic international conferences. Join us at from June 19th - 21st at the . Thanks to popular demand, two of our awesome engineers and will be hosting a workshop on on Thursday, June 19th. Additionally, our much-loved and heralded Community Manager, , will present her talk at 10:30 AM on Saturday, June 21st. As always, the whole crew welcomes you to swing by and say hi at the Elasticsearch table. Come and share your stories with us, hear about what’s going on and pick up some fun swag to boot! On June 19th & 20th in the heart of the city, we’ll have a team at . Our CTO, , will treat the crowd to a . Shay will also join from to tell the story of When you’re not in sessions, grab a coffee and follow the scent of delicious warm Dutch syrup waffles which we’ll be baking fresh at our booth. If you’re feeling healthy - and please don’t, stroopwaffles are amazing! - we’ll have cool swag up for grabs too. London Elasticsearch will be on stage during at the first ever at the London EXCEL on June 17th. Our own will be telling you all about how to Mark’s talk is at 16:00 in the Big Data & Analytics theatre. Can’t make Mark’s talk, but have a need to quench your Elasticsearch thirst whilst in London? The has got you covered as they will be hosting a special meetup on June 18th starting at 17:00 at The Marketplace. We’ll have lightning talks from Elasticsearch users Cogenta and Betfair and we’ll follow those with networking drinks until 18.30. Paris If you’re a fan of Drupal, don’t miss out on the session where our own will speak at the on June 18th at . Doors open at 18:45. Where in the world is Elasticsearch next week? It’s going to be a quiet week on the conference and meet up front for us as the whole company will be converging on our HQ in Amsterdam for our annual all hands gathering. For those already in or surreptitiously visiting Amsterdam, we’ll be on Friday, June 27th at 18:00 in . You’ll have the chance to schmooze and kvetch with tons of our team members while enjoying delicious snacks and drinks. Want to join us? . And remember, if you’re giving a talk, hosting a meetup or otherwise sharing the Elasticsearch love, . We’d love to feature your knowledge sharing bits here! ","locales":"","title":"Where in the World is Elasticsearch? - June 16, 2014"}
{"index":{}}
{"author":"Alex Brasetvik","category":"","publish_date":"2014-06-13T00:00:00.000Z","url":"/blog/found-logstash-openshift","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud. Get started analyzing your logs on OpenShift. While OpenShift lets you tail the logs of your apps, the Elasticsearch/Logstash/Kibana trinity gives you a very flexible and powerful toolchain to visualize and analyze these logs. This article explains how to make a Logstash cartridge on OpenShift. The cartridge feeds your logs into Elasticsearch where you can use the Kibana visualization engine to follow trends, detect anomalies and inspect incidents in your environment. ","locales":"","title":"Logstash on OpenShift"}
{"index":{}}
{"author":"Boaz Leskes","category":"Engineering","publish_date":"2014-06-09T00:00:00.000Z","url":"/blog/marvel-1-2-1-released","seo_title":"","content":" Today, we are happy to announce the release of . This is a bug fix release, fixing a data shipping issue in the new information which can affect large clusters. We recommend you upgrade if you are experiencing problems with the new Shard Allocation dashboard and/or if you are seeing the following error in one of your Elasticsearch node's logs:[2014-06-05 10:47:48,683][ERROR][marvel.agent] [Bandit] exporter [es_exporter] has thrown an exception: java.lang.IllegalStateException: array not available at org.elasticsearch.common.bytes.PagedBytesReference.array(PagedBytesReference.java:289) at org.elasticsearch.marvel.agent.exporter.ESExporter.addXContentRendererToConnection(ESExporter.java:209) To upgrade, you must install the latest Marvel plugin on all your ES nodes. As with any other Java plugin, you will need to restart nodes (one by one) in order for the version to become active. This is described in more details on the Marvel .As always, we welcome all feedback so we can continue improving Elasticsearch Marvel. Please send questions, praise or pains to the . ","locales":"","title":"Marvel 1.2.1 Released"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-06-11T00:00:00.000Z","url":"/blog/2014-06-11-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. Elasticsearch core Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases. Slides & Videos slides from last week's Hadoop Summit. Video Coming Soon! at Berlin Buzzwords 2014: Elasticsearch's Query Domain Specific Language (DSL) - Not Just for Wizards! Talking about how we use Scala and at — Skot Mahr (@lazyvalue) on Elasticsearch's Percolator at Berlin Buzzwords 2014 on Apache Lucene 4 from Berlin Buzzwords 2014 . at talking about ELK in a DevOps environment for tonight's San Francisco meetup. — elasticsearch (@elasticsearch) Where to find Us We'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking or hosting a meetup, let our Community Manager,, know! Austria The has scheduled their first meetup for Thursday, June 12th. Please join us at 7 PM to hear from on What's New in Elasticsearch. France If you're a fan of Drupal and Elasticsearch, please join on June 18th at the . Doors open at 6:45 PM. Israel The is hosting a workshop on Applied Cloud Computing with Google Cloud Platform at Google Tel Aviv. Topics will include BigQuery with Logstash as Application Log Analysis Platform. The workshop runs from 9:30-13:30 on July 7th. The Netherlands We've got not one but awesome conferences going on in Amsterdam the week of June 16th: Plus, we're hosting at 6:00 PM in Amsterdam. You'll have the chance to meet tons of our core developers, as the whole company will be visiting our EU HQ that week for our annual all hands gathering. Romania will speak at on Big data real time search and analytics. Topconf Bucharest runs from June 10-13th and Costin will speak at 3:20 PM on June 12th. United Kingdom United States Where to Find You Our Community Manager, , is hard at work to help folks create more Elasticsearch meetup groups and to help meetup organizers find more speakers. If you are interested in either effort, . Oh yeah, we're also . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticearch, Kibana, and Logstash than where you rest your head. Trainings If you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elasticsearch - June 11, 2014"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2014-06-07T00:00:00.000Z","url":"/blog/world-elasticsearch-7","seo_title":"","content":" Welcome to this week’s ! Several members of our core developer team are speaking at conferences in Bucharest and London and we’ve got meetups from Paris to San Francisco. Read on to find where you can hear all about the ELK stack and Elasticsearch awesomeness! A snapshot of last week’s Bay Area Search group meetup in San Jose - on all things Elasticsearch. Topconf Bucharest This international brand of Software conference takes place June 10-13. Don’t miss presentation, , on Thursday, June 12 at 15:20 in Room Asia. If you have any questions about Big Data/Hadoop architecture, Real-time search in distributed systems or Data visualization, make sure to say hi to Costin during the conference. Devoxx UK Running June 12-13, Devoxx UK offers a packed schedule including presentations and hands-on labs delivered by industry veterans, mavericks and rising stars, both UK- and internationally- based. Our own will be giving a on on Thursday, June 12 in Room 1 from 16:00-16:50. Meetups Norway Comperio will host a  on June 11 at 8:30 with various talks and case studies on search and analytics matters.  will talk about \"How to combine Elasticsearch, Logstash and Kibana to get new real-time analysis of almost all types of structured and unstructured data sources\". Mark's session begins at 10:00 @ The Thief in Tjuvholmen. Paris The will be hosting an evening of fun @ Dailymotion on Tuesday June 10. The French connection, David Pilato, and will be getting together for some Elasticsearch talks at 19:30. There are only 2 spots left, so make sure to secure your seat if you are in the area! San Fransisco and will be talking all things ELK stack at the From a general overview to best practices for configuring your ELK cluster, Kurt and Gaurav will educate you how the ELK stack provides the perfect tool set for collaboration across teams, not just for Devs and Ops, but for ALL your colleagues. Doors open at 6:30. Vienna The  has announced that its first Elasticsearch meetup will be taking place on Thursday, June 12 starting at 19:00. will be covering all the shiny new features in Elasticsearch 1.0. Don’t miss it! Whittier - SoCal Whittier, Southern California - not the most well-known spot on earth. But wait! and are in the area and would love to speak to you and answer any questions you may have. Meet them on Tuesday June 10 for a couple of beers, some snacks and probably a few pizzookies at at 6:00. RSVP on the meetup page. Lots of great stuff on the docket for this upcoming week. Remember, if you’re giving a talk, hosting a meetup or otherwise sharing the Elasticsearch and ELK stack love, . We’d love to feature your knowledge sharing bits here! And with that all the Elasticsearch fans at Puppet Camp DC wave thank you and see you soon! ","locales":"","title":"Where in the World is Elasticsearch? - June 07, 2014"}
{"index":{}}
{"author":"Kevin Kluge","category":"Engineering","publish_date":"2014-06-06T00:00:00.000Z","url":"/blog/tool-help-routing-issues-elasticsearch-1-2-0","seo_title":"","content":" ","locales":"","title":"A tool to help with routing issues from Elasticsearch 1.2.0"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-06-04T00:00:00.000Z","url":"/blog/2014-06-04-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. Wait, a realtime patch and vulnerability dashboard using elasticsearch and kibana? Why yes! (note: still beta) — Michael Henry (@neoCrimeLabs) Elasticsearch core Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases. MT : Standing room only for 's talk on for real-time analytics — Paige Roberts (@RobertsPaige) Slides & Videos One of the best talks Scoring for human beings by . Get the video when it's out ! — Lucian Precup (@lucianprecup) You got it, Lucian! on Scoring for Human Beings at Berlin Buzzwords 2014 on Elasticsearch Aggregations at Berlin Buzzwords 2014 on Two Use Cases for Scaling Data with Elasticsearch at Berlin Buzzwords 2014 Where to find Us We'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking or hosting a meetup, let our Community Manager,, know! RT \"“: Impressive presentations tonight about by \"\" — Brussels DataScience (@DataScienceBe) Austria The has scheduled their first meetup for Thursday, June 12th. Please join us at 7 PM to hear from on What's New in Elasticsearch. France Israel The is hosting a workshop on Applied Cloud Computing with Google Cloud Platform at Google Tel Aviv. Topics will include BigQuery with Logstash as Application Log Analysis Platform. The workshop runs from 9:30-13:30 on July 7th. The Netherlands We've got not one but awesome conferences going on in Amsterdam the week of June 16th: Norway Romania will speak at on Big data real time search and analytics. Topconf Bucharest runs from June 10-13th and Costin will speak at 3:20 PM on June 12th. New Zealand The in Auckland will get together on June 7th to talk Getting Control of Your Logs. Lots of Logstash love on offer. Switzerland Alexander Reelsen and will be speaking at the on June 7th. Alex will discuss What's new in Elasticsearch and Britta will cover the Significant Terms Aggregation. Doors open at 7 PM. United Kingdom United States Where to Find You Our Community Manager, , is hard at work to help folks create more Elasticsearch meetup groups and to help meetup organizers find more speakers. If you are interested in either effort, . Oh yeah, we're also . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticearch, Kibana, and Logstash than where you rest your head. Trainings Attending training, and learning a lot of new things from the experts. Great product and training. — Trent Swanson (@trentmswanson) Thanks, Trent! Glad it was useful for you. :) If you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elasticsearch - June 04, 2014"}
{"index":{}}
{"author":"Clinton Gormley","category":"Engineering","publish_date":"2014-07-09T00:00:00.000Z","url":"/blog/elasticsearch-1-2-2-released","seo_title":"","content":" <!--<br /> div.itemizedlist { margin-top: -15px}<br /> --> Today, we are happy to announce the bugfix release of , based on Lucene 4.8.1. You can download it and read the full changes list here: . Elasticsearch 1.2.2 is the latest stable release. Windows users and users with heavy indexing requirements should upgrade. The full change log is available in the , but we will highlight the three most important ones below: ","locales":"","title":"Elasticsearch 1.2.2 released"}
{"index":{}}
{"author":"Zachary Tong","category":"Engineering","publish_date":"2014-07-08T00:00:00.000Z","url":"/blog/quick-tips-regex-filter-buckets","seo_title":"","content":" A while ago on Twitter, someone was asking if aggregations could be used to categorize data based on irregular product codes. They were combining data from several legacy systems, so the product codes were erratic and not internally consistent. There were two classes of product codes: The goal was to determine how many of each type existed, and how many were missing certain meta-data tags. Many people are stumped by situations like this, since it is initially unclear how to categorize two groups of product codes that are only related to each other by the structural format of their ID. Option 1: Pre-parse with Grok and Logstash The best solution is to deal with inconsistencies like this at the input level. For example, you can build a to identify the two different patterns and tag them appropriately. Once tagged, it is trivial to do aggregations on the structured document data. If a new pattern occurs in your data that doesn’t match the pattern, Logstash will emit a tag. This has the benefit of improving search results, since only properly tagged documents will be visible to search. You can go back and fix/reindex the “broken” data at your lesiure and iteratively improve your search results through better-tagged data. Option 2: Regex to the rescue But things are not always this simple. What if you just indexed 10TB of data and realized you forgot to include appropriate pre-parsing? Re-indexing would be a pain, or potentially impossible. We need a solution that operates on your existing data. The key is to use a regular expression and filter buckets. A will hold all documents matching its filtering criteria. If we place a inside the bucket, we can find all product IDs matching a certain pattern. Once documents have been sorted into one of the filter buckets, we can apply other bucketing and metrics to derive statistics. Let’s take a look at a very simple example. First we index some data with mixed product codes: PUT /test/data/_bulk {\"index\":{}} {\"product_code\" : \"AB123\"} {\"index\":{}} {\"product_code\" : \"XY345\"} {\"index\":{}} {\"product_code\" : \"AZ987\"} {\"index\":{}} {\"product_code\" : \"ZZ192\"} {\"index\":{}} {\"product_code\" : \"A99999\"} {\"index\":{}} {\"product_code\" : \"A12345\"} {\"index\":{}} {\"product_code\" : \"A98765\"} {\"index\":{}} {\"some_other_field\" : \"xyz\"} {\"index\":{}} {\"some_other_field\" : \"123\"} Then we can run a very simple aggregation to sort out the various codes: GET /test/data/_search?search_type=count { \"aggs\" :{ \"total_count\" : { \"global\" : {} }, \"XX999\" : { \"filter\" : { \"regexp\":{ \"product_code\" : \"[a-z]{2}[0-9]{3}\" } } }, \"X99999\" : { \"filter\" : { \"regexp\":{ \"product_code\" : \"[a-z]{1}[0-9]{5}\" } } }, \"no_format\" : { \"missing\" : { \"field\" : \"product_code\" } } } } This query will give a document count for each product code matching the two regex patterns. The bucket, which will get , will show all documents that don’t have any product code at all. From this base, it is easy to add extra metrics, such as the average price of each product, or the average number sold each day for the last month, etc. The takeaway tip The key to this tip is the bucket. This bucket accepts Elasticsearch filter, which means you can construct the same kind of complex filtering operations which you already use in search requests. And because these are filters, they enjoy all the performance benefits inherent to filters. Starting in Elasticsearch version 1.3.0, you will also have access to the (note the plural). Although functionally equivalent to using multiple filter buckets, the simplifies the syntax for applying multiple filters at the same time. So next time you need to aggregate some statistics, but are stumped by the irregular or inconsistent nature of your data, remember the bucket (and the upcoming bucket). ","locales":"","title":"Quick Tips: regex filter buckets"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2014-07-07T00:00:00.000Z","url":"/blog/world-elasticsearch-9","seo_title":"","content":" The week before last week all Elasticsearch ELKs were in the wild (or at the beach) in Amsterdam at our #ESAllHands meeting... on the speed lane! — Dimitri Marx (@MarxDimitri) ...This week we are back with some awesome events and meetups happening in Barcelona, Cologne, Montreal, Paris, Philadelphia, Seoul and Tel Aviv!Mimacom and Elasticsearch are on the road in Barça! Our local Elasticsearcher will be giving a talk on on Thursday July 10th at the . Are you in the area and don’t want to miss out on interesting tech talk and coffee? Then sign up .Want to learn how to work with public, conversational, real-time data? Make sure to head down to the . Our own will present a workshop together with Sylvain Carle on . July 9th, 1pm, sign up .If you feel like some great talks on all things Elasticsearch you should read on. On Wednesday July 9th, our German partner is hosting their 2nd meetup and we are lucky to be part of it! will kick it off with a talk about and will bring it home with a more specific look into . now!We are pleased to announce our on Monday 7th July where two great speakers will present their use cases within two very different business industries. Sébastien Capillier will talk about how is monitoring their logs with Elasticsearch and Kibana and Alexandre Fricker will present their Logstash, Redis, Elasticsearch, Kibana and Marvel ecosystem and explain how has been using it for different use cases. Amusez-vous!Our 2nd Philadelphia meetup on July 9th is almost here! Last time folks made an introduction to Elasticsearch and the new features in 1.0.x. This time you will hear an interesting use case story about how uses Elasticsearch on both and . There are still some free spots left, make sure you .Thanks to the lovely , we have our next , scheduled for July 10th complete with Elasticsearchers from all over the world. If you are in the Seoul area you should make sure to say hi to and as it is rather seldom to meet them and hear them talk that far East. :)The in Tel Aviv is going to host a day of education on Applied Cloud Computing with the Google Cloud Platform. Among the list of great talks around how to apply these great features to everyday tasks of developers and DevOps folks will speak about and will also show a demo.That's it for this week, we look forward to meeting you at one of these exciting places!And remember, if you’re giving a talk, hosting a meetup or otherwise sharing the Elasticsearch love, . We’d love to feature your knowledge sharing bits here! ","locales":"","title":"Where in the World is Elasticsearch? - July 07, 2015"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-07-02T00:00:00.000Z","url":"/blog/2014-07-02-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. Elasticsearch core Full house at last week's ELK Stack open space session at DevOps Days Silicon Valley Photo credit: Ilan Rabinovitch Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases. OpenTable <3s Elasticsearch LOL our logging ElasticSearch cluster has just reached 1 Billion documents :) — Paul Stack (@stack72) Slides & Videos Costin Leau's Presentation from the recent Hadoop Summit North America 2014 explains the significant terms aggregation, a feature that allows to users to identify terms that are relevant to a particular set of documents Jean Baptiste Favre shares details on Blablacar's system architecture and how they use the ELK stack (en français) Where to find Us We'd love to feature all the great Elasticsearch, Logstash, and Kibana presentations and meetups happening worldwide in this section. If you're speaking or hosting a meetup, let our Community Manager, , know! France The 8th Elasticsearch France Meetup will take place in Paris on July 7th at Coworkshop. Space is very much limited, so take a look and to save your place. Germany Israel Italy The very first Elasticsearch Italy Meetup has been scheduled for July 16th! Please join us to hear from on What's New in Elasticsearch. You'll also be treated to a case study talk on how Lutech uses Elasticsearch in its Threat Management System for Breach Detection, Intelligence & Response. Doors open at 6:00 PM, and . Japan The will convene on July 14th at 6:30 PM. Register now to get a chance to hear from our core developers and . Even cooler, our CTO will be attending this meetup, so register now to save your place! Korea The will convene in Seoul on July 10th at 7 PM. Register now to get a chance to hear from our core developers and . New Zealand The Auckland JVM Users Group will rebooting their meetup series, with their newest offering focused on Elasticsearch. You can join them on Tuesday, July 22nd at 6:00 PM, and . Spain Clinton Gormley will be speaking on Scaling Real-Time Search and Analytics with Elasticsearch at on July 10th. Clint takes the stage at 9:15 AM. United Kingdom United States Where to Find You Our Community Manager, , is hard at work to help folks create more Elasticsearch meetup groups and to help meetup organizers find more speakers. If you are interested in either effort, . Oh yeah, we're also  . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticearch, Kibana, and Logstash than where you rest your head. Trainings If you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This week in Elasticsearch - July 02, 2014"}
{"index":{}}
{"author":"Kevin Kluge","category":"","publish_date":"2014-06-24T00:00:00.000Z","url":"/blog/logstash-1-4-2","seo_title":"","content":" We released Logstash 1.4.2 today. This is a bug fix release that includes an important fix for a security vulnerability that was present in previous versions of Logstash. We recommend that users of Logstash's zabbix or nagios_nsca outputs upgrade immediately. Deployments that do not use the zabbix or the nagios_nsca outputs are not vulnerable and do not need to upgrade for this reason. the vulnerability The vulnerability impacts deployments that use the either the zabbix or the nagios_nsca outputs. In these cases, an attacker with an ability to send crafted events to any source of data for Logstash could execute operating system commands with the permissions of the Logstash process. Deployments that do not use the zabbix or the nagios_nsca outputs are not vulnerable and do not need to upgrade for this reason. We have added this vulnerability to our and are working on filling out the CVE. We would like to thank Jan Karwowski and Danila Borisiuk for reporting the issue and working with us on the resolution. remediations An upgrade to Logstash 1.4.2 will address the issue. This is our recommended path. Some deployments may be able to remove the zabbix and nagios_nsca outputs from their configuration. This is a viable option to remediate until an upgrade can be performed. We have also released a patch for the 1.3.x series of Logstash releases. This patch can be applied to address the vulnerability. This patch is available as an option to upgrade to 1.4.2. If you apply the patch, you do not need to upgrade to 1.4.2 to fix the vulnerability. In order to apply the patch for the 1.3.x series, do the following on each Logstash host.  This example uses 1.3.3, but you can also use these steps for 1.3.0 - 1.3.2. # mkdir -p /tmp/logstash-patch/logstash/outputs # wget -O /tmp/logstash-patch/logstash/outputs/zabbix.rb https://github.com/elasticsearch/logstash-contrib/raw/v1.4.2/lib/logstash/outputs/zabbix.rb # wget -O /tmp/logstash-patch/logstash/outputs/nagios_nsca.rb https://github.com/elasticsearch/logstash/raw/v1.4.2/lib/logstash/outputs/nagios_nsca.rb # jar uf logstash-1.3.3-flatjar.jar -C /tmp/logstash-patch/ logstash/outputs/zabbix.rb -C /tmp/logstash-patch/ logstash/outputs/nagios_nsca.rb other fixes The 1.4.2 release includes a number of other fixes. For Logstash core: And for logstash-contrib: You can read the full , or jump right to the page. ","locales":"","title":"Logstash 1.4.2 released, vulnerability fixed"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-06-18T00:00:00.000Z","url":"/blog/2014-06-18-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. explains Elasticsearch in just 90 seconds .... Elasticsearch Core on all things Elasticsearch from the recent Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases. The `Elasticsearch::Persistence::Model` has been released to Rubygems. Details here: — Karel Minařík (@karmiq) Got plans on July 2nd? Join & 1500+ of your fellow enthusiasts for our biggest webinar yet — elasticsearch (@elasticsearch) Slides & Videos Michael Kaisser on Geospatial Analysis of Social Media posts with Elasticsearch from Berlin Buzzwords 2014 Just published my slides about (add advanced search to your legacy app): — David Pilato (@dadoonet) Matthew Britt from the University of Michigan showcases how to better handle HPC logs using Logstash From June's Elasticsearch London Meetup With excellent and useful information for non-hipsters, too! Jens Kohl's overview of the Elasticsearch PHP client Itamar Syn-Hershko shares the Ultimate Guide to Elasticsearch plugins at Berlin Buzzwords 2014 ","locales":"","title":"This Week in Elasticsearch - June 18, 2014"}
{"index":{}}
{"author":"Aaron Mildenstein","category":"Engineering","publish_date":"2014-06-13T00:00:00.000Z","url":"/blog/elasticsearch-curator-version-1-1-0-released","seo_title":"","content":" ","locales":"","title":"Elasticsearch Curator -- Version 1.1.0 Released"}
{"index":{}}
{"author":"Livia Froelicher","category":"Engineering","publish_date":"2014-06-06T00:00:00.000Z","url":"/blog/berlin-buzzwords-fever","seo_title":"","content":" Last week was all about - the EU startup hub's most exciting conference on storing, processing and searching large amounts of digital data. Over 600 attendees from every continent save Antarctica, with visitors from Chile, India, Korea, South Africa and the US. Among these, a total of 15 Elasticsearchers were onsite to share the love and spread knowledge about our products and contributions to the open source community. Take a minute to watch   on Apache Lucene 4: The fantastic Kulturbrauerei – an old beer brewery – was once more the perfect atmosphere for some great and inspiring discussions in the expo hall. And people really loved our swag. Thanks a lot Elasticsearch for protecting me from bad architecture nightmare :) — Loïc Bertron (@loicbertron) , ,,and rocked their sessions without leaving any seats empty. (We'll be featuring all of our employees talks from Buzzwords in our weekly blog newsletter, .) (photo source: ) Berlin Buzzwords wasn't just about talks. As a gold sponsor Elasticsearch had the chance to provide a fun chill & play area with a cool foosball table, comfy outdoor recliners and a green powered DIY smoothie bike. Here's taking a ride: The search for hydration continues!@kimchy cycles his way to a smoothie — sejal korenromp (@sejiek) After two days of conference talks and social gatherings, it was time for a deep-dive into the Elasticsearch code. The Elasticsearch hackathon was a great round off of the entire event. Hosted by , we had a great facility to make this event truly amazing. Thanks again to the , especially brand new community manager Cristina Santamarina, and all other sponsors who contributed to this hackathon! Florian and Felix will be sharing a write up of the hackathon with us next week. Today's hackathon. Thanks for all! — Leslie Hawthorn (@lhawthorn) Finally, a BIG thank to Newthinking (especially) for their fantastic organization before and throughout the whole conference. It was a pleasure to be part of it and discover a bunch of new glowing Elasticsearch & ELK stack users! ","locales":"","title":"Berlin Buzzwords Fever"}
{"index":{}}
{"author":"Shay Banon","category":"News","publish_date":"2014-06-05T00:00:00.000Z","url":"/blog/series-c-financing-sharing-fruit-labors","seo_title":"","content":" You may have already , but in case you missed it, Elasticsearch just closed a $70M funding round led by NEA Ventures. Needless to say, we’re very excited about what this means for the future of Elasticsearch and the ELK stack. We hope that you’ll join us in celebrating today! So, what do we plan to do with our new found venture capital? More of the same, and even more. We’ll continue to make the ELK stack open source and ever improving. Always. We’ll continue adding more commercial offerings to our product line. We’ll continue hiring great employees. And we’ll keep . At times like these, anyone would find themselves reflecting on what they’ve built. Just over four years ago, I created Elasticsearch with the vision of building a system architected for scale and the cloud. A few years later, I was fortunate to be joined by Steven, Simon and Uri so we could build a company around the Elasticsearch project. Today, we employ nearly one hundred people in 9 countries and provide an end to end, full stack, search, analytics, logging and data visualization engine. It’s been a humbling ride! In particular, though, I am proud of our open source foundations and how our work empowers the community to innovate. When you look at major community users like Mozilla and Wikipedia, it’s incredible to know that our open source products support their missions. Not only are we helping our customers at major financial institutions to and newsrooms to to their reader engagement analytics. But our creations also support global access to free knowledge and the creation of an open, innovative internet. All while making sure that big companies can keep their systems up and running, and to you. We’re thrilled we can share the successes we’ve enjoyed with all of you today. You keep telling us what you need from the ELK stack, and we’ll keep making great products for our customers and community! (The apple photo is courtesy of . The apples were designed with love by nature and our marketing team in Amsterdam.) ","locales":"","title":"Our Series C Financing: Sharing the Fruit of Our Labors"}
{"index":{}}
{"author":"Steven Schuurman","category":"News","publish_date":"2014-06-05T00:00:00.000Z","url":"/blog/elasticsearch-raises-70m-series-c-nea","seo_title":"","content":" With unbelievable pride, I'd like to announce that we just closed another amazing round of financing. In this Series C round, which was led by New Enterprise Associates (NEA) with contributions from Benchmark Capital and Index Ventures, we raised $70M. NEA General Partners Scott Sandell and Harry Weller led NEA's investment in Elasticsearch. Harry will be joining our Board of Directors, and you can look forward to his thoughts on our round later today.Over the course of three rounds of financing, Elasticsearch has raised a grand total of $104M since we started seeking capital just 18 months ago.The last year and a half has been an amazing adventure, and quite frankly I only feel like we're just about to shift into the highest gear. With some of the highest caliber customers already on board, from Facebook to The Guardian, our products are powering some of the world's most sophisticated search and analytics engines. In addition to our most well-known use case, businesses are now commonly using our logging solution. Bloomberg uses the Elasticsearch ELK stack to centralize 1.5B log lines/day, generated across 1,000s of production machines. The ability to centralize them in one place to troubleshoot any potential issues saves their 2,000 programmers an immense amount of time and hassle.Even better, we've built a world-class team to fuel our product development and commercial activities. We've benefited greatly from the support from our board members Rod Johnson, Peter Fenton and Mike Volpi in our search for new business and talent. The experience these guys bring to the table is invaluable to us, as we're moving quickly to respond to the immense market demand for the ELK stack. As we'll only be expanding our efforts from here, we chose to go with an investment firm with deep open source and enterprise software experience at scale. One that could help us grow aggressively and propel our products offerings forward. Of all the top-tier Venture Capital firms out there, we believe the team at NEA stands out as the one best positioned to help us execute against our plans during this phase of extremely rapid growth. We are nothing short of humbled by the fact that after having signed up Benchmark and Index, we get to work with this third team of wonderfully talented people. Together, Benchmark, Index and NEA are some of the best investment firms and people in open source.The company will be using the additional funds to continue to expanding our investment in our open source products, whilst also expanding our commercial presence across the globe. Elasticsearch's dual headquarters in Los Altos, California and Amsterdam, The Netherlands house about half of our global team. We have regional offices in Berlin, London and Phoenix, but they aren't the only places employees call home. We also have developers in Canada, the Czech Republic, France, Spain, Romania and states across the US. To facilitate our growth and remain close to our user base, we make our company's distributed nature work to our advantage. If you want to know more, check out I did with FAST company.Elasticsearch has always been a wonderful place to work. We offer our people - in all disciplines - the chance to work in an open and vibrant culture and learn from some of the best and brightest in our industry. With these additional funds, we're looking to continue our track record of hiring the best talent our industry has to offer. If you're interested in helping us build the world's next great software company, take a peek at . We're incredibly excited about what the future holds. My thanks to all of our investors, and a huge thanks to all of our employees worldwide. We wouldn't be able to share today's news without all of your hard work, passion and dedication to a simple and beautiful customer experience! ","locales":"","title":"Elasticsearch Raises $70M in Series C from NEA"}
{"index":{}}
{"author":"Leslie Hawthorn","category":"News","publish_date":"2014-06-03T00:00:00.000Z","url":"/blog/hello-hadoop-summit","seo_title":"","content":" Greetings and good morning, San Jose! If you're attending , we want to make sure that you know where to find us, and what we have on.We'll have tons of folks from our team to chat with you about all things Elasticsearch and Apache Hadoop, plus how the ELK Stack can give you massive insights into your data in real-time. We have an expo table and we're at your service!We're even more excited to invite you to join session today. Costin is the lead developer of Elasticsearch for Apache Hadoop, and his talk will cover using Elasticsearch, Hadoop and Storm. Join Costin at 4:35 PM today in the Future of Hadoop track!Costin will explore using Apache Hadoop as a data platform, Apache Storm for real-time computation, data ingestion and orchestration and Elasticsearch for performing advanced real-time searches. The session will also have a particular focus on the architectural challenges of bridging batch and real-time systems and how to overcome them, keeping a close eye on performance and scalability.We're looking forward to seeing everyone here at the San Jose Convention Center, and we'll be there throughout the conference. See you soon! ","locales":"","title":"Hello from Hadoop Summit! "}
{"index":{}}
{"author":"Chris Earle","category":"Engineering","publish_date":"2014-06-04T00:00:00.000Z","url":"/blog/marvel-1-2-released","seo_title":"","content":" Today, we are excited to announce the release of Elasticsearch Marvel 1.2. With this upgrade, we are bringing some amazing new features to our Marvel users. These features include the shard allocation dashboard, improved navigation and customization, and updates to the Sense knowledge base to bring it up to date with the latest Elasticsearch 1.2.1 release. As always the new release works with Elasticsearch version 0.90.9 and up. Shard Allocation Dashboard We’ve added a new dashboard for visualizing the shard allocation of the cluster for nodes and indices. With this dashboard you can now see how the cluster is allocated and how the indices are distributed among the nodes. You can also go back in time to see how your cluster has changed over the past few minutes, hours and beyond. This feature should be a big help for diagnosing why your cluster went red at 3 am. And here is a demo video showing off the new features…   Improved Navigation and Customization In previous versions, saving a dashboard would detach it from the Marvel dashboard list and place it into a completely different drop-down – similar to how Kibana works. We felt like this was impacting the user experience, and wanted to make saving customizations a much smoother experience. With this in mind, we refactored the top navigation to simplify things. We’ve replaced the default Kibana dashboard drop down with a single save button that allows the user to customize their dashboards and replace the defaults. When the user customizes the “Overview” dashboard and saves it, the Marvel navigation will then use the customized saved version from that point on. Updates to Sense to include Elasticsearch 1.2 Features To keep up with Elasticsearch’s new features we’ve upgraded Marvel’s knowledge base to include the latest APIs from Elasticsearch 1.2. The updated APIs include the new percentiles, significant terms, and cardinality aggregations. The following endpoints additions are also included: , <code>_cat/segments, , <code>_count, and . Upgrade Instructions To upgrade, you must install the latest Marvel plugin on all your Elasticsearch nodes. As with any other Java plugin, you will need to restart nodes (one by one) in order for the version to become active. Please refer to the documentation for . For a complete change list, see . We welcome all feedback so we can continue improving Elasticsearch Marvel. Please send questions, praise or pains to the . ","locales":"","title":"Marvel 1.2 Released"}
{"index":{}}
{"author":"Leslie Hawthorn","category":"","publish_date":"2014-06-02T00:00:00.000Z","url":"/blog/world-elasticsearch-6","seo_title":"","content":" Welcome to this week's ! Several members of our core developer team are speaking at conferences throughout the EU, and we've got meetups on from Belgium to Zurich. Read on to find where you can hear about all things ELK Stack, and a special chance to do live Q&A with our CTO, . From last week's Berlin Buzzwords' Adventures - Adrien Grand on Elasticsearch Aggregations Norwegian Developers Conference NDC Oslo will welcome more than 1600 attendees this week, covering topics from Cloud to User Experience. Our very own , creator of NEST, will teach you all about implementing search and provide a tour of Apache Lucene under the hood. Martijn's presentation, , on Friday, June 6th at 15:00 in Room 6. If you have any questions on .Net and Elasticsearch, make sure to say hi to Martijn during the conference! PyCon Russia Bringing together lovers of all things Python in Russia, this annual 2 day conference takes place June 2-3rd near Yekaterinburg. If you're joining us for the festivities, please plan to attend presentation on exploring your data, . Honza will take the stage in the main room at 14:10 on Day 2 of the conference. Don't forget to say hello in the hallway track - Honza is our go to guy for all things Python and Elasticsearch! Topconf Bucharest Topconf's vision is to be the conference series that inspires attendees with information on how to optimize business critical IT systems and to take future IT technologies and put them to the best possible use for customers. , the creator of Elasticsearch for Apache Hadoop, will present on . His session will focus on big data through the lens of the Hadoop platform and teach you how Map/Reduce, Hive, Pig or Cascading jobs can leverage a search engine to significantly speed up execution and enhance their capabilities. You can catch Costin's talk at 15:20 in the Asia room on Thursday, June 12th. Costin will also be at the conference in the hallway track if you want to talk about how to make your life with Hadoop better! Mimacom Days Zurich Our partner Mimacom to give attendees the opportunity to network and hear about the latest cutting-edge technologies that can drive their businesses' success. This Wednesday, you can hear from , Elasticsearch core developer, on Elasticsearch - Beyond Full-Text Search. Alex's talk starts at 9:45, directly after the conference's opening remarks. Alex will be at Mimacom Days Zurich throughout the day, so make sure to say hello and ask him about all things JVM, concurrency, scalability and Elasticsearch! You can get your Elasticsearch hackfest on this week with the BigBoards Garage Meetup group in Aarschot. Meetups San Jose There are several great talks happening at meetups worldwide this week, but we're particularly excited to let folks know that Shay Banon, creator of Elasticsearch, will be holding a live Q&A session at this week's ! Join Shay and a bunch of our core developers at eBay's San Jose campus tomorrow, June 3rd. Doors open at 6:30 PM. Thanks to the Bay Area Search Meetup and eBay for hosting us! Aarschot, Belgium The will be hosting an evening of fun and hacking on Logstash & Kibana on Tuesday night. Everyone will be getting together at 19:30 for the festivities. what cool things you build during the meetup! Austin The will be hosting an introduction to various technologies, including Logstash, Docker, OpenStack and more. Doors open at 7:00 PM on Wednesday, and this meetup promises to be a great teaser for all the great content on offer at the upcoming Texas Linux Fest. (Psst: you can get an from Aaron Mildenstein, Logstash Core Developer, at TLF!) Breizh, France The Breizh JUG will welcome our very own , to talk about all things Elasticsearch this Thursday. Amongst other topics, David will cover how Elasticsearch can make your life far easier than using plain old SQL queries. Doors open at 18:00. London The is getting together this Wednesday 93 Feet East. In addition to our fabulous venue, there will be three presentations ","locales":"","title":"Where in the World is Elasticsearch? - June 02, 2014"}
{"index":{}}
{"author":"Leslie Hawthorn","category":"User Stories","publish_date":"2014-05-29T00:00:00.000Z","url":"/blog/openstack-elastic-recheck-powered-elk-stack","seo_title":"","content":" Sean Dague at the recent OpenStack Summit in Atlanta on Every day, the OpenStack project runs hundreds of patches through its continuous integration system to assure code consistency, functionality and smooth integration with other projects in the OpenStack ecosystem. This process works exceptionally well for a majority of patches, but as with any system pushing so much data through development infrastructure, there are times when there are failures unrelated to the patches being tested. It may be that a VM goes down unexpectedly during a test, an external resource is unavailable (nameserver, package repository, etc), a service unrelated to the test locks up or one of many race conditions or other transient bugs pops up. Gatekeeper Graph Showing VMs Used over the Course of a Day Historically, the team worked to track these issues in bug reports, which developers could submit and then search through at to see if their failed change was tied to one of these types of “transient” bugs. This process was largely manual and only gave limited data about the frequency and types of patches these bugs were occurring on. It was also difficult to determine whether a “new” bug was only impacting your change, whether it had been going on for some time without being reported or whether the bug had simply gone away. There was also no automated way of notifying a developer that their change had encountered a known bug:  they had to check the rechecks page themselves. Old Manual Rechecks Page When a developer ran into one of these bugs, they would then rerun the tests with a comment referencing the bug in our code review system. Enter Elasticsearch Back at the OpenStack Summit in the fall of 2012, Clark Boylan and Sean Dague came up with some ideas around a more automated solution, using Elasticsearch to address many of the issues with the manual process so that developers could be automatically notified if their change hit a known bug. In 2013, Clark began implementing an Elasticsearch + Logstash + Kibana (the ELK stack) solution for the OpenStack infrastructure. The stable setup he finally came up with is fully open source and documented at . Over that summer, Sean created some sample code to talk to the web service. Joe Gordon and Matthew Treinish then turned the sample code into in September of 2013, when stress on the project infrastructure hit a high point and manual rechecks were common. With now in place, contributors can: Format for this change is , so for , a file called with data from the bug would be created containing: query: > message:\"SSHTimeout: Connection to the\" AND message:\"via SSH timed out.\" AND tags:\"console.html\" AND NOT build_name:\"check-tempest-dsvm-neutron-heat-slow\" (You can see live files here: .) Just like everything else in the OpenStack ecosystem, this patch is then reviewed by peers and then merged into the system to be used. From there, the page is automatically generated with all bugs included as queries in . The page includes frequency graphs and links to resources for each bug: . Home Page Automatic responses are then fed into our code review system from when a patch hits a known bug, e.g.: Automatic Response to Code Review from Now, when known failures are encountered, subsequent developers running into the same issue are automatically notified and can take the appropriate action immediately without having to search through the bug tracker. This innovation has been a boon for our developers, who can now move along more efficiently with development instead of getting stuck on transient bugs. Overview of Flow by Sean Dague By using the ELK stack and , we have given members of our community tools that allow them to: You can find out more about here: Thanks to Clark Boylan, Joe Gordon and Sean Dague for reviewing this article, and to Matt Riedemann and Matthew Treinish who keep reviews for this project coming along as additional core reviewers. ","locales":"","title":"Openstack Elastic-recheck: Powered by the Elk Stack"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-05-28T00:00:00.000Z","url":"/blog/2014-05-28-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. Elasticsearch core Today's hackathon. Thanks for all! — Leslie Hawthorn (@lhawthorn) Elasticsearch Ecosystem Here’s some more information about what is happening in the ecosystem we are maintaining around the ELK stack – that’s Elasticsearch plus Logstash and Kibana – including plugin and driver releases. Introduces Kibana at last week’s Slides & Videos We’re got a bunch of slide & video love from Berlin Buzzwords and beyond! ","locales":"","title":"This Week in Elasticsearch - May 28, 2014"}
{"index":{}}
{"author":"Kevin Kluge","category":"Engineering","publish_date":"2014-06-03T00:00:00.000Z","url":"/blog/elasticsearch-1-2-1-released","seo_title":"","content":" We released today. This is a bug fix release that includes the fix for a severe bug in Elasticsearch 1.2.0. We recommend that anyone using version 1.2.0 upgrade immediately to this 1.2.1 release. We removed the 1.2.0 release from our download site and repositories due to the severity of the bug. A Fix for Document Routing and Duplicate Documents There was  in Elasticsearch 1.2.0 that could have a number of bad side effects on the cluster. Possible side effects include: The 1.2.1 release fixes this bug. The bug fix will restore access via get to the documents that were indexed prior to the upgrade to 1.2.0. An upgrade to 1.2.1 will not correct duplicate documents. Also, an upgrade to 1.2.1 will break access via get to documents that were indexed since the upgrade to 1.2.0.  We are investigating what tools we can create to help with the diagnosis and correction of these problems. We will post an update as soon as possible with this information. Other Fixes Elasticsearch 1.2.1 also fixes a . We found that the  was too conservative and could prevent memory requests that should have been allowed. We have effectively disabled this new circuit breaker for 1.2.1. The fielddata circuit breaker continues to work as it has in previous releases. There is also a . A mapping that contained the parameter  in the root type would produce a MapperParsingException. For example: PUT test1 { \"mappings\": { \"type_name\": { \"include_in_all\": false, \"properties\": {} } } } would trigger the bug, effectively removing this feature from 1.2.0. If such a mapping is present when upgrading to 1.2.0, then the mapping for this index can be corrupted once a document with a new field is indexed. 1.2.1 restores the correct behavior of . Conclusion Users with Elasticsearch 1.2.0 should immediately upgrade to 1.2.1. We will be working to see what tools we can create to help users that have experienced problems from 1.2.0 and we will update on our progress soon. Please  and let us know what you think. You can report any issues on our . ","locales":"","title":"Elasticsearch 1.2.1 Released"}
{"index":{}}
{"author":"Konrad Beiske","category":"","publish_date":"2014-05-30T00:00:00.000Z","url":"/blog/found-extending-the-scripting-module","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud. How to Add Another Scripting Language You can use this post as a starting point to make a language plugin for any scripting engine that implements the JSR-223 interfaces. ","locales":"","title":"Extending the Scripts Module"}
{"index":{}}
{"author":"Costin Leau","category":"Engineering","publish_date":"2014-05-27T00:00:00.000Z","url":"/blog/es-hadoop-2-0-g","seo_title":"","content":" I am elated to of Elasticsearch for Apache Hadoop 2.0 GA. Elasticsearch for Apache Hadoop, affectionately known as es-hadoop, enables Hadoop users and data-hungry businesses to enhance their workflows with a full-blown search and analytics engine, in real-time. es-hadoop is open source and works across different Hadoop, Cascading, Apache Hive and Apache Pig versions and across multiple Hadoop , whether vanilla Apache Hadoop, Cloudera CDH, Hortonworks HDP, MapR or Pivotal. No dependencies, all the functionality. Native Integration with Hadoop Using Elasticsearch with Hadoop has never been easier. Thanks to our deep API integration, interacting with Elasticsearch is similar to interacting with HDFS resources, whatever the environment used, from plain Map/Reduce, to Cascading plans, Pig scripts and Hive queries. For each environment, es-hadoop provides a interface that one can use to read, write and query Elasticsearch transparently:  the dedicated Map/Reduce /, Cascading /, Hive / and Pig /tions take care of the heavy lifting so you do not have to fiddle with data conversion or network communicating with Elasticsearch. If your data happens to be JSON that's fine by us:  es-hadoop supports that too. Pure Map/Reduce model Most importantly, the Map/Reduce model in Hadoop is mapped on top of your Elasticsearch cluster:  by leveraging Elasticsearch distributed architecture es-hadoop operation scales out, being executed in parallel across the target shards. In other words, whenever a write or a read is issued, es-hadoop will dynamically determine the number of shards used for the target index and, for each one, use a dedicated task to push/pull the data in parallel, enabling the operation to scale out with the data. Moreover, es-hadoop has full insight into the data topology used underneath so it can run its tasks with the data, a great performance boost in deployments where Elasticsearch and Hadoop clusters run side by side. Portability Elasticsearch Hadoop is actively tested against various Hadoop distributions (such as vanilla Hadoop, CDH, HDP, MapR, Pivotal). Whether you are using Hadoop 1.x or 2.x, the so-called old () or the new () API, vanilla Hadoop or a certain distribution, we invest heavily in ensuring that es-hadoop works reliably no matter your Hadoop environment. Operational Ease es-hadoop provides a single binary (~350 KB jar) for its entire feature set, and those interested in saving a few KBs can use the jars. Without any dependencies, each jar can be used as is, easily embedded inside Hadoop jobs or provisioned throughout the cluster. At runtime, the firewall-friendly HTTP/REST protocol is used while HTTP and SOCKS proxies (with or without authentication) being supported for those running in locked down networks. Production-ready, at Scale We are happy to report that es-hadoop is being used in multiple data-intensive environments:  in a recent example, a large financial institute that stores all of their raw access logs in Hadoop – billions of documents – has been using es-hadoop to index the data into Elasticsearch and then visualize it using Kibana. This approach allowed the customer to have near real-time visibility into their data through Kibana, yet also run batch oriented jobs over all their raw data when needed. By combining Hadoop and Elasticsearch, organizations gain a scalable, distributed platform that enables fast search and data discovery across tremendous amounts of information. And through es-hadoop, this is easier than ever. But don't take our word for it, es-hadoop 2.0 GA, try it out and what you think! ","locales":"","title":"Elasticsearch for Apache Hadoop 2.0 GA Released"}
{"index":{}}
{"author":"Leslie Hawthorn","category":"","publish_date":"2014-05-26T00:00:00.000Z","url":"/blog/world-elasticsearch-berlin-buzzworks-edition","seo_title":"","content":" Welcome to the latest edition of . For this week, the answer is largely \"at \"! If you're joining us in Berlin, there's a full docket of Elasticsearch content on offer at the conference. If you're not, no worries - the conference organizers will be taping and publishing all of these sessions for your later enjoyment and edification. Plus, we've got Bulgaria Web Summit at the end of the week and plenty of meetup goodness worldwide. Sit back, relax, and enjoy the shows! Berlin Buzzwords Now in its 5th edition, Berlin Buzzwords brings together experts in all of our favorite buzzwords: cloud, search, storage, scale and beyond. The conference runs from May 25-26th in Berlin at Kulturbraurei, capping off with hackathons running in various locations throughout the city on Wednesday, the 27th. When you're not attending sessions, you can visit us anytime at the Elasticsearch stand in the Palais Building, where we can offer you knowledge, swag and delicious smoothies! All things Elasticsearch at Buzzwords: Monday, May 25th Tuesday, May 27th Wednesday, May 28th The fine folks at the Elasticsearch User Group Berlin have once again organized an Elasticsearch Hackathon for Buzzwords! We're nearly at capacity, so if you are interested in attending so we can try to find a place for you. Thanks again to for organizing! Bulgaria Web Summit The Bulgaria Web Summit is an all day event on Saturday, May 31st with a number of great talks on offer. If you're attending, make sure to stop by session at 16:45 on Explore your data. Honza promises us 60% code & console commands, no slides, all how Elasticsearch helps you make sense of your data. Meetups From Belgium to Washington, D.C, here's where to get your Elasticsearch Meetup on this week: Monday, May 25th Tuesday, May 26th Wednesday, May 27th And that's a wrap for this week. Remember, if you're giving a talk, hosting a meetup or otherwise sharing the Elasticsearch love, . We'd love to feature your knowledge sharing bits here! ","locales":"","title":"Where in the World is Elasticsearch: Berlin Buzzwords Edition"}
{"index":{}}
{"author":"Sejal Korenromp","category":"News","publish_date":"2014-05-23T00:00:00.000Z","url":"/blog/elasticsearch-teams-mit-sloan-data-analytics-hackathon","seo_title":"","content":" Following from the success and popularity of the we participated in late 2013, last week we sponsored the for our latest offering to Elasticsearch aficionados. More than 50 software engineers, business students and other open source software enthusiasts signed up to participate, and on a Saturday to boot! The full day's festivities included access to a huge storage and computing cluster, and everyone was set free to create something awesome using Elasticsearch. <p> <img class=\"alignnone\" alt=\"\" src=\"https://lh4.googleusercontent.com/yeI6ABiqGMWK6t-LfGLTVZEPd2AVGoRQ5VXV5kmYhhAA97CaU5CMHcQ2yRqi8QWrrzmAIw0CUnkuIAcVZt2ChuvC2Yoxw_RdiEcX4vshwADqcBL_aLnFzWIMN-3bpS1WQg\" width=\"624px: \" height=\"181px: \" style=\"margin: 10px: \" > </p> <p class=\"blog-img-caption\"> <a href=\"http://twitter.com/imotov\">Igor Motov</a> teaches the crowd about all things ELK Stack </p> With no time to lose, we kick started the session with and , two of our software engineers. They gave an overview of the , followed by an in-depth tutorial to get new users up to speed with the search engine's features and implementation. Even those never exposed to these technologies were wowed, especially by Kibana and how its simplicity and beauty provides a powerful data visualization solution. Binh Ly in action: Kibana, Simple & Beautiful The participants then split into 5 teams, and from that moment on it was heads down hacking time. Whilst the teams were diving into real world data sets – including Tweets, Wikipedia and datasets – our expert duo were at hand to answer questions, help troubleshoot, and advise and inspire along the way. No hackathon is complete without an element of competition, so with a prize incentive up for grabs, the teams were unstoppable. The five finalists came up with some incredibly cool hacks, including: Chris had the idea of taking ebooks from , extracting content and metadata, indexing it into Elasticsearch and providing a single search box that allows you to easily search through all the texts. Project Gutenburg contains 45K ebooks and a public domain API. He used Python to index the content into Elasticsearch and Node.js to build a search webpage. He was also thinking of eventually allowing multiple users to do collaborative annotations on each ebook. This team looked at sentiment analysis for a number of food chains: Burger King, McDonald's, Subway and Chipotle. Using Twitter mentions, they looked for these brand names and associated sentiment words. e.g. “like\" or “dislike\" terms such as “awesome,\" “great,\" “best,\" “me gusto,\" “fun,\" “worst,\" “bad,\" etc. Based on their counts and ratios, they determined that Burger King had the most positive sentiment, and Chipotle had the most negative sentiment. Invaluable data for the next snack attack you might have, really. <p> <img class=\"alignnone\" alt=\"\" src=\"https://lh6.googleusercontent.com/G4EGElp5xqq7yf0aD5gD2nLllS0GEGJqUu00WT1p6J-fpdaJSJghZHYXRmVOmpt9zGm8EYpRP9maFtED5vnZy-erxKuKsfX8NkrqDZeLUOtHshsE4fWlyxHWHdc68Y48og\" width=\"533px: \" height=\"249px: \" style=\"margin: 12px: \" > </p> <p class=\"blog-img-caption\"> Happy hacking! </p> Theja had the idea of taking data on papers submitted to the conference, extracting metadata and PDF content, indexing it into Elasticsearch and making it searchable. Currently, only very few pieces of metadata (e.g. title and author) are searchable online, so making the body/content of these papers searchable benefits research by making discovery of these papers much easier. He used Python to do all the processing and indexing, standardizing the data all into a common schema (e.g. paper ID, filename/url, keywords, authors, body, etc.). Theja then used Kibana to dig into his newly generated dataset, as well as perform some interesting aggregations. He came up with some interesting discoveries, such as what nationalities are the most common amongst paper submitters, the most common t","locales":"","title":"Elasticsearch Teams up with MIT Sloan for Data Analytics Hackathon"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-05-21T00:00:00.000Z","url":"/blog/2014-05-21-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project.Elasticsearch Core NOC Display with switching Tabs, powered by , and some jQuery to parse JSON Output — Andri Steiner (@andristeiner) Elasticsearch EcosystemHere's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases. . is a happy customer RT speaking about at . — Grant Gochnauer (@GrantGochnauer) ","locales":"","title":"This Week in Elasticsearch - May 21, 2014"}
{"index":{}}
{"author":"Zachary Tong","category":"Engineering","publish_date":"2014-05-20T00:00:00.000Z","url":"/blog/quick-tips-negative-connotation-filter","seo_title":"","content":" global wrapper ","locales":"","title":"Quick Tips: Negative Connotation Filter"}
{"index":{}}
{"author":"Leslie Hawthorn","category":"","publish_date":"2014-05-12T00:00:00.000Z","url":"/blog/world-elasticsearch-4","seo_title":"","content":" Welcome to the latest installment of Where in the world is Elasticsearch. We've got and speaking at conferences en France, plus plenty of awesome meetups featuring our core devs!DjangoCon EUIf you happen to find yourself on the beautiful Île des Embiez in France, you're probably already at . Good for you, my friend! Join Honza for his talk and enjoy hacking from the beach!dotScaleThe folks have organized another fantastic program in Paris. David Pilato and will help kick off the festivities on dotScale Workshop Day! Join David and Adrien this Saturday, May 17th at from 2-6 PM for a hands on . David and Adrien will be on hand for whole dotScale conference, so make sure to say hello in the hallways.MeetupsWhere to get your tasty ELK stack tidbits, from Algeria to the Philippines.Tuesday, May 13th Wednesday, May 14thJoin David Pilato, Adrien Grand and at the . Doors open at 6:30 PM in Paris. Costin will talk Elasticsearch and Apache Hadoop, and our friends at Nuxeo will share the story of how Elasticsearch made their lives better. Many thanks to the folks at Nuxeo for having us, and for speaking!Thursday, May 15thThe Barcelona on Rails Meetup will welcome to speak on ! Door open at 7:00 PM. Many thanks to the Barcelona Rails folks and XING for hosting us!More to Come...We've got a packed event calendar next week. Stay tuned to this space for more news next Monday! ","locales":"","title":"Where in the World is Elasticsearch? - May 12, 2014"}
{"index":{}}
{"author":"Kurt Hurtado","category":"Engineering","publish_date":"2014-05-07T00:00:00.000Z","url":"/blog/logstash-1-4-1-released","seo_title":"","content":" The Elasticsearch Logstash team is pleased to announce the release of version 1.4.1. This is a minor version containing mostly bugfixes and minor feature enhancements. This version of Logstash now contains an embedded version of Elasticsearch 1.1.1, as well as Kibana 3.0.1. We strive to constantly improve all aspects of our software, both glorious and mundane. To that end, we have updated our documentation, which, as always, is driven by our community of users - both via feedback and actual user documentation submissions. Also, the project's testing infrastructure received some love in this cycle, as our team has been busy improving the specs and tests as well as our continuous integration system. Lastly, the packaging has been greatly improved, ensuring users on all of our supported platforms have great experiences when deploying Logstash! Inputs, Outputs and Filters Packaging improvements You can read the full , or jump right to the . ","locales":"","title":"Logstash 1.4.1 has been released!"}
{"index":{}}
{"author":"Clinton Gormley","category":"Engineering","publish_date":"2014-05-22T00:00:00.000Z","url":"/blog/elasticsearch-1-2-0-released","seo_title":"","content":" Today, we are happy to announce the release of , based on Lucene 4.8.1, along with a bug fix release . You can download them and read the full change lists here: Elasticsearch 1.2.0 is a bumper release, containing over 300 new features, enhancements, and bug fixes. You can see the full changes list in the , but we will highlight some of the important ones below: Breaking changes While there are a few more breaking changes than just those listed here, most of those probably won’t affect you. The following, however, are very important: Java 7 required Elasticsearch now requires Java 7 and will no longer work with Java 6. We recommend using Oracle’s JDK 7u55 or JDK 7u25. Avoid any of the updates in-between as they contain a nasty bug which can cause index corruption. Dynamic scripting disabled by default Elasticsearch allows the use of scripts in several APIs: document updates, searches and aggregations. can be loaded from disk (static scripts) or specified directly within a request (dynamic scripts). Unfortunately MVEL, the current default scripting language, does not support sandboxing, meaning that a dynamic script can be used to do pretty much anything that the user can do. While it has been possible to disable dynamic scripting for a long time, we’ve decided to change the default to disable dynamic scripting out of the box. See . Watch this space for a blog post giving more details about the future of scripting in Elasticsearch. Field data and filter caches The JVM heap has to be shared by a number of competing resources such as field data, filter caching, index buffering, aggregations, etc. Field data in particular can be greedy and, in the past, has caused a number of users to experience OOM conditions. We added the to try to prevent these OOMs. Initially we set the default circuit breaker limit to 80% of the heap size, but that appears to have been too generous. We have now changed the default circuit breaker limit to of the JVM heap, and the filter cache to of the heap. Some Logstash users and other users of time-based indices might find that queries that worked correctly the day before have now suddenly stopped working. The reason for this failure is that the field data cache is full of old data which is no longer being used, so the circuit breaker is refusing to load more field data. This can be worked around either by or by setting the (which is unbounded by default) to a value like of the heap. We hope to have a better answer for this in the next release. Gateways removed The shared filesystem, S3 and HDFS gateways have been deprecated for a long time, and they have finally been removed. The functionality should be used instead. New features and enhancements The improvements in this release are heavily focussed on performance and resource usage, specifically during indexing and aggregating. Indexing and merging We tend to think of indexing and merging as separate functions, but really they are very closely related. The indexing process takes the docs in the indexing buffer and writes them to disk as a small segment. Having too many segments slows down indexing and searching, so the merge process merges smaller segments into bigger segments in the background. There is a balance between the size of new segments and the speed at which your changes become searchable. Very large merges can swamp the I/O on a node, slowing down other functions like search. To control this we have merge throttling, which slows down the merge speed to 20 MB/s by default. However, it is quite possible that the indexing rate is so high that merges just can’t keep up, leading to an explosion of segments. This hurts indexing and searching. To improve the interplay of all of these factors, we have: Of course, it is difficult to provide good defaults both for users with spinning disks and users with SSD. If you have spinning disks, you may consider dropping from its default value of to . If you are just indexing, wit","locales":"","title":"Elasticsearch 1.2.0 and 1.1.2 released"}
{"index":{}}
{"author":"Rashid Khan","category":"Engineering","publish_date":"2014-05-15T00:00:00.000Z","url":"/blog/kibana-3-1","seo_title":"","content":" Kibana 3.1 has been tagged and released! In addition to the usual smattering of bug fixes and small tweaks we have added a few fun features we’d love to share with you.Extended statisticsThe stats panel has been expanded to include all eight statistics Elasticsearch makes available. Of course all can be toggled, and sorted.A stats button in the tableIn addition to the terms button, we’ve added a stats button to create an adhoc stats panel. The button is type aware:  it only appears for numeric fields.Drag it to the dashboardIn addition to the new stats button in the table, both the terms and stats panels in the table can be dragged directly onto the dashboard by grabbing their move icon. This means no more going through the menus if all you need is a quick panel, just drag it from the table onto your existing dashboard. After you’ve dragged it to the dashboard, grab the right edge of the panel and drag to resize!Help us out!Have a great idea for a feature? Find a bug and want to fix it? Head over to the . Have some really great ideas? And the passion and skills to implement them? We’re working on the next generation of Kibana right now, ","locales":"","title":"Kibana 3.1 is out!"}
{"index":{}}
{"author":"Njal Karevoll","category":"","publish_date":"2014-05-13T00:00:00.000Z","url":"/blog/found-managing-elasticsearch-fields-when-searching","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud. Controlling the number of fields returned for search requests is an important aspect of maximizing Elasticsearch's performance. In this article we'll look at how we can selectively return only the fields we're interested in for each search hit in order to optimize our usage pattern. ","locales":"","title":"Managing Elasticsearch Fields When Searching"}
{"index":{}}
{"author":"Richard Pijnenburg","category":"Engineering","publish_date":"2014-05-06T00:00:00.000Z","url":"/blog/logstash-puppet-module-0-5-0-released","seo_title":"","content":" Today, we are happy to announce the release of our . You can download it and read the full changes list here: What's New Like we did in the , we've made a number of improvements, including a better test suite, and switching to for easier configuration. We have worked hard to incorporate the package changes that were made in logstash 1.4.x, and now you can install the contrib package directly from the repository or other remote source. We have also made some minor improvements like fixing inline documentation and, with the help of the community, found and squashed some minor bugs. We're always working to improve the quality and support of our Puppet offerings, and we hope you enjoy these latest enhancements. As always, you can give us your feedback or report problems on our page. ","locales":"","title":"Logstash Puppet module 0.5.0 released"}
{"index":{}}
{"author":"Leslie Hawthorn","category":"","publish_date":"2014-05-05T00:00:00.000Z","url":"/blog/world-elasticsearch-3","seo_title":"","content":" Welcome to the merry month of May! As we start this month's adventures, we've got a full slate of awesome talks coming up this week. If you're attending any of these conferences, we hope that you'll stop by to meet our team members!DevOps Days Austin Our very own Logstash core developer and creator of , , will be at . The conference takes place on May 5th and 6th, and you can meet up with Aaron during the festivities at the Elasticsearch table. Even cooler, Aaron is hoping to schedule some time during the conference's open space hours to talk logging, the ELK stack and other topics near and dear to the heart of ops humans (and developers!). Make sure to say stop by our table and say hello to Aaron!MonitoramaThe North American edition of , an open source monitoring conference and hackathon, kicks off at 9 AM today in Portland, Oregon, US. In addition to all of the great talks on the agenda, , creator of Kibana, will host a workshop on Wednesday, May 7th just after the opening welcome at 9:45 AM. Check out the workshop, and make sure to say hi to Rashid and in the hallway track! Espirit JUG Day TunisiaThe will hold their annual conference on May 7th and 8th in Ariana. , Elasticsearch Developer Advocate and DJ extraordinaire, will host two sessions: Make sure to attend David's talks. If everyone is excited about it, perhaps you can convince him to spin some tunes like he did recently at Devoxx France! 15th Fórum Internacional de Software Livre and Brasil MeetupsElasticsearch is very excited to send our first speaker from the company to Brazil! Leslie Hawthorn, our Community Manager, will return to FISL once again to speak on . She will speak on May 7th and runs from May 7-10th in Porto Alegre.Leslie's talk will be in English but most of the talks on the program are given in BR-PT. Leslie will also be on hand to answer questions you may have about Elasticsearch, Logstash and Kibana. You can visit with Leslie after her talk or in the Elasticsearch booth in the exhibits area.During her visit for FISL 15, Leslie will also speak at the first ever on May 5th.If you are attending FISL 15 or make your home in or near Porto Alegre, please join us for the first on May 8th.Elasticsearch and MIT Sloan Data Analytics HackathonOn May 10th in Cambridge, Masschusetts, US, Elasticsearch will along with the MIT Sloan Data Analytics Club. In addition to the chance to hack on Elasticsearch, network with other cool folks and eat delicious noms, we'll feature something a little different: data science for the non-computer scientist. We know that there are many people who need to make sense of their data, not just those with experience as technologists. If that sounds like you, we'll have from the Elasticsearch engineering team on hand to help you get started using Kibana, the data visualization tool that makes extracting insights from your data simple and beautiful., Elasticsearch core developer, will also be on hand to mentor attendees coming up to speed on Elasticsearch. And to make awesome things. :)The hackathon is open to anyone, not just students. Space is limited, so !But Wait, There's More ... Got Meetups?We've got a number of meetups happening this week, and even more awesome Elasticsearch content on offer from the wider community: Phew. That's one busy week! We'll keep bringing you news of where we'll be each week, so stay tuned to this blog or for the latest news. ","locales":"","title":"Where in the World is Elasticsearch? - May 05, 2014"}
{"index":{}}
{"author":"Luca Cavanna","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-04-30T00:00:00.000Z","url":"/blog/2014-04-30-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. Elasticsearch core Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases. Slides & Videos ","locales":"","title":"This Week in Elasticsearch - April 30, 2014"}
{"index":{}}
{"author":"Boaz Leskes","category":"","publish_date":"2014-05-12T00:00:00.000Z","url":"/blog/marvel-1-1-1-released","seo_title":"","content":" Today, we are happy to announce the release of . This is a bug fix release, fixing an issue with the correct interpretation of timeout settings by the Marvel agent. Although this bug is not critical, we felt it was important enough to justify a release. While this release is beneficial for all users, we especially recommend it if you are experiencing occasional timeout errors such as these: [2014-05-12 21:52:41,133][ERROR][marvel.agent.exporter ] [Robert da Costa] error connecting to [10.255.255.1:9200] java.net.SocketTimeoutException: connect timed out To upgrade, you must install the latest Marvel plugin on all your ES nodes. As with any other Java plugin, you will need to restart nodes (one by one) in order for the version to become active. You may follow a similar procedure to the rolling upgrade of Elasticsearch itself, described in more details . As always, we welcome all feedback so we can continue improving Elasticsearch Marvel. Please send questions, praise or pains to the . ","locales":"","title":"Marvel 1.1.1 released"}
{"index":{}}
{"author":"Livia Froelicher","category":"News","publish_date":"2014-05-06T00:00:00.000Z","url":"/blog/railsgirls-amsterdam-free-ruby-rails-workshop-girls","seo_title":"","content":" We’re very happy that Elasticsearch will be sponsoring RailsGirls Amsterdam! Rails Girls Amsterdam is a two days Ruby on Rails workshop, on June 6 and 7, for absolute beginners in the world of programming and web development. Rails Girls wants to empower girls to build capacity and acquire the tools they need to conquer the last online frontier. Our aim is to provide tools and a community where women come together to study technology and build their ideas. Come join us and learn sketching, prototyping, basic programming and get introduced to the world of technology! Rails Girls was born in Finland, but is nowadays a global, non-profit volunteer community. Thanks to Elasticsearch we can provide the girls some awesome refreshments! Are you or do you know a woman who wants to learn programming for free? Applications for Rails Girls Amsterdam are ! Don’t miss out! Apply at ! ","locales":"","title":"A Little Elasticsearch Hometown Love for Rails Girls Amsterdam"}
{"index":{}}
{"author":"Luca Cavanna","category":"Engineering","publish_date":"2014-04-29T00:00:00.000Z","url":"/blog/aliases-ftw","seo_title":"","content":" Index aliases are supported in elasticsearch for some time now. Using aliases you can easily abstract your physical indices away, so that only logical indices are exposed to the users and a lot of interesting things become possible. For instance you can , or even scale out without reindexing your data when your index has reached its full capacity. Aliases can be created, modified and deleted through proper aliases api:  elasticsearch makes it even easier to create them though. The idea is that aliases should be used in every situation, right from the beginning of every project, which is why we made it possible to specify them while creating an index, as well as when creating index templates. Aliases upon index creation As of , aliases can be provided as part of the create index api. Here is an example: curl -XPUT localhost:9200/logs-2014-04-14 -d ' { \"aliases\" : { \"april_2014\" : {}, \"year_2014\" : {} }, \"settings\" : { \"number_of_shards\" : 3, \"number_of_replicas\" : 0 } } ' The above request creates a new index called and associates the aliases and with it. Aliases support in index templates From it is also possible to specify aliases when registering . This comes in very handy especially when dealing with time based data, for instance daily indices. You can just create a couple of index templates as follows: curl -XPUT localhost:9200/_template/template_2014 -d ' { \"template\" : \"logs-2014-*\", \"aliases\" : { \"logs-2014\" : {} } } ' curl -XPUT localhost:9200/_template/template_2014_04 -d ' { \"template\" : \"logs-2014-04-*\", \"aliases\" : { \"logs-2014-04\" : {} } } ' As a result, whenever you create an index for year 2014, it will get automatically added to the alias. Also, whenever an index for april 2014 is created, it will get added to the existing alias that holds all indices for april 2014. The above examples contain simple aliases but let’s not forget that an alias can hold routing keys and a filter too. Also, because index templates allow you to associate an alias with an index whose name is not known yet, it is possible to use the placeholder as part of the alias name, which will get automatically replaced with the index the template is getting applied to at index creation time. curl -XPUT localhost:9200/_template/template_2014 -d ' { \"template\" : \"logs-2014-*\", \"aliases\" : { \"{index}-alias\" : {} } } ' Note that it is only possible to use the whole index name as part of the alias name. Although it would be nice to be able to use a portion of the index name and do much more complex things, we decided to keep it simple for now, but there’s already a to address this. A big thank you to for his contribution on these improvements! We look forward to your feedback on the or ! ","locales":"","title":"Aliases for the Win"}
{"index":{}}
{"author":"Alexander Reelsen","category":"Engineering","publish_date":"2014-04-29T00:00:00.000Z","url":"/blog/dry-keeping-your-queries-short","seo_title":"","content":" With the release of it is now possible to have query and search templates for all your requests. This blog post explains the how and why. Most of us know this principle from programming: Don’t repeat yourself. Make sure, you only write code once and leave out repetitive code that does not change. The same applies for search requests. There should be no need to repeat parts of a query that do not change a lot. However, even for executing a query via HTTP, you are required to repeat a similar JSON data structure over and over again. Template Query The first DRY helper for you is the . This allows you to specify a specific query where one part is the template, which simply is a mustache template, and one part parameters, which are then compiled together, before the query is executed as a normal query. Take a look at this simple example: GET /_search { \"query\": { \"template\": { \"query\": {\"match_{{template}}\": {}}, \"params\" : { \"template\" : \"all\" } } } } Now this does not save you a lot of code exactly, does it? Very true, let us make this example more usable. First, most queries are not a simple match_all query, but rather a longer query where usually a single search phrase (possibly consisting of several terms) is put into several fields like this: { \"query\": { \"template\": { \"query\": { \"bool\" : { \"must\" : [ { \"match\" : { \"name\": \"{{name}}\" } } ], \"should\" : [ { \"match\" : { \"firstname\": \"{{name}}\" } } ] } }, \"params\" : { \"name\" : \"alexander\" } } } } Again, this query did not become any shorter. So the next step is not to send the query with every request, but maybe have it already stored on the server side. This is exactly one of the features of the query template. You can put the query part into a file in directory, for example , as mustache is used for rendering. Then the query is suddenly short like this: GET /_search { \"query\": { \"template\": { \"query\": \"my-script\", \"params\" : { \"name\" : \"alexander\" } } } } You can add the script file to elasticsearch while running and it will pick it up automatically without the need for a restart. So, now we are saving some bytes per request. But can we do better? There is still some redundancy above like the part. Also, it would be nice to maybe template the full request, and have script aggregations or highlighting fields for example. Search template GET /_search/template { \"template\": { \"query\": { \"term\": { \"{{field}}\" : \"{{value}}\" } }, \"aggs\" : { \"{{field}}\" : { \"terms\" : { \"field\": \"{{field}}\"} } } }, \"params\": { \"field\" : \"name\", \"value\" : \"alexander\" } } As you can see, you can template the whole request. And again you can refer to an already stored script and shorten it dramatically! GET /_search/template { \"template\": \"my-request\", \"params\": { \"field\" : \"username\", \"value\" : \"alexander\" } } In addition, you can use more complex features of the of the mustache templating engine, see the . There are several reasons for this feature. Saving some bytes on the wire might be one:  only allowing to execute a couple of predefined search operations (like a functionality, or executing A/B tests by easily specifying different queries, or adding some ACL driven filters to all queries) might be another. This could also become a point of sharing queries between applications, perhaps even written in different languages. Up next… Storing the script in the directory of elasticsearch still implies you have to copy it manually to each node. Another idea might be to store it in an index or the cluster state. We will work on that as well. Also, feel free to drop us some feedback, if you think mustache is a good fit here or if you think it makes sense to support other template languages. As mustache is a so-called logic less template language, some people might consider it too limited, so we are eager to know about your use-case. In addition, we’re discussing if it is a good idea to allow for a pure parameter driven get request like this without a body: An advantage of this","locales":"","title":"DRY - Keeping Search Requests Short"}
{"index":{}}
{"author":"Leslie Hawthorn","category":"","publish_date":"2014-05-19T00:00:00.000Z","url":"/blog/world-elasticsearch-5","seo_title":"","content":" Welcome to our weekly news bite about where you can find all sorts of Elasticsearch & ELK stack goodness. This week, will get you ready for the wide world of Elasticsearch and Kibana, while will teach you how to love your logs with Logstash - the L in the Elasticsearch ELK stack - and Elasticsearch & Kibana.DatabeatGood morning, San Francisco! Headed to today and tomorrow? Make sure to say hello to our resident Elasticsearchians, and as you wend your way through The Grand Hyatt Union Square. And block your calendar now for 3:10 PM today, as you won't want to miss Graham Tackley, Director of News and Architecture at The Guardian, on User Behavior Analytics Frameworks to Drive Revenue. Solutions LinuxThe team putting on are at it again, bringing you the edition of the conference this week on May 20th and 21st in Paris. Join David Pilato on Wednesday, May 20th at 11:40 AM in Salle Monet for a one hour workshop, including a feature overview of Elasticsearch and an introduction to Kibana. You can also visit our team throughout the conference in Booth A18. Come on over and say \"Bonjour!\"GOTO ChicagoWith this on offer, we just had to go, too. Join us at on May 20th and 21st at the Drake Hotel. We'll have and other folks from Elasticsearch Inc there to answer all of your questions and to offer you some cool ELK toy goodness. Plus, we've got a meetup on in Chicago !You can visit Binh and the rest of the Elasticsearch team at Booth #2. GlueconGluecon 2014 kicks off on Tuesday, May 20th with a pre-conference camp, CampDevOps @ Gluecon. Even if you're not joining us for Gluecon on May 21st & 22nd, you can still attend CampDevOps. Just !For those who will be attending Gluecon in Broomfield, Colorado, plan to hear from the creator of Logstash, Jordan Sissel. Jordan will be covering Love Your Logs with Elasticsearch ELK at 11:05 AM in Breakout Room #2, directly after the conference opening keynotes on Wednesday.Jordan and other stalwart folks from the Elasticsearch team will be hanging out at the Elasticsearch table, T8, when not presenting. Visit us to get some cool Loggy and Elasticsearch stickers!BreizhCampDavid Pilato will conclude his Elasticsearch love powered tour of France this week in Rennes at . The conference runs from May 21-23rd, with David leading a hands on Elasticsearch and Kibana workshop at 10:30 AM on Thursday. Make sure to attend David's workshop and chat with him during the breaks about all things ELK stack! If that's not enough Elasticsearch goodness for you, Jérôme Mainaud will cover Five Ingredients to Spice up Your Elasticsearch experience on Thursday evening. Bonjour, Jérôme! Nous sommes impatients de vous rencontrer à BreizhCamp!Polyglot VancouverThe fine folks at the Polyglot Unconference in Vancouver, BC, Canada are hosting their annual event on May 23-25th. The conference kicks off with Ganesh Swami presenting a half day workshop on .MeetupsSince you are not Dr. Who and are therefore likely without a Tardis, you may not be able to join us for all these conferences. But we have lots of great meetups happening this week. Please join us in a city near you for more informal get togethers, including refreshments and tasty talk treats!San FranciscoOkay, so we took this one out of alphabetical order, but that's because we're welcoming a speaker from overseas. Please join us tomorrow, Tuesday, May 20th to hear from Graham Tackley, Director of Architecture at The Guardian News and Media. Graham will discuss how to build an analytics platform that allowed everyone in The Guardian's newsroom to deep dive into reader engagement - all in real-time. But wait, there's more! Kirsten Stewart from Desk.com will present on the challenges, complexities, and triumphs Desk has experienced giving their users a near real-time experience using various tools and configurations of Elasticsearch. We're at capacity now, but there's still . If you cannot join us in person, we'll be taping the talks, so stay tuned to th","locales":"","title":"Where in the World is Elasticsearch? - May 19, 2014"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-05-14T00:00:00.000Z","url":"/blog/2014-05-14-week-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. Elasticsearch Core Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases. Slides & Videos Where to Find Us We'd love to feature all the great Elasticsearch, Logstash and Kibana presentations and meetups happening worldwide in this section. If you're speaking or hosting a meetup, let our Community Manager, , know! Australia The wonderful Mark Walkom, Community Organizer of the Elasticsearch Sydney Meetup, will discuss How to on May 20th. Join Mark at Google Sydney for the May SAGE-AU Meetup at 6 PM. Austria The has scheduled their first meetup for Thursday, June 12th. Please join us at 7 PM to hear from on What's New in Elasticsearch. Bulgaria will discuss how to Explore Your Data using Elasticsearch at the . The conference takes place on May 31st in Sofia. Canada The fine folks at the Polyglot Unconference in Vancouver, BC are hosting their annual event on May 23-25th. The conference kicks off with Ganesh Swami presenting a on Getting Started with Elasticsearch. France Germany Norway will be speaking on from text to full-text search at the conference. The show runs from June 2-6th. Poland The Torun JUG will get together on May 28th at 6 PM to talk about , with a spotlight on Elasticsearch. Romania will speak at on Big data real time search and analytics. Topconf Bucharest runs from June 10-13th and Costin will speak at 3:20 PM on June 12th. Russia will speak at . The schedule is still being finalized, but mark your calendars for June 2nd and 3rd. If you're heading to PyCon Ru, make sure to say hello to Honza! Spain has been invited to speak at the Barcelona on Rails Meetup on May 15th. Join him for a presentation on Doors open at 7 PM, and thanks to the fine folks at XING for hosting us! Switzerland United Kingdom United States Where to Find You Our Community Manager, , is hard at work to help folks create more Elasticsearch meetup groups and to help meetup organizers find more speakers. If you are interested in either effort, . Oh yeah, we're also  . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticsearch, Kibana and Logstash than where you rest your head. Trainings If you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elasticsearch - May 14, 2014"}
{"index":{}}
{"author":"Costin Leau","category":"","publish_date":"2014-05-08T00:00:00.000Z","url":"/blog/es-hadoop-20-rc1","seo_title":"","content":" I am glad to report that 2.0 RC1 has been . As the label implies, this release brings the current development iteration close to fruition. Since the , exactly one month ago, several improvements have been made: Index time/date-based formatting If you are dealing with time-based data (such as logs), es-hadoop can determine and format the target index/type based on the data being processed, entry by entry. This works transparently across libraries (Map/Reduce, Cascading, Hive, Pig) or, if opted so, on the raw JSON: Support for update scripting The and has been extended to allow the use of scripts and to mirror the Elasticsearch update API. Furthermore, the script parameters can be extracted dynamically, at runtime, from the data being processed. As you would expect, this works consistently raw JSON and Hadoop libraries. Upgrade to the latest Apache Hive and Pig We are actively monitoring the releases in the Hadoop ecosystem. es-hadoop 2.0 RC1 is not just compatible with the latest Apache Pig (0.12.1) and Apache Hive (0.13.0), it also supports the newly introduced types (like ) while preserving backwards compatibility. Increase the version from 1.3 to 2.0 While reviewing the list of changes since the of Elasticsearch for Apache Hadoop started, we quickly realized the version needs to reflect the plethora of new features and functionality added. And thus 1.3 became 2.0. In addition to all these updates, es-hadoop has been extensively tested across various Hadoop distributions to ensure full compatibility:  whatever your environment, we want to make sure es-hadoop works flawlessly. Besides bug-fixes, the new release contains improved error and logging messages (especially when it comes to connectivity and network issues) to speed up the recovery process. As always, we appreciate feedback - please take 2.0 RC1 and ","locales":"","title":"Elasticsearch for Apache Hadoop 2.0 RC1 released"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-05-07T00:00:00.000Z","url":"/blog/2014-05-07-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. Elasticsearch core Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases. Slides & Videos Where to find Us We'd love to feature all the great Elasticsearch, Logstash and Kibana presentations and meetups happening worldwide in this section. If you're speaking or hosting a meetup, let our Community Manager, , know! Austria The has scheduled their first meetup for Thursday, June 12th. Please join us at 7 PM to hear from on What's New in Elasticsearch. Brasil Elasticsearch is very excited to send our first speaker from the company to Brasil! Leslie Hawthorn, our Community Manager, will be returning to FISL once again to speak on . Leslie will speak on May 7th and runs from May 7-10th in Porto Alegre. Her talk will be in English but most of the talks on the program are given in BR-PT. Leslie will also be on hand to answer questions you may have about Elasticsearch, Logstash and Kibana. Bulgaria will discuss how to Explore Your Data using Elasticsearch at the . The conference takes place on May 31st in Sofia. Denmark The inaugural will convene on Tuesday, May 13th at 7 PM. Please join us to hear from , and our friends at Falcon Social. France Germany Norway will be speaking on at the conference. The show runs from June 2-6th. Romania will speak at on Big data real time search and analytics. Topconf Bucharest runs from June 10-13th and Costin will speak at 3:20 PM on June 12th. Russia will speak at . The schedule is still being finalized, but mark your calendars for June 2nd and 3rd. If you're heading to PyCon Ru, make sure to say hello to Honza! Spain has been invited to speak at the Barcelona on Rails Meetup on May 15th. Join him for a presentation on Doors open at 7 PM, and thanks to the fine folks at XING for hosting us! Switzerland Tunisia will speak at the Esprit JUG Days in Ariana. The conference runs from May 7th and 8th. You may want to visit the for more details on the group or just . United Kingdom United States ","locales":"","title":"This week in elasticsearch - May 07, 2014"}
{"index":{}}
{"author":"Livia Froelicher","category":"News","publish_date":"2014-05-05T00:00:00.000Z","url":"/blog/elasticsearch-deutschland-experience-nosql-matters-cologne","seo_title":"","content":" Last week, we attended the 5th edition of the . Hosted at the Mediapark in Cologne, more than 200 attendees were treated to both a great venue and a ton of mind blowing talks about new products, use cases and field reports of day-to-day operation of NoSQL infrastructures.Our own – Software Developer on the Elasticsearch Marvel team – gave an in-depth presentation on Elasticsearch: a Deep Dive into Analytics Using Aggregations.The organizers gave out happy and sad faces at the exit to each talk so folks could rate material during each session they attended with a quick +1/-1.The “smiley likes\" were all over during Boaz's talk. Folks were very keen to know more about our products. Boaz gave several impromptu demos tailored to specific audience after his talk. For those of you who couldn't make it join us for the presentation, you may enjoy Boaz's .Boaz was also selected to deliver at the conference wrap up lightning talks. He gave a short 5 minutes intro to Marvel, complete with another cool demo. If you're interested in learning more, you may want to .A BIG thank you to and the entire NoSQL Matters Cologne team for all their hard work to bring all of us together and to give us the opportunity to learn from each other. It was a pleasure to be part of it and discover a bunch of happy users of Elasticsearch and meet more folks who want to check it out! ","locales":"","title":"The Elasticsearch Deutschland Experience at NoSQL Matters Cologne"}
{"index":{}}
{"author":"Daniel Palay","category":"","publish_date":"2014-04-29T00:00:00.000Z","url":"/blog/world-elasticsearch-2","seo_title":"","content":" In this week's search for Elasticsearchers, we turn our attention to the town of Cologne as our very own and come to town for . There will be plenty of Elasticsearch and Logstash stickers and even a few stuffed ELKs there for you to get your hands on. But fancy schwag aside, if you are in Cologne, you are really going to want to catch Boaz and his talk happening at 13:45 Tuesday. Can't make it to NoSQL Matters? Don't worry, we've got you covered. We recently held a community meet-up at our office in Amsterdam – where we highlighted the new features in and Boaz (yea, he's a busy speaking and coding machine) demoed our cluster-monitoring product . Enjoy and we'll catch you next time on Where in the World is Elasticsearch ","locales":"","title":"Where in the World is Elasticsearch? - April 29, 2014"}
{"index":{}}
{"author":"Shay Banon","category":"Engineering","publish_date":"2014-04-26T00:00:00.000Z","url":"/blog/resiliency-elasticsearch","seo_title":"","content":" This blog post tries to explain our thought processes around how we work in Elasticsearch and Apache Lucene to address resiliency. It ended up being much longer post than I intended, but I hope you will find it well worth the read. The first question is: why would we even care about resiliency in Elasticsearch? Is it not \"just\" a search system, one that simply mirrors a primary \"source of truth\" data source? Surely you can always reindex the data? Well, to a degree, this is a correct. Many systems out there have their \"single source of truth\" outside of Elasticsearch. If your SSoT is another datastore or flat files in S3 or HDFS, you can always reindex the data. But, the fact that you can reindex the data doesn't mean that we are happy with the fact that you *have to do this*. We have users of Elasticsearch storing petabytes of data - reindexing all of that data would take an unacceptably long time! For that reason, we strive to ensure that Elasticsearch doesn’t lose data, ever. A major step towards keeping data safe was the addition of the snapshot/restore API. While it was possible to do backups in Elasticsearch before, it was fiddly and inefficient. With the incremental backups available in snapshot/restore, you can now recover your data even when your cluster suffers massive failure, be it hardware or user induced (think DELETE /*). But restoring data still takes time. We also want to ensure that your live cluster is resilient and never loses data. How do we address the resiliency aspects of Elasticsearch? Our process is quite simple. We work through known issues based on how likely are they to occur, and how broad the scope of applying the fix. If we suspect there is a bug in Apache Lucene, we put all our resources on it to try to find and fix it. Quickly! Lucene is at the core of Elasticsearch. We feel deeply responsible for helping Lucene to move forward (see ), it’s a very high priority for us. Lucene is a widely used library and, outside of the context of Elasticsearch, us helping to fix anything related to resiliency in Lucene means that we have a karma multiplier that goes beyond just Elasticsearch. We do not take this responsibility lightly. A lot also happens somewhat behind the scenes. For example, a few months ago we were getting reports of users losing data in Elasticsearch. Among them was GitHub, one of our customers. We had a few engineers fully dedicated to figuring out why it happened. It took us a few months to nail down the problem. We started by examining each step within Elasticsearch itself – when shards are allocated, when shards are deleted – hardening each step to add resiliency. In the end, it turned out to be a bug in Lucene, which could result in a whole index being deleted! We were super excited that we had managed to track down this bug and commit the fix. (). We’re doing the same thing with the upcoming Lucene 4.8 release. So far there have been two bugs in Lucene 4.8 that we have managed to uncover and fix: (wipes index), (zero byte files). That’s not to say that there aren't bugs in Elasticsearch itself. A few months ago, we found that using multiple data paths in Elasticsearch could result in Elasticsearch wrongly reporting a corrupted index. Again, this bug was one that took us quite some time to find, but it was a high priority for us. How do we make sure our system is resilient? There are two aspects to it. First is our test infrastructure. One of the amazing things that happened in Apache Lucene that helped to make it super resilient was the use of randomized testing (see for background). Think of randomized testing as a test infrastructure that is \"predictably irrational.\" Tests usually stagnate towards stability, and randomized testing allows you to make your test infrastructure a living organism that keeps on trying to be, effectively, \"predictably irrational\". Another part of the test infrastructure is what Simon has us all fondly referring to as “evil tests.” ","locales":"","title":"Resiliency and Elasticsearch"}
{"index":{}}
{"author":"Andrew Cholakian","category":"","publish_date":"2014-04-26T00:00:00.000Z","url":"/blog/found-function-scoring","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud. We'll cover the basics of scoring using functions while taking a look at some use cases where functional scoring techniques are highly useful and effective. ","locales":"","title":"A Gentle Intro to Function Scoring"}
{"index":{}}
{"author":"Alexander Reelsen","category":"Engineering","publish_date":"2014-04-24T00:00:00.000Z","url":"/blog/enriching-searches-open-geo-data","seo_title":"","content":" Have you ever found some neat public datasets where you thought it might make sense to include it in your application to optimize a specific feature, even a minor feature? Sure you have! This blog post shows you how to use Logstash to get an external dataset in the desired format, using Kibana to check if it is applicable, and then making sure the data gets indexed correctly into Elasticsearch so it can do the heavy lifting in production. Getting the data I will assume from now on that you have a current Elasticsearch and Logstash installation available. I will use and in these examples. is a German website, which includes German geo information in SQL and CSV formats. As we want to store data in Elasticsearch, SQL is not an option, neither is CSV out of the box, however we can use Logstash to preprocess and index data in Elasticsearch. The file we are going to parse is a list of German cities, including all of their zip codes. The dataset is available and these files are licensed as . Data format Taking a look at the data reveals it consists of a number of columns: Interesting fields to play around with might be , , , , and . More about that soon… Indexing the data into Elasticsearch Using the Logstash CSV filter The next step is to get the data into Elasticsearch. The first step is to create a Logstash configuration, so I’ll copy this configuration into a file named . Note that we are using a filter called “csv”, even though the field separator is a tab, and not a comma. input { stdin {} } filter { # Step 1, possible dropping if [message] =~ /^#/ { drop {} } # Step 2, splitting csv { # careful... there is a \"tab\" embedded in the next line: # if you cannot copy paste it, press ctrl+V and then the tab key to create the control sequence # or maybe just tab, depending on your editor separator => ' ' quote_char => '|' # arbitrary, default one is included in the data and does not work columns => [ 'id', 'ags', 'name_uc', 'name', 'lat', 'lon', 'official_description', 'zip', 'phone_area_code', 'population', 'area', 'plate', 'type', 'level', 'of', 'invalid' ] } # Step 3, possible dropping if [level] != '6' { drop {} } # Step 4, zip code splitting if [zip] =~ /,/ { mutate { split => [ \"zip\", \",\" ] } } # Step 5, lat/lon love if [lat] and [lon] { # move into own location object for additional geo_point type in ES # copy field, then merge to create array for bettermap mutate { rename => [ \"lat\", \"[location][lat]\", \"lon\", \"[location][lon]\" ] add_field => { \"lonlat\" => [ \"%{[location][lon]}\", \"%{[location][lat]}\" ] } } } # Step 6, explicit conversion mutate { convert => [ \"population\", \"integer\" ] convert => [ \"area\", \"integer\" ] convert => [ \"[location][lat]\", \"float\" ] convert => [ \"[location][lon]\", \"float\" ] convert => [ \"[lonlat]\", \"float\" ] } } output { elasticsearch { host => 'localhost' index => 'opengeodb' index_type => \"locality\" flush_size => 1000 protocol => 'http' } } Before going on, this is how you would call Logstash in order to index the data. You should fire up Elasticsearch first. cat DE.tab | logstash-1.4.0/bin/logstash -f opengeodb.conf So, quite a lot of stuff going on here (might take a minute, depending highly on your hardware). The first thing you might notice: the Logstash configuration does not ingest events using a input. The reason for this is, that the file input behaves like a under UNIX systems and waits for new data to be appended to the file. The data file we have however has a defined end, so it is more appropriate to read of all its data using the input. The section consists of five steps. So, lets go through each and every step and explain what it does Step 1 – Exclude comments The first step drops comments, which are defined as lines starting with a hash. This is needed because the supplied tab-separated file’s first line is such a comment, which contains the names of the columns, and there is no need to index that. Step 2 – CSV extraction The second ste","locales":"","title":"Enriching Your Searches With Open Geo Data"}
{"index":{}}
{"author":"Luca Cavanna","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-04-23T00:00:00.000Z","url":"/blog/2014-04-23-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. Elasticsearch core Photo courtesy of Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases. Photo courtesy of Slides & Videos You may also be interested in our about all the talks and other awesome from Elasticsearch folks last week at DevNation, co-located with Red Hat Summit. Where to find Us We'd love to feature all the great Elasticsearch, Logstash and Kibana presentations and meetups happening worldwide in this section. If you're speaking or hosting a meetup, let our Community Manager, , know! Austria The just formed! Join the group now to get updates on their first meeting. Brasil Elasticsearch is very excited to send our first speaker from the company to Brasil! Leslie Hawthorn, our Community Manager, will be returning to FISL once again to speak on . Leslie will speak on May 7th and runs from May 7-10th in Porto Alegre. Her talk will be in English but most of the talks on the program are given in BR-PT. Leslie will also be on hand to answer questions you may have about Elasticsearch, Logstash and Kibana. Bulgaria will discuss how to Explore Your Data using Elasticsearch at the . The conference takes place on May 31st in Sofia. France Germany Spain has been invited to speak at the Barcelona on Rails Meetup on May 15th. Join him for a presentation on Doors open at 7 PM, and thanks to the fine folks at XING for hosting us! Tunisia will speak at the Esprit JUG Days in Ariana. Further details of the conference schedule are forthcoming, but mark your calendar for May 7th and 8th. In the meantime, you may want to visit the . United Kingdom United States Where to Find You Our Community Manager, , is hard at work to help folks create more Elasticsearch meetup groups and to help meetup organizers find more speakers. If you are interested in either effort, . Oh yeah, we're also  . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticearch, Kibana and Logstash than where you rest your head. Trainings If you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elasticsearch - April 23, 2014"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2014-04-17T00:00:00.000Z","url":"/blog/investing-apache-lucene","seo_title":"","content":" As you know, Apache Lucene is at the heart of Elasticsearch, and when we formed the company around Elasticsearch, we made a commitment to make sure we drive Apache Lucene forward. With , , , and , we already have a very strong group of Lucene committers that help to harden Lucene and keep adding enhancements to it. Other Elasticsearch employees invest a lot of time in Lucene as well, for example, with spending a considerable amount of time in Lucene a few months ago. In keeping with our commitments to improving core Lucene, I am happy to announce that and have joined Elasticsearch. We hope you already notice the extra work that has gone into improving Apache Lucene in the past few weeks since they joined us. It is humbling to have such a wonderful group of Apache Lucene committers at Elasticsearch, and to have the opportunity to help innovation in the Lucene world happen at an even faster pace. Welcome, Mike and Robert! ","locales":"","title":"Investing in Apache Lucene"}
{"index":{}}
{"author":"Clinton Gormley","category":"Engineering","publish_date":"2014-04-23T00:00:00.000Z","url":"/blog/multi-field-search-just-got-better","seo_title":"","content":" The is the go-to query for matching on a single field. It understands the field mapping and uses the appropriate analyzer for the field, it can match any word or require all words (with set to or ), or it can match a minimum number or percentage of words with . It can do fuzzy matching and phrase or proximity matching. In short, it is very flexible and very powerful. Multi-field search, on the other hand, is hard. Elasticsearch provides the , which makes multi-field search look simple: But in reality, it is not as simple as it looks. Unless you understand how the query works, you will often use it incorrectly and get suboptimal results. Elasticsearch v1.1.0 added some new features to the query which make multi-field search much more powerful and easier to use. Types of multi-field search How you search across multiple fields depends on how your data is indexed and the type of search that you need. There are three main scenarios: Best matching field When searching across multiple fields for a single “concept”, you want to look for as many words as possible within the same field. For instance, “brown fox” in a single field is more meaningful than “brown” in one field and “fox” in the other. In other words, you’re looking for the single field. This type of query can be executed by running a query against each field, and choosing the relevance from the best matching field, using the : The query accepts a parameter which tells it how to execute the query. The default type is , which results in exactly the same query as we have above: This query, as written above, will choose the best matching field, but will ignore other lesser matches. We can still take these secondary matches into account by specifying the parameter: The above query will still use the from the best matching field, but will also add in the from any other matching fields, multiplied by . Most matching fields Often we index the same text with several different analyzers, perhaps as stemmed and unstemmed, with synonyms, with shingles for proximity matching, with edge-ngrams for autocomplete etc. In this case, we want to query all of the fields and add up the from each match to find the documents with the . We could write such a query by wrapping individual clauses with a : This is the same query that would be executed by the query when the parameter is set to : You can give extra “weight” to one or more fields by specifying a on that field, using the caret () syntax: In the above query, the field is twice as important as the other fields. Cross field matching Finally, we often need to search for entities whose data is spread across multiple fields, such as when we search for \"John Smith\" in the and fields of a object. In this case, we want to find as many individual words as possible in . The approach may appear to be the answer here, but there are several reasons why it will not give good results. Both and are queries — they match each field separately. This means that: One solution to this problem is just to index the data from and into the single field , which we can do automatically with this mapping: Then we can just query the field with a simple query. That said, it is often useful to be able to achieve the same thing across multiple fields. Elasticsearch v1.1.0 added the new execution type which allows you to do just that: The approach first analyzes the query string into individual terms, then it looks for each term in any field, much like this: The and parameters would work as you expect, as each word is queried (and so can be counted) separately. But this still leaves the problem of term frequencies. In the above query, would still score higher than . In fact, the approach doesn’t use queries. Instead it uses a special query which combines the term frequency of with the term frequency of and uses that value for both fields. In other words, it treats and as if they were . It has certain advantages over","locales":"","title":"Multi-Field Search Just Got Better"}
{"index":{}}
{"author":"Michael McCandless","category":"Engineering","publish_date":"2014-04-18T00:00:00.000Z","url":"/blog/java-1-7u55-safe-use-elasticsearch-lucene","seo_title":"","content":" This post was co-authored with .Apache Lucene's  are so stressful that they also happily serve as strong test cases for the JVM itself.Over time, Lucene's tests, running many times per day on multiple Jenkins installations with random JVM options, different operating systems and garbage collectors, have uncovered a number of exciting JVM bugs. There's a .For example, the very first Java 1.7 release had a , which was fixed with the first update (1.7.0u1). Fortunately, Oracle's QA team has been working more closely with Lucene developers since then, helping to iterate on new issues, notifying us on new early-access Java releases, etc.One recent bug, , would cause SEGV or, worse, silent corruption in the index term vectors. Because of this bug, we've had to recommend Lucene users stick with Java 1.7u25, now almost a year old.At Oracle, Vladimir Kozlov has worked very hard and , as of . It's so hard to imagine how he could track down this bug, when debugging tricky Lucene test failures is hard enough, so we asked him how does that, and he explained that he steps through megabytes of assembly instructions, correlates them back to the particular Java sources, thinks about which parts of the Hotspot compiler may have resulted in each part of the assembly.  This all sounds exceptionally challenging, and we are happy Vladimir is able to do it so well!Finally, that fix has now been back-ported to Java's 1.7.x branch, and released as of . The Lucene developers have upgraded Lucene's continuous Jenkins builds to 1.7u55 and no tests are failing, so we are comfortable and can now recommend that 1.7u55 is safe for Elasticsearch and all other Lucene based applications. ","locales":"","title":"Java 1.7u55 is Safe for Use With Elasticsearch"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-04-17T00:00:00.000Z","url":"/blog/2014-04-17-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. Elasticsearch core Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases. Slides & Videos Where to find Us We'd love to feature all the great Elasticsearch, Logstash and Kibana presentations and meetups happening worldwide in this section. If you're speaking or hosting a meetup, let our Community Manager,, know! Austria The just formed! Join the group now to get updates on their first meeting. Brasil Elasticsearch is very excited to send our first speaker from the company to Brasil! Leslie Hawthorn, our Community Manager, will be returning to FISL once again to speak on . Leslie will speak on May 7th and runs from May 7-10th in Porto Alegre. Her talk will be in English but most of the talks on the program are given in BR-PT. Leslie will also be on hand to answer questions you may have about Elasticsearch, Logstash and Kibana. Canada will be attending and the accompanying Django sprints. Make sure to stop by and hear more from him during his poster session . France Germany Japan The has been scheduled by Jun Ohtani. Please plan to join the meeting at 7 PM on April 21st. Spain has been invited to speak at the Barcelona on Rails Meetup on May 15th. Join him for a presentation on Doors open at 7 PM, and thanks to the fine folks at XING for hosting us! Tunisia will speak at the Esprit JUG Days in Ariana. Further details of the conference schedule are forthcoming, but mark your calendar for May 7th and 8th. In the meantime, you may want to visit the . United Kingdom United States Where to Find You Our Community Manager, , is hard at work to help folks create more Elasticsearch meetup groups and to help meetup organizers find more speakers. If you are interested in either effort, . Oh yeah, we're also . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticearch, Kibana and Logstash than where you rest your head. Trainings If you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elasticsearch - April 17, 2014"}
{"index":{}}
{"author":"Zachary Tong","category":"Engineering","publish_date":"2014-04-22T00:00:00.000Z","url":"/blog/averages-can-dangerous-use-percentile","seo_title":"","content":" With the release of Elasticsearch 1.1.0, there is a new metric aggregation available to users: the . Percentiles tell you the value at which a certain percentage of your data is included. So a 95th percentile tells you the value which is greater than or equal to 95% of your data. Ok…but why is that useful? Imagine you are the administrator for a large website. One of your goals is to guarantee fast response times to all website visitors, no matter where in the world they live. How do you analyze your data to guarantee that the latency is small? Most people reach for basic statistics like mean, median or max. Each have their place, but for populations of data they often hide the truth. Mean and median tend to hide outliers, since the majority of your data is “normal”. In contrast, the max is a hypercondriac and easily distorted by a single outlier. Let’s look at a graph. If you rely on simple metrics like mean or median, you might see a graph that looks like this: That doesn’t look so bad, does it? Average and median response time is around 50ms, and creeps up to 100ms for a little while. A different truth is apparent when you include the 99th percentile: Wow! That certainly doesn’t look good at all! At 9:30am, the mean is telling you . In contrast, the 99th percentile says , which is a very different picture. One percent of all your customers are experiencing 800+ ms latencies, which could be very bad for business. Using the percentile The new percentile metric works just like the simpler stats metrics like and . It is a metric that can be applied to any aggregation bucket. The percentile metric will then calculate a set of percentiles based on the documents that fall within the bucket. Let’s look at a simple example: curl -XGET localhost:9200/website/logs/_search -d ' { \"aggs\" : { \"load_time_outlier\" : { \"percentiles\" : { \"field\" : \"load_time\" } } } }' By default, the metric will calculate a set of default percentiles () and return you the value for each one: { ... \"aggregations\": { \"load_time_outlier\": { \"1.0\": 15, \"5.0\": 20, \"25.0\": 33, \"50.0\": 38, \"75.0\": 45, \"95.0\": 60, \"99.0\": 867 } } } Often, only the extreme percentiles are important to you, such as the 95th and 99.9th percentile. In this case, you can specify just the percentile you are interested in: curl -XGET localhost:9200/website/logs/_search -d ' { \"aggs\" : { \"load_time_outlier\" : { \"percentiles\" : { \"field\" : \"load_time\" , \"percents\" : [ 95, 99.9 ] } } } }' Being a metric, we can nest it inside of buckets to get more sophisticated analysis. Going back to our original goal — detecting high latency based on geographical location — we can build an aggregation that buckets users by their country and then computes a percentile on curl -XGET localhost:9200/website/logs/_search -d ' { \"aggs\" : { \"countries\" : { \"terms\" : { \"field\" : \"country_code\" }, \"aggs\" : { \"load_time_outlier\" : { \"percentiles\" : { \"field\" : \"load_time\" , \"percents\" : [ 95 ] } } } } } }' And now we can see that Antarctica has a particularly slow 95th percentile (for some strange reason): { ... \"aggregations\": { \"country\" : { \"buckets\": [ { \"key\" : \"AY\", \"doc_count\" : 20391, \"load_time_outlier\": { \"95.0\": 1205 } }, ... percentiles are (usually) approximate All good things come at a price, and with percentiles it usually boils down to approximations. Fundamentally, percentiles are very expensive to calculate. If you want to calculate the 95th percentile, you need to sort all your values from least to greatest, then find the value at This works fine for small data that fits in memory, but simply fails when you have terrabytes of data spread over a cluster of servers (which is common for Elasticsearch users). The exact method just won't work for Elasticsearch. Instead, we use an algorithm called (). Without getting bogged down in technical details, it is sufficient to make the following claims about T-Digest: The following chart shows the relative","locales":"","title":"Averages Can Be Misleading: Try a Percentile"}
{"index":{}}
{"author":"Gaurav Gupta","category":"","publish_date":"2014-04-16T00:00:00.000Z","url":"/blog/joined-elasticsearch","seo_title":"","content":" Hello Elasticsearchers! I’m Gaurav Gupta, and I recently joined Elasticsearch as VP of Product Management. A little bit about me and why I’m so excited to be part of the team! My love for all things data and search started in the late 90’s when I first tried out Google. For the first time, the data on the web truly became useful to me. When I joined Splunk in 2008, I found a kind of corollary to Google. Splunk leveraged search, but explored an entirely different type of data. Machine data was a kind of metadata of the world – the logs, metrics and spew that come out of servers, applications, network devices, and sensors that run our world. It was possibly thousands of times larger than the entirety of human-generated data (the stuff that Google mostly searched) and its growth seemed limitless. In the world of Big Data, “human” and “machine” generated data have largely remained distinct in terms of both tools and technical approaches. Elasticsearch brings these two together for the first time. Its engine can tackle both document search and machine data analytics with an elegant simplicity and speed. The sheer number of use cases is a product guy’s dream (or nightmare, depending on how you see it!) The Elasticsearch community really “gets” it. Hop onto twitter, read the blog posts, hang out in the IRC channel, or better yet talk to some customers of Elasticsearch. People LOVE our products, and they’re excited to contribute back to make it even better. I can’t wait to be a part of it. Elasticsearch is building one of the few truly geographically distributed startups I know of. As a result, we are able to hire the best and brightest technical and business minds regardless of where they live. I live in San Francisco, but I regularly communicate with people living in Amsterdam, Germany, the UK, and all over the US. We all get together 1-2x a year and with modern technology coupled with the right culture, it works. My recent experiences with other open source “Big Data” infrastructure companies (ie Hadoop, NoSQL etc) left me puzzled. Getting those solutions up and running, let alone getting something useful, took days and often weeks. Elasticsearch and the ELK stack is a dream in comparison. Beyond the surface, lies great power. Everything is designed with flexibility in mind: clean APIs, JSON everywhere, and a product built with scalability and reliability as a first class citizen. It can just as easily live on-premise as it can in the cloud. And being open source means that we are by nature open and transparent, quality conscious, and focused on serving our community. I look forward to meeting many of you! Feel free to reach out anytime via email at gaurav.gupta (at) elasticsearch.com or on twitter at . ","locales":"","title":"Why I joined elasticsearch - April 16, 2014"}
{"index":{}}
{"author":"Daniel Palay","category":"News","publish_date":"2014-04-19T00:00:00.000Z","url":"/blog/week-red-hat","seo_title":"","content":" For those of you who couldn’t join us in San Francisco this week, we had quite the busy week. – creator of Logstash – was part of a star-studded in San Francisco this past Tuesday night. Jordan – pictured left (via ) was joined by – author of , – Developer and Platform Evangelist for Red Hat Enterprise Linux, and – Principal Open Shift Architect at Red Hat. The quartet led a vibrant discussion on the best practices for system and application monitoring. Our own was on hand for the BoF and captured the essence of the discussion. – \"Everyone seems to have all these tools. That they hate.\" Problem is not fully solved. — Leslie Hawthorn (@lhawthorn) Jordan also joined the Elasticsearch team at our Red Hat Summit booth. Partnering up with fellow Logstash engineer , Jordan led 3 standing room only demos on the ELK Stack. If you couldn’t make any of Jordan & Kurt’s demos this week or just wanted more information, take a moment to watch this video from our recent and listen to how Bloomberg is utilizing the entire stack.   As you can see by the pictures from the and the Bloomberg video, simple and beautiful data visualization is a key part of the ELK Stack. The creator of the K part of ELK – – just . If you have a few minutes this weekend and simple and beautiful data visualizations interest you, we recommend taking a moment to watch it. ","locales":"","title":"The Week that Was at Red Hat Summit & DevNation"}
{"index":{}}
{"author":"Clinton Gormley","category":"Engineering","publish_date":"2014-04-16T00:00:00.000Z","url":"/blog/elasticsearch-1-1-1-1-0-3-released","seo_title":"","content":" Today we are happy to announce the release of , based on Lucene 4.7.2 and , based on Lucene 4.6.1. You can download them and read the full changes list here: Both of these releases contain important bug fixes, but we recommend upgrading to version 1.1.1 unless you have a particular reason for staying with the 1.0 branch. ","locales":"","title":"Elasticsearch 1.1.1 and 1.0.3 Released"}
{"index":{}}
{"author":"Mark Harwood","category":"Engineering","publish_date":"2014-04-15T00:00:00.000Z","url":"/blog/significant-terms-aggregation","seo_title":"","content":" Strap on your goggles... In the movie , the alien has a sophisticated thermal imaging system that allows him to single out his human prey by observing the heat differences between their bodies and the environment in which they are hiding. The new behaves like the Predator's vision, identifying interesting things that stand out from the background (not by observing heat differentials but by observing term frequency differentials). Terms of interest in a result set stand out clearly like the heat signal of a monosyllabic Austrian bodybuilder sweating behind a fern. Revealing the uncommonly common The trick behind the significant terms aggregation is in spotting terms that are significantly more common in a result set than they are in the general background of data from which they are drawn. These are what you might call uncommonly common terms and examples of the real insights these can give include: In the following sections we present worked examples of just some of the useful applications of this new feature: Use case: Geographic anomalies This XKCD cartoon neatly summarises the issue with the typical forms of mapping analysis: The significant terms aggregation can help overcome this problem. Let's first take all of the UK crime data for last year and break the reports down into geographic areas using the and with a simple like this: curl -XGET \"http://localhost:9200/ukcrimes/_search\" -d' { \"query\": { \"aggregations\" : { \"map\" : { \"geohash_grid\" : { \"field\":\"location\", \"precision\":5, }, \"aggregations\":{ \"most_popular_crime_type\":{\"terms\":{ \"field\" : \"crime_type\", \"size\" : 1}} } } } }' We end up with an XKCD-style map effectively showing us a population distribution and the less-than useful insight that anti-social behaviour is the most popular crime type : However, if we use the aggregation we can get a more interesting insight into the data and reveal the unusual occurrences of crime in each location: curl -XGET \"http://localhost:9200/ukcrimes/_search\" -d' { \"query\": { \"aggregations\" : { \"map\" : { \"geohash_grid\" : { \"field\":\"location\", \"precision\":5, }, \"aggregations\":{ \"weirdCrimes\":{\"significant_terms\":{\"field\" : \"crime_type\", \"size\":1}} } } } }' If we show only the top scoring areas, we move away from focusing purely on the most populated areas and the most common crime and begin to find the anomalies in our data: Here, we see a relatively remote area with a disproportionately large number of Possession of Weapon crimes. If we zoom in, we can see from the sky why this is the case - this is the location of Stansted airport where passengers are routinely searched as they transit through the airport. Other spots around the country have their own curiosities - the fields where drug-related crimes peak as part of annual music festivals, the year-round bicycle thefts from university towns like Cambridge, and the prisons where it would seem a crime conducted against a fellow criminal is not really a crime so is registered with the type Other. Use case: Root cause analysis The maintains a database of car fault reports and, like many systems for fault reports, there is a product ID and a free-text description with each report. Using the aggregation you can identify the common reasons for product failures by examining the free-text descriptions of each product. Example querycurl -XGET \"http://localhost:9200/nhtsa/_search\" -d' { \"aggregations\" : { \"car_model\":{ \"terms\":{\"field\" : \"car_model\", \"size\" : 20}, \"aggregations\":{ \"reasons_for_failure\" : { \"significant_terms\":{\"field\" : \"fault_description\", \"size\" : 20} } } } } }' Example results\"aggregations\": { \"car_model\": { \"buckets\": [ { \"key\": \"Taurus\", \"doc_count\": 3967, \"reasons_for_failure\": { \"doc_count\": 3967, \"buckets\": [ { \"key\": \"coil\", \"doc_count\": 250, \"score\": 0.544, \"bg_count\": 1115 }, { \"key\": \"mounts\", \"doc_count\": 178, \"score\": 0.3969, \"bg_count\": 777 }, { \"key\": \"spring\", \"doc_count\": 261, \"score\": 0.3668, \"bg_count\": 1706 }, ... To make these keywords a m","locales":"","title":"Significant Terms Aggregation"}
{"index":{}}
{"author":"Martijn Laarman","category":"Engineering","publish_date":"2014-04-10T00:00:00.000Z","url":"/blog/introducing-elasticsearch-net-nest-1-0-0-beta1","seo_title":"","content":" It’s been 4 months since was released and a lot has happened since then. Elasticsearch released 1.0 and even version 1.1. With this release announcement, I’m pleased to say NEST has finally caught up and brings (almost) all the awesome that Elasticsearch 1.0 and 1.1 bring to the .NET world. Even more exciting – as of today, Elasticsearch.Net and NEST are both officially supported .Net clients for Elasticsearch! (For those just learning about Elasticsearch.Net and NEST, Elasticsearch.Net is the low level client and NEST is the high level client for users of .Net and Elasticsearch.) Before diving in all the technical details of the NEST 1.0 beta release, I would first like to express a sincere thank you to all of you who’ve reached out on Twitter, GitHub issues and via email. Your kind words made all the difference these past 4 months as I busily switched jobs, refactored the client and implemented 1.0/1.1 features. I am happy to say that I have formally joined Elasticsearch Inc and will now be fully dedicated to making Elasticsearch in the .NET world awesome! So, what does ‘beta’ mean? All tests are passing, but we are still waiting for at least one new feature to land. Additionally, beta means that some new features may change before general release. Most importantly, this beta period is intended to solicit as much feedback as possible from everyone on breaking changes/oversights/bugs. Breaking Changes With this 1.0 release, we will have many breaking changes. The NEST releases have been around since 2010 and the internals were showing their age. This 1.0 release represents an almost complete refactoring of NEST. A separate section that lists the breaking changes . If you find any breaking changes that are not documented, we’d love to hear about them in . Not one but two clients NEST 0.12.0 introduced a client interface called which allowed you to build your own requests and responses without having to worry about building endpoints. Much of the work for this 1.0 beta release has been around refactoring this functionality out of the NEST assembly and making NEST’s strongly typed use internally. Elasticsearch.Net Elasticsearch.Net’s client is now called and is almost completely generated , exposing all the Elasticsearch 1.1 endpoints. It brings a client that’s well aligned in spirit and architecture with the other official Elasticsearch client libraries. Elasticsearch.Net leaves all the request and response mapping up to you although it comes with support for mapping responses into a dynamic container (a slightly modified DynamicDictionary from ) out of the box. Elasticsearch.Net is very much intended to be used by other high level clients. Another big new feature is built-in support. This allows the client to retry failed connections on different nodes and sniff the cluster for live nodes when a node fails or at certain configurable intervals. Elasticsearch is elastic, and the client should be, too. Elasticsearch 1.0.0-beta is a prerelease package so make sure you adjust your filter inside the package manager: or pass the flag to nuget from the console: Please read to find out more. NEST NEST’s client classes have completely been rewritten and internally use and still expose the Elasticsearch.Net client. It benefits from it’s cluster failover support and it no longer has complicated internal path builders. NEST is highly opinionated on how you should build and consume Elasticsearch responses, and comes with a strongly typed Query DSL. NEST has really high coverage of mapped endpoints, and the unmapped API’s are all identified and documented on the . Changelog This release consists of 300+ commits over a 4 month timespan so I will just list the highlights: Aggregations All of Elasticsearch’s (1.0 and 1.1!) aggregations have been mapped in NEST 1.0. Snapshot and Restore You can now script your create/delete/get repositories/snapshotting/restoring ","locales":"","title":"Introducing Elasticsearch.Net and NEST 1.0.0-beta1"}
{"index":{}}
{"author":"Alex Brasetvik","category":"","publish_date":"2014-04-08T00:00:00.000Z","url":"/blog/found-sizing-elasticsearch","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud. Scaling up and outWe're often asked \"How big a cluster do I need?\", and it's usually hard to be more specific than \"Well, it depends!\". There are so many variables, where knowledge about your application's specific workload and your performance expectations are just as important as the number of documents and their average size. In this article we won't offer a specific answer or a formula, instead we will equip you with a set of questions you'll want to ask yourself, and some tips on finding their answers. ","locales":"","title":"Sizing Elasticsearch"}
{"index":{}}
{"author":"Adrien Grand","category":"","publish_date":"2014-04-03T00:00:00.000Z","url":"/blog/count-elasticsearch","seo_title":"","content":" One feature that has been requested landed in Elasticsearch and is available in the 1.1.0 release: the ability to count unique values for a particular field. There are lots of interesting things that this feature allows to compute: Example This feature is exposed under the form of an aggregation called , so that you can benefit from all the goodness of aggregations, especially composability. So taking back the unique number of visitors on your website as an example, you could put this aggregation under a aggregation in order to see the trend over months: curl -XGET \"http://localhost:9200/_search\" -d' { \"aggs\": { \"monthly\": { \"date_histogram\": { \"field\": \"timestamp\", \"interval\": \"month\" }, \"aggs\": { \"visitor_count\": { \"cardinality\": { \"field\": \"ip_address\" } } } } } }' Under the hood As long as a dataset is small and can fit on a single machine, computing the number of unique values is simple and is just a matter of adding elements into a and returning its size. However, memory usage of this solution is linear with the number of unique values, which makes it impractical to evaluate high cardinalities. Another issue is that if you want to compute the cardinality of a dataset that is stored on several machines, summing up the cardinalities returned on each machine is not good enough since there might be overlap. So you actually need to stream these sets to a single location where they can be merged in order to know the actual cardinality. Fortunately, there are other algorithms that address these challenges, in particular and . I won't explain how they work given that there are already some that do it. However, here are a few important things to know about these algorithms: Because of the different characteristics of these algorithms, there is something interesting that can be done: using on small cardinalities and on larger ones. This way, we would have the best of both worlds: excellent precision on small cardinalities and fixed memory usage whatever the cardinality to estimate is. But this also means that a needs to be slightly modified in such a way that it can be upgraded to an counter. This is exactly what the algorithm, that we use for the aggregation, does. Precision and memory usage As we just saw, precision degrades at some point in order to keep memory usage bounded. Elasticsearch makes this threshold configurable through the parameter. For example, if you configure a of , you could expect precision to be excellent if the return value is < 1000 and a bit more approximate otherwise. The memory usage of this aggregation also depends on this parameter: for a value of , you should expect a memory usage of about bytes per shard per aggregation bucket. For example, we built the following chart that computes the relative error for various sets of random values, depending on the precision threshold and the actual cardinality: For all 3 thresholds, counts have been accurate up to the configured threshold (although not guaranteed, this is likely to be the case). Please also note that even with a threshold as low as , the relative error remained way under under 5%, even when counting millions of items. Hopefully this post gives you insights about what the aggregation does and how it works. We look forward to your feedback on the or ! ","locales":"","title":"Count on Elasticsearch!"}
{"index":{}}
{"author":"Clinton Gormley","category":"Releases","publish_date":"2014-03-25T00:00:00.000Z","url":"/blog/elasticsearch-1-1-0-released","seo_title":"","content":" #index #post_content p { margin: 0.5em }<br /> Today we are happy to announce the release of , based on Lucene 4.7, along with bug fix releases and :You can download them and read the full changes list here: ","locales":"","title":"Elasticsearch 1.1.0, 1.0.2 and 0.90.13 released"}
{"index":{}}
{"author":"Daniel Palay","category":"","publish_date":"2014-04-14T00:00:00.000Z","url":"/blog/world-elasticsearch","seo_title":"","content":" Welcome to our new series, where we’ll give you a sneak peak of what conferences we’ll be attending and other places to find us. We know the series doesn’t come with its , but it will come along with great insights on our future presentations. For our regular readers, we hope these posts give you another reason to visit us at an event near you. So, without further ado, where in the world is Elasticsearch going to be this week? Not only will we have a booth (#1113) on the exhibit hall floor - right next to the crowd favorite developer lounge - but we’ll also have several Elasticsearch talks through out the conference. Here’s who and what you can hear: Kevin Kluge, Vice President of Engineering When: Thursday, April 17th @ 9:45 am Where: Room 220 Jordan Sissel, Creator-Logstash When: Tuesday, April 15th @ 8:00 pm Where: Room 212 Now that you know where we’ll be, come see what we have in store. But if you can't join us, follow us on for updates! ","locales":"","title":"Where in the World is Elasticsearch? - April 14, 2014"}
{"index":{}}
{"author":"","category":"","publish_date":"2014-04-10T00:00:00.000Z","url":"/blog/elasticsearch-mapr-hadoop-best-worlds","seo_title":"","content":" It’s been a big week for Elasticsearch and Hadoop! On Tuesday, we announced the release of Elasticsearch for Apache Hadoop 1.3 M3, which includes a . And just last week, we in the Hadoop ecosystem, with MapR Technologies. We’re beyond thrilled to partner with MapR and to help their customers - and ours - add real-time search and analytics capabilities to their MapR Hadoop Distribution clusters. With the combination of MapR and Elasticsearch, developers gain a scalable, distributed architecture to quickly perform search and discovery across tremendous amounts of information. The combined solution is already in use at leading enterprise companies including Solutionary, the leading pure-play managed security service provider, and several Fortune 100 financial services institutions. For example, we worked closely with MapR Technologies to help a large financial institute store all of their raw access logs - billions of documents - in Hadoop. The documents were indexed into Elasticsearch using the Elasticsearch for Apache Hadoop integration, then visualized using Kibana. This approach allowed the customer to have near real time visibility into their data through Kibana, yet also run batch oriented jobs over all their raw data when needed. Moreover, by using the Elasticsearch for Apache Hadoop, our data search and analytics capabilities were also available while executing the aforementioned Map/Reduce, Hive, or Pig jobs. The distributed nature of the Map/Reduce model fits really well on top of Elasticsearch because we correlate the number of Map/Reduce tasks with the number of Elasticsearch shards for a particular query. So every time a query is run, the system dynamically generates a number of Hadoop splits proportional to the number of shards available so that the jobs are run in parallel – your Hadoop cluster scales easily alongside Elasticsearch and vice-versa. Best of all, Elasticsearch for Apache Hadoop provides a single jar that enables real time search and analytics across different Hadoop, Cascading, Hive and Pig versions and across multiple Hadoop distributions, whether it is vanilla Apache Hadoop, CDH, HDP, MapR or Pivotal. No dependencies, all the functionality. We look forward to hearing what you’re building with Elasticsearch and MapR. Most of all, we want to hear how using this software together makes your life better, and how we can improve. any time! ","locales":"","title":"Elasticsearch & MapR Hadoop: The Best of Both Worlds"}
{"index":{}}
{"author":"Richard Pijnenburg","category":"Engineering","publish_date":"2014-04-07T00:00:00.000Z","url":"/blog/elasticsearch-puppet-module-0-3-0-released","seo_title":"","content":" Today we are happy to announce the release of our Elasticsearch Puppet modules version 0.3.0. You can download them and read the full changes list here: What's New We've improved support for different Linux distributions and added OpenSuse to our list of supported distros. Debian 6 support has also been improved. To improve the support for the defaults files we've implemented . Improved testing For the past few weeks, we've worked hard on improving the quality of these modules. With the implementation of System Tests, we now have several layers of testing: Puppet lint allows us to validate the code against the recommended Puppet style guidelines from the . This makes sharing code easier as everyone holds to the same guidelines as much as possible. The syntax validation step validates the code against the without having to run the actual code. Rspec tests are the first step in actual validation of the module. This step runs the code and validates different test scenarios and is the first step in catching errors. System testing is the final step in the testing process. These tests run the actual code on real systems to validate it all actually works. Conclusion We will continue to improve the quality and support of our Puppet modules. You can report any problems on our page. ","locales":"","title":"Elasticsearch Puppet Modules 0.3.0 Released"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-04-02T00:00:00.000Z","url":"/blog/2014-04-02-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project.Elasticsearch Core Elasticsearch EcosystemHere's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases. Slides & Videos ","locales":"","title":"This Week in Elasticsearch - April 02, 2014"}
{"index":{}}
{"author":"Clinton Gormley","category":"News","publish_date":"2014-03-20T00:00:00.000Z","url":"/blog/elasticsearch-definitive-guide","seo_title":"","content":" For a long time, new users have struggled to get started with our reference docs, and for a long time we have been promising “something better”. Today, we are proud to announce the early release edition of , which will be published by O’Reilly Media. It can be read online at and it will be available to purchase in printed form or as an eBook from O’Reilly at . This book is suitable for novices and experienced users alike. We expect you to have some programming background and, although not required, it would help to have used SQL and have some database experience. We explain concepts from first principles, helping novices to gain a sure footing in the complex world of search. The reader with a search background will also benefit from this book. Elasticsearch is a new technology which has some familiar concepts. The more experienced user will gain an understanding of how those concepts have been implemented and how they interact in the context of Elasticsearch. It was very important to us to ensure that this book would be freely available to anybody who needs it. A lot of time and effort was invested in ensuring that it could be freely read online, while still making it available in eBook and printed format. We would like to thank O’Reilly for their support, cooperation and flexibility. They are one of the few publishers who really understand open source and have already made a number of their works available online for free. It is an honour to join their stable of authors. The book is a long way from being finished — there is a lot to write about! However, we felt that there was enough content to be useful to many users already. Release soon release often! We will be uploading new chapters as they are finished. If you spot any errors in the book, or have suggestions for improving a section, we would love to hear from you. Please open an issue or a pull request on the GitHub repo: . We will need you to sign our standard to ensure that your changes can be incorporated in the printed version of the book. Happy reading! ","locales":"","title":"Elasticsearch - The Definitive Guide"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-03-19T00:00:00.000Z","url":"/blog/2014-03-19-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project.Elasticsearch Core Elasticsearch EcosystemHere's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases. Slides & Videos Where to Find UsWe'd love to feature all the great Elasticsearch, Logstash and Kibana presentations and meetups happening worldwide in this section. If you're speaking or hosting a meetup, let our Community Manager,, know!Canada will be attending and the accompanying Django sprints. Make sure to stop by and hear more from him during his poster session .France and will both be at Devoxx France 2014, where they will co-present the . Devoxx France runs April 16-18th in Paris.Germany Italy JapanThe has been scheduled by Jun Ohtani. Please plan to join the meeting at 7 PM on April 21st.The NetherlandsThe group will convene on Thursday, April 3rd at 6:30 PM at Elasticsearch's EU HQ in Amsterdam. Attendees will be treated to a deep dive on new features in Elasticsearch 1.0 by and a demo of Elasticsearch Marvel by . New ZealandThe will talk all things Elasticsearch on Tuesday, March 25th. Doors open at 6 PM.NorwayThe has been scheduled for April 3rd at 6 PM. Details on the presentations are forthcoming, but you can register to attend now.South Africa will take the stage at ScaleConf to talk . The conference runs April 10th and 11th in Cape Town.United Kingdom will share What's New in Elasticsearch 1.0 with the on March 21st. Doors open at 6:15 PM.United States Where to Find YouOur Community Manager, , is hard at work to help folks create more Elasticsearch meetup groups and to help meetup organizers find more speakers. If you are interested in either effort, .Oh yeah, we're also . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticearch, Kibana and Logstash than where you rest your head.TrainingsIf you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elasticsearch - March 19, 2014"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-04-09T00:00:00.000Z","url":"/blog/2014-04-09-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. Elasticsearch Core Photo courtesy of . Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases. Photo courtesy of . Slides & Videos   Where to Find Us We'd love to feature all the great Elasticsearch, Logstash and Kibana presentations and meetups happening worldwide in this section. If you're speaking or hosting a meetup, let our Community Manager, , know! Brasil Elasticsearch is very excited to send our first speaker from the company to Brasil! Leslie Hawthorn, our Community Manager, will be returning to FISL once again to speak on . Leslie will speak on May 7th and runs from May 7-10th in Porto Alegre. Her talk will be in English but most of the talks on the program are given in BR-PT. Leslie will also be on hand to answer questions you may have about Elasticsearch, Logstash and Kibana. Canada France Germany Italy David Pilato will present Make sense of your (BIG) data! at the The conference takes place in Rome on April 11-12th, and David will speak at 2:10 PM on the 12th. Japan The has been scheduled by Jun Ohtani. Please plan to join the meeting at 7 PM on April 21st. Romania will be speaking at the , covering an Introduction to Elasticsearch. Doors open at 6 PM. Spain has been invited to speak at the Barcelona on Rails Meetup on May 15th. Join him for a presentation on Doors open at 7 PM, and thanks to the fine folks at XING for hosting us! South Africa will take the stage at ScaleConf to talk . The conference runs April 10th and 11th in Cape Town. Tunisia will speak at the Esprit JUG Days in Ariana. Further details of the conference schedule are forthcoming, but mark your calendar for May 7th and 8th. In the meantime, you may want to visit the . United Kingdom United States Where to Find You Our Community Manager, , is hard at work to help folks create more Elasticsearch meetup groups and to help meetup organizers find more speakers. If you are interested in either effort, . Oh yeah, we're also  . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticearch, Kibana and Logstash than where you rest your head. Trainings If you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elasticsearch - April 09, 2014"}
{"index":{}}
{"author":"Costin Leau","category":"","publish_date":"2014-04-08T00:00:00.000Z","url":"/blog/elasticsearch-apache-hadoop-1-3-m3","seo_title":"","content":" I am happy to announce that 1.3 M3 has been . Elasticsearch for Apache Hadoop provides a single jar that enables real time search and analytics across different Hadoop, Cascading, Hive and Pig versions and across multiple Hadoop distributions, whether it is vanilla Apache Hadoop, CDH, HDP, MapR or Pivotal. No dependencies, all the functionality. Besides a healthy dose of bug fixes, the last (planned) milestone in 1.3 adds a series of improvements and features, such as: Multi-index Writes When indexing data, it is common to `split` it into different buckets based on its content. For example, log data is typically indexed per date (per-day, week or month), which makes it easy to both handle and manage the data and its life-cycle. es-hadoop 1.3 M3 brings this functionality to the table, allowing data in Hadoop to be indexed in real-time and based on its content, regardless of whether you are using Map/Reduce, Cascading, Hive or Pig. If we want to index our media based on its type, we can simply define the following target resource: At runtime, the field is being extracted, the actual index/type resolved and the data properly dispatched. The actual value extraction happens transparently whether you are using a Cascading tuple or a Hive table and also, no matter if you passing in native types or raw JSON. See of the user guide for more information. Support for automatic time-based formatting is for 1.3-RC1, so one could use logstash-like patterns: ( ) Metrics Getting insight into how your jobs are performing is crucial for maximizing performance, tracking behavior and diagnosing issues. That is why in M3, we have added that cover extensively the entire I/O spectrum of es-hadoop. Simply upgrade and you will get the stats automatically logged into the console for each Map/Reduce job: All the stats are exposed through the Hadoop infrastructure and are available through the standard APIs. There are extra steps or configurations that need to be setup - everything is already included. Mapping Typo Suggestions Typos happen to everyone (to some more often than to others), and it can be quite annoying discovering that your query is incorrect because there is no or in your data. es-hadoop tries to help out: This validation can be if you wish to prevent your potentially expensive job from running with typos. Proxy (HTTP and SOCKS) support If your network has access restrictions, with M3 you can use both HTTP and SOCKS proxies to transparently route connections from Hadoop to Elasticsearch (and back :)). Both open and authorized proxies are supported. Plus, the proxying is scoped, so es-hadoop connections do not interfere with the rest of the JVM. Same binary for both Hadoop 1.x and 2.x es-hadoop supports both Hadoop 1.x and 2.x environments. Since Hadoop 2.x is not binary compatible with Hadoop 1.x (for package) this resulted in two separate jars that had to be used, one for each environment. Needless to say, it was easy to mix the two, until now. In M3, we introduce a single jar that works across both Hadoop environments! Modular jars Speaking of binaries, in M3 we also introduced one jar per module. es-hadoop as a whole takes about ~300 KB which gives you integration with Map/Reduce, Hive, Pig and Cascading. Yet we understand that some folks might want to use Elasticsearch just with Cascalog or only do real-time search with Hive. For this reason, one can now get a jar, for each integration, with a dedicated Maven POM, slimmer in size and that can run stand-alone, no other jars required. We have also expanded the configuration option for each integration, allowing a single job to read and write data to different Elasticsearch indices, extended Cascading integration with per- configuration and (run ANSI SQL in Hadoop against Elasticsearch) and improved exception reporting and handling. Feedback welcome We are quite excited about the upcoming 1.3 release and we hope you are too! So go ahead, for a spin and what you think","locales":"","title":"Elasticsearch for Apache Hadoop 1.3 M3 is out"}
{"index":{}}
{"author":"Konrad Beiske","category":"","publish_date":"2014-03-26T00:00:00.000Z","url":"/blog/found-elasticsearch-snapshot-and-restore","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud. Behind the Scenes Let's take a closer look at Elasticsearch's snapshot and restore module and the files used to store snapshots, exemplified with snapshots on S3. ","locales":"","title":"Snapshot and Restore"}
{"index":{}}
{"author":"Jordan Sissel","category":"","publish_date":"2014-03-14T00:00:00.000Z","url":"/blog/logstash-1-4-0-rc1-released","seo_title":"","content":" We are pleased to announce that the first release candidate for Logstash 1.4.0 is available! There are no new 'big' features since 1.4.0 Beta 2, only a few bug fixes! You can read the , or jump right ! We'll be running some longer tests on this over the next few days, and with your help, will be releasing 1.4.0 GA shortly! Happy Logstashing! ","locales":"","title":"Logstash 1.4.0 RC1 released"}
{"index":{}}
{"author":"Morten Ingebrigtsen","category":"","publish_date":"2014-03-13T00:00:00.000Z","url":"/blog/found-using-kibana-for-twitter-intelligence","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud. In this article we will set up a simple and easy to follow example on how to use Elasticsearch and Kibana for basic business intelligence. In our demo we will be using real world Twitter data which we'll feed into Elasticsearch and then inspect and analyze it by using the Kibana dashboard. ","locales":"","title":"Using Kibana for Business Intelligence"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-03-12T00:00:00.000Z","url":"/blog/2014-03-12-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. Elasticsearch core Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around Elasticsearch, including plugin and driver releases, and news about Logstash and Kibana. Slides & Videos Where to find Us We'd love to feature all the great Elasticsearch, Logstash and Kibana presentations and meetups happening worldwide in this section. If you're speaking or hosting a meetup, let our Community Manager, , know! Australia and will head Down Under to teach training courses in Melbourne and Sydney. On March 13th you can join us for the , or come by the on March 17th. Austria will speak at the tonight, March 12th. Doors open at 7 PM. Canada France Germany Italy will present Make sense of your (BIG) data! at the The conference takes place in Torino on April 3rd, and David will speak at 10:30 AM. Japan The has been scheduled by Jun Ohtani. Please plan to join the meeting at 7 PM on April 21st. New Zealand The will talk all things Elasticsearch on Tuesday, March 25th. Doors open at 6 PM. Norway The has been scheduled for April 3rd at 6 PM. Details on the presentations are forthcoming, but you can register to attend now. South Africa will take the stage at ScaleConf to talk . The conference runs April 10th and 11th in Cape Town. United Kingdom will share What's New in Elasticsearch 1.0 with the on March 21st. Doors open at 6:15 PM. United States Where to Find You Our Community Manager, , is hard at work to help folks create more Elasticsearch meetup groups and to help meetup organizers find more speakers. If you are interested in either effort, . Oh yeah, we're also  . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticearch, Kibana and Logstash than where you rest your head. Trainings If you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elasticsearch - March 12, 2014"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-03-05T00:00:00.000Z","url":"/blog/2014-03-05-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. Elasticsearch core Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around Elasticsearch, including plugin and driver releases, and news about Logstash and Kibana. Slides & Videos Where to find Us We'd love to feature all the great Elasticsearch, Logstash and Kibana presentations and meetups happening worldwide in this section. If you're speaking or hosting a meetup, let our Community Manager,, know! Australia and will head Down Under to teach training courses in Melbourne and Sydney. On March 13th you can join us for the , or come by the on March 17th. Austria will speak at the on March 12th. Canada Czech Republic will be speaking at the on March 5th. Doors open at 7 PM. France will be speaking on the ELK stack - that's Elasticsearch, Logstash and Kibana - and Elasticsearch Marvel on March 13th in Paris. The talks are free of charge but . Germany Italy will present Make sense of your (BIG) data! at the The conference takes place in Torino on April 3rd, and David will speak at 10:30 AM. Japan The has been scheduled by Jun Ohtani. Please plan to join the meeting at 7 PM on April 21st. The Netherlands and will be attending the this Friday in Rotterdam. Say hello to them in the hallways! South Africa will take the stage at ScaleConf to talk . The conference runs April 10th and 11th in Cape Town. United Kingdom United States Where to Find You Our Community Manager, , is hard at work to help folks create more Elasticsearch meetup groups and to help meetup organizers find more speakers. If you are interested in either effort, . Oh yeah, we're also . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticearch, Kibana and Logstash than where you rest your head. Trainings If you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elasticsearch - March 05, 2014"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-02-26T00:00:00.000Z","url":"/blog/2014-02-26-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. Elasticsearch core Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around Elasticsearch, including plugin and driver releases, and news about Logstash and Kibana. Slides & Videos Where to find Us We'd love to feature all the great Elasticsearch, Logstash and Kibana presentations and meetups happening worldwide in this section. If you're speaking or hosting a meetup, let our Community Manager,, know! Australia and will head Down Under to teach training courses in Melbourne and Sydney. On the 13th of March you can join us for the meetup, or come by the on March 17th. Austria will speak at the on March 12th. Canada Czech Republic will be speaking at the on March 5th. Doors open at 7 PM. Germany Italy will present Make sense of your (BIG) data! at the The conference takes place in Torino on April 3rd, and David will speak at 10:30 AM. Japan The has been scheduled by Jun Ohtani. Please plan to join the meeting at 7 PM on April 21st. Slovakia will speak at the on February 27th. The conference takes place in Bratislava. South Africa will take the stage at ScaleConf to talk . The conference runs April 10th and 11th in Cape Town. United Kingdom United States Where to Find You Our Community Manager, , is hard at work to help folks create more Elasticsearch meetup groups and to help meetup organizers find more speakers. If you are interested in either effort, . Oh yeah, we're also . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticearch, Kibana and Logstash than where you rest your head. Trainings If you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elasticsearch - February 26, 2014"}
{"index":{}}
{"author":"Clinton Gormley","category":"","publish_date":"2014-03-03T00:00:00.000Z","url":"/blog/renaming-perl-client","seo_title":"","content":" ","locales":"","title":"Renaming the Perl client"}
{"index":{}}
{"author":"Kevin Kluge","category":"","publish_date":"2014-02-25T00:00:00.000Z","url":"/blog/elasticsearch-0-90-12-1-0-1-released","seo_title":"","content":" Today we are happy to announce the release of and  elasticsearch 1.0.1 Thanks to everyone for the positive feedback on 1.0.0, including the bug reports!  We have responded to the quick feedback with the 1.0.1 release, which is now the current stable release in the 1.0 series.  Details of the changes are available in the . Some of the notable changes are: We recommend that users of Elasticsearch from all versions upgrade to 1.0.1 and not 1.0.0.  This path will avoid the issue where reading Lucene memory usage could cause excessive CPU usage. You can . elasticsearch 0.90.12 This is the current stable release in the 0.90 series and contains a number of  bug fixes and small enhancements that you can read about in the . Please note that we are decreasing the rate of development of the 0.90.x series of releases so that we can focus our attention on 1.0.x and future releases.  Some of the notable changes are: We recommend that users of Elasticsearch from the 0.20.x series or earlier upgrade to 0.90.12 and not 0.90.11, or 1.0.1.  This path will avoid the issue where getting Lucene memory usage could cause excessive CPU usage. You can .   ","locales":"","title":"Elasticsearch 1.0.1 and 0.90.12 Released"}
{"index":{}}
{"author":"Chris Earle","category":"Engineering","publish_date":"2014-02-18T00:00:00.000Z","url":"/blog/restricting-users-kibana-filtered-aliases","seo_title":"","content":" location ~ ^/((,?)${remote_user}-d+.d+.d+)+/_search$ { proxy_pass http://127.0.0.1:9200:  proxy_read_timeout 90:  } One question we often get with Kibana is, “How do you restrict the data for different users?” Our go to answer has always been to proxy the requests through Nginx and use filtered aliases to segment the data. The typical response to this is, “Uh… Okay I will look into it.” This blog post will take that advice one step further and give you a working example of exactly what’s needed to accomplish this task. For our example, we are going to use web server logs that segment the users based on the host name. The incoming log will look something like this: { \"@timestamp\": \"2014-02-04T11:46:16.164Z\", \"ip\": \"106.115.144.245\", \"extension\": \"css\", \"response\": \"200\", \"country\": \"IN\", \"tags\": [ \"warning\", \"info\"], \"referrer\": \"http://twitter.com/success/pyotr-kolodin\", \"agent\": \"Mozilla/5.0 (X11:  Linux x86_64:  rv:6.0a1) Gecko/20110421 Firefox/6.0a1\", \"clientip\": \"106.115.144.245\", \"bytes\": 6091.388051980175, \"request\": \"/terry-hart.css\", \"host\": \"astronauts.com\", \"responseTime\": 303, \"message\": \"106.115.144.245 - - [2014-02-04T11:46:16.164Z] \\\"GET /terry-hart.css HTTP/1.1\\\" 200 6091.388051980175 \\\"-\\\" \\\"Mozilla/5.0 (X11:  Linux x86_64:  rv:6.0a1) Gecko/20110421 Firefox/6.0a1\\\"\" } Assuming the log data is coming in via Logstash, we can setup the following translate filter to add a user field based on the host: filter { translate { field => \"host\" destination => \"user\" dictionary => [ \"astronauts.com\", \"buzz\", \"nasa.org\", \"gus\", \"space.com\", \"shakey\", \"rocketmen.org\", \"hotdog\" ] } } With the user field added to the data, we can now setup our first filtered alias for a user using the Sense interface in : POST _aliases { \"actions\": [ { \"add\": { \"index\": \"logstash-2014.02.03\", \"alias\": \"buzz-2014.02.03\", \"filter\": { \"term\": { \"user\": \"buzz\" } } } } ] } Any request that goes to will now include a term filter on the field for . The one gotcha for this system is that an alias will need to be setup for every user for each daily Logstash index. Elasticsearch currently does not have a feature for setting up dynamic aliases upon index creation, but the good news is that . For now, we will need to use a nightly cron to setup our user aliases. require 'elasticsearch' require 'hashie' # Connect to the ElasticSearch cluster client = Elasticsearch::Client.new # Get all the users and map them to an array resp = Hashie::Mash.new client.search index: \"logstash-*\", body: { size: 0, facets: { users: { terms: { field: 'user' } } } } users = resp.facets.users.terms.to_a.map { |f| f.term } # Get a list of all the indexes and aliases aliases = Hashie::Mash.new client.indices.get_aliases aliases.each_pair do |index,aliases| # Match the all the Logstash indexes and get the Logstash # date stamp from the index name. matches = /logstash-(d{4}.d{2}.d{2})/.match index if matches # Loop through each user and check to see if the index exists # if it doesn't then create the new alias and add a term filter. users.each do |user| aliasName = \"#{user}-#{matches[1]}\" unless aliases.aliases[aliasName] puts \"Creating alias #{aliasName} for #{index}\" client.indices.put_alias index: index, name: aliasName, body: { filter: { term: { user: user } } } end end end end The next piece of the puzzle is setting up Nginx to serve the Kibana interface with basic auth and to proxy the requests to the user’s aliases. There is a in the Kibana Github repo that we will use as a starting point. We need to add basic auth to the top of configuration along with modifying some of the rewrite rules to use the filtered aliases and user specific indexes. You can view the . The trickiest part to setup is translating the requests to the user’s aliases. Kibana will often send requests like , which will need to be translated to . Nginx doesn’t have a simple find and replace feature, so we need to dust off our hacker skills and","locales":"","title":"Restricting Users for Kibana with Filtered Aliases"}
{"index":{}}
{"author":"Clinton Gormley","category":"","publish_date":"2014-02-12T00:00:00.000Z","url":"/blog/1-0-0-released","seo_title":"","content":" Today we are proud to announce the release of GA, based on Lucene 4.6.1. This release is the culmination of 9 months of work with almost 8,000 commits by 183 contributors! A big thank you to everybody who has made this release possible. You can . The main features available in 1.0 are: Migrating to 1.0.0 We took advantage of a major version bump to remove cruft and to fix a number of inconsistencies. Our goal is that the user interface should be intuitive — you shouldn’t even need to consult the docs for common requests because the API should be obvious. The good news is that a number of APIs are much simpler. Unfortunately, this means that there were a few backwards incompatible changes. We have put together a guide to help you migrate to v1.0.0: . Please read this carefully, backup your data and test your application thoroughly before upgrading. You will need to do a when upgrading, but we hope to make these a thing of the past in the 1.x branch — a number of features have been added to facilitate rolling upgrades going forward. The list all of the changes since v1.0.0.RC2, but you should also check the release notes for: If you are using the official Elasticsearch clients, please see the appropriate docs for instructions about how to use them with v1.0.0: Please and ! ","locales":"","title":"1.0.0 Released"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-03-26T00:00:00.000Z","url":"/blog/2014-03-2-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project.Elasticsearch Core Elasticsearch EcosystemHere's some more information about what is happening in the ecosystem we are maintaining around the ELK stack - that's Elasticsearch plus Logstash and Kibana - including plugin and driver releases. Slides & Videos ","locales":"","title":"This Week in Elasticsearch - March 26, 2014"}
{"index":{}}
{"author":"Jordan Sissel","category":"News","publish_date":"2014-03-20T00:00:00.000Z","url":"/blog/logstash-1-4-0-ga-unleashed","seo_title":"","content":" Today, we are releasing Logstash 1.4.0 GA!  This release is the culmination of several months of adding features to and maturing Logstash.To recap, here are some of the key features and changes from 1.3 to 1.4: Also we wanted to give a big THANK YOU to the community for all their help with tests and the patches that went into this release! The Logstash user base does so much to move the project forward, and Logstash is so much better for it.  This release had some great features, but we are particularly excited about the quality and maturity improvements in it.You can read the , or jump right !As always, we welcome and encourage feedback. Check out the release and what you think. Happy Logstashing! ","locales":"","title":"Logstash 1.4.0 GA Has Been Unleashed"}
{"index":{}}
{"author":"Rashid Khan","category":"","publish_date":"2014-03-18T00:00:00.000Z","url":"/blog/kibana-3-0-0-ga-now-available","seo_title":"","content":" Today is a big day for Elasticsearch and the Kibana team. After 5 milestone releases and over 1000 commits, we’re happy to announce the release of Kibana 3.0.0 GA. Over the last year, Kibana has moved from a simple interface to search logs to a fully featured, interactive analysis and dashboard system for any type of data. Everyday, we’re incredibly inspired by the people who tell us they’ve solved major problems, optimized their existing deployments and found insights in places they never imagined. . Changes since milestone 5 We’re thrilled to say there have been many bugs fixed since milestone 5, but we’ve introduced no breaking changes! One hundred percent of the focus has been on delivering a stable, consistent experience. What do I need to do to upgrade? If you’re coming from milestone 5? Nothing! Simply install and go. If you’re upgrading from milestone 4 your dashboards schema is compatible, but you should know that the has been deprecated (.) If you’re coming from milestone 4, you may need to clear your browser cache upon upgrading. We have put in place a number of cache busting strategies to limit the need for this in the future. More information We’ve written some great tutorials that you can find in the section. If you’re using Kibana for the first time, make sure to read the . Additionally, these pieces of documentation will be useful to getting you up and running: Getting Started Videos We’ve also created some getting-started videos to guide you through Kibana’s many panel types. These two are the first in a series, so stay tuned to this blog to learn when more are on offer. For a deeper dive, you can attend our live Kibana webcast, hosted by yours truly, on April 9th. You can to attend the session, which will include live Q&A following the presentation.   Introduction to the Kibana Terms Panel   Using the Kibana Table Panel Let Us Know What You Think and check out all of the updates! As always, ! ","locales":"","title":"Kibana 3.0.0 GA is now available!"}
{"index":{}}
{"author":"Boaz Leskes","category":"","publish_date":"2014-03-13T00:00:00.000Z","url":"/blog/marvel-1-1-0-released","seo_title":"","content":" Today, we are happy to announce the release of . This is the first feature release since the introduction of Marvel, slightly more than a month ago. We've gotten great feedback from our users thus far, and Marvel 1.1.0 includes a couple of new features based on their requests. Here are the highlights: Elasticsearch 1.0 Support in Sense Sense's knowledge base has been extended to incorporate new features introduced in Elasticsearch 1.0. Most notably, autocomplete suggestions are now available for both the Aggregations and Snapshot and Restore APIs. All have also been incorporated. These updates will make it easier to both explore these new APIs via Sense and discover how your API calls need to change when moving to 1.0. Visual Improvements to the Nodes & Indices Overview Changes to the Nodes & Indices section provide new visual cues to help you understand the state of your cluster. The current master node is indicated with a little star. If a node drops off the cluster and stops sending data, the corresponding row will be greyed out to indicate that the data is stale and some action is needed. The same thing happens if an index was deleted, indicating the index no longer exists. Data Reduction One of the goals when building Marvel was to offer in-depth monitoring and insights into the heart of Elasticsearch. As such, Marvel's agent shipped statistics on the cluster and index levels, even all the way to shard level information - be it a primary shard or a replica. Since the release, we have learned that shard level statistics generate considerable amounts of data and some users find they do not provide sufficient value to be worth the extra resources. Shard level statistics are useful to pinpoint bad performance to the level of a specific shard misbehaving. However, in practically all cases the unintended behavior is caused by load on the Node hosting the shard. As Marvel already makes Node level information readily available, we have decided to disable shipping of shard level statistics by default. You can still enable them, in run-time, if needed. We expect this change to result in big resource savings, especially in deployments with many indices. To help even more, we have also increased the default sampling rate from 5 seconds to 10 seconds. Dynamically Updatable Settings Marvel now allows you to use the to change some of its settings. You can now temporarily disable data shipping and change the target to which the agent sends data without restarting the cluster. This will be very helpful during maintenance and upgrades. For a complete change list, see . As mentioned at the start of this post, we welcome all feedback so we can continue improving Elasticsearch Marvel. Please send questions, praise or pains to the . ","locales":"","title":"Marvel 1.1.0 Released"}
{"index":{}}
{"author":"Shay Banon","category":"Engineering","publish_date":"2014-03-06T00:00:00.000Z","url":"/blog/tribe-node","seo_title":"","content":" At Elasticsearch, we like to think different about data and how to manage it. We know that companies can innovate only if they truly understand all aspects of their business through their data. We see Elasticsearch being used for many different use cases within individual organizations: logging critical visitor data in one division, analyzing financial transactions in another, and driving insights from social data in yet another division. Often, these departments are spread out across the globe. What if, instead of each division having their own data silo, we could produce one coherent view of data from across the entire organization? What new insights could be gained if we were able to connect all of this data? To answer this question, we created the Tribe node. The Tribe node connects to multiple Elasticsearch clusters and allows you to view them as if they were one big cluster. You may remember that this functionality was partially the promise of Federated Search. What makes the Tribe node unique is that it doesn’t impose any restrictions on cross-cluster search or, for that matter, on any other core APIs. Technically, the Tribe node is quite simple: it connects to multiple clusters and registers to receive their cluster events. Any time a cluster event occurs, the Tribe node acts on it by merging the different cluster events/states into a single global cluster state that can be used by all the different APIs. This means that searching on 10 shards in a single cluster, or searching across 10 shards where 6 of them exist on 1 cluster and the other 4 exist on another, is exactly the same operation. The Tribe node supports almost all APIs, with the exception of meta-level APIs such as the Create Index API. Meta-level APIs must be processed by the elected master node, but the Tribe node effectively has no single elected master. Instead, operations like creating an index must be executed on the individual cluster. Another important point to note when using the Tribe node: index names must be unique across all clusters. Because the cluster states from multiple clusters are merged into a single global cluster state, if an index with the same name exists on two clusters then one of the two indices will simply be ignored. Using the Tribe node couldn’t be simpler. Here is a demonstration of how to test it out by running two clusters on your local machine: # first, start a single node LDN cluster bin/elasticsearch --cluster.name ldn # second, start another single node HK cluster bin/elasticsearch --cluster.name hk Elasticsearch makes it easy to run multiple instances on the same machine for testing purposes. We have just started two nodes, each belonging to a different cluster: the first LDN cluster uses ports 9300/9200 and the second HK cluster uses ports 9301/9201. # now, let's start a Tribe node that connects to both clusters bin/elasticsearch --tribe.ldn.cluster.name ldn --tribe.hk.cluster.name hk This Tribe node started with ports 9302/9202. Now, let’s see which nodes are part of the Tribe node cluster state: # see all the nodes that are part of the Tribe node (9202) state curl 'localhost:9202/_cluster/state/nodes?pretty' # response { \"nodes\" : { \"lykJOKu2Shaa0v4jjt9d4g\" : { \"name\" : \"Man-Eater/hk\", \"transport_address\" : \"inet[/10.12.1.196:9303]\", \"attributes\" : { \"tribe.name\" : \"hk\", \"client\" : \"true\", \"data\" : \"false\" } }, \"fuFL42E1S_GSe_miEQKOvg\" : { \"name\" : \"Man-Eater/ldn\", \"transport_address\" : \"inet[/10.12.1.196:9304]\", \"attributes\" : { \"tribe.name\" : \"ldn\", \"client\" : \"true\", \"data\" : \"false\" } }, \"I8iGiHehQES9G-ZWbw2roQ\" : { \"name\" : \"Stygyro\", \"transport_address\" : \"inet[/10.12.1.196:9300]\", \"attributes\" : { \"tribe.name\" : \"ldn\" } }, \"4rVSDX2vQNe4b6WGMqhH7A\" : { \"name\" : \"Sepulchre\", \"transport_address\" : \"inet[/10.12.1.196:9301]\", \"attributes\" : { \"tribe.name\" : \"hk\" } }, \"s7QC7w2gTpyu_pDM14JitQ\" : { \"name\" : \"Man-Eater\", \"transport_address\" : \"inet[/10.12.1.196:9302]\", \"attributes\" : { \"client\" : \"true\", \"data\" : \"false\" } } } } Let me e","locales":"","title":"Tribe Node"}
{"index":{}}
{"author":"Aaron Mildenstein","category":"","publish_date":"2014-03-01T00:00:00.000Z","url":"/blog/logstash-1-4-0-beta-2","seo_title":"","content":" We are pleased to announce the second beta release of Logstash 1.4.0! The following major features are: Other points of note: Since the first beta we've had tremendous support from the community!  We have had numerous bugs filed and patches submitted.  We've been really pleased by your enthusiasm and support! Thank you! You can view the full changelog for this beta here:  What’s the ‘beta’ mean? All tests are passing, but we are leaving some time to allow the community to kick the tires and take it for a test drive. Additionally, beta means that some new features may change before general release. Get started and download the new tarball package:   ","locales":"","title":"Logstash 1.4.0 beta 2"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-02-19T00:00:00.000Z","url":"/blog/2014-02-19-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project.Elasticsearch Core Elasticsearch EcosystemHere's some more information about what is happening in the ecosystem we are maintaining around Elasticsearch, including plugin and driver releases, and news about Logstash and Kibana. Slides & Videos Where to Find UsWe'd love to feature all the great Elasticsearch, Logstash and Kibana presentations and meetups happening worldwide in this section. If you're speaking or hosting a meetup, let our Community Manager,, know!Australia Austrian will talk all about Elasticsearch at the on March 12th. Doors open at 7 PM.Canada Czech RepublicKarel Minařík will be speaking at the on March 5th. Doors open at 7 PM.Germany ItalyDavid Pilato will present Make sense of your (BIG) data! at the The conference takes place in Torino on April 3rd, and David will speak at 10:30 AM.bedJapanThe has been scheduled by Jun Ohtani. Please plan to join the meeting at 7 PM on April 21st.Slovakia will speak at the on February 27th. The conference takes place in Bratislava.South Africa will take the stage at ScaleConf to talk . The conference runs April 10th and 11th in Cape Town.United Kingdom United States Where to Find YouOur Community Manager, , is hard at work to help folks create more Elasticsearch meetup groups and to help meetup organizers find more speakers. If you are interested in either effort, .Oh yeah, we're also . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticearch, Kibana and Logstash than where you rest your head.TrainingsIf you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elasticsearch - February 19, 2014"}
{"index":{}}
{"author":"Jordan Sissel","category":"","publish_date":"2014-02-19T00:00:00.000Z","url":"/blog/logstash-1-4-0-beta1","seo_title":"","content":" We are pleased to announce the beta release of Logstash 1.4.0! This release series is primarily focused on improving quality, maturity, and ease of use. The following major features are included in this beta: Other points of note: You can view the full changelog for this beta here: What’s the ‘beta’ mean? All tests are passing, but we are still waiting for at least one new feature to land. Additionally, beta means that some new features may change before general release. Get started and download the new tarball package: A tarball? What happened to the jar file? Read on, my inquisitive friend! Jar? Tarball! Past releases have been a single jar file which included all Ruby and Java library dependencies to eliminate deployment pains. The jar file served us well, but over time we found Java’s default heap size, garbage collector, and other settings weren’t well suited to Logstash. In order to provide better Java defaults, we’ve changed to releasing a tarball (.tar.gz) that includes all the same dependencies (yay!). What does this mean to you? Instead of running you run (for Windows users, ) One pleasant side effect of using a tarball is that the Logstash code itself is much more accessible and able to satisfy any curiosity you may have. Contrib plugins package Logstash has grown brilliantly over the past few years with great contributions from the community. Now having 165 plugins, it became hard for us (the Logstash engineering team) to reliably support all the wonderful technologies in each contributed plugin. We combed through all the plugins and picked the ones we felt strongly we could support, and those now ship by default with Logstash. All the other plugins are now available in a package. All plugins continue to be and free, of course! Installing plugins from the package is very easy and documented on the package site. A bonus effect of this decision is that the default Logstash download size shrank by 19MB compared to the previous release because we were able to shed some lesser-used dependencies. You can learn more about the plugin package here: New release cycle This beta release is the first step in our new release process. Going forward, Logstash release cycles will more closely mirror Elasticsearch’s model of releases. Each new major release will start as a beta, continuing to a release candidate once all features are complete, and finally, becoming a general release once we feel it is production-ready. Maintenance of major versions will include bug fixes only. New features will wait until the next major release (for example, until 1.5.0). A more formal release cycle lets us more effectively set expectations about changes going into Logstash and is intended to help make you happier when it comes to upgrades and bug fixes! Better documentation We’ve got a brand new that aims to more effectively educate new users about how to use Logstash. Beyond that, we took a look at the most popular plugins and put lots of effort into improving them. Try out the new beta and what you think! ","locales":"","title":"Logstash 1.4.0 beta1"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-02-12T00:00:00.000Z","url":"/blog/2014-02-12-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. Elasticsearch core Photo Credit: Igor Motov Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around Elasticsearch, including plugin and driver releases, and news about Logstash and Kibana. Slides & Videos Where to find Us We'd love to feature all the great Elasticsearch, Logstash and Kibana presentations and meetups happening worldwide in this section. If you're speaking or hosting a meetup, let our Community Manager,, know! Australia and will headed Down Under to teach training courses in Melbourne and Sydney, and we're working to schedule meetups during their visit. We hope to have full details by next week's edition of , but for now you can save the date. We're targeting March 13th for the Melbourne Meetup and March 24th for Sydney. Canada France David will speak on at Microsoft Tech Days in Paris. David's presentation is scheduled for February 13th at 4:30 PM. Slovakia will speak at the on February 27th. The conference takes place in Bratislava. United Kingdom United States Where to Find You Our Community Manager, , is hard at work to help folks create more Elasticsearch meetup groups and to help meetup organizers find more speakers. If you are interested in either effort, . Oh yeah, we're also . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticearch, Kibana and Logstash than where you rest your head. Trainings If you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elasticsearch - February 12, 2014"}
{"index":{}}
{"author":"Andrew Cholakian","category":"","publish_date":"2014-02-18T00:00:00.000Z","url":"/blog/found-text-analysis-part-2","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud. This is a follow up article where we'll continue to survey various analyzers, each of which showcases a very different approach to parsing text. This is the second of two articles about analyzers. If you haven't yet read the first article on this topic, , it is strongly recommended that you read that first. In this article we'll be diving a little deeper into the catalog of Elasticsearch analyzers. The intention of these articles is to take users on a guided tour of some of the most useful analyzers. There's no need to remember everything here, but if you know all these tools are available you'll be able to create much better queries using Elasticsearch. ","locales":"","title":"All About Analyzers, Part Two"}
{"index":{}}
{"author":"Alex Brasetvik","category":"","publish_date":"2014-02-11T00:00:00.000Z","url":"/blog/found-beginner-troubleshooting","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud. We'll look at common problems when getting started with Elasticsearch. ","locales":"","title":"Troubleshooting Elasticsearch searches, for Beginners"}
{"index":{}}
{"author":"Shelby Sturgis","category":"Engineering","publish_date":"2014-02-10T00:00:00.000Z","url":"/blog/data-visualization-elasticsearch-aggregations","seo_title":"","content":" For those of you familiar with , you know that its an amazing modern, scalable, full-text search engine with and the at its core. Elasticsearch allows users to query their data and provides efficient and blazingly fast look up of documents that make it perfect for creating real-time analytics dashboards. Currently, Elasticsearch includes , a functionality that allows users to compute aggregations of their data. For example, a user with twitter data could create buckets for the number of tweets per year, quarter, month, day, week, hour, or minute using the , making it quite simple to create histograms. Faceted search is a powerful tool for data visualization. is a great example of a front-end interface that makes good use of facets. However, there are some major restrictions to faceting. Facets do not retain information about which documents fall into which buckets, making complex querying difficult. Which is why, Elasticsearch is pleased to introduce the framework with the 1.0 release. Aggregations rips apart its faceting restraints and provides developers the potential to do much more with visualizations. Aggregations (=Awesomeness!) Aggregations is \"faceting reborn\". Aggregations incorporate all of the faceting functionality while also providing much more powerful capabilities. Aggregations is a generic but extremely powerful framework for building any type of aggregation. There are several different types of aggregations, but they fall into two main categories: bucketing and metric. Bucketing aggregations produce a list of buckets, each one with a set of documents that belong to it (e.g., , , , , , ). Metric aggregations keep track and compute metrics over a set of documents (e.g., , , , , , ). Using Aggregations for Data Visualization (with D3) Lets dive right in and see the power that aggregations give us for data visualization. We will create a donut chart and a dendrogram using the Elasticsearch aggregations framework, the , and . If you are new to Elasticsearch, it is very easy to get started. Visit the Elasticsearch page to learn how to download, install, and run Elasticsearch version 1.0. Requirements: * a link to the index.html and main javascript file can be found . Alternatively, you can clone the on github. Uploading Data to our Elasticsearch Index We will need to create an index and add some data. Since the (American) football season just ended, let's send it off by exploring some NFL data. For this tutorial, we will only be concerned with . Download the dataset (nfl_2013.json) and its mappings (nfl_mapping.json) using the link above. Fire up Elasticsearch from your localhost. Now, let's add an index and some data. From the terminal: curl -XPOST localhost:9200/nfl?pretty curl -XPUT localhost:9200/nfl/2013/_mapping?pretty -d @nfl_mapping.json curl -XPOST localhost:9200/nfl/2013/_bulk?pretty --data-binary @nfl_2013.json The NFL data file consists of play-by-play data with 18 fields. { def: \"HOU\", // defensive team defscore: \"13\", // score for defensive team description: \"(11:34) S.Ridley right guard for 8 yards TOUCHDOWN.\", // play description down: \"2\", // down for current play (up to 4) gameid: \"20130113_HOU@NE\", // game id with date and teams min: \"26\", // minutes remaining in game nextscore: \"7\", // ??? off: \"NE\", // offensive team offscore: \"17\", // score for offensive team qtr: \"3\", // quarter scorechange: \"0\", // amount offensive score changed scorediff: \"4\", // difference in score season: \"2012\", // nfl season (year) sec: \"34\", // seconds remaining within minute of play series1stdn: \"1\", // ??? teamwin: \"1\", // either 1 or 0, demarks the team winning togo: \"6\", // number of yards to go to reach a first down ydline: \"8\" // yard line ball is on to start play } Setting up our HTML and Javascript Files Lets begin by creating a directory for our project. Lets call it nfl. Now add index.html and a scripts subdirectory. Within scripts, place our ","locales":"ko-kr","title":"Data visualization with Elasticsearch aggregations and D3"}
{"index":{}}
{"author":"Livia Froelicher","category":"User Stories","publish_date":"2014-02-07T00:00:00.000Z","url":"/blog/ladders-elasticsearch-marvel","seo_title":"","content":" We've been seeing tremendous growth in attendance at Elasticsearch meetups, and I'm personally quite excited to see new groups springing up worldwide. Today, we're excited to share more from Monday's at . In this video, Peter Pathirana, Lead Engineer, Recommendations Infrastructure, Platform Team and co-author of , treats us to an in-depth view of how The Ladders uses Elasticsearch to power their job-matching service for career-driven professionals. Following the talk, Shay Banon, our CTO and creator of Elasticsearch, answers extensive audience Q&A on all things Elasticsearch, including our brand new management & monitoring product, . The whole meetup proceedings are chock full of juicy architectural bits and the Q&A quite wide ranging, but here are some of the highlights: #MonitoringLove Use Case High Points   Thanks again to Peter and the whole team at The Ladders for hosting us and sharing their story! As always, if you're giving a talk on Elasticsearch, Elasticsearch Marvel, Logstash or Kibana, I would love to . We're proud and excited to support user group activities and I'm available to help you get the word out about your talk and help you get the pizza to go with it. Happy viewing! ","locales":"","title":"The Ladders and Elasticsearch Marvel"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-02-05T00:00:00.000Z","url":"/blog/2014-02-05-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the , as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. Elasticsearch core Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around Elasticsearch, including plugin and driver releases, and news about Logstash and Kibana. Slides & Videos Where to find Us We'd love to feature all the great Elasticsearch, Logstash and Kibana presentations and meetups happening worldwide in this section. If you're speaking or hosting a meetup, let our Community Manager,, know! Czech Republic Honza Kral will give two presentations at : and . Honza's presentations take place on Friday, February 7th, and the conference runs from the 7th through the 9th. France Germany Japan Thanks to Jun Ohtani, the 3rd Elasticsearch Meetup will be held in Tokyo on February 7th starting at 7 PM. Please remember to . Sweden The has just been scheduled for February 5th. will cover Elasticsearch and Chef, and will discuss What's New in 1.0 with Aggregations. United Kingdom United States Where to Find You Our Community Manager, , is hard at work to help folks create more Elasticsearch meetup groups and to help meetup organizers find more speakers. If you are interested in either effort, . Oh yeah, we're also . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticearch, Kibana and Logstash than where you rest your head. Trainings If you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elasticsearch - February 05, 2014"}
{"index":{}}
{"author":"Costin Leau","category":"Engineering","publish_date":"2014-02-06T00:00:00.000Z","url":"/blog/elasticsearch-hadoop-1-3-m2","seo_title":"","content":" I am happy to announce that the second milestone of 1.3 (also known as es-hadoop) has been released. M2 brings several new major features to the table, including: Support for Elasticsearch 1.0 (RC1+) es-hadoop supports Elasticsearch 1.0 RC1 or higher while preserving compatibility with the Elasticsearch 0.90.x line. The code base automatically detects the target Elasticsearch version and uses the appropriate . id awareness es-hadoop 1.3 M1 enabled new indexes to be created in Elasticsearch directly from Hadoop jobs. M2 takes this feature several steps forward, enabling both index creation and updating. Furthermore, one can specify all the meta-data options if needed: document , , and . Field aliasing Higher level abstractions on Map/Reduce, like Cascading, Apache Hive and Apache Pig provide mapping and data types on top of raw data for easier manipulation. With es-hadoop, one can also define field aliasing, decoupling the Hadoop libraries structural declaration from the underlying document fields for cleaner syntax and improved readability. Field extraction Performance is a top priority for es-hadoop. As such, M2 only loads the data that it must from Elasticsearch:  rather than loading the entire document, es-hadoop retrieves just the required fields, also known as . This implementation of projection results in less network, CPU and memory usage. JSON support Besides supporting the Map/Reduce, Cascading, Hive and Pig data types, M2 also allows JSON data to be indexed. Don’t forget that es-hadoop can now extract the document metadata (such as ) if instructed to do so. Parallel writes and ingest mitigation Users with high ingestion volumes will be happy to hear that M2 provides significant updates on the write front. Now, all writes to Elasticsearch are parallelized on the target shards:  this implementation prevents network bottlenecks as the load is spread across multiple nodes. In case of excessive loads on Elasticsearch, regardless of the reason, 1.3 M2 automatically retrieves the rejected payload before ingesting further data. This temporary throttling prevents data spillage. Improved I/O In a similar vein, es-hadoop 1.3 M2 provides automatic cluster discovery and, in case of network errors, automatic fall back to the rest of the available nodes. Snapshot/Restore for HDFS Last but not least, 1.3 M2 introduces support for through a separate, umbrella project (). Once installed in the Elasticsearch cluster, this standalone plugin enables any storage (from the omnipresent HDFS to pluggable implementations such as or ) to be used for backing up and restoring data within running Elasticsearch clusters. Of course, this release also includes the usual bug fixes, compatibility improvements and refinements. As always, we welcome feedback so and what you think. P.S. If you are migrating from M1, you might want to read up on the . ","locales":"","title":"Elasticsearch for Apache Hadoop 1.3 M2 released"}
{"index":{}}
{"author":"Clinton Gormley","category":"","publish_date":"2014-02-03T00:00:00.000Z","url":"/blog/0-90-11-1-0-0-rc2-released","seo_title":"","content":" Today we are happy to announce the release of and , both of which are based on Lucene 4.6.1. Elasticsearch 0.90.11 This is the current stable release in the 0.90 series and contains a number of small bug fixes and enhancements which you can read about in the . Some of the notable changes are: We recommend that users of older versions upgrade to 0.90.11, but users of 0.90.10 don't need to upgrade unless they are being affected by one of the issues listed in the release notes. You can . Elasticsearch 1.0.0.RC2 This is the final release candidate in the 1.0 branch and contains a number of small bug fixes, consistency improvements and a few enhancements, including the ones listed above. You can see the details in the . We don't intend to make any more changes other than bug fixes to the 1.0 branch before the final release of 1.0. Please download this release candidate and be mean to it! If anything breaks, before 1.0. You can . ","locales":"","title":"0.90.11 and 1.0.0.RC2 released"}
{"index":{}}
{"author":"David Pilato","category":"Engineering","publish_date":"2014-01-31T00:00:00.000Z","url":"/blog/azure-cloud-plugin-for-elasticsearch","seo_title":"","content":" In cloud environments like Azure, multicast is often (always?) forbidden. So you need to provide a list of nodes to help Elasticsearch to discover the nodes of the cluster. Starting new instances could be then tricky as you have to maintain a minimal list of nodes. And what happens when a virtual machine goes down? When it comes back up, it could have a new IP address. So you need to edit unicast settings for each node, right? We are pleased to announce the first release of . This first release uses the Azure API for the unicast discovery mechanism and simplifies your cluster growth management a lot. Azure Virtual Machine DiscoveryAzure plugin uses to perform automatic discovery, which is similar to multicast discovery in multicast-friendly environments. You just have to: And… You're done! Want More Details?Suppose that you want to build an Ubuntu 13 virtual machine running Elasticsearch. Let's say that you already have an with your already defined and uploaded to Azure, that you have installed , that you have a ready to use. # You first need to generate a java keystore (azurekeystore.pkcs12) # from your existing ssh key (azure-private.key) and certificate (azure-certificate.pem) openssl x509 -outform der -in azure-certificate.pem -out azure-certificate.cer openssl pkcs8 -topk8 -nocrypt -in azure-private.key -inform PEM -out azure-pk.pem -outform PEM openssl x509 -inform der -in azure-certificate.cer -out azure-cert.pem cat azure-cert.pem azure-pk.pem > azure.pem.txt openssl pkcs12 -export -in azure.pem.txt -out azurekeystore.pkcs12 -name azure -noiter -nomaciter # Deploy an Ubuntu image on an extra small instance in West Europe: azure vm create azure-elasticsearch-cluster \\ b39f27a8b8c64d52b05eac6a62ebad85__Ubuntu-13_10-amd64-server-20130808-alpha3-en-us-30GB \\ --vm-name myesnode1 \\ --location \"West Europe\" \\ --vm-size extrasmall \\ --ssh 22 \\ --ssh-cert /tmp/azure-certificate.pem \\ elasticsearch password1234!! # \"elasticsearch/password1234!!\" are the SSH login/password for this instance. # Connect to your instance when started # SSH settings for convenience HOST=myescluster.cloudapp.net SSH_OPTIONS=\"-o User=elasticsearch -o IdentityFile=/tmp/azure-private.key -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null\" # Copy your keystore to the VM scp $SSH_OPTIONS /tmp/azurekeystore.pkcs12 $HOST:/home/elasticsearch # Connect to the VM ssh $SSH_OPTIONS $HOST Install either the latest OpenJDK using or and then install Elasticsearch and its Azure cloud plugin: curl -s https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-0.90.10.deb \\ -o elasticsearch-0.90.10.deb sudo dpkg -i elasticsearch-0.90.10.deb sudo /usr/share/elasticsearch/bin/plugin -install \\ elasticsearch/elasticsearch-cloud-azure/1.0.0.alpha1 Edit and add: cloud.azure.management: subscription.id: your_azure_subscription_id cloud.service.name: your_azure_cloud_service_name keystore: path: /home/elasticsearch/azurekeystore.pkcs12 password: your_password_for_keystore discovery: type: azure Restart Elasticsearch and you're done! Now this instance uses the Azure API to get a list of available nodes. sudo service elasticsearch restart Scaling Out!Hey! But we have started only one node! It's not really a cluster, right? Let's scale out and bring more nodes to the party! # From your local machine, shutdown azure node and create an image: azure vm shutdown myesnode1 azure vm capture myesnode1 esnode-image --delete # Start 10 instances: for x in $(seq 1 10) do echo \"Launching azure instance #$x...\" azure vm create azure-elasticsearch-cluster \\ esnode-image \\ --vm-name myesnode$x \\ --vm-size extrasmall \\ --ssh $((21 + $x)) \\ --ssh-cert /tmp/azure-certificate.pem \\ --connect \\ elasticsearch password1234!! done You should now have a cluster running with 10 nodes! What's Next?First, we love to hear feedback from our community! Feel free to ask questions on the and raise issues or ask for feature requests on . Pull requests a","locales":"","title":"Azure Cloud Plugin for Elasticsearch"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-01-29T00:00:00.000Z","url":"/blog/2014-01-29-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the GitHub repositories, as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. Elasticsearch core Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around Elasticsearch, including plugin and driver releases, and news about Logstash and Kibana. Slides & Videos Where to find Us We'd love to feature all the great Elasticsearch, Logstash and Kibana presentations and meetups happening worldwide in this section. If you're speaking or hosting a meetup, let our Community Manager,, know! Belgium Czech Republic Honza Kral will give two presentations at : and . Honza's presentations take place on Friday, February 7th, and the conference runs from the 7th through the 9th. France Germany Japan Thanks to Jun Ohtani, the 3rd Elasticsearch Meetup will be held in Tokyo on February 7th starting at 7 PM. Please remember to . Sweden The has just been scheduled for February 5th. Further details are forthcoming, but you can look forward to presentations from and . United Kingdom United States Where to Find You Our Community Manager, , is hard at work to help folks create more Elasticsearch meetup groups and to help meetup organizers find more speakers. If you are interested in either effort, . Oh yeah, we're also . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticearch, Kibana and Logstash than where you rest your head. Trainings If you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elasticsearch - January 29, 2014"}
{"index":{}}
{"author":"Boaz Leskes","category":"","publish_date":"2014-01-29T00:00:00.000Z","url":"/blog/building-marvel","seo_title":"","content":" Yesterday, our latest product, . Judging by the twitter storm alone, people are just as excited about it as we are. Today, I would like to take the opportunity to tell more about how it works and how it came to be. Marvel is the result of all of our experiences helping users and providing support to customers. Most importantly, the product has come from our own needs for its capabilities and insights. What happened at 3 AM this morning? Since Elasticsearch , its adoption has been nothing short of impressive. As the number of users has grown, so have the number of questions and requests for help on the and in IRC (#elasticsearch on ). We also have numerous inquiries that come in from our customers on dedicated support contracts. The questions come, of course, in many flavors. Some would be about how to best use a feature, or help with a certain aspect of the Query DSL. Others would report a problem and ask for help figuring out what has happened, be it about an exception from the logs, garbage collection taking longer than ideal or an indication that memory has run out. Sometimes a quick API call would be enough to find out the cause of the problem and resolve it quickly. Other times, issues are a result of a more complex sequence of events. Take, for example, the following scenario: you are the proud owner of a cluster. You have an application which analyzes time-based data. Following best practices, you have your recent data on powerful SSD-equipped machines. As data gets older, you have a nightly cronjob that uses Elasticsearch's to indicate the data should be moved to cheaper & less capable machines. One morning, just as you walk into the office, you notice that one of the old-data machines is stressing out and run out of capacity. But why? And why now? There is no spike in search traffic, and indexing goes to the more powerful nodes. The only change you can think of is that nightly cronjob, but it runs at 3 AM, 6 hours before the node started having problems. As it happens, the cronjob started a chain of events that led to the current situation. In order not to impact performance, Elasticsearch throttles moving data around. Since the job issued the command at 3 AM, more and more data moved. Only a couple of hours later, the node's maximum capacity was reached. Once you've found out this information and spent some time digging into logs and thinking hard, the solution is easy: temporarily move some data back to the SSD machines, provision another cheap node and move the data to it. Upon reflection, you realize you need some way of seeing cluster behavior over time. This functionality would have allowed you to see the increasing usage trend and to easily trace back the problem to its start at 3 AM in the morning. It would also have allowed you to see that the same thing happened yesterday, and the day before, except then it was not a problem - yet. It would also be great if just at the beginning of that trend, you could see an indication that Elasticsearch had started to relocate data … but we'll get to that later. As you can imagine, such stories are not unique. In fact, things can get even more complex. Repeatedly, we ask customers to send us the logs from all their nodes. We also ask to call the stats API repeatedly while running heavy queries, what management commands were actually issued, when they were issued, etc. Once we get the information, we scan it carefully and try to correlate the different information streams and compare them to how we know Elasticsearch behaves. The process is manual, intensive and time consuming. Especially during those moments where time is not necessarily on your side. Being engineers, we kept thinking about how we could improve and automate this process. We wanted to build something that would make our own lives easier and help all of our users: tools to monitor clusters & to collect and analyze the vast number of statistics Elasticsearch exposes. To accomplish this, we needed a place to store all this dat","locales":"","title":"Why We Built Marvel"}
{"index":{}}
{"author":"Rashid Khan","category":"Engineering","publish_date":"2014-01-27T00:00:00.000Z","url":"/blog/whats-cooking-kibana","seo_title":"","content":" Elasticsearch 1.0 is almost here, and the Kibana team is gearing up for release right alongside. Along with the usual bug fixes and small tweaks, we've got some great new features for you in our next release: Panel Groups Panels are now organized into groups that happily contain as many panels as you you’d like. Rows collapse cleanly and panels consume no resources when hidden. Chart Markers Correlate deployments, logins and other critical events with changes in traffic, memory consumption or load. Chart markers let you input a custom query to be used to draw your important events inline with a time series chart. Ad hoc Filters Create your own query filters and save them for later. Filters are stored along with the dashboard and can be toggled on and off to compare the data subsets you define. Top-N Queries Click the colored dot next to a query to do more than set the query color. The new top-N query finds the most popular terms in a field and uses them to compute new queries. Stats Panel The Stats Panel distills a search down into a single, meaningful number. Terms_Stats mode Bytes served by country? Revenue per customer? Per-page memory usage? The terms_stats mode of the terms panel has everything you need. Coming Soon The next release of Kibana will occur in the coming weeks, but you can grab the current version . If you really want to get your hands dirty, pop on over to the . Feedback is welcome and encouraged, so keep those coming! ","locales":"","title":"What's Cooking in Kibana"}
{"index":{}}
{"author":"Andrew Cholakian","category":"","publish_date":"2014-01-22T00:00:00.000Z","url":"/blog/found-text-analysis-part-1","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud. In this article we'll survey various analyzers, each of which showcases a very different approach to parsing text. ","locales":"","title":"All About Analyzers, Part One"}
{"index":{}}
{"author":"Steven Schuurman","category":"News","publish_date":"2014-01-28T00:00:00.000Z","url":"/blog/introducing-elasticsearch-marvel-native-monitoring-deployments","seo_title":"","content":" Native Management and Monitoring for DeploymentsToday, we're making an announcement that's probably the most important one Elasticsearch has made since we founded the company.With unbelievable pride and excitement we would like to introduce you to – our brand new management & monitoring product. Marvel is 100% native to Elasticsearch, built from scratch by the team that also develops Elasticsearch and is effectively the #1 requested ecosystem product requested for our product stack. We hope you will be as excited about it as we are!What does Marvel do Exactly?Marvel solves one of the most important challenges anyone who has deployed Elasticsearch has encountered: how to gain a complete view of the state of a deployment and how to manage clusters towards optimal health. Although Elasticsearch exposes a very rich set of system statistics through its Stats API, translating those outputs into actionable cluster health information is an entirely different issue. Marvel instantly provides much-needed visibility into a deployment, both in real-time and historically. Native Monitoring for DeploymentsMarvel is directly connected with every node in an Elasticsearch deployment and records and visualizes all metrics produced by each node in a cluster, allowing the operator to monitor the current, real-time state as well as the historical state of the cluster. This level of visibility into the Operating System, the JVM, and the various services that are running within Elasticsearch and Lucene, combined with low level system metrics like machine load, CPU, disk, network and memory usage, provide invaluable insights that weren't available before. Although deceivingly simple, being able to correlate core system metrics with Elasticsearch and Lucene specific metrics, is unbelievably valuable. No other off-the-shelf monitoring system is able to provide this level of transparency into your Elasticsearch deployment.How to get MarvelGetting Marvel is simple - you can install it via the command line or download it and install as you would any other Elasticsearch plugin. You can learn more at The Rules are SimpleWe decided to make Marvel completely free for development use and downloadable without any registration of any kind. Marvel is a commercial product though, so if you want to use it to for your live deployment you need to buy a license. If you're an Elasticsearch subscription customer, you get a full license to use Marvel included with your subscription package. If you're not already a subscription customer, you can find all our product information at .And no, $500 for the first five nodes isn't a typo. We've priced our offering to be affordable to any and all Elasticsearch users. We certainly hope you enjoy using Marvel as much as we enjoyed building it for you! ","locales":"","title":"Introducing Elasticsearch Marvel"}
{"index":{}}
{"author":"Luca Cavanna","category":"","publish_date":"2014-01-22T00:00:00.000Z","url":"/blog/this-week-in-elasticsearch-2014-01-22","seo_title":"","content":" Welcome to  . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the GitHub repositories, as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. Elasticsearch core Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around Elasticsearch, including plugin and driver releases, and news about Logstash and Kibana. Slides & Videos Where to find Us We'd love to feature all the great Elasticsearch presentations happening worldwide in this section. If you're speaking on Elasticsearch, ! Belgium will be speaking on What's New in Elasticsearch 1.0 with Aggregations at the in Brussels on Friday, January 31st. Doors open at 6 PM. and Honza Kral will be attending on February 1st and 2nd. Stop by the Elasticsearch table to say hello! We'll be in Building K on the 2nd Floor.Leslie Hawthorn will be speaking at the on DevOps: For Happier, More Productive People. Infrastructure.Next takes place on February 5th in Ghent, following .Czech Republic Honza Kral will give two presentations at : and . Honza's presentations take place on Friday, February 7th, and the conference runs from the 7th through the 9th. France Germany Japan Thanks to Jun Ohtani, the 3rd Elasticsearch Meetup will be held in Tokyo on February 7th starting at 7 PM. Please remember to . Sweden The has just been scheduled for February 5th. Further details are forthcoming, but you can look forward to presentations from and . United Kingdom Elasticsearch will have two sessions at , which takes place March 3-7th. You can join for a tutorial on plus see and Graham Tackley co-present on . Make sure to stop by our booth to say hello! United States Where to Find You Our Community Manager, , is hard at work to help folks create more Elasticsearch meetup groups and to help meetup organizers find more speakers. If you are interested in either effort, . Oh yeah, we're also  . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticearch, Kibana and Logstash than where you rest your head. Trainings If you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elasticsearch - January 22, 2014"}
{"index":{}}
{"author":"Igor Motov","category":"Engineering","publish_date":"2014-01-21T00:00:00.000Z","url":"/blog/introducing-snapshot-restore","seo_title":"","content":" In the last year, we saw a tremendous increase in adoption of Elasticsearch by many companies. As more and more companies are using Elasticsearch as an integral part of their business, high availability of Elasticsearch becomes increasingly important. With the help of automatic replication and failover, Elasticsearch provides a stable, highly available search and analytics platform. However, while replication can protect a cluster from hardware failures, it doesn’t help when someone accidentally deletes an index. Anyone that relies on an Elasticsearch cluster needs to perform regular backups. It has always been possible to backup an Elasticsearch cluster. However, until version 1.0 the backup process involved turning off index flushing, identifying locations of primary shards on the file system, copying the data and then remembering to turn on flushing again. We believe that simple is best, and the previous backup process in Elasticsearch didn’t quite fit the definition of simple. That’s why in v1.0 we are introducing a new Snapshot & Restore API that should make backup process much easier. In v1.0, backup is a simple and straightforward process. First, Elasticsearch needs to know where to backup data, which is done by registering a backup repository: $ curl -XPUT 'http://localhost:9200/_snapshot/my_backup' -d '{ \"type\": \"fs\", \"settings\": { \"location\": \"/mount/backups/my_backup\", \"compress\": true } }' Currently, we support and repositories. is coming soon. Once Elasticsearch knows about a repository, it’s possible to make a backup of the entire cluster with a single command: $ curl -XPUT \"localhost:9200/_snapshot/my_backup/snapshot_1?wait_for_completion=true\" Snapshots can be created on a live cluster that continues to perform indexing and search operations. A snapshot captures the point-in-time view of the index at the moment when a snapshot process has started. It makes the backup image of the index consistent. Restore is even simpler: $ curl -XPOST \"localhost:9200/_snapshot/my_backup/snapshot_1/_restore?wait_for_completion=true\" It’s possible to restore indices within a live cluster as well. However, indices have to be closed prior to restore. We are planning to make it possible to restore open read-only indices in a future release. Both backup and restore operations are incremental, which means that only files that changed since the last snapshot will be copied into the repository or restored into an index. Incremental snapshots allow performing the snapshot operation as frequently as needed without too much disk space overhead. Users can now easily create a snapshot before upgrade or a risky change in the cluster and quickly rollback to the previous index state if things go wrong. The snapshot/restore mechanism can be also used to synchronize data between a “hot” cluster and a remote, “cold” backup cluster in a different geographic region for fast disaster recovery. We are very excited about this new feature. We like to think of incremental backup as a time machine for your data. We are confident that everyone who relies on Elasticsearch as a critical component in their system and cannot afford down time for re-indexing will find the new Snapshot and Restore mechanism really helpful. We welcome your feedback – the Snapshot & Restore API and what you think! ","locales":"","title":"Introducing Snapshot & Restore"}
{"index":{}}
{"author":"Aaron Mildenstein","category":"Engineering","publish_date":"2014-01-20T00:00:00.000Z","url":"/blog/curator-tending-your-time-series-indices","seo_title":"","content":" backgroundA few years ago, I was managing an Elasticsearch, Logstash and Kibana (ELK) stack and needed a way to automatically clean up indices older than 30 days. After reading the API documentation and getting some help from the community in the #logstash and #elasticsearch IRC channels, I realized that this was fairly easy to set up with simple scripting and cron. curl -XDELETE 'localhost:9200/logstash-2014.01.01?pretty' Sure, this works, and it’s not terribly hard to generate dates, but I wanted something a bit more elegant. In the beginning…So, I started writing a script in python. It did the job with a cleaner command line and a single target number of days to keep, so I shared it with the larger community. Others polished it up and added new functionality. I wrote another script that also allowed me to optimize — which really just means “merge the segments in each shard until no more than n segments exist per shard — old indices. These scripts have now been merged and enhanced to become a single, helpful tool to help manage your older indices like the fine works of art they are! Introducing CuratorHere are a few of the index operations you can do with Curator: Installing CuratorAs of this writing, Curator is at release 0.5.1 and works with versions up through 0.90.10. Curator should also should be compatible with Elasticsearch version 1.0 (which is still only at RC1). We’ll be testing to ensure compatibility with each release. It currently resides in a git repository. In the near future, it will be a pip installable package (it’s python-based). Don’t let that scare you away from using it, though! If you have python and pip installed on your machine, installation is as simple as: git clone https://github.com/elasticsearch/curator.git pip install -r requirements.txt After that, you should be able to run this: $ ./curator.py -v curator.py 0.5.1 How-To and examplesBefore we get to the examples, in context. The list is long (and included at the end of this post), but illustrative of how much control you have. Please note where defaults have been listed. If you like them, you do not need to specify those flags. Now let’s go through some simple examples to illustrate how Curator can make your ELK stack better, and even more responsive. DeleteLet’s say you want to keep no more than 90 days of indices. The command is simple: $ curator.py --host my-elasticsearch -d 90 Here the -d specifies the number of days. Simple, right? Delete by spaceThis is a special case where you might want to delete indices in excess of some number of gigabytes (starting with the oldest): $ curator.py --host my-elasticsearch -C space -g 10024 Here you see that we specified curation (-C) by space, and a number of (-g) gigabytes (10024, or 10TB). The -g argument will accept a decimal value, e.g. 1.5, 0.5, etc. CloseClosing an index is handled by the : The open and close index APIs allow to you close an index, and later on open it. A closed index has almost no overhead on the cluster (except for maintaining its metadata), and is blocked for read/write operations. A closed index can be opened which will then go through the normal recovery process.Closing an index means it’s still there, but not searchable. Why is that useful? Imagine that you have an obligation to keep indices for 90 days, but rarely if ever do you search through an index over 30 days old. In this case, you can close the indices which will save valuable resources (heap space, in particular). This means your cluster will have more memory for searches and indexing! And if you ever need the data in those indices, you can open them again with an API call and they’ll be there again. $ curator.py --host my-elasticsearch -c 30 -d 90 This builds on our previous example. This will close indices older than 30 days and delete indices older than 90 days. Still quite simple! Disable Bloom Filters Don’t worry, the script checks to make s","locales":"","title":"Curator: Tending your time-series indices"}
{"index":{}}
{"author":"Drew Raines","category":"Engineering","publish_date":"2014-01-17T00:00:00.000Z","url":"/blog/introducing-cat-api","seo_title":"","content":" Background Perhaps the biggest success of Elasticsearch is its APIs. Interacting with the system is so simple it still catches me off guard, four years after I first tried it out. The engine powering this simplicity is JSON, a straightforward form of structured text birthed out of the rise of JavaScript. JSON is easy to understand and parse, and because of that, supported by almost every programming language in existence. Humans, however, are not programming languages. JSON’s strength is that it’s plaintext, which makes it possible for our eyes to parse, but merely looking at human-readable characters isn’t the same as actually understanding the information. With any more than the most trivial structure in a JSON doc, we typically reach for the nearest pretty-printer. Unfortunately, pretty-printing often still does not translate into actionable knowledge. In fact, the addition of whitespace often makes life even more difficult as it eats up precious space in a terminal window. “Not a problem,” you say. “JSON is so simple all I need is $LANG and five minutes.” Unfortunately, JSON and $LANG, along with speaking, walking, producing coherent sentences, and almost any other task in life, is a bit harder when you’re woken up from deep sleep by your phone alerting you to a system outage. The 3 AM Page Imagine this has just happened to Teresa. The monitoring system noticed that her cluster is red. A common first step in this moment of life as an Elasticsearch cluster administrator is to take a look at the logs on the master node. Which node is master? Armed with the comprehensive output of the , she’s off to the races. % curl 'es1:9200/_cluster/state?pretty' { ... \"master_node\" : \"Wjf_YVvySoK8TE41yORt3A\", ... OK, not quite. That’s just the node ID. node is that? ... \"nodes\" : { \"56RhV2ecT3OIZFzUYVYwNQ\" : { \"name\" : \"Midnight Sun\", \"transport_address\" : \"inet[/192.168.56.20:9300]\", \"attributes\" : { } }, \"pyqzjh_nRx6rapL-CBvsyA\" : { \"name\" : \"Urthona\", \"transport_address\" : \"inet[/192.168.56.40:9300]\", \"attributes\" : { } }, \"Wjf_YVvySoK8TE41yORt3A\" : { \"name\" : \"Lasher\", \"transport_address\" : \"inet[/192.168.56.10:9300]\", \"attributes\" : { } }, ... Her tired eyes move back and forth a few times and figure out that in order to get to she must connect to . She grumbles and pores over the logs. She notices that a node has failed some pings. She’s no fool. She’s seen this before and it wasn’t the network. The JVM is likely in trouble on that node. First up, she checks on the cluster’s health. % curl es1:9200/_cluster/health?pretty { \"cluster_name\" : \"foo\", \"status\" : \"red\", \"timed_out\" : false, \"number_of_nodes\" : 4, \"number_of_data_nodes\" : 4, \"active_primary_shards\" : 283, \"active_shards\" : 566, \"relocating_shards\" : 0, \"initializing_shards\" : 0, \"unassigned_shards\" : 1 } Uh oh! Red! Teresa is a bit paranoid, so she checks health across the whole cluster. % for i in 1 2 3 4 5:  do curl es${i}:9200/_cluster/health:  echo:  done {\"cluster_name\":\"foo\",\"status\":\"red\",\"timed_out\":false,\"number_of_nodes\":4,\"number_of_data_nodes\":4,\"active_primary_shards\":283,\"active_shards\":566,\"relocating_shards\":0,\"initializing_shards\":0,\"unassigned_shards\":1} {\"cluster_name\":\"foo\",\"status\":\"red\",\"timed_out\":false,\"number_of_nodes\":4,\"number_of_data_nodes\":4,\"active_primary_shards\":283,\"active_shards\":566,\"relocating_shards\":0,\"initializing_shards\":0,\"unassigned_shards\":1} {\"cluster_name\":\"foo\",\"status\":\"red\",\"timed_out\":false,\"number_of_nodes\":4,\"number_of_data_nodes\":4,\"active_primary_shards\":283,\"active_shards\":566,\"relocating_shards\":0,\"initializing_shards\":0,\"unassigned_shards\":1} {\"cluster_name\":\"foo\",\"status\":\"red\",\"timed_out\":false,\"number_of_nodes\":4,\"number_of_data_nodes\":4,\"active_primary_shards\":283,\"active_shards\":566,\"relocating_shards\":0,\"initializing_shards\":0,\"unassigned_shards\":1} A bit verbose, but it got the job done. All the nodes agree:  at least, the ones that are responding. A node definitely is missing, which","locales":"","title":"Introducing: The cat API"}
{"index":{}}
{"author":"Clinton Gormley","category":"Engineering","publish_date":"2014-01-10T00:00:00.000Z","url":"/blog/0-90-10-released","seo_title":"","content":" Today, we are happy to announce the release of E, which is based on Lucene 4.6. This is the current stable release in the 0.90 series. You can download it .This release fixes an . If you have set to point to multiple directories, you should upgrade to 0.90.10. More details about this bug can be found below. You can read about other bug fixes and enhancements in the .To explain the problem, a shard requires a single file called which stores the latest generation number. If there is more than one file, the index may (incorrectly) appear to be corrupted. When using multiple data paths, the file could be written to any of the listed directories, so causing duplication and the appearance of corruption. This can prevent a shard from recovering, with error messages such as:Failed to start shard, message [IndexShardGatewayRecoveryException[[my_indexname][2] shard allocated for local recovery (post api), should exist, but doesn't, current files: [_66wz.fdx, ...]]:  nested: FileNotFoundException[segments_4ws]:  ]] Often users have resorted to deleting the shard and thus losing data. This by ensuring that the file is only ever written to one location. If you encounter this apparent index corruption in a running system, you can it by deleting all files called . It is advisory only, and Lucene can recover correctly without it.This release also includes a big speed up when calculating geo-distances. We have changed the distance calculation to use the and we use to calculate cosine and arcsin. Distance calculations are now 99.9% accurate and ! If you need absolutely accurate calculations, you can set the to instead of the new default .We hope you enjoy this new release. Please , and let us know what you think. ","locales":"","title":"Elasticsearch 0.90.10 Released"}
{"index":{}}
{"author":"Njal Karevoll","category":"","publish_date":"2014-01-07T00:00:00.000Z","url":"/blog/found-mapping-workflow","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud. In this article we explore a workflow for exploring new data via mapping refinements. We index an example document, look at its default mapping and iteratively improve on it to get us one step closer to our goal. ","locales":"","title":"A Data Exploration Workflow for Mappings"}
{"index":{}}
{"author":"Richard Pijnenburg","category":"Engineering","publish_date":"2013-12-20T00:00:00.000Z","url":"/blog/apt-and-yum-repositories","seo_title":"","content":" global wrapper ","locales":"","title":"Announcing DEB and RPM Repositories"}
{"index":{}}
{"author":"Clinton Gormley","category":"Engineering","publish_date":"2013-12-18T00:00:00.000Z","url":"/blog/0-90-8-released","seo_title":"","content":" global wrapper ","locales":"","title":"Elasticsearch 0.90.8 Released"}
{"index":{}}
{"author":"Clinton Gormley","category":"","publish_date":"2014-01-15T00:00:00.000Z","url":"/blog/1-0-0-rc1-released","seo_title":"","content":" Today we are happy to announce the release of , which is based on Lucene 4.6. This is our first (and hopefully last) release candidate before version 1.0.0 stable. You can . In the four years that Elasticsearch has been in development, it has accumulated some cruft: there are some inconsistent APIs and parameters. We are using this release to try to fix that. Our goal is that the user interface should be intuitive — you shouldn’t even need to consult the docs for common requests because the API should be obvious. While we try very hard to maintain backwards compatibility, some of these changes require us to break with the past. To help you migrate your application to 1.0, we have put together a . The full list of all changes is available in the . New features and enhancements An Elasticsearch release wouldn’t be complete without new toys, and this release is no different: Federated search The first new toy is the (experimental!) which joins multiple clusters and act as a federated client. Almost all operations are supported: distributed search, suggestions, percolation. You can even index into multiple clusters with the tribe node. Alternatively, you can set a tribe node to not allow any write operations, making it read-only. See the for more information. Scale and stability Several of our clients and users are using Elasticsearch at humongous scale, pushing the boundaries of what is possible. Their experiences have helped us to find the breaking points in Elasticsearch, and to improve them. The result is improved stability and scale for all of us. Cluster state processing (eg creating indices, mappings, nodes joining and leaving, shard allocation) has been streamlined and takes 5% of the time that it used to. Shard allocation and recovery has been improved and several bugs have been fixed. See , , , , , , , , , and . Memory usage and limits One of the biggest causes of instability in Elasticsearch is : field values have to be loaded into memory to make aggregations, sorting and scripting perform as fast as they do. Up until now, it was difficult to prevent this field data from using all available memory and throwing an OOM error. The new will throw an exception if you try to exceed the , which defaults to 80% of the heap size. You can read more about it in the . Other memory optimizations include the ability to We try to play nicely with the JVM to reduce garbage collection and to keep request latency low. This can be difficult to do when large temporary data structures are required to service requests. The new PageCacheRecycler provides us with pages of memory that can be reused for subsequent requests without interfering with the young generation heap. See and . Other new features What’s next This is a big release — it reflects just how many improvements we wanted to get into version 1.0 of Elasticsearch. Part of the changes include a greatly expanded test suite. But our test suite can never cover everything, so we need your help with verifying this release. Please download and test out and report any problems that you find. This will help us get to version 1.0 stable more quickly! ","locales":"","title":"1.0.0.RC1 released"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-01-08T00:00:00.000Z","url":"/blog/2014-01-08-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the GitHub repositories, as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. We've been out for a bit due to the end of the year holidays, so we have even more great information to share with you this week.Elasticsearch Core Elasticsearch EcosystemHere's some more information about what is happening in the ecosystem we are maintaining around Elasticsearch, including plugin and driver releases, as well as news about Logstash and Kibana. Slides & Videos Where to Find UsBelgium and will be attending on February 1st and 2nd. Stop by the Elasticsearch table to say hello!Czech RepublicHonza Kral will give two presentations at : and . Honza's presentations take place on Friday, February 7th, and the conference runs from the 7th through the 9th.France Germany JapanThanks to Jun Ohtani, the 3rd Elasticsearch Meetup will be held in Tokyo on February 7th starting at 7 PM. Please remember to .Netherlands will present From A to JSON - an Overview of Elasticsearch at the in Rotterdam. Doors open tomorrow night, January 9th, at 6 PM. United Kingdom will talk about What's New in Elasticsearch 1.0 at in London on January 15th. Attendance is free of charge, though registration is required. Doors open at 5 PM.United States Where to Find YouOur Community Manager, Leslie Hawthorn, is hard at work to help folks create more Elasticsearch meetup groups and to help meetup organizers find more speakers. If you are interested in either effort, .Oh yeah, we're also . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticsearch, Kibana and Logstash than where you rest your head.TrainingIf you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elasticsearch - January 08, 2014"}
{"index":{}}
{"author":"Aaron Mildenstein","category":"","publish_date":"2013-12-24T00:00:00.000Z","url":"/blog/logstash-1-3-2-released","seo_title":"","content":" Hello again friends! is now available! We recently implemented a new feature to help make Logstash and Elasticsearch get along more usefully (the new indexing template, it is awesome!). However, 1.3.0 had a bug that prevented it from actually working, and 1.3.1 had a different bug that caused the data to be stored in a way that made searches work inconsistently with previous releases. The 1.3.2 release has a few bug fixes and a big performance improvement for date processing (json codec, date filter, etc will all benefit here). You can to see the whole list. Some highlights: That's it for now! Happy Logstashing! ","locales":"","title":"Logstash 1.3.2 Released!"}
{"index":{}}
{"author":"Spencer Alger","category":"Engineering","publish_date":"2013-12-17T00:00:00.000Z","url":"/blog/client-for-node-js-and-the-browser","seo_title":"","content":" global wrapper ","locales":"","title":"Announcing Elasticsearch.js for Node.js and the Browser"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2014-01-16T00:00:00.000Z","url":"/blog/2014-01-16-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the GitHub repositories, as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. Elasticsearch core Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around Elasticsearch, including plugin and driver releases, as well as news about Logstash and Kibana. Slides & Videos Where to find UsBelgium will be speaking on What's New in Elasticsearch 1.0 with Aggregations at the in Brussels on Friday, January 31st. Doors open at 6 PM. and Honza Kral will be attending on February 1st and 2nd. Stop by the Elasticsearch table to say hello!Leslie Hawthorn will be speaking at the on DevOps: For Happier, More Productive People. Infrastructure.Next takes place on February 5th in Ghent.Czech Republic Honza Kral will give two presentations at : and . Honza's presentations take place on Friday, February 7th, and the conference runs from the 7th through the 9th. France Germany Japan Thanks to Jun Ohtani, the 3rd Elasticsearch Meetup will be held in Tokyo on February 7th starting at 7 PM. Please remember to . United Kingdom Elasticsearch will have two sessions at , which takes place March 3-7th. You can join for a tutorial on plus see and Graham Tackley co-present on . Make sure to stop by our booth to say hello! United States Where to Find You Our Community Manager, Leslie Hawthorn, is hard at work to help folks create more Elasticsearch meetup groups and to help meetup organizers find more speakers. If you are interested in either effort, . Oh yeah, we're also . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticearch, Kibana and Logstash than where you rest your head. Trainings If you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elasticsearch - January 16, 2014"}
{"index":{}}
{"author":"Konrad Beiske","category":"","publish_date":"2014-01-14T00:00:00.000Z","url":"/blog/found-bm-vs-lucene-default-similarity","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud. A follow up on the similarity article where we compare the precision and recall of the two models using Wikipedia articles. ","locales":"","title":"BM25 vs Lucene Default Similarity"}
{"index":{}}
{"author":"Leslie Hawthorn","category":"","publish_date":"2014-01-07T00:00:00.000Z","url":"/blog/ignore-filters-latest-feature-elmah-io-courtesy-elasticsearch","seo_title":"","content":" Today we’re bringing you another story of Elasticsearch in the field: elmah.io and its latest feature, . Elmah.io is cloud based error logger for .NET web applications. Thomas Ardal, one of their backend developers, was kind enough to share his story of implementing the latest functionality for elmah.io in just 30 minutes, all based on Elasticsearch’s . At , we’ve set a course to create the best cloud based error logging framework for .NET web applications:  simply add a NuGet package to your web project, and all of your website errors are logged to elmah.io. On the elmah.io site, you can search through all of your errors and logs using various search input. All of the errors we receive are indexed in a cluster of Elasticsearch instances, making it highly available and super-fast to search through all possible errors. Let’s talk about how we’ve used some of the nice features of Elasticsearch for some of our recently added functionality. Like any good company, we eat our own dog food and use elmah.io to perform all logging for our own website. One day, I noticed new errors in our logs with the user agent saying something like *bot*. You’ve probably seen something similar yourself:  Google or someone else is trying to request pages that don’t exist on the server or somehow manipulate the URL in an unintended way. We could sit down and implement special code handling bot requests, but what we really want is a way to tell elmah.io not to index certain types of errors that don’t yield useful, actionable information. That’s why we decided to build . In this blog post, I will tell you how we did that using C#, NEST and Elasticsearch’s Percolator API. In short, the Percolator API implements a sort of reverse query in Elasticsearch. Usually you index documents and query them. Using the Percolator, you index queries and then ask if Elasticsearch has queries which match documents – perfect use-case for implementing ignore queries! were something we had been thinking about for some time. We wanted a solution that was easy for our users to setup and possible for us to implement without using months of development time. After reading a blog post explaining the Percolator API in Elasticsearch, I got curious and, before I knew it, I had implemented a prototype of the feature. To get started, users need to input search queries telling elmah.io what errors to ignore. We built a new UI for this task: The user adds their own queries or chooses one of the templates defined at the bottom. Implementing this save in C# is easy using the wonderful client package for , written by Martijn Laarman: var connectionSettings = new ConnectionSettings(url):  connectionSettings.SetDefaultIndex(indexName):  var elasticClient = new ElasticClient(connectionSettings):  var registerPercolator = elasticClient.RegisterPercolator(id, p => p.QueryString(qs => qs.Query(query))):  We start by creating a new instance pointing to the index that we want to add the ignore filter to. Then, we call the method, which takes an ID that we generate from a random number, as well as a query. We’ve decided to let the user input Lucene queries in the UI, which uses the in Elasticsearch. This solution is not optimal, because users may incidentally write slow queries this way. There’s a new feature in recent versions of Elasticsearch called , which we plan to migrate to in the next version of elmah.io. And that’s it! The user is now able to register ignore filters, based on Lucene queries and the Percolator API. Each time we receive an error, we ask the Percolator for queries (ignore filters) matching the new error using NEST: var percolateResponse = elasticClient.Percolate(errorDocument):  if (percolateResponse.OK && percolateResponse.Matches.Any()) { return Request.CreateResponse(HttpStatusCode.OK):  } In text: if the percolate request were OK and the request actually returne","locales":"","title":"Ignore Filters: The Latest Feature in elmah.io, Courtesy of Elasticsearch"}
{"index":{}}
{"author":"Clinton Gormley","category":"Engineering","publish_date":"2013-12-23T00:00:00.000Z","url":"/blog/0-90-9-released","seo_title":"","content":" Today, just in time for the holidays, we are happy to announce the release of E, which is based on Lucene 4.6. This is the current stable release in the 0.90 series. You can download it .Unfortunately, 0.90.8 had a bug in the API which might report too few shards if one of your indices is status `red`. While this doesn't put your data at risk, it might cause sysadmins to suffer a moment of extreme panic.While we were about it, we found a bug in the which could, in rare cases, cause search requests to hang. And lastly, a bug was introduced in 0.90.8 which prevented index templates from being loaded when placed under the config directory.Apologies for the quick re-release, but we hope it makes your holiday more festive and relaxed. Thanks to those who reported these bugs so promptly. Please and let us know what you think. ","locales":"","title":"Elasticsearch 0.90.9 released"}
{"index":{}}
{"author":"Alex Brasetvik","category":"","publish_date":"2013-12-20T00:00:00.000Z","url":"/blog/found-elasticsearch-aggregations","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud. Elasticsearch's faceting feature has made it extremely popular not just for realtime search, but also for analytics. With its new aggregations framework, it'll take you even further. - <p>We’ve made <a href=\"https://www.found.no/play/gist/cab4eaf924717c5d1c92\">a runnable example in Play</a>. (See <a href=\"/blog/found-presenting-play/\">Presenting Play</a> for an introduction on how Play works). In the figure below is the resulting aggregation definition, a tree visualization, and parts of an example result:  showing how things get nested. </p> <p> <img src=\"/assets/bltb4f98197e5aeb7cf/stackoverflow-tree.svg?uid=bltb4f98197e5aeb7cf\" data-sys-asset-uid=\"blt651afbb922035410\" alt=\"A nested aggregation - the query, its tree and sample result\" style=\"max-width:100%: \" > </p><figcaption>A nested aggregation - the query, its tree and sample result</figcaption> <p>First, the <code>terms</code>-aggregation on the <code>tags</code>-field will produce one bucket per <code>tag</code>. The documents in each bucket are then fed to an <code>avg</code>-aggregator on the <code>comment_count</code> field for each document in the bucket. The documents are also sent to a <code>nested</code>-aggregator. The <code>nested</code>-aggregator pulls up the <code>comments</code> of each document in the bucket, and these inner comments are passed to a <code>terms</code>-aggregator. That <code>terms</code>-aggregator produces buckets of comments per author. Last, these are passed through a <code>max</code>-aggregator, to find the highest comment score. </p> <p>Many things are happening here, but if you think of how documents pass through a tree of aggregators step by step, complex problems can be broken down into a tree of simple operations. </p> </section> <section id=\"more-examples\" class=\"level2\"> <h2><a href=\"#more-examples\">More examples</a></h2> <p>We have made some <a href=\"/blog/found-presenting-play\">Plays</a> that contain various annotated examples. Check them out and have a play. You can adjust the documents and searches and press <code>Run</code> (or <code>Ctrl</code>+<code>Enter</code>) to experiment! When there are multiple searches in the example, they are delimited with <code>---</code> and show their results in different tabs (e.g. “Search #1”, “Search #2”, &hellip: ) </p> <p>The examples do not have lots of complex documents, but enough to demonstrate some concepts. (Note: As of this writing, there is <a href=\"https://github.com/elasticsearch/elasticsearch/pull/4472\">a bug related to sorting terms aggregations based on sub-aggregations</a>. Play should work, however. This will likely be fixed before 1.0.) </p> <ul> <li><a href=\"https://www.found.no/play/gist/8053573\">Find servers that report the most variance in CPU-usage</a>. Logstash now makes it quite easy to report metrics to Elasticsearch.</li> <li><a href=\"https://www.found.no/play/gist/8053633\">For ticket sales, find top-selling organizations, their most popular events and get a date histogram of revenue per event</a></li> <li><a href=\"https://www.found.no/play/gist/8053769\">HTTP request logs: Find hosts with slow requests, and per host what’s slow</a></li> <li><a href=\"https://www.found.no/play/gist/8053943\">Hierarchically faceting products by category and store</a></li> <li><a href=\"https://www.found.no/play/gist/cab4eaf924717c5d1c92\">The Stackoverflow example from above</a></li> </ul> </section> ","locales":"","title":"Elasticsearch's New Aggregations"}
{"index":{}}
{"author":"Andrew Cholakian","category":"","publish_date":"2013-12-17T00:00:00.000Z","url":"/blog/found-fuzzy-search","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud. Elasticsearch's Fuzzy query is a powerful tool for a multitude of situations. Username searches, misspellings, and other funky problems can oftentimes be solved with this unconventional query. In this article we clarify the sometimes confusing options for fuzzy searches, as well as dive into the internals of Lucene's FuzzyQuery. ","locales":"","title":"How to Use Fuzzy Searches in Elasticsearch"}
{"index":{}}
{"author":"Livia Froelicher","category":"","publish_date":"2013-12-24T00:00:00.000Z","url":"/blog/holiday-viewing-pleasure-facebook-elasticsearch","seo_title":"","content":" Our days are a bit quiet here at Elasticsearch this week, as we and many of our friends and colleagues spend time with our loved ones for the holidays. We won't be bringing you our usual update in , but didn't want to leave anyone hungry for Elasticsearch awesome. So, without further ado, we bring you this video from the , in which you'll learn more about Facebook's use of Elasticsearch, including:   You'll also hear from me on Elasticsearch 1.0 and enjoy some lively audience Q&A. Many thanks to Facebook for hosting us and sharing their Elasticsearch use cases. We hope you enjoy the video. what you think. Happy holidays to all of you celebrating them this week and next, and happy hacking on Elasticsearch! ","locales":"","title":"Facebook & Elasticsearch: for Your Holiday Viewing Pleasure"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2013-12-18T00:00:00.000Z","url":"/blog/2013-12-18-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the GitHub repositories, as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. Elasticsearch core Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around Elasticsearch, including plugin and driver releases, as well as news about Logstash and Kibana. Slides Where to find Us Israel The first ever will take place on December 30th, beginning at 7 PM. Join Elasticsearch core developer , who will present on what's new in Elasticsearch 1.0, and Itamar Syn-Hershko, who will discuss Monitoring online conversation at scale using Elasticsearch. France will talk about in Angers at the 14th of January. Netherlands will talk about Elasticsearch at in Rotterdam at the 9th of January. United Kingdom Mark Harwood will talk about Elasticsearch 1.0 at in London at the 15th of January. Where to Find You Our Community Manager, Leslie Hawthorn, is hard at work to help folks create more Elasticsearch meetup groups and to help meetup organizers find more speakers. If you are interested in either effort, . Oh yeah, we're also . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticsearch, Kibana and Logstash than where you rest your head. Training If you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elasticsearch - December 18, 2013"}
{"index":{}}
{"author":"Aaron Mildenstein","category":"Engineering","publish_date":"2013-12-17T00:00:00.000Z","url":"/blog/logstash-collectd-input-plugin","seo_title":"","content":" A few weeks ago I finished the initial release of the Logstash collectd input plugin. I'm really excited about this new feature! Some of you may be wondering why we would go to this effort, seeing how collectd data isn't exactly, well, . We consider any data that has a corresponding timestamp to be an event in Logstash. The data collectd sends is also timestamp + metric data. It will fit right alongside your other log data as a valuable companion. Now you can see if there's a corresponding change in disk I/O or CPU load when you see certain log entries, or vice versa! The possibilities are vast! I can't wait to hear how you use this plugin to help visualize your data! collectd configuration The simplest way to get started is to configure collectd. You may or may not already have a collectd setup where you are, but your configuration file could be as simple as this: Hostname \"host.example.com\" LoadPlugin interface LoadPlugin load LoadPlugin memory LoadPlugin network <Plugin interface> Interface \"eth0\" IgnoreSelected false </Plugin> <Plugin network> <Server \"10.0.0.1\" \"25826\"> </Server> </Plugin> These options will send collectd information consisting of cpu load, memory stats, and network traffic information via UDP to the IP 10.0.0.1 on port 25826. For your own setup just replace 10.0.0.1 with the IP or hostname of your Logstash instance. The collectd plugin will populate the Logstash event host field with whatever is in the “Hostname\" directive rather than what reverse lookup finds. If otherwise unset, the default configuration will send values every 10 seconds. You can learn more about collectd configuration . This example is only a tiny sample of the kind of plugins and configurations collectd has to offer. A comprehensive list can be found . As with Logstash, you can write your own plugins for collectd so there are virtually endless possibilities! Logstash configuration Now that we have our collectd ready to send let's configure our Logstash instance input { collectd {} } Yep, that's it. Pretty crazy, right? We try to come up with sane defaults for everything. The full configuration explanations are . Let me explain some important ones. So, what is the result of this little example? I ran this test on my 2013 MacBook Air, with the data coming from a nearby computer with 2 Ethernet ports and 16G of memory. This is traffic from a single box over a 1 hour period. As you can see from this graph the memory is broken down into blocks: , , , . The CPU load histogram should be fairly self-explanatory, as are the network I/O charts. You can clearly see peaks every 5 minutes in network traffic on EN1 with little other traffic usually. The version of Kibana I am using here (as of 16 Dec 2013) was downloaded straight from the master branch on GitHub and is not an officially released version. I needed this version as it enabled me to do derivative graphing, where each subsequent point is the difference between the current and previous values. This is necessary with network values as collectd measures them as it simply sends the counter values from the kernel store (similar to SNMP network data). If you need this feature now you too can use the most current development version. The release of Kibana coming in Jan 2014 (coinciding with ES 1.0 release) will have this feature. The stats themselves are unimpressive, seeing how they are from a personal server on home network with limited traffic. So I fired up , the collectd traffic generator. This was the result: Keeping in mind that this was tested on a MacBook Air I thought it was a pretty good show: 3000 events per second. If I had configured servers to send an average of 30 events every 10 seconds (3 events per second) that amounts to my laptop being able to process a continuous stream of events from 1000 servers! So, there's a brief introduction to the collectd plugin. Happy Logstashing! ","locales":"","title":"Logstash Collectd Input Plugin"}
{"index":{}}
{"author":"Aaron Mildenstein","category":"","publish_date":"2013-12-13T00:00:00.000Z","url":"/blog/new-in-logstash-1-3-elasticsearch-index-template-management","seo_title":"","content":" For a long time in the Logstash community we’ve been advising users to apply an index mapping template. There are a number of compelling reasons to do this, including: Not having that last option set has resulted in a near-constant stream of similar questions, “Why do the terms break up on hyphens?” and others of this sort. To answer these questions and prevent future questions like it from being asked, it became one of the first tasks I was assigned to work on as an employee at Elasticsearch. Important note The plugin uses Java API calls to manage the template, while the plugin uses REST API calls. With this comes an important caveat. In order to use template management with the plugin you must be using version 0.90.5 or newer. The Java API calls did not exist prior to that. If you attempt to use Logstash v1.3+ with a version of Elasticsearch older than 0.90.5 with the output plugin the template management features will not work and there will be a stack trace in the log files indicating the absence of those API calls. The REST API has no such constraints. If you are using the output with an older version of Elasticsearch it will still attempt to assign the template. Some new template options may not exist in very old versions of Elasticsearch, so be sure to upgrade. Upgrading is good. You want all of the performance benefits and new features that come with new releases, right? Configuration options. Common to both the and plugin are the following options : The option is boolean and is only used to disable the automatic template feature since it is on by default. Who would use this feature? Why wouldn’t you want the awesomeness of automatic template management? One such reason might be that you have dynamically named indices. For example, if you want a different index name for production logs than staging, but in the same Elasticsearch cluster, you could configure that in Logstash: output { elasticsearch { cluster => \"mycluster\" manage_template => false index => \"%{segment_name}-logstash-%{+YYYY.MM.dd}\" } } In this example the index is determined in part by date, and in part by a variable, . When using complex index names we recommend setting the template manually. The option determines what name the template will be stored as in Elasticsearch. The default is . There is an important caveat to note with this setting. If you change the option in a fully configured and running system you’ll still have a template stored under that name. In this case you may want to clean out the old template so it’s not left around. curl -XDELETE http://localhost:9200/_template/OLD_template_name?pretty where is the previous template name. The configuration option is useful if you want to use the template engine but provide your own template instead of the included one. An example might be: output { elasticsearch { cluster => \"mycluster\" template => \"/path/to/mytemplate.json\" } } Because the default is to manage templates, this configuration would read the JSON from the indicated file and attempt to upload the template if there isn’t one already there. This brings me to… The option will always overwrite the indicated template in Elasticsearch with either the one indicated by or the included one. This option is set to by default. If you always want to stay up to date with the template provided by Logstash, this option could be very useful to you. Likewise, if you have your own template file managed by puppet, for example, and you wanted to be able to update it regularly, this option could help there as well. Conclusion Hopefully this helps make the configuration options more clear. Happy Logstashing! ","locales":"","title":"New in Logstash 1.3: Elasticsearch Index Template Management"}
{"index":{}}
{"author":"Alexander Reelsen","category":"","publish_date":"2013-12-11T00:00:00.000Z","url":"/blog/2013-12-11-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the GitHub repositories, as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. Also, do not miss our webinar tomorrow! Elasticsearch core Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around Elasticsearch, including plugin and driver releases, as well as news about Logstash and Kibana. Slides Where to find UsFrance will speak at the . David's talk will cover all things Elasticsearch, including new features in 1.0. Doors open at 7:15 PM. Israel The first ever will take place on December 30th, beginning at 7 PM. Join Elasticsearch core developer , who will present on what's new in Elasticsearch 1.0, and Itamar Syn-Hershko, who will discuss Monitoring online conversation at scale using Elasticsearch. Netherlands will talk about Elasticsearch at in Rotterdam at the 9th of January. Where to Find You Our Community Manager, Leslie Hawthorn, is hard at work to help folks create more Elasticsearch meetup groups and to help meetup organizers find more speakers. If you are interested in either effort, . Oh yeah, we're also . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticearch, Kibana and Logstash than where you rest your head. Trainings If you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elasticsearch - December 11, 2013"}
{"index":{}}
{"author":"Alex Brasetvik","category":"","publish_date":"2013-12-09T00:00:00.000Z","url":"/blog/found-elasticsearch-security","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud.A Brief Overview of Running Elasticsearch SecurelyElasticsearch does not perform authentication or authorization, leaving that as an exercise for the developer. This article gives an overview of things to keep in mind when you configure the security settings for your Elasticsearch cluster, providing users with (limited) access to your cluster when you cannot necessarily (entirely) trust them. : Elastic has released , a product which provides comprehensive security for Elasticsearch, including encrypted communications, role-based access control, AD/LDAP integration and Auditing. The following article was authored before Shield was available. ","locales":"","title":"Securing Your Elasticsearch Cluster"}
{"index":{}}
{"author":"Jordan Sissel","category":"Engineering","publish_date":"2013-12-12T00:00:00.000Z","url":"/blog/logstash-1-3-1-released","seo_title":"","content":" Hello friends! We have released with lots of new fixes and features. You can , but I’d like to highlight two of the new features, both of which were implemented by . First, in our mission to make it easy to integrate with lots of tools, we have a new input plugin, collectd. This plugin lets you receive metrics from agents with logstash and ship them anywhere you want. Want to know more? Check out the . Second, as a way to make logstash have the best possible default behavior for the most users, we now have logstash providing its own . This index template was built to try and solve some common problems for users by using better default analyzers and mappings specific to logging use cases. To demonstrate how this new feature works, let’s look at some apache logs in Kibana. I’ve parsed these apache logs with the following logstash config: filter { grok { match => { \"message\" => \"%{COMBINEDAPACHELOG}\" } } date { match => [ \"timestamp\", \"dd/MMM/yyyy:HH:mm:ss Z\" ] } } The filter configuration above uses grok with the built-in apache log pattern to parse apache logs into separate fields such as the request path, http response code, bytes sent, user agent, etc. The second filter takes the original timestamp field in the apache log and parses it to be used as the canonical timestamp of the event – this gives you more accurate search results over time and also lets you ingest old log data correctly. Now, a common search pattern is to ask for the top N of something. In Kibana, you can either use the ‘top N’ query or you can use a pie chart, depending on your goals. In this example, I’ll just use a pie chart. Adding a pie chart with mode ‘terms’ on the ‘request’ field gets me this: Most folks, in this situation, sit and scratch their heads, right? I know I did the first time. I’m pretty certain “docs” and “centralized” aren’t valid paths on the logstash.net website! The problem here is that the pie chart is built from a . With the default text analyzer in elasticsearch, a path like “/docs/1.3.1/filters/” becomes 3 terms {docs, 1.3.1, filters}, so when we ask for a terms facet, we only get individual terms back! Index templates to the rescue! The we provide adds a “.raw” field to every field you index. These “.raw” fields are set by logstash as “not_analyzed” so that no analysis or tokenization takes place – our original value is used as-is! If we update our pie chart above to instead use the “request.raw” field, we get the following: Much better! And because we still index both the terms and the not-analyzed parts for each field, you can still do simple term queries like “request:docs” to find all requests with ‘docs’ in the text. I hope this helps explain the new feature. Happy logstashing! ","locales":"","title":"Logstash 1.3.1 Released!"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2013-12-04T00:00:00.000Z","url":"/blog/2013-12-04-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the GitHub repositories, as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. Elasticsearch core Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around Elasticsearch, including plugin and driver releases, as well as news about Logstash and Kibana. Slides Where to find UsFrance Israel The first ever will take place on December 30th, beginning at 7 PM. Join Elasticsearch core developer , who will present on what's new in Elasticsearch 1.0, and Itamar Syn-Hershko, who will discuss Monitoring online conversation at scale using Elasticsearch. Netherlands will present at the Open Source Conference in Amsterdam on December 6th. Shay's talk will start at 4:05 PM. United States Where to Find You Our Community Manager, Leslie Hawthorn, is hard at work to help folks create more Elasticsearch meetup groups and to help meetup organizers find more speakers. If you are interested in either effort, . Oh yeah, we're also . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticearch, Kibana and Logstash than where you rest your head. Trainings If you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elasticsearch - December 04, 2013"}
{"index":{}}
{"author":"Steven Schuurman","category":"News","publish_date":"2013-12-03T00:00:00.000Z","url":"/blog/how-the-world-is-using-elasticsearch","seo_title":"","content":" It's been just over a year since Elasticsearch Inc was born, and about 3.5 years since our CTO, Shay Banon, first released Elasticsearch. From that time to now, we've brought almost 40 developers on board to grow and maintain the ecosystem around Elasticsearch, Logstash and Kibana, and we've seen our software downloaded almost half a million times per month. Just a week ago, we hit the memorable milestone of 5,000,000 downloads, an impressive number by anyone's standard. The fact that nearly 4,000,0000 of those were downloaded over the last 12 months, and that number is growing at a rate of nearly 500,000 a month makes it even more special. Elasticsearch, Logstash and Kibana are on fire, baby! Members of all startups will consistently tell you how proud they are of their world-changing technology, phenomenal growth and market traction, and we are - of course - no exception. However, we'd like you to hear these things from our customers and our users, not from us. To that end, we've had a robust case study program going for the past few months, leading to some pretty amazing stories of how Elasticsearch has helped companies create new revenue streams, improve their customer interaction, create stellar new application functionality and save money. Today, we'd like to share some of these case studies with you and, more importantly, to ask for you to let us know how you're using Elasticsearch.   Elasticsearch Powers Social, ECommerce, Education and Much More… We've seen a number of compelling use cases for Elasticsearch across the full spectrum of social companies, including: Klout If you're one of the many household names using Klout to create campaigns that target social influencers, you're using Elasticsearch. at this case study to find out how Elasticsearch made it possible for Klout to provide their forthcoming self-service option to their customers, which Klout predicts will allow them to at least double their current revenues. XING XING is the leading business social network in Europe, with half its users located in Germany and the other half throughout the rest of Europe, Asia and Australia. XING has called their relationship with Elasticsearch a strategic partnership, far beyond a simple customer and service provider relationship. about how we've forged these deep ties with our customer by enabling XING to keep their users' updates flowing in real-time. GitHub Elasticsearch empowers GitHub's 4 million 'social coders' through providing search across GitHub's 8 million + code repositories. The GitHub team also makes use of Elasticsearch to monitor for abuse using some fairly clever logging hacks. You can get all the details in our .   Focusmatic Focusmatic provides its customers with brand insights from across all aspects of the social web: blogs and micro-blogging sites, video content, news outlets and social networks. Thanks to Elasticsearch, Focusmatic was able to scale to meet the needs of their current customers and, more significantly, be prepared to meet the needs of many more new customers, all at 75% less cost. how Elasticsearch made it possible for Focusmatic to “continue their business.\" Cogenta Cogenta provides retail market data to high-end brand retailers in Europe, with plans currently underway to expand into 20 countries worldwide. Their real-time pricing intelligence platform tracks and analyzes competitive data on more than 20 million products. at how Elasticsearch helped Cogenta prepare for this customer base expansion at a 40% lower total cost of ownership. Datadog Datadog is a SaaS monitoring service startup for IT, operations and development teams that enables these teams to better analyze metrics and events, ensuring their operations continue smoothly. By using Elasticsearch, Datadog was able to scale to take on larger customers who produce 500x more events than Datadog could originally handle. You can how Elasticsearch helped Datadog increase their event handling cap","locales":"","title":"How the World is Using Elasticsearch"}
{"index":{}}
{"author":"Clinton Gormley","category":"Engineering","publish_date":"2013-12-02T00:00:00.000Z","url":"/blog/1-0-0-beta2-released","seo_title":"","content":" Today we are delighted to announce the release of , the second beta release on the road to 1.0.0 GA. The new features we have planned for 1.0.0 have come together more quickly than we expected, and this beta release is chock full of shiny new toys. Christmas has come early! We have added: Please , try it out, break it, figure out what is missing and . Our next release will focus on cleaning up inconsistent APIs and usability, plus fixing any bugs that are reported in the new functionality, so your early bug reports are an important part of ensuring that 1.0.0 GA is solid. This is a beta release – it is not production ready, features are not set in stone and may well change in the next version, and once you have made any changes to your data with this release, it will no longer be readable by older versions! Snapshot / Restore While it has always been possible to backup a live index on Elasticsearch, the process was a bit finicky. The long awaited snapshot/restore API makes it easy. First, define a  — the place where your backups will live: curl -XPUT 'http://localhost:9200/_snapshot/my_backup' -d '{ \"type\": \"fs\", \"settings\": { \"location\": \"/mount/backups/my_backup\" } }' Currently we support the shared filesystem () repository, which needs to be writable by all nodes. In the future we will add support for S3, HDFS, GlusterFS, Google Compute Engine and Microsoft Azure. With the repository in place, you can tell Elasticsearch to create a snapshot named with: curl -XPUT localhost:9200/_snapshot/my_backup/snapshot_1 You can snapshot the whole cluster or just specific indices. The best part is that  — it only copies the segments that have changed since the last snapshot was made. This makes the snapshotting process faster and lighter (you can snapshot every 5 minutes if you want to) and uses up less storage in the repository. You can restore the whole cluster, with or without persistent cluster settings, or just individual indices: curl -XPOST localhost:9200/_snapshot/my_backup/snapshot_1/_restore Later on we plan on making cross data-center replication possible by adding the ability to do incremental restores into a read-only index. You can . API JSON is a great… for computers. But at 3 AM when you’re trying to figure out what is happening in your cluster, we humans prefer a simpler text format, easier to read and use with command line tools like and . The new API is the system administrator’s friend:  — it formats the results in a columnar fashion for easy reading and parsing. For instance, to find out which indices are status : curl localhost:9200/_cat/indices | grep ^yell yellow foo 5 1 4 0 17kb 17kb Column headers are not returned by default, but you can request them by adding the parameter to the query string: curl localhost:9200/_cat/recovery?v index shard target recovered % ip node wiki1 2 68083830 7865837 11.6% 192.168.56.20 Adam II wiki2 1 2542400 444175 17.5% 192.168.56.20 Adam II Above we can see that two shards are recovering after a node failure, and that they are 12% and 18% complete, respectively. There are endpoints for many admin APIs, including allocation, indices, nodes, and shards. See the for details. Aggregations Aggregations are “facets reborn”. Facets are amazingly powerful — they allow you to summarise vast amounts of data in the context of a user’s query on the fly, without the need for slow batch precalculations. However, the way they are implemented limits them in two important ways: Aggregations change all this. There are two types of aggregation (or ): aggregators which allow you to divide documents up into separate buckets eg , , , , , etc and aggregators which perform some calculation on the documents in each bucket, eg , , etc. Buckets can be sub-divided into smaller buckets, and metrics can be calculated for any bucket at any level. In the following example, we run a query looking for all tweets which mention , count the most popular hashtags overall, count t","locales":"","title":"1.0.0.Beta2 Released"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2013-11-27T00:00:00.000Z","url":"/blog/2013-11-27-this-week-in-elasticsearch","seo_title":"","content":" Welcome to  . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the GitHub repositories, as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. Elasticsearch core Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around Elasticsearch, including plugin and driver releases, as well as news about Logstash and Kibana. Slides Where to find Us France David Pilato will speak at Drupagora on December 5th, covering . David's talk will begin at 3 PM. Netherlands will present at the Open Source Conference in Amsterdam on December 6th. Shay's talk will start at 4:05 PM. Spain will speak at the NoSQL matters conference in Barcelona on . Clinton's talk takes place on Saturday, November 30th at 10:30 AM, and the conference runs November 29-30th. United Kingdom United States Where to Find You Our Community Manager, Leslie Hawthorn, is hard at work to help folks create more Elasticsearch meetup groups and to help meetup organizers find more speakers. If you are interested in either effort, . Oh yeah, we're also  . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticearch, Kibana and Logstash than where you rest your head. Trainings If you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This week in elasticsearch - November 27, 2013"}
{"index":{}}
{"author":"Adrien Grand","category":"Engineering","publish_date":"2013-11-28T00:00:00.000Z","url":"/blog/disk-based-field-data-a-k-a-doc-values","seo_title":"","content":" Elasticsearch is not just about full-text search, and many users are actually not using Elasticsearch for full-text search at all but for analytics though facets. This approach works well, but, as you probably know, faceting or sorting on a field requires loading field values into in-memory data structures that we call . It is very common that takes several (tens of) gigabytes of memory. Memory is rather cheap, so it is usually not a problem to get boxes with enough memory. However, this can raise issues at the JVM level: major garbage collections on a heap of several tens of gigabytes can easily take several seconds during which your application will be unresponsive. Careful JVM tuning can help prevent this issue, but ideally field data should be stored .Doc Values to the Rescue is a feature that will be available in the forthcoming Elasticsearch 1.0 release. You can already check it out in our . are a Lucene 4.x feature which allow for storing field values on disk in a column stride fashion, which is filesystem cache friendly and suitable for custom scoring, sorting or faceting, exactly like field data. It was only natural to build a new field data backend based on , and this new implementation has several benefits compared to the traditional field data implementations that Elasticsearch builds by uninverting the inverted index: On the other hand, are going to make indices bigger — unless it allows for not indexing the field, eg. if the field is used solely for sorting — and intensive work loads such as faceting will be slightly slower.When Should I Use Doc Values? can be used as a drop-in replacement for uninverted in most cases, but there are a few cases where they can be particularly helpful: How to Enable Doc Values are an index time decision, so they need to be enabled in the mappings before indexing the first document. Here is an example of a string field definition that can be used for sorting and faceting, but not searching:\"my_field\": { \"type\": \"string\", \"index\": \"no\", \"fielddata\": { \"format\": \"doc_values\" } } For now, doc values are supported on non-analyzed string fields and numeric fields (byte, short, integer, long, float, double, date).As you can see, fields don't need to be indexed to enable . And once are enabled, all operations working on top of like sorting or faceting will transparently use under the hood.We would love to get your feedback on . Try out our latest beta release and ! ","locales":"","title":"Disk-Based Field Data a.k.a. Doc Values"}
{"index":{}}
{"author":"Konrad Beiske","category":"","publish_date":"2013-11-26T00:00:00.000Z","url":"/blog/found-similarity-in-elasticsearch","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud.A Brief Introduction to the Similarity Models AvailableElasticsearch now supports replacement of the default similarity model. In this article we will look into what a similarity model is with a special focus on tf-idf and bm25. ","locales":"","title":"Similarity in Elasticsearch"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2013-11-21T00:00:00.000Z","url":"/blog/2013-11-2-this-week-in-elasticsearch","seo_title":"","content":" Welcome to  . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the GitHub repositories, as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. Elasticsearch core Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around Elasticsearch, including plugin and driver releases, as well as news about Logstash and Kibana. Slides Where to find Us France David Pilato will speak at Drupagora on December 5th, covering . David's talk will begin at 3 PM. Netherlands Norway and will speak at the on November 26th. Full details on their talks and other presentations by community members in Oslo will be published next week. Spain will speak at the NoSQL matters conference in Barcelona on . Clinton's talk takes place on Saturday, November 30th at 10:30 AM, and the conference runs November 29-30th. United Kingdom Where to Find You Our Community Manager, Leslie Hawthorn, is hard at work to help folks create more Elasticsearch meetup groups and to help meetup organizers find more speakers. If you are interested in either effort, . Oh yeah, we're also  . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticearch, Kibana and Logstash than where you rest your head. Trainings If you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elasticsearch - November 21, 2013"}
{"index":{}}
{"author":"Florian Hopf","category":"","publish_date":"2013-11-13T00:00:00.000Z","url":"/blog/found-sense-a-cool-json-aware-interface-to-elasticsearch","seo_title":"","content":"  This article will introduce you to Sense, a Chrome plugin for Elasticsearch. It offers autocompletion, code highlighting and formatting and can help you with exploring Elasticsearch. ","locales":"","title":"Sense - a Cool JSON Aware Interface to Elasticsearch"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2013-11-14T00:00:00.000Z","url":"/blog/2013-11-14-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the GitHub repositories, as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project.Elasticsearch Core Elasticsearch EcosystemHere's some more information about what is happening in the ecosystem we are maintaining around Elasticsearch, including plugin and driver releases, as well as news about Logstash and Kibana. SlidesCheck out presentations from Lucene revolutions, talking about and . Also at the ruby user group in San Francisco. Last, talked about using .Where to Find UsAustraliaClinton Gormley will be speaking at the inaugural on November 18th and the on November 21st. Clinton will cover new features forthcoming in 1.0 and will be joined by other speakers from the Melbourne and Sydney community. We'll have full details for you next week.BelgiumIf you're heading to Devoxx Belgium 2013, be sure to check out on Elastify Your App: From SQL to NoSQL in Less than One Hour. David will speak on Wednesday, November 13th at 3:10 PM in Room 6. David will also give a lightning talk, Make Sense of Your (BIG) Data on Thursday, November 14th at 1:10 PM in Room 9. Devoxx Belgium runs from November 11-15 at the Metropolis Business Center in Antwerp.France Germany organizes the , and will be hosting a meetup on November 20th. The meetup begins at 6:45 PM.NetherlandsElasticsearch will be at the on November 20-22nd in Amsterdam. Stop by our table to say hello, and make sure to catch and Gerard de Vos' workshop on Elasticsearch, taking place on Wednesday, November 20th. Also you can easily join the on the 20th as well in the evening. Even if you attend the Cloudstack Collaboration Conference, feel free to come over, as the event starts a bit later.Norway and will speak at the on November 26th. Full details on their talks and other presentations by community members in Oslo will be published next week.Spain will speak at the NoSQL matters conference in Barcelona on . Clinton's talk takes place on Saturday, November 30th at 10:30 AM, and the conference runs November 29-30th.United Kingdom United States Where to Find YouOur Community Manager, Leslie Hawthorn, is hard at work to help folks create more Elasticsearch meetup groups and to help meetup organizers find more speakers. If you are interested in either effort, .Oh yeah, we're also . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticearch, Kibana and Logstash than where you rest your head.TrainingsIf you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elasticsearch - November 14, 2013"}
{"index":{}}
{"author":"Alex Brasetvik","category":"","publish_date":"2013-11-13T00:00:00.000Z","url":"/blog/found-presenting-play","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud. ","locales":"","title":"Presenting Play: A Preview of an Elasticsearch Playground"}
{"index":{}}
{"author":"","category":"Engineering","publish_date":"2013-11-25T00:00:00.000Z","url":"/blog/percolator-redesign-blog-post","seo_title":"","content":" The percolator is essentially search in reverse, which can by confusing initially for many people. This post will help to solve that problem and give more information on the redesigned percolator. We have added a lot more features to it to help users work with percolated documents/queries more easily. In normal search systems, you store your data as documents and then send your questions as queries. The search results are a list of documents that matched your query. With the percolator, this is reversed. First, you store the queries and then you send your 'questions' as documents. The percolator results are a list of queries that matched the document. So what can do percolator do for you? The percolator can be used for a number of use cases, but the most common is for alerting and monitoring. By registering queries in Elasticsearch, your data can be monitored in real-time. If data with certain properties is being indexed, the percolator can tell you what queries this data matches. For example, imagine a user \"saving\" a search. As new documents are added to the index, documents are percolated against this saved query and the user is alerted when new documents match. The percolator can also be used for data classification and user query feedback. So how does it work in Elasticsearch? Data and queries are two separate things, but are both expressed in JSON. By using this common property, we can send queries to Elasticsearch and they will be stored as documents...but that alone isn't enough. Elasticsearch needs to treat it as a query, not a document. The current percolator system stores queries in a dedicated index. Whenever queries are registered with the percolator index, the query part is extracted and turned into a real query, which is kept around for later usage via the percolate api. The percolate api is the equivalent to the search api for percolating. Also the percolate api is one of the APIs that is fully real-time, meaning that as soon as a query is registered with the percolator it available for use. Let take a look at the percolator already available in many Elasticsearch releases: # Register a query for <= 0.90.x percolator. curl -XPUT 'localhost:9200/_percolator/my-index/my-id' -d '{ \"query\" : { \"match\" : { \"body\" : \"coffee\" } } }' # <= 0.90.x percolate api curl -XPUT 'localhost:9200/my-index/my-type/_percolate' -d '{ \"doc\" : { \"title\" : \"Coffee percolator\", \"body\" : \"A coffee percolator is a type of ...\" } }' The system index that is used to register a query as shown in the first request in the above sample is a single primary shard index that has replica shards on each data node and these settings are fixed. The last request in the above sample sends a percolate request to Elasticsearch and will yield the following result: { \"ok\" : true \"matches\" : [\"my-id\", ...] } This image below illustrates how a client executes a percolate request. It doesn't matter where the percolate request is executed, because each data node has a _percolate shard (p1 squares), that sits next to all the other shards (other squares) a data node may have. The current percolator has been around since ES version and since then more people have started using it with more and more queries. The percolator works fine until a certain amount of queries are store in it, but then the percolate execution time begins to increase to a level where people are less comfortable using it in production. It scales roughly linear to the number of registered queries. In order to get around this limitation, queries can be partitioned against multiple indices or the percolate query mechanism can be used in order to reduce the percolator execution time. Even with these workarounds, the current percolator has fundamental scaling limits, which becomes obvious when people are percolating massive amounts of queries. So we had to go back to the drawing board and come up with a different solution for how to scale the percolator. The good news is that we did and came up with a","locales":"","title":"Redesigned Percolator"}
{"index":{}}
{"author":"Andrew Cholakian","category":"","publish_date":"2013-11-19T00:00:00.000Z","url":"/blog/found-keeping-elasticsearch-in-sync","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud. One of the trickiest parts of integrating Elasticsearch into an existing app is figuring out how to manage the flow of data from an authoritative data source, such as an SQL database, into Elasticsearch. In most cases this means utilizing Elasticsearch's bulk API. It also implies designing an application to effectively make data available in an efficient, robust, on-time manner. This usually requires modifying an application's workflow to replicate data in batches. This article is a survey of various patterns for accomplishing this task. ","locales":"","title":"Keeping Elasticsearch in Sync"}
{"index":{}}
{"author":"Clinton Gormley","category":"","publish_date":"2013-11-13T00:00:00.000Z","url":"/blog/0-90-7-released","seo_title":"","content":" Today we are happy to announce the release of , which is based on Lucene 4.5.1. This is the current stable release in the 0.90 series and we recommend upgrading. You can download it . This is largely a bug fix release. Most changes deal with edge cases (you can read about them in the ) but there are a few important fixes which are worth mentioning: Norms and memory use In an effort to make Elasticsearch as responsive as possible, 0.90.6 introduced a change which loaded all into memory whenever a new merged segment was created. For some users, this turned out to be an optimization too far. Field norms provide statistics about field length and are an important variable in TF/IDF, Lucene’s default similarity algorithm. However, they are only useful when you actually run a full-text query on a field. Many users, especially our Logstash users, store many text fields in Elasticsearch which they never search on, which means that we never need to load the norms for these fields. This “eager” loading of norms used up much more RAM than before. In 0.90.7 we reverted this change (see ) and in a later version we will add a per-field option which will allow you to opt in to eager norms loading, where it makes sense for you. See . Lucene and memory use A was causing a delay in cleaning up old files which manifested itself in increased memory use, especially when using the store. This bug has been fixed in Lucene 4.6 and the fix has been backported to this release as well. See . Postings highlighter The newly added Postings highlighter got off to a false start. Highlighting worked just fine... until there was more than one segment. (Note: you can never have too many tests). See . It also failed on fields whose index name and field name were different. See . And finally, an enhancement! The postings highlighter now also supports a separate . See . Other Details of other changes are available in the . Many thanks to everybody who submitted bug reports and pull requests. Your help is greatly appreciated. ","locales":"","title":"0.90.7 released"}
{"index":{}}
{"author":"Leslie Hawthorn","category":"","publish_date":"2013-11-12T00:00:00.000Z","url":"/blog/building-enterprise-content-management-systems-on-elasticsearch","seo_title":"","content":" Today we’re bringing you another story of Elasticsearch’s use in the field: Vodori and Pepper, their home grown enterprise content management system based on Alfresco and Elasticsearch. is a full service digital agency that blends Strategy, Design, Technology, and Product to deliver high-velocity digital global marketing solutions. They are based in Chicago, Illinois, US and they’re the gracious hosts of the . Grant Gochnauer Vice President and Co-Founder of Vodori was kind enough to share their story with us. Two and a half years ago, Vodori set out to redesign its enterprise content management system, Pepper, to delivery high-velocity digital marketing to our customers. With a global scale and complex content management and distribution requirements, Pepper had to be flexible, scalable, and of course provide rich search capabilities. In 2010, we evaluated technologies that would enable us to meet our product goals and decided on two core technologies: Alfresco and Elasticsearch. Why Elasticsearch? Before diving into our decision to use Elasticsearch, it helps to understand a bit more about some of our requirements. Content management systems base their data storage on an allowing developers to specify how content types are defined and relate to one another in a system. For example, we might have an object called “Document” that contains properties such as “title” and “filename”. We can then extend “Document” with a child object in our object model – a “WebDocument”. It has additional properties such as “seoDescription” and “url”. “WebDocument” inherits the properties defined on “Document”. Pepper provides an object model as part of the core product but allows (and encourages) implementation teams to extend this model depending on the business requirements for the solution being delivered. Alfresco provides its own “object model” out of box and we extend this concept significantly for Pepper in our searching paradigm. Because project implementations can contain widely varied content types and properties, we have to support the myriad ways our customers may want to search and filter content. While Alfresco implements CMIS, the industry standard for interacting with content data (creating, updating, searching), the performance did not meet Pepper’s specific needs. Our challenge: how to get content out of Alfresco (and by extension Pepper) in such a way that: Enter Elasticsearch. After evaluating a number of technology options, we selected Elasticsearch because it met our requirements and also because of the trust we had in the people and product gained through the successful use of Shay Banon’s previous technology, Compass. By introducing this facade layer on top of Alfresco, we not only introduced a high-performance and scalable way to interact with our data, it also gave us complete control of how we search content. Let’s look at an example: Pepper Library One of the many ways users can find and interact with content is through the Pepper Library. We allow customers to toggle a number of parametric filters as the results appear through an endless scroll list. Elasticsearch provides everything we need to implement this user experience out of the box. When users toggle filters along the left panel, we simply construct the appropriate Elasticsearch query using the TermsFilter combined with a BoolFilter and Elasticsearch immediately provides the data we need. We also provide for free-text search by using the search box in the upper right corner of the Library. Thankfully this is also extremely easy to do with Elasticsearch – even when searching with the left panel filters applied. If a user types in a search query, we simply append a Match Query to the existing BoolFilter in our previous query. Elasticsearch happily returns what we asked for. We can further customize the way free-text search works b","locales":"","title":"Building Enterprise Content Management Systems on Elasticsearch"}
{"index":{}}
{"author":"Luca Cavanna","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2013-11-06T00:00:00.000Z","url":"/blog/2013-11-06-this-week-in-elasticsearch","seo_title":"","content":" Welcome to  . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the GitHub repositories, as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. Elasticsearch core Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around Elasticsearch, including plugin and driver releases, as well as news about Logstash and Kibana. Slides Check out presentation on and slides on . Both presentations were delivered at this week's . Where to find UsAustralia Clinton Gormley will be speaking at the inaugural on November 18th and the on November 21st. Clinton will cover new features forthcoming in 1.0 and will be joined by other speakers from the Melbourne and Sydney community. We'll have full details for you next week. Belgium If you're heading to Devoxx Belgium 2013, be sure to check out on . David will speak on Wednesday, November 13th at 3:10 PM in Room 6. David will also give a lightning talk, on Thursday, November 14th at 1:10 PM in Room 9. Devoxx Belgium runs from November 11-15 at the Metropolis Business Center in Antwerp. Estonia will explore at on Thursday, November 7th at 3:30 PM. Topconf runs November 6th and 7th in Tallinn. France Germany Ireland will present at on Thursday, November 7th at 2:45 PM. Lucene Revolution takes place November 4th - 7th in Dublin. Japan The Elasticsearch User Group in Japan will meet for a on November 12th in Tokyo. The meeting begins at 7 PM. Netherlands Elasticsearch will be at the on November 20-22nd in Amsterdam. Stop by our table to say hello, and make sure to catch and Gerard de Vos' workshop on Elasticsearch, taking place on Wednesday, November 20th. Norway and will speak at the on November 26th. Full details on their talks and other presentations by community members in Oslo will be published next week. Spain will speak at the NoSQL matters conference in Barcelona on . Clinton's talk takes place on Saturday, November 30th at 10:30 AM, and the conference runs November 29-30th. United Kingdom United States Where to Find You Our Community Manager, Leslie Hawthorn, is hard at work to help folks create more Elasticsearch meetup groups and to help meetup organizers find more speakers. If you are interested in either effort, . Oh yeah, we're also  . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticearch, Kibana and Logstash than where you rest your head. Trainings If you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elasticsearch - November 06, 2013"}
{"index":{}}
{"author":"Rashid Khan","category":"","publish_date":"2013-11-05T00:00:00.000Z","url":"/blog/kibana-3-milestone-4","seo_title":"","content":" Kibana 3: Milestone 4 has been released and brings with it a wealth of performance, usability and visualization enhancements. Let’s take a look at a few of the significant changes. If you’re still on Milestone 3, check out for even more new features. An all new look Kibana’s panels have been overhauled to feature more prominent labeling, easier to use buttons and links, and an entirely new style. The changes result in improved usability, as well as a space efficient design that allows for great data density and a more consistent UI. Kibana’s new look Consistent query and filter placement To improve UI harmony, the query and filter panels get their own collapsible, pull-down sections right under the navigation bar. There is no longer a need to place these essential panels yourself, as they’re included in every dashboard by default. Like many of Kibana’s features, they can be disabled entirely from the dashboard configuration dialog. 100% new time range selector If you’re familiar with Kibana’s 2 year history, you might know that there have been several time picker evolutions. The new time picker is a total rewrite that not only uses less space than the old one, but is simpler and easier to use. By moving this important component off the main dashboard, Kibana now has more room to devote to important data and charts. Plus, the new filter format implements Elasticsearch’s so there’s no need to reselect a time frame to move your time window:  every search will update the window automatically. An all new time picker Filterable field list Find fields quickly and easily with the type-to-filter feature of the table. Filterable field lists Ad-hoc facets And, once you’ve found those field,s analyze them quickly with ad-hoc terms facets. Simply click a field and select a visualization to see the top 10 term matches for that field. Exploration is even easier No need to add a panel. Pie chart on the fly! Dynamic dashboards and URL parameters Kibana 3: Milestone 4 can now take input via URL parameters! This much anticipated feature comes in two flavors: templated dashboards and scripted dashboards. Kibana 3: Milestone 4 ships with 2 examples designed to work flawlessly with Logstash and gives you a foundation to build your own dashboards with. Templated dashboards are easy to create by simply exporting your dashboard schema to a file, editing it and adding it to your app/dashboards directory. For example, take this section from logstash.json () Templated dashboards use “handlebar syntax” to add dynamic section to the JSON based dashboard scheme. Here we’ve replaced the contents of the query key with an expression that translates to: . Now we could access this dashboard with a URL such as More complex scripted dashboards Scripted dashboards are an even more powerful method of taking URL parameters, using all of the power of JavaScript to compose a complete dashboard object. An example, logstash.js () is included in app/dashboards. Because scripted dashboards are entirely JavaScript, we can perform complex operations, like splitting URL parameters. For example, in this URL we are searching for , in the , and showing in the table. Note the important change from to in the URL: Go get it now Milestone 4 is a big step forward for both implementers and users. It is simultaneously more powerful and more simple. And, of course. Kibana continues to integrate seamlessly with Logstash, including the recently released . Kibana can be downloaded directly from elastic.co . Also, we’re hiring Do you want to get paid to work on open source? Are you passionate about JavaScript, with a head full of data visualization ideas? We’re way more interested in what you know and what you can do than where you live. . ","locales":"","title":"Kibana 3: Milestone 4"}
{"index":{}}
{"author":"Clinton Gormley","category":"","publish_date":"2013-11-06T00:00:00.000Z","url":"/blog/1-0-0-beta1-released","seo_title":"","content":" Today we are delighted to announce the release of , the first public release on the road to 1.0.0. The countdown has begun! You can . In each beta release we will add one major new feature, giving you the chance to try it out, to break it, to figure out what is missing and to tell us about it. Your use cases, ideas and feedback is essential to making Elasticsearch awesome. The main feature we are showcasing in this first beta is . This is a beta release - it is not production ready, features are not set in stone and may well change in the next version, and once you have made any changes to your data with this release, it will no longer be readable by older versions! Distributed percolation For those of you who aren’t familiar with percolation, it is “search reversed”. Instead of running a query to find matching docs, percolation allows you to find queries which match a doc. Think of people registering alerts like: . Percolation has been supported by Elasticsearch for a long time. In the current implementation, queries are stored in a special index which is replicated to all nodes, meaning that all queries exist on all nodes. The idea was to have the queries alongside the data. But users are using it at a scale that we never expected, with hundreds of thousands of registered queries and high indexing rates. Having all queries on every node just doesn’t scale. Enter . In the new implementation, queries are registered under the special type within the same index as the data. This means that queries are distributed along with the data, and percolation can happen in a distributed manner across potentially all nodes in the cluster. It also means that an index can be made as big or small as required. The more nodes you have the more percolation you can do. We have removed the ability to percolate while indexing a document, as that didn’t scale either. But we’ve added a host of new features that makes percolation awesome. Check out the . Doc Values The most common problem that users have with Elasticsearch is with fielddata. In order to sort or facet on field values, those values need to be easily accessible. By far the fastest way of accessing field values is by loading them into memory, which is known as fielddata. Especially string fields can use large amounts of RAM and can cause slow garabage collection and even out of memory exceptions. The other big new feature in this release is the addition of doc values. With this new functionality, field values can be stored on disk at index time, instead of in memory. It’s not as fast as holding fielddata in memory - facets take longer because they need to hit disk - but this is achieved with a fraction of the memory, meaning that you can now calculate facets, sort on fields or access field values in scripts for much larger datasets than was previously possible. Doc values need to be setup in the field mapping at index time, and work on numeric, geo and string values, both single and multi-valued. They cannot be enabled on analyzed string fields as that would require a second analysis phase. This may change in the future. Check out the . Stopwords disabled by default The analyzer, the default analyzer used by Elasticsearch, comes with stopwords enabled. Not only that, it uses English stopwords by default. It doesn't matter what language your text is in, or which full text field you're indexing, it'll have stopwords removed. This can produce unexpected confusion, like wondering why doesn't match anything, or why you can't find any plays called \"To be or not to be\". With modern hardware and queries like the , stopwords are less useful than they used to be. This release sets the default stopwords list for the analyzer to empty, meaning that no stopwords are removed by default. You can still enable stopwords where you want them, but you can do that where you choose to. For the sake of backwards compatibility, indices created with a previous version of Elasticsearch will continue to use th","locales":"","title":"1.0.0.Beta1 released"}
{"index":{}}
{"author":"Njal Karevoll","category":"","publish_date":"2013-11-05T00:00:00.000Z","url":"/blog/found-elasticsearch-networking","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud.This article introduces the networking part of Elasticsearch. We look at the network topology of an Elasticsearch cluster, which connections are established between which nodes and how the different Java clients works. Finally, we look a bit closer on the communication channels between two nodes. ","locales":"","title":"Elasticsearch Internals: Networking Introduction"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2013-10-30T00:00:00.000Z","url":"/blog/2013-10-30-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the GitHub repositories, as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. Elasticsearch core Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around Elasticsearch, including plugin and driver releases, as well as news about Logstash and Kibana. Upcoming Webinar will be presenting on October 31st. You can watch live at 9 AM Pacific Time, 5 PM in the UK or 6 PM CET, followed by real-time Q&A with Honza and other members of the Elasticsearch core developer team. For those who are not able to join the broadcast, it will be archived for later playback. Registration is required. Slides Where to find Us Australia We're working on scheduling meetups in Sydney on November 18th and Melbourne on November 21st to coincide with our Elasticsearch training courses. We'll update everyone once all details are confirmed. Belgium If you're heading to Devoxx Belgium 2013, be sure to check out on . David will speak on Wednesday, November 13th at 3:10 PM in Room 6. David will also give a lightning talk, on Thursday, November 14th at 1:10 PM in Room 9. Devoxx Belgium runs from November 11-15 at the Metropolis Business Center in Antwerp. Brasil Elasticsearch community members in Brasil have created a Brasilian Portugese mailing list for folks to discuss Elasticsearch in their native tongue. You can now and we will have an update on their first meetup coming soon. Estonia will explore at on Thursday, November 7th at 3:30 PM. Topconf runs November 6th and 7th in Tallinn. France Germany Ireland will present at on Thursday, November 7th at 2:45 PM. Lucene Revolution takes place November 4th - 7th in Dublin. Japan The Elasticsearch User Group in Japan will meet for a on November 12th in Tokyo. The meeting begins at 7 PM. Netherlands Elasticsearch will be at the on November 20-22nd in Amsterdam. Stop by our table to say hello! United Kingdom United States Call for Speakers and Community Organizers Our Community Manager, Leslie Hawthorn, is hard at work to help folks create more Elasticsearch meetup groups and to help meetup organizers find more speakers. If you are interested in either effort, . Where to Find You Are you hosting an Elasticsearch meetup or giving a talk about Elasticsearch? We would love to know so we can feature that information in future editions of . Just let our  know what you're up to and we're happy to help promote your efforts. Oh yeah, we're also . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticearch, Kibana and Logstash than where you rest your head. Trainings If you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elasticsearch - October 30, 2013"}
{"index":{}}
{"author":"Clinton Gormley","category":"Engineering","publish_date":"2013-11-04T00:00:00.000Z","url":"/blog/0-90-6-released","seo_title":"","content":" Today we are happy to announce the release of , which is based on Lucene 4.5.1. This is the current stable release in the 0.90 series and we recommend upgrading. You can download it . In this release there have been big improvements to highlighting including the new highlighter, automatic script reloading, better handling of edge cases associated with cluster-level changes, and many small bug fixes and enhancements which you can read about in the . Postings highlighter The new highlighter is faster, requires less disk space than the fast-vector-highlighter, and is “sentence aware” which should result in more meaningful snippets. In order to use it, the field mapping must be configured with set to instead of the default . For instance: curl -XPUT localhost:9200/my_index -d ' { \"mappings\": { \"my_type\": { \"properties\": { \"title\": { \"type\": \"string\", \"analyzer\": \"english\", \"index_options\": \"offsets\" } } } } } ' With the mapping setup correctly, we can index some documents: curl -XPOST localhost:9200/my_index/my_type/_bulk -d ' {\"index\":{\"_id\": 1}} {\"title\": \"The quick brown fox jumped over the lazy dog\"} {\"index\":{\"_id\": 2}} {\"title\": \"Brown foxes do love jumping, especially over dogs\"} ' And search them: curl -XGET localhost:9200/my_index/my_type/_search?pretty -d ' { \"query\": { \"match\": { \"title\": \"Jumping brown foxes\"} }, \"highlight\": { \"fields\": { \"title\": {} } } } ' Each result from the above request is accompanied by a nicely highlighted snippet: Other highlighting improvements Thanks go to Nik Everett, a frequent contributor to Elasticsearch, who has added the ability to specify a separate query just for highlighting (see ) and the ability to return a simple excerpt when there are no words that can be highlighted (see in ). More accurate terms facet Because terms facets are calculated by combining the results from multiple shards, it is possible that each shard has a different list, resulting in inaccurate global counts. This release introduces the parameter which allows you to fetch more results from each shard, while still returning only (default ) results to the user. Pulling more results from each shard reduces the inaccuracy in global counts. Reload scripts automatically Scripts are used in many APIs in Elasticsearch, eg for scoring, script fields, faceting etc. A script can either be specified in the request itself, or named scripts can be loaded from the directory on each node. Previously, changing configured scripts was a tiresome process which involved updating and restarting all nodes. Now, a new watcher will check for changes in the scripts directory every 60 seconds (configurable with ) and load new scripts, reload changed scripts or delete removed scripts automatically. See for more. Pretty is prettier This is a very simple change, but removes a common source of annoyance. Pretty-printed results now have a newline character appended to make console output easier to read. We hope you enjoy this new release. Please , and let us know what you think. ","locales":"","title":"0.90.6 Released"}
{"index":{}}
{"author":"Steven Schuurman","category":"News","publish_date":"2013-10-31T00:00:00.000Z","url":"/blog/elasticsearch-and-hortonworks-partner-what-you-need-to-know","seo_title":"","content":" What You Need to KnowWe've got some exciting news to share around Elasticsearch and Hadoop. Elasticsearch and have strengthened their relationship by , which has resulted in Elasticsearch now being the first certified search vendor. With both Elasticsearch and Hortonworks being leaders in the open source space, this partnership will lead to a valuable integration between Elasticsearch and Hadoop that anyone using the open source products will benefit from. This is what Hortonworks said about our newly formed partnership:“The over 400,000 downloads a month is a testament to the level of trust put into Elasticsearch. Since Elasticsearch and Hadoop are such critical elements to the data analytics infrastructure for a huge number of users, we're excited by this opportunity to join together and reach beyond our users' expectations.\" Shaun Conolly, Vice President, Corporate Strategy at HortonworksElasticsearch's real-time data exploration, analytics, logging and search features combine really well with Hadoop and make for a powerful combination:  very useful to anyone handling large volumes of data on a day to day basis. Why This MattersOur customers can now enhance their Hortonworks Hadoop based workflows with our rich query language, designed to help businesses ask better questions, get clearer answers and better analyze their business metrics, all in real-time. Elasticsearch plays well with all Hadoop distributions, including MapR, Cloudera, Pivotal HD and Amazon EMR, and we have plans to provide specific integration with Hortonworks' HDP platform. This is what Shay had to say about our partnership with Hortonworks:“We're moving quickly here at Elasticsearch. Just a few weeks ago, we announced , so it's exciting to announce Elasticsearch now works with Hortonworks' HDP Hadoop distribution. We're constantly working to improve our product so we can provide the most value to all of our users.\" Shay Banon, founder and CTO of ElasticsearchHow Your Hadoop Infrastructure Can Benefit by integrating with ElasticsearchOften, data stores integrated into Hadoop can become a bottleneck due to the number of requests generated by the tasks running in the cluster for each job. The distributed nature of the Map/Reduce model fits really well on top of Elasticsearch because we correlate the number of Map/Reduce tasks with the number of Elasticsearch shards for a particular query. So every time a query is run, the system dynamically generates a number of Hadoop splits proportional to the number of shards available so that the jobs are run in parallel - your Hadoop cluster scales easily alongside Elasticsearch and vice-versa.Finally, Elasticsearch provides near real-time responses (think milliseconds) that significantly improve a Hadoop job's execution and the cost associated with it, especially when running on 'rented resources' such as Amazon EMR. We've got even more news in the works about Elasticsearch's integration with all things Hadoop. Follow this blog for future details! And, how you're using Elasticsearch with Hadoop. We would love to hear about it. ","locales":"","title":"Elasticsearch and Hortonworks Partner"}
{"index":{}}
{"author":"Morten Ingebrigtsen","category":"","publish_date":"2013-10-29T00:00:00.000Z","url":"/blog/found-indexing-for-beginners-part3","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud. What Does an Index Look Like? In the previous article we looked at how search engines parses, analyzes and tokenizes text. In this article we will explore in more detail what an index looks like. ","locales":"","title":"Indexing for Beginners, Part 3"}
{"index":{}}
{"author":"Leslie Hawthorn","category":"Engineering","publish_date":"2013-10-24T00:00:00.000Z","url":"/blog/tiny-data-rapid-development-with-elasticsearch","seo_title":"","content":" Last weekend we participated in , a 48 hour hacking contest to build something fancy on top of Ruby (nowadays, Rails is not required). Beyond fancy, we wanted to build something that solves a real problem that we could continue developing after the hackathon concluded. It turns out that we found a common scratch to itch: our entire group was interested in . The Project Sadly, there are almost no good learning resources for sign language on the internet. If material is available, licensing is a hassle or both the licensing and the material is poorly documented. Documenting sign language yourself is also hard, because producing and collecting videos is difficult. You need third-party recording tools, video conversion and manual categorization. That's a sad state in a world where every notebook has a usable camera built in! Our idea was to leverage modern browser technologies to provide an easy recording function and a quick interface to categorize the recorded words. The result is . We Have a Hundred Problems... Handling UserMedia (video and audio) in the browser is still in its infancy. Especially with recording, which mostly works through elaborate hacks like that need to be set up very carefully. Given only about 24 hours of full work - our team valued sleep and rest - we expected recording and proper video conversion to take up most of our time. ... The Data Store Shouldn't Be One of Them Thus, we were looking for a data store that: The last point stems from the contest rules: all applications have to run on a Linode instance with 1GB of memory. Considering that we have to convert videos on the same machine, we cannot be wasteful. Our problem is language and searching, so Apache Lucene is an obvious pick. Three of our team members had already worked with Elasticsearch and the decision was made quickly:  we used Elasticsearch and couldn't be happier with it. Elasticsearch for Tiny Data Elasticsearch supported the goals of our fast prototype very well: Setting up a reasonably configured elasticsearch instance as a developer is as easy as: With packages available for many platforms, Elasticsearch is quickly configured on the server as well. Also, contrary to popular belief, Elasticsearch can be run in a small memory space by setting to a small value. Given that we expected less than 5000 documents for our initial prototype, we were able to easily fit Elasticsearch in less around 256MB of memory. Setting up the new was also a breeze, it integrates well with the connection-less data models provided by . On the content side, we had 3 main problems: tagging, flagging and transcription. The transcriptions to a video should be searchable. At the same time, we want to flag videos by a fixed set of criteria (if a word is vulgar or an insult, we want to know). Tagging is free-form and allows users to categorize words. All this is possible using Elasticsearch's dynamic mapping. Our main datatype ended up looking roughly like this: { \"transcription\": \"hello from seemespeak\", \"tags\": [\"funny\"], \"flags\": [\"casual\"], \"reviewed\": false, \"language\": \"ASL\" } This is a very workable application-side data format that Elasticsearch's dynamic mapping indexes very well. More specifically, it gives you all you need to search it as expected: We found that the Elasticsearch's query language makes is very easy to build queries iteratively. We started with a plain query to give frontend developers enough content to start building and added filters and randomizations over the first day. By the end of the first day, we were done with all search scenarios. The current SeeMeSpeak is a prototype. We didn't venture into fine-tuning by implementing custom analyzers, e.g. for proper stemming and synonym searches. We were confident that we could do this on the second day, but wanted to invest our time in other things, like translating our German corpus to English for our English-speaking visitors and translati","locales":"","title":"Tiny Data: Rapid Development with Elasticsearch"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2013-10-23T00:00:00.000Z","url":"/blog/2013-10-23-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the GitHub repositories, as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. Elasticsearch core Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around Elasticsearch, including plugin and driver releases, as well as news about Logstash and Kibana. Slides While not slides per se, you can check out the interactive coder oriented Elasticsearch tutorial used for his presentation at last week's GOTO Berlin conference. We also now have a for all presentations given by Elasticsearch's employees that may be a useful reference resource for you. Where to find UsBelgium If you're heading to Devoxx Belgium 2013, be sure to check out on . David will speak on Wednesday, November 13th at 3:10 PM in Room 6. David will also give a lightning talk, on Thursday, November 14th at 1:10 PM in Room 9. Devoxx Belgium runs from November 11-15 at the Metropolis Business Center in Antwerp. Estonia will explore at on Thursday, November 7th at 3:30 PM. Topconf runs November 6th and 7th in Tallinn. France Germany Ireland will present at on Thursday, November 7th at 2:45 PM. Lucene Revolution takes place November 4th - 7th in Dublin. Switzerland David Pilato will be in Geneva on October 24th and 25th for the . You can visit with David in the Elasticsearch booth, and make sure to check out his presentation on on Thursday at 4:30 PM. You may also want to join David for the NoSQL Fireside Chat, also taking place on Thursday, starting at 2:30 PM. United Kingdom United States Where to Find You Are you hosting an Elasticsearch meetup or giving a talk about Elasticsearch? We would love to know so we can feature that information in future editions of . Just let our  know what you're up to and we're happy to help promote your efforts. Oh yeah, we're also . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticearch, Kibana and Logstash than where you rest your head. Trainings If you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elasticsearch - October 23, 2013"}
{"index":{}}
{"author":"Alex Brasetvik","category":"","publish_date":"2013-10-22T00:00:00.000Z","url":"/blog/found-cluster-management","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud. ","locales":"","title":"Managing Elasticsearch with Found"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2013-10-16T00:00:00.000Z","url":"/blog/2013-10-16-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the GitHub repositories, as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. Elasticsearch core Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around Elasticsearch, including plugin and driver releases, as well as news about Logstash and Kibana. Slides Last week, the Core Developer Team presented several lightning talks on Elasticsearch to more than 65 people at the Elasticsearch Netherlands User Group Meeting. You can check out on Speakerdeck and read excellent on the lightning talks. You might also want to check out the slides from , a presentation by at . Where to find UsEstonia will explore at on Thursday, November 7th at 3:30 PM. Topconf runs November 6th and 7th in Tallinn. France will present at the Nantes Java User Group meeting on November 4th. The meeting begins at 7:00 PM. Germany Ireland will present at on Thursday, November 7th at 2:45 PM. Lucene Revolution takes place November 4th - 7th in Dublin. Norway Alex Brasetvik from Found will present at the javaBin Trondheim User Group on October 22nd. Poland Honza Kral will present at PyCon Poland on Friday, October 18th. The conference runs Thursday, October 17th through Sunday, October 20th in Szczyrk. Switzerland David Pilato will be in Geneva on October 24th and 25th for the . You can visit with David in the Elasticsearch booth, and make sure to check out his presentation on on Thursday at 4:30 PM. United Kingdom  will give a talk at about how to on Wednesday, October 30th. United States Where to Find You Are you hosting an Elasticsearch meetup or giving a talk about Elasticsearch? We would love to know so we can feature that information in future editions of . Just let our  know what you're up to and we're happy to help promote your efforts. Oh yeah, we're also . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticearch, Kibana and Logstash than where you rest your head. Trainings If you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elasticsearch - October 16, 2013"}
{"index":{}}
{"author":"Konrad Beiske","category":"","publish_date":"2013-10-15T00:00:00.000Z","url":"/blog/found-city-bikes-reindexing-for-filters","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud.Reindexing and Query Optimization with FiltersIn this article we will look at basic memory optimization of our queries and while doing so, we will write a small script for reindexing our data. TODO: Terms query filter, siden stativene ikke flytter på seg kan vi bruke navn som er mere cachbart? ","locales":"","title":"City Bikes Part two"}
{"index":{}}
{"author":"Costin Leau","category":"Engineering","publish_date":"2013-10-03T00:00:00.000Z","url":"/blog/elasticsearch-and-hadoop","seo_title":"","content":" global wrapper ","locales":"","title":"Elasticsearch and Hadoop"}
{"index":{}}
{"author":"Njal Karevoll","category":"","publish_date":"2013-10-02T00:00:00.000Z","url":"/blog/found-elasticsearch-internals","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud.This article gives an overview of the Elasticsearch internals. I will present a 10,000 foot view of the different modules that Elasticsearch is composed of and how we can extend or replace built-in functionality using plugins. ","locales":"","title":"Elasticsearch Internals: an Overview"}
{"index":{}}
{"author":"Livia Froelicher","category":"News","publish_date":"2013-10-16T00:00:00.000Z","url":"/blog/from-amsterdam-with-love-elasticsearchs-second-company-all-hands","seo_title":"","content":" Elasticsearch's Second Company All Hands This post was co-authored with . Last week, the Elasticsearch crew all converged on our EU headquarters in Amsterdam, The Netherlands for five days of collaboration, hacking and community outreach. This meeting was the first time most of us had met in person, as the company has grown by more than 15 employees since our last All Hands in April 2013. We can't recount all the great stuff we did in one blog post, but we wanted to share some highlights of our meeting, particularly our development discussions. Development Discussions We spent several days with all the developers talking about progress we have made thus far and brainstorming about where we should go across all our different products, Elasticsearch, Logstash, Kibana, Elasticsearch-Hadoop, and the various language clients. Elasticsearch We checked status of features aimed at our 1.0 release planned for early next year. We chatted with about , talked about the , and discussed . We also brainstormed about some future work we would like to see in Elasticsearch, and we would love to share some of these initial thoughts with all of you: Field Data Heavy brainstorming on how to improve the experience of users when using features that require loading all field data and the potential memory problems they run into. The discussions ranged from talking about what use cases “doc values\" will help solve (which has implemented, watch out for a forthcoming blog post), improved memory usage, and potentially other storage means for the field data. Machine Learning Britta led a wonderful session about ML, asking all of us what we would like to see in ML, and to see what directions might be interesting for us to pursue in the future. We ended up breaking things into several buckets, amongst them classification, predictive functions, sentiment analysis, NLP, and more. The session was mainly exploratory in terms of understanding what we have out there and what might apply to Elasticsearch. Changes API One of the exciting features that we want to try and tackle post 1.0 release is registering for changes happening in an index. We mainly brainstormed about how something like this can be implemented - it is quite a complicated feature. We had a couple of different options for storing or virtually storing the change log, but the best choice isn't yet clear. Backwards Compatibility The plan is to have wire level backwards compatibility in 1.0, and we also discussed what it means to have backwards compatibility on the index level over multiple Elasticsearch versions (for example, in keeping backward comp. on the analysis chain level). Field Collapsing / Inner Hits We again fleshed out what is needed in order to properly support field collapsing in a distributed environment execution, as well as the ability to get inner hits (for nested / parent child cases). We have a good idea on the type of refactoring we need in our search execution infrastructure, and hope to tackle it post 1.0. Tests If you haven't noticed, and the rest of us have been hard at work at Elasticsearch to improve our testing, mainly around introducing randomized testing and improving our integration tests. We continued the discussion regarding how to move forward with our testing enhancements, including other places where we can benefit from creating an infrastructure for our tests. Test Infrastructure has been working on setting up an extensive test infrastructure for all our products, including running all our tests over multiple JDK versions, multiple operating systems, multiple machine types and other variants. We test all our products using it, and we plan to open it up relatively soon for people to see it. Performance Infrastructure We also discussed how to automate our benchmarking code and make it both consistent and applicable across different projects. We brainstormed on creating an infrastructure that all our projects can use, and c","locales":"","title":"From Amsterdam with Love"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2013-10-09T00:00:00.000Z","url":"/blog/2013-10-09-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the Github repositories, as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project. Elasticsearch core Elasticsearch Ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around Elasticsearch, including plugin and driver releases, as well as news about Logstash and Kibana. Slides Where to find UsGermany Hungary  will talk about Elasticsearch at  in Budapest on October 12th. RuPy is a conference to bring together programmers and communities of different programming languages like Ruby, Python, Clojure or JavaScript. Netherlands You can meet almost all of the Elasticsearch, Kibana and Logstash developers tomorrow in Amsterdam at the and ask us whatever you want to know. In addition to our Q&A session, will talk about running Elasticsearch on bare metal and will talk about our Hadoop integration. United Kingdom  will give a talk at about how to on Wednesday, October 30th. United States Where to Find You Are you hosting an Elasticsearch meetup or giving a talk about Elasticsearch? We would love to know so we can feature that information in future editions of . Just let our  know what you're up to and we're happy to help promote your efforts. Oh yeah, we're also . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticearch, Kibana and Logstash than where you rest your head. Trainings If you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elasticsearch - October 09, 2013"}
{"index":{}}
{"author":"Clinton Gormley","category":"","publish_date":"2013-10-04T00:00:00.000Z","url":"/blog/docs-docs-docs","seo_title":"","content":" There's not much point in having an amazing piece of software, if users can't figure out how to use it. Over the years, we've had several complaints about our documentation: it's difficult to navigate, it's difficult to know where to start, etc. This is not unusual -- documentation is the bugbear of most software projects. But that doesn't mean we can't make it better! So let's tell you what we've done so far and what we've got planned. Docs and code, code and docs Documentation should be an accurate reflection of the code it describes. Up until now, we've had the code and the documentation in separate repositories. Now, the code lives with the docs in the . This is good news both for users and developers. Developers can include code and documentation in the same pull request and users can now access that they are using. We build a separate set of documentation for each major version, ie , , . Changes between point versions are marked up as in this example in the . So far, we have focussed on migrating the docs to the new build system. There are still a few niggles in the new docs. Please bear with us as we work on fixing them. Next we plan on improving them: making them more consistent, more complete, better explanations and more examples. This is a long-term project but you should notice a steady improvement in the documentation over time. We welcome feedback, suggestions and pull-requests! One build system to rule them all All of our projects will be using the same framework to provide their documentation. We have chosen to use as our markup language, because it is easy to read, very expressive and, via DocBook, gives us the assurance that we don't have any bad links. Also, because we build all docs from each project together, we can ensure that cross-document links work and continue to work when edits are made later on. Another good reason to use AsciiDoc and DocBook is that DocBook can produce output as HTML, PDF, EPUB and Mobi... and it is the preferred format for ! The Definitive Guide Yes! We are in the process of writing , to be published by O'Reilly Media. It was important for us to find the right partner for this book: we are an open-source company and we wanted the book to be open-source too. O'Reilly has a long track record of supporting the open-source world and we are thrilled to be working with them on this important project. While our reference documentation is intended to explain how to use a particular API and what parameters it accepts, the book will focus on how to approach and solve problems: how to improve search relevance, how to handle multiple languages, how to tune Elasticsearch for your workload. It will be suitable for beginners who know nothing about search, but will have enough meat in it to satisfy the more experienced as well. The book will be freely available online in HTML format both on our website and on O'Reilly's site and, when finished, will be available to purchase in hard copy from O'Reilly. The first part of the book is due to be published online in December this year. New chapters will be added as they are finished, and hardcopies will available for purchase in mid-2014. ","locales":"","title":"Docs! Docs! Docs"}
{"index":{}}
{"author":"Boaz Leskes","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2013-10-02T00:00:00.000Z","url":"/blog/2013-10-02-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the Github repositories, as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project.Elasticsearch core Elasticsearch EcosystemHere's some more information about what is happening in the ecosystem we are maintaining around Elasticsearch, including plugin and driver releases, as well as news about Logstash and Kibana. Slides Where to find Us will be speaking tonight at the Belux Elasticsearch meetup about the which will be part of the 1.0 version.David Pilato will treat attendees of m to a his presentation , as well as lead an . Open World Forum takes place October 3rd-5th in Paris.GermanyThe will convene for their regular monthly meeting on October 29th. will talk about Elasticsearch at  in Budapest on October 12th. RuPy is a conference to bring together programmers and communities of different programming languages like Ruby, Python, Clojure or JavaScript.United Kingdom will give a talk at about how to on Wednesday, October 30th.United States Where to Find YouAre you hosting an Elasticsearch meetup or giving a talk about Elasticsearch? We would love to know so we can feature that information in future editions of . Just let our  know what you're up to and we're happy to help promote your efforts.Oh yeah, we're also . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticsearch, Kibana and Logstash than where you rest your head.TrainingIf you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elasticsearch - October 02, 2013"}
{"index":{}}
{"author":"Leslie Hawthorn","category":"User Stories","publish_date":"2013-10-01T00:00:00.000Z","url":"/blog/using-elasticsearch-and-logstash-to-serve-billions-of-searchable-events-for-customers","seo_title":"","content":" sends and receives a lot of emails and we track and store every event that happens to every email. It adds up to billions of events per month that we need to make available to our customers, along with the ability easily parse through this data with full text search. Below is a detailed account of this challenge and how we solved it with the help of Elasticsearch and Logstash (note: we were delighted to hear about shortly after completing this project). ","locales":"","title":"Using ElasticSearch and Logstash to Serve Billions of Searchable Events for Customers"}
{"index":{}}
{"author":"Zachary Tong","category":"","publish_date":"2013-10-15T00:00:00.000Z","url":"/blog/meet-the-devs-meetup-in-amsterdam","seo_title":"","content":" h2 {clear:left: } Last Thursday, the group held its eighth meetup. This meetup was a bit special, however, since it also coincided with the Elasticsearch company meeting. Rather than the traditional two presentation format, the Elasticsearch developers gave a number of lightning talks about what they have been working on. A big thanks to for hosting the meetup! Around 65 people showed up, with lots of great discussion before, during and after the event. Drew Raines - Life after EC2 Elasticsearch talks are often at a very high level: a new feature, a new type of query, how to implement XYZ functionality. talk went the other direction and discussed the ramifications of running on bare metal hardware. In particular, he discussed how he and debugged a performance quirk with their new cluster. Long story short, they discovered that the default Linux I/O scheduler is atrocious when using SSDs, or RAID over fast disks. After switching their scheduler to Noop or Deadline, they saw a remarkable performance increase! Check out for more details (especially if you are using RAID or SSDs!) Costin Leau - Real-time data with Hadoop talked about his new integration. He highlighted what Hadoop is good at...and where it is lacking. Real-time processing, analytics, and geo operations are all difficult for Hadoop. Elasticsearch, on the other hand, excels at these problems. Costin explained how Elasticsearch can be integrated with an existing Hadoop installation to provide functionality that is difficult or slow in Hadoop. He then showed examples in several popular Hadoop frameworks and some benchmark data. Costin's slides are . Clinton Gormley, Karel Minarik, Honza Kral, Zachary Tong - Unleashing the Clients The client team gave a short overview of the . They talked about the motivations behind creating the clients, as well as the need for a unified interface, consistent testing framework and pluggable components. Britta Weber - Function Score Query talked about the new . Partially known for its outrageously awesome , the function score allows you to tweak scoring with complicated mathematical functions. Britta discussed how it is used and some example scenarios. Igor Motov - Snapshot and Restore presented the Snapshot and Restore functionality that he has been working on. Slated for version 1.0, Snapshot and Restore will allow users to snapshot their cluster to a shared repository (S3, shared FS, etc). Using another API, they can restore these incremental snapshots to the cluster. Snapshot and Restore will make backing up and delayed replication vastly easier in the future. You can get more detail from . Alexander Reelsen - Completion Suggester discussed the new . Using advanced Finite State Transducers, the completion suggester provides autocomplete-style suggestions in milliseconds. Importantly, it only requires one request, as opposed to approaches like ngrams which require multiple round-trips - users need suggestions they finish typing. You can learn more from . Uri Boness - Aggregations demoed his much-anticipated . Designed to replace facets, aggregations provide a framework where individual aggregations can be composed into nearly limitless combinations. He first built an example that mirrored a traditional facet, then proceeded to enhance it with more nested aggregations. Simon Willnauer - Lucene Pipeline wrapped up the meetup by talking about some low-level improvements that will be coming to Apache Lucene in the future. In particular, he discussed his frustrations with the standard Apache Lucene highlighters, as well as depreciating Span queries in favor of adding payloads to non-span queries. You'll enjoy , including the obligatory photo of his fellow Apache Lucene committer Uwe Schindler. Wrap up Big thanks to everyone who showed up for the meetup, and everyone who stayed around to chat after the presentations. The Elasticsearch team had a great time presenting and really enjoyed fielding questions about their respectiv","locales":"","title":"Meet-the-Devs Meetup in Amsterdam"}
{"index":{}}
{"author":"Igor Motov","category":"","publish_date":"2013-10-10T00:00:00.000Z","url":"/blog/hopper-hosts-hackathon-with-elasticsearch","seo_title":"","content":" Last week, I attended the first . This all day event was co-organized by , a Cambridge, Massachusetts, US based company that is using Elasticsearch to plow through a massive amount of travel data. Also organizing the hackathon were the fine folks from the , a diverse group of Elasticsearch users in the Greater Boston area. The hackathon gathered more than 40 software engineers, students and other search and open source software enthusiasts eager to learn more about Elasticsearch and share their knowledge with others. The goal of the Hackathon organizers was to create an inclusive event that would be interesting to both experts and novices alike, so we started with to bring all attendees up to speed before getting started coding. During a typical one-day hackathon, attendees have only 5-7 hours to write code, and getting stuck during these hours can derail otherwise interesting project. In order to improve participants’ experiences and allow them to make as much progress as possible on their projects in just a few short hours, the Hackathon organizers convened a group of local Elasticsearch experts from Hopper, Traacker and Elasticsearch Inc as mentors. I was one of the six mentors during this hackathon, and it was an amazing experience. This event wasn’t my first Elasticsearch-themed hackathon, but what made it very different for me was the number of new Elasticsearch users and the amount of progress that they made during a very short period of time. This Hackthon was the first where we introduced the and , and I think this two projects made all the difference. With the lightweight API, new users were able to very quickly translate concepts described in the elasticsearch.org user guide into working code. At the same time, Kibana made it simple to look at very complex data without requiring a lot of new code. In just seven hours by combining Twitter river with Kibana, one of the teams managed to create Twitter analytics solution for analyzing demographics of topics of interest (in their case tweets about Justin Bieber). Two winning teams analyzed Twitter to determine the popularity of TV shows and did sentiment analysis on political topics. But it wasn't all about Twitter:  other hacks ranged from finding the shortest path between two Wikipedia articles to finding hidden relationships between Enron employees based on their email messages. Two teams won prizes in this hackathon, and it was wonderful to be a part of something where everyone was learning stuff, having fun and left with a sense of accomplishment. Many thanks to our sponsors: Elasticsearch Inc., SoftLayer and Traackr. You can read even more about the hackathon on the . ","locales":"","title":"Hopper Hosts Hackathon with Elasticsearch"}
{"index":{}}
{"author":"Morten Ingebrigtsen","category":"","publish_date":"2013-10-08T00:00:00.000Z","url":"/blog/found-indexing-for-beginners-part2","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud.Document Parsing and TokenizationWe continue the Indexing for beginners article with an introduction to the document parsing process and use some space to explain the concept of tokenization and tokens. ","locales":"","title":"Indexing for Beginners, Part 2"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2013-09-26T00:00:00.000Z","url":"/blog/2013-09-26-this-week-in-elasticsearch","seo_title":"","content":" Welcome to  . In this roundup, we try to inform you about the latest and greatest changes in elasticsearch. We cover what happened in the Github repositories, as well as many elasticsearch events happening worldwide, and give you a small peek into the future of the project. Elasticsearch core Elasticsearch ecosystem Here's some more information about what is happening in the ecosystem we are maintaining around elasticsearch, including plugin and driver releases, as well as news about logstash and kibana. Slides Where to find UsDenmark will be attending to with a few other folks from the Elasticsearch team on September 30th - October 2nd. You can pop by our booth to meet us, or just head to Alex's talk . France David will also give attendees of Open World Forum a taste of , as well as lead an . Open World Forum takes place October 3rd-5th in Paris. Norway The fine folks spearheading the will host their inaugural meetup on Thursday, September 26th. will join the festivities to talk about new features in the works for Elasticsearch. Hungary will talk about elasticsearch at in Budapest on 12th of October. RuPy is a conference to bring together programmers and communities of different programming languages like Ruby, Python, Clojure or JavaScript. Ukraine kicks off on Friday, September 27th in Kiev. Karel Minarik will be there, giving a presentation on the first day of the conference. United States Where to Find You Are you hosting an Elasticsearch meetup or giving a talk about Elasticsearch? We would love to know so we can feature that information in future editions of . Just let our know what you're up to and we're happy to help promote your efforts. Oh yeah, we're also . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for elasticearch, kibana and logstash than where you rest your head. Trainings If you are interested in elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elasticsearch - September 26, 2013"}
{"index":{}}
{"author":"Clinton Gormley","category":"","publish_date":"2013-09-24T00:00:00.000Z","url":"/blog/unleash-the-clients-ruby-python-php-perl","seo_title":"","content":" Today, the Elasticsearch developer team is excited to announce the release of official Elasticsearch clients for Ruby, Python, PHP and Perl, with other languages to follow in the future. All of the clients are licensed under the Apache 2 open source license. Why are we providing official clients? The Elasticsearch community has done a great job over the years of providing clients for many languages. The feature set of these clients varies greatly: some are low level thin wrappers, some are high level with many abstractions. Some focus on the Elasticsearch API, while others focus on the networking layer. Some have extensive coverage of the Elasticsearch APIs and some just implement the few APIs that the developer needs. And they are all implemented in the preferred style of the developer. A number of our users have multiple languages in their codebase and want to use Elasticsearch from all of them. Differences between clients can make this more complicated than it need be. What do these clients provide? We have written these clients with the following goals in mind: No opinions! It should be as simple as possible to go from the directly to the client of your choice. All of the Elasticsearch APIs provided by these clients are direct translations of the native Elasticsearch REST interface. There should be no guessing required. Play nicely with the cluster While it is easy to make HTTP calls to Elasticsearch, handling dynamic node detection or failover when nodes disappear is tricky. These clients provide a solid networking base for playing nicely with the cluster. Full, consistent coverage of the APIs We have implemented the full REST interface. Nothing is missing. And the method calls and parameters are consistent between languages: you can make the same calls from any of the supported clients. That said, each client still feels like it belongs to the language it is written in. It won’t feel like you are programming Java in Ruby, or C++ in PHP. Transport abstraction These clients are not tied to a particular networking module. Different HTTP backends can be plugged in for different environments, or to match the HTTP client that you are already using. But this abstraction also allows us to plugin different transport protocols in the future, which are more efficient than HTTP. Extend and conquer The API that these clients provide is a thin wrapper around the REST interface. Perhaps their style is different from your own, and you’d prefer more abstractions. We hear you, so we have built these clients to be extensible. We have tried to do the hard stuff for you, to provide you with a solid foundation for building your own interface.This is the beauty of open source: our users will have great ideas that haven’t even occurred to us. Now you also have the tools to implement them, without having to start from scratch. Supported by Elasticsearch Because we wrote these clients and they are tested by us, we can provide official support for them. When bugs are found, we can fix them quickly – it’s our job. Downloading the clients Each client is available from GitHub and also from the standard distribution network for each language, where one exists. This is the first release of these clients. Please download them, try them out and give us feedback. What doesn’t work? What is missing? What can we do better? Elasticsearch for Perl Installing cpanm Elasticsearch Example usage use Elasticsearch:  # Connect to localhost:9200 my $es = Elasticsearch->new():  # Round-robin between two nodes my $es = Elasticsearch->new( nodes => [ 'search1:9200', 'search2:9200' ] ):  # Connect to cluster at search1:9200, sniff all nodes # and round-robin between them my $es = Elasticsearch->new( nodes => 'search1:9200', cxn_pool => 'Sniff' ):  # Index a document $es->index( index => 'my_app', type => 'blog_post', id => 1, body => { title => 'Elasticsearch clients', content => 'Interesting content...', date => '2013-09","locales":"","title":"Release the clients!  Ruby, Python, PHP, Perl"}
{"index":{}}
{"author":"Njal Karevoll","category":"","publish_date":"2013-09-23T00:00:00.000Z","url":"/blog/found-elasticsearch-mapping-introduction","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud. This article will give an introduction to the mapping feature of Elasticsearch. We'll define the key terms and take a closer look at what mapping is, when we specify it, how it is structured and how we can apply it to our data. ","locales":"","title":"An Introduction to Elasticsearch Mapping"}
{"index":{}}
{"author":"Clinton Gormley","category":"Engineering","publish_date":"2013-09-16T00:00:00.000Z","url":"/blog/0-90-4-released","seo_title":"","content":" global wrapper ","locales":"","title":"Elasticsearch 0.90.4 released"}
{"index":{}}
{"author":"Alex Brasetvik","category":"","publish_date":"2013-09-16T00:00:00.000Z","url":"/blog/found-elasticsearch-from-the-bottom-up","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud. In this article series, we look at Elasticsearch from a new perspective. We'll start at the \"bottom\" (or close enough!) of the many abstraction levels, and gradually move upwards towards the user-visible layers, studying the various internal data structures and behaviours as we ascend. ","locales":"","title":"Elasticsearch from the Bottom Up, Part 1"}
{"index":{}}
{"author":"Morten Ingebrigtsen","category":"","publish_date":"2013-09-13T00:00:00.000Z","url":"/blog/found-indexing-for-beginners-part1","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud.The Missing Document on Search Engine Indexing ","locales":"","title":"Indexing for Beginners, Part 1"}
{"index":{}}
{"author":"Rashid Khan","category":"Kurrently in Kibana","publish_date":"2013-09-19T00:00:00.000Z","url":"/blog/this-week-in-kibana","seo_title":"","content":" Histogram zero filling The histogram panel received a makeover that brought proper light weight zero filling. This means that at intervals where a query should have 0 results, it will properly appear as such, instead of drawing a sloped line to the next point. The zero filling also means that histogram stacked bars will always appear in the same order top to bottom. In addition, the stacked tooltip now allows you to choose between cumulative and individual modes. Micro analysis of array fields Array fields can now be handled as singles or groups in the micro analysis panel. For example, if I have an array of tags I could see either the 10 most common tags, or the 10 most common ways that tags appear together _source as the default table field If you don't select any fields for your table, Kibana will now show you the json source of your even by default until you select some fields to show Configurable field trimming Noticed the '...' at the end of the _source in the above screenshot? Table fields can be trimmed by a configurable \"factor\". The factor is essentially the maximum length of a field, divided by the number of columns in the table, after which the field will be trimmed to nicely fit. For example, if my trim factor was 300, and I had 3 columns in the table, fields would be trimmed to a max of 100 character, after which '...' would be added. Of course the entirety of the field is still available in the expanded details view About that details view You may know that you can see a table of event fields by click on a table row. Now you have a choice of how you'd like to see the details of an event, including syntax highlighted JSON, as well as the raw unhighlighted JSON. Lighter, faster, smaller, better Kibana has an all new build system! This new system allows us to build an optimized, minified, awesomulated distribution of Kibana. It also has the benefit of cleanly clearing old caches when you upgrade. Regular builds of Kibana are being published at for your convenience. That zipball can be extracted right to your webserver. You can still run it right out of the if you want. However instead of copying the entire repo, you need only upload the src/ directory to your webserver. We recommend grabbing the built version however as its much faster. ","locales":"","title":"This Week in Kibana"}
{"index":{}}
{"author":"Alex Brasetvik","category":"","publish_date":"2013-09-17T00:00:00.000Z","url":"/blog/found-elasticsearch-in-production","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud. Elasticsearch easily lets you develop amazing things, and it has gone to great lengths to make Lucene's features readily available in a distributed setting. However, when it comes to running Elasticsearch in production, you still have a fairly complicated system on your hands: a system with high demands on network stability, a huge appetite for memory, and a system that assumes all users are trustworthy. These articles cover some of the lessons we've learned from securing and herding hundreds of Elasticsearch clusters. Elastic provides , a product which offers comprehensive security for Elasticsearch, including encrypted communications, role-based access control, AD/LDAP integration and auditing. The following article was authored before Shield was available. This article is introductory. Its goal is to give an overview of important aspects of running and maintaining Elasticsearch clusters (or other distributed search engines), and to motivate learning more about them. It also aims to explain the importance of having enough memory and how to achieve high availability. Hopefully, this article will help you set reasonable expectations in terms of what Elasticsearch can (and cannot) do for you. In the future, we'll add more thorough articles about each of the covered topic. These are the topics we will cover in this article: It is no coincidence that is largely based on the amount of memory for your cluster. ","locales":"","title":"Elasticsearch in Production"}
{"index":{}}
{"author":"Alex Brasetvik","category":"","publish_date":"2013-09-15T00:00:00.000Z","url":"/blog/found-elasticsearch-as-nosql","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud. .Can Elasticsearch be used as a \"NoSQL\"-database? NoSQL means different things in different contexts, and interestingly it's not really about SQL. We will start out with a \"Maybe!\", and look into the various properties of Elasticsearch as well as those it has sacrificed, in order to become one of the most flexible, scalable and performant search and analytics engines yet. ","locales":"","title":"Elasticsearch as a NoSQL Database"}
{"index":{}}
{"author":"Luca Cavanna","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2013-09-11T00:00:00.000Z","url":"/blog/2013-09-11-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the Github repositories, as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project.Elasticsearch core Elasticsearch ecosystemHere's some more information about what is happening in the ecosystem we are maintaining around Elasticsearch, including plugin and driver releases, as well as news about Logstash and Kibana. Slides Where to find UsAustraliaOn September 19th, the will have talk by RealEstate.com.au about their migration to Elasticsearch.Czech RepublicShould you find yourself in Prague for , stop in to see and co-present .Denmark will be attending to with a few other folks from the Elasticsearch team on September 30th - October 2nd. You can pop by our booth to meet us, or just head to Alex's talk .France Germany and will be attending Monitorama on September 19-20th in Berlin. Ping either of them on Twitter if you want to talk all things Elasticsearch over a tasty beverage.NorwayThe fine folks spearheading the will host their inaugural meetup on Thursday, September 26th. will join the festivities to talk about new features in the works for Elasticsearch.Ukraine kicks off on Friday, September 27th in Kiev. Karel Minarik will be there, giving a presentation on the first day of the conference.United States Where to Find YouAre you hosting an Elasticsearch meetup or giving a talk about Elasticsearch? We would love to know so we can feature that information in future editions of . Just let our know what you're up to and we're happy to help promote your efforts.Oh yeah, we're also . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for Elasticearch, Kibana and Logstash than where you rest your head.TrainingIf you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elasticsearch - September 11, 2013"}
{"index":{}}
{"author":"Boaz Leskes","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2013-09-04T00:00:00.000Z","url":"/blog/2013-09-04-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this format we try to inform you about the latest and greatest changes in Elasticsearch. We try to cover what happened in the GitHub repositories, as well as all the events happening about Elasticsearch and give you a small peek in the future.Elasticsearch core Elasticsearch ecosystemWe try to give you some more information about what is happening in the ecosystem we are maintaining around Elasticsearch, be it plugin or driver releases or news about Kibana. Elasticsearch communityGot an interesting open source project, plugin, driver or anything else for Elasticsearch? Here is your time to shine! Just drop us a note and we will list it here (and on the .org website, of course!). Meetups Also, if you are interested in a core Elasticsearch training, the next locations are London at the 9th of September each (special discount ), Paris at the 16th of September (with a 10% rebate, if you sign up by Sep 6th) and San Francisco at September 23rd.  For more locations, check the If you are interested in all this, we are . We are interested in your skills, not in your location. Just drop us a note. ","locales":"","title":"This Week in Elasticsearch - September 04, 2013"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2013-09-18T00:00:00.000Z","url":"/blog/2013-09-18-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this roundup, we try to inform you about the latest and greatest changes in Elasticsearch. We cover what happened in the Github repositories, as well as many Elasticsearch events happening worldwide, and give you a small peek into the future of the project.Elasticsearch core Elasticsearch ecosystemHere's some more information about what is happening in the ecosystem we are maintaining around Elasticsearch, including plugin and driver releases, as well as news about Logstash and Kibana. Slides Where to find UsAustraliaOn September 19th, the will have talk by RealEstate.com.au about their migration to Elasticsearch.Austria will be presenting at EJUG Austria on September 25th.Czech RepublicShould you find yourself in Prague for , stop in to see and co-present .Denmark will be attending to with a few other folks from the Elasticsearch team on September 30th - October 2nd. You can pop by our booth to meet us, or just head to Alex's talk .France Germany NorwayThe fine folks spearheading the will host their inaugural meetup on Thursday, September 26th. will join the festivities to talk about new features in the works for Elasticsearch.Ukraine kicks off on Friday, September 27th in Kiev. Karel Minarik will be there, giving a presentation on the first day of the conference.United States Where to Find YouAre you hosting an Elasticsearch meetup or giving a talk about Elasticsearch? We would love to know so we can feature that information in future editions of . Just let our know what you're up to and we're happy to help promote your efforts.Oh yeah, we're also . If you'd like us to find you for employment purposes, just .  We care more about your skill set and passion for elasticearch, kibana and logstash than where you rest your head.TrainingIf you are interested in Elasticsearch training we have courses taught by our core developers coming up in: ","locales":"","title":"This Week in Elasticsearch - September 18, 2013"}
{"index":{}}
{"author":"Clinton Gormley","category":"Engineering","publish_date":"2013-09-17T00:00:00.000Z","url":"/blog/0-90-5-released","seo_title":"","content":" The Elasticsearch dev team are pleased to announce the release of , which is based on Lucene 4.4. You can download it .We released 0.90.4 yesterday with great expectations, and a few hours later had our expectations dashed by a silly bug in the plugin install script, which installed site plugins (e.g. head, bigdesk, paramedic) incorrectly. We apologize for that. That bug has been fixed and we've added the appropriate tests. However, that is not all. Things move fast in the Elasticsearch world, so we've taken advantage of this quick release to fix a few other bugs: Windows SupportAnd finally, an enhancement for those of you using Elasticsearch on Windows: we have added support for running Elasticsearch as a service on Windows , via the script. You can now do:> service.bat Usage: service.bat install|remove|start|stop|manager [SERVICE_ID] > service install Installing service : 'elasticsearch-service-x64' Using JAVA_HOME (64-bit): c:\\jvm\\jdk1.7 The service 'elasticsearch-service-x64' has been installed. > service start The service 'elasticsearch-service-x64' has been started > service stop The service 'elasticsearch-service-x64' has been stopped > service remove The service 'elasticsearch-service-x64' has been removed All the details are available in the . If you are not affected by any of the above issues then there is no need for you to upgrade from 0.90.4.Thank you for your feedback on the release of 0.90.4 yesterday - it allowed us to find and fix the plugin script error quickly. ","locales":"","title":"Elasticsearch 0.90.5 Released"}
{"index":{}}
{"author":"Konrad Beiske","category":"","publish_date":"2013-09-14T00:00:00.000Z","url":"/blog/found-leader-election-in-general","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud.Leader election is one of the most tricky things to do in distributed systems. At same time, understanding how a leader is elected and the responsibilities of the leader is key to understanding a distributed system. ","locales":"","title":"Leader Election, Why Should I Care?"}
{"index":{}}
{"author":"Njal Karevoll","category":"Engineering","publish_date":"2013-09-10T00:00:00.000Z","url":"/blog/found-writing-a-plugin","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud.WARNING: This article contains outdated information. We no longer recommend taking its advice.Using plugins, it's possible to add new functionality to Elasticsearch without having to create a fork of Elasticsearch itself. In this article, we will go through the steps required to create a new Elasticsearch plugin from the ground up. ","locales":"","title":"Writing an Elasticsearch Plugin: Getting Started"}
{"index":{}}
{"author":"Boaz Leskes","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2013-08-28T00:00:00.000Z","url":"/blog/2013-08-28-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this format we try to inform you about the latest and greatest changes in elasticsearch. We try to cover what happened in the github repositories, as well as all the events happening about elasticsearch and give you a small peek in the future. Elasticsearch core Elasticsearch ecosystem We try to give you some more information about what is happening in the ecosystem we are maintaining around elasticsearch, be it plugin or driver releases or news about kibana. Elasticsearch community Got an interesting open source project, plugin, driver or anything else for elasticsearch? Here is your time to shine! Just drop us a note and we will list it here (and on the .org website, of course!). Meetups Also, if you are interested in a core elasticsearch training, the next locations are Toronto at the 29th of August,  London and New York at the 9th of September each and Paris at the 16th of September. For more locations, check the  If you are interested in all this, we are . We are interested in your skills, not in your location. Just drop us a note. ","locales":"","title":"This Week in Elasticsearch - August 28, 2013"}
{"index":{}}
{"author":"Konrad Beiske","category":"","publish_date":"2013-09-10T00:00:00.000Z","url":"/blog/found-elasticsearch-and-the-discovery-plugin","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud. The Zen discovery plugin might seem like magic, but replacing it is actually not that hard. Getting down and dirty with Zen might very well pay off in your particular setup. ","locales":"","title":"Elasticsearch and the Discovery Plugin"}
{"index":{}}
{"author":"Konrad Beiske","category":"","publish_date":"2013-09-06T00:00:00.000Z","url":"/blog/found-city-bikes-and-elasticsearch-facets","seo_title":"","content":" This article refers to our hosted Elasticsearch offering by an older name, Found. Please note that Found is now known as Elastic Cloud. A small demo of using facets in Elasticsearch on time series. ","locales":"","title":"City Bikes and Elasticsearch Facets"}
{"index":{}}
{"author":"","category":"","publish_date":"2013-08-27T00:00:00.000Z","url":"/blog/welcome-jordan-logstash","seo_title":"","content":" Today is a defining day in the history of our company: We are proud to announce that Jordan Sissel, the creator of Logstash and a good friend, is joining Elasticsearch! This means that Elasticsearch, the company, now provides a fully open source product stack for logging and events management: for log processing, Elasticsearch as the real time analytics and search engine, and (created by Rashid Khan) as the visual front end. Neither Jordan nor Logstash really need an introduction, but I'd like to give you an idea about why this is amazing news for so many Elasticsearch and Logstash users. About Logstash Logstash, which just released version 1.2.0, is one of the most popular open source logs and events shipper/processor out there. It consumes logs (eg by tailing log files), processes and enriches the data and stores it in Elasticsearch. This means that your logging data can now be analyzed in real time. Kibana is a visual web front end which allows you to explore and monitor the analytics that matter to you. The trio of Logstash, Kibana and Elasticsearch is already the most popular open source solution to logs management. The three products work together beautifully, which is not surprising, given that Jordan, Rashid and I have known each other for a long time and have worked closely on ensuring that our products work well together. Now with Jordan joining us full time, we will build a team dedicated to Logstash development as well, and will have more time to work together on developing the many new ideas we have for new features and smoother integration. Logstash / Kibana support Ever since the company started offering SLA based support subscriptions, we have received requests to offer commercial support for Logstash. We’re happy to report that, as of today, our support customers will receive Logstash and Kibana support as part of their existing subscriptions – at no additional cost. The same applies to future customers: our company will support Logstash and Kibana in addition to Elasticsearch, as part of our standard support subscriptions without any change to the pricing. Our vision Even though Elasticsearch itself was not designed specifically to be a logging product, the logging use case has contributed heavily to its popularity. There are a few reasons for this: What is a log? Logs used to be an (often indecipherable) line of text intended for offline human analysis of what went wrong. Today, a log can be any piece of structured or unstructured data, usually associated with a timestamp, that may come from access logs, application logs, or even tweets, financial transactions, audit events, etc. Elasticsearch was built from the ground up to handle any type of data and, over the years, the time based data model has proved to be a very good fit for Elasticsearch. Moreover, the ability to slice, dice and aggregate data on the fly, based on any field in the logs has freed users from worrying about how to turn their raw logs into valuable insights. When Rashid joined our company, Kibana was tied to data generated by Logstash. Since then, Rashid and our team have been heavily at work building Kibana 3. The new version of Kibana today allows users to explore any time based data stored in Elasticsearch including, obviously, our vision of what constitutes a log. Logstash shares the same vision. Effectively, Logstash is a generic system to process events. It provides a pluggable pipeline to combine different ways of inputting data, enriching it, and outputting the results. The plethora of inputs, filters, and outputs, and the amazing community that have developed around them, makes Logstash a Swiss Army knife suitable for almost any type of data munging. This same thread runs through all of our products, and our vision for redefining the logging space is not just a happy coincidence. In the same way that we have redefined search with Elasticsearch, we want to redefine the log with Logstash. Single context Users have many different types of data in","locales":"","title":"Welcome Jordan & Logstash"}
{"index":{}}
{"author":"Zachary Tong","category":"","publish_date":"2013-08-26T00:00:00.000Z","url":"/blog/stop-stopping-stop-words-a-look-at-common-terms-query","seo_title":"","content":" Stop words are a fundamental part of Information Retrieval lore. Common wisdom dictates that you should identify and remove stop words from your index. This increases both performance (fewer terms in your dictionary) and more relevant search results. Elasticsearch supports stop word removal through the , but a new query was recently added which makes this filter unnecessary: . Background Before we go much farther, Why would you want to remove stop words anyway? In most cases, stop words add little semantic value to a sentence. They are filler words that help sentences flow better, but provide very little context on their own. Stop words are usually words like “to”, “I”, “has”, “the”, “be”, “or”, etc etc. Even worse than being void of useful information…they are everywhere! Stop words are the most frequent words in the English language. Because of this, most sentences share a similar percentage of stop words. Stop words bloat your index without providing any extra value. If they are both common lacking in much useful information, why not remove them? Removing stop words helps decrease the size of your index as well as the size of your query. Fewer terms is always a win with regards to performance. And since stop words are semantically empty, relevance scores are unaffected. Stop words are useless… …unless you actually need them. The classic example is a phrase from Shakespeare: This sentence is composed entirely of stop words, and would therefore not be represented in your index. That’s a problem. At this point, people usually start maintaining multiple mappings of their data: one with stop words removed, and one that has stop words intact. They are boosted differently, to favor phrases with no stop words. This procedure, however, eliminates the benefits of stop word removal. Instead of decreasing the number of terms being searched (and indexed), we just doubled it by indexing the field in two different ways! Enter Common Terms The new Common Terms Query is designed to fix this situations, and it does so through a very clever mechanism. At a high level, Common Terms analyzes your query, identifies which words are “important” and performs a search using just those words. Only after documents are matched with important words are the “unimportant” words considered The motivation behind Common Terms is to leverage the power of stop word removal (faster searches) without eliminating stop words entirely (because they can contribute to score sometimes). You can have your cake and eat it too! How Common Terms works Let’s first look at how you construct a Common query: { \"common\": { \"body\": { \"query\": \"this is bonsai cool\", \"cutoff_frequency\": 0.001 } } } Several things happen when you execute this query: Internally, the query is rewritten into roughly this representation: { \"bool\": { \"must\": [ { \"term\": { \"body\": \"bonsai\"}}, { \"term\": { \"body\": \"cool\"}} ], \"should\": [ { \"term\": { \"body\": \"this\"}} { \"term\": { \"body\": \"is\"}} ] } } It should be easy to see how this can greatly affect performance. The low frequency words act as a pre-filter, drastically reducing the number of terms that need to be evaluated/scored against the full index. Adaptive Stop Word Evaluation With traditional stop word schemes, you must first create a list of stop words. Every domain is unique when it comes to stop words: there are no “pre-made” stop word lists on the internet. As an example, consider the word ““. For most businesses, “video” is an important word – it shouldn’t be removed. But if you are Youtube, “video” is probably mentioned in thousands of places…it is definitely a stop word in this context. Traditional stop word removal would need a human to sit down, compile a list of stop words, add it to Elasticsearch and then routinely maintain the list with additions/deletions. In contrast, Common Terms adapts to your domain. Words with high ","locales":"","title":"Stop Stopping Stop Words: a Look at Common Terms Query"}
{"index":{}}
{"author":"Alexander Reelsen","category":"Engineering","publish_date":"2013-08-22T00:00:00.000Z","url":"/blog/you-complete-me","seo_title":"","content":" Effective search is not just about returning relevant results when a user types in a search phrase, it's also about helping your user to choose the best search phrases. Elasticsearch already has functionality which can correct the user's spelling after they have searched. Now, we are adding the which can make suggestions . Giving the user the right search phrase before they have issued their first search makes for happier users and reduced load on your servers. Consider this feature experimental at the moment! Things might change/break in future releases. Why Another Suggester? It was already possible to make suggestions using existing functionality in Elasticsearch, like prefix queries and ngrams, so why have we added a dedicated completion suggester? There are a few reasons: SPEED Remember, we're making suggestions while the user types, so results need to be shown to the user within a few milliseconds, even after taking network latency into account! A full-blown search has to examine too many terms (and their frequencies) to perform sufficiently fast for this purpose. Instead, we use an in-memory data structure called an FST which contains valid suggestions and is optimised for fast retrieval and memory usage. Essentially, it is just a graph. For instance, and FST containing the words , , , and would look like this: All we do is start on the left and follow the paths to the right. If the user types an , then we can see that there is only one possible completion: , so we can immediately complete that word. If the user types an , then we can provide a list of all the \"m\" words. As soon as they type , then we can autocomplete the word \"marriot\". Following this in-memory graph is blazingly fast, as you will see from the benchmarks later in this blogpost. Real Time Suggesters in Lucene are built in-memory by loading the completion values from the index, then building the FST. This can be a slow, resource intensive process. And, as soon as the index changes, the FST needs to be rebuilt. \"Real time search\" is a mantra of Elasticsearch. It is not acceptable to return out of date suggestions, nor to require a full rebuild whenever the index changes. Instead of building the FST at search time, we now build an FST per-segment at index time. Essentially, whenever a new segment is written to disk, we also write the FST to a file in a format which is fast to load into memory when required. Instead of consulting a single FST for the whole index, we query each per-segment FST to produce a unified list of results. Having an FST per segment also means that the completion suggester , in exactly the same way as for full text search: the more nodes you add, the more you can scale. Readability A user searching for may type in any number of search phrases: However, we want our suggestions to be explicit, leaving the user in no doubt about the page they will see if they click on our suggestion. All of the above inputs should return the single, nicely formatted suggestion of . Custom Ordering We want suggestions to be presented in a particular order, but this order is not the same as the ordering we need for full text search. Full text search uses TF/IDF (term frequency/inverse document frequency) to find the documents that are most relevant for the given search phrase. For suggestions, we may want common terms to appear at the top of the list. However common terms have high document frequencies, which their relevance. Alternatively, we may want a completely custom ordering applied to suggestions, which is independent of term frequency. For instance, we may want to promote hotels which have received good user ratings, or promote recent blogposts over older blogposts. The completion suggester gives us complete control over the order of suggestions but, by default, treats more common suggestions as more important. Use Case: Hotel Bookings To demonstrate the power of the , let's start with a simple function on a hotel bookings website. We'll build on this exam","locales":"","title":"You Complete Me"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2013-08-07T00:00:00.000Z","url":"/blog/2013-08-07-this-week-in-elasticsearch","seo_title":"","content":" Welcome to the sixth issue of . In this format we try to inform you about the latest and greatest changes in elasticsearch. We try to cover what happened in the github repositories, as well as all the events happening about elasticsearch and give you a small peek in the future. Elasticsearch core Elasticsearch ecosystem We try to give you some more information about what is happening in the ecosystem we are maintaining around elasticsearch, be it plugin or driver releases or news about kibana. Elasticsearch community Got an interesting open source project, plugin, driver or anything else for elasticsearch? Here is your time to shine! Just drop us a note and we will list it here (and on the .org website, of course!). Meetups Also, if you are interested in a core elasticsearch training, the next locations are Boston next week, Amsterdam (sold out!) at the 20th and Austin at the 24th og August. For more locations, check the If you are interested in all this, we are . We are interested in your skills, not in your location. Just drop us a note. ","locales":"","title":"This Week in Elasticsearch - August 07, 2013"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2013-07-31T00:00:00.000Z","url":"/blog/2013-07-31-this-week-in-elasticsearch","seo_title":"","content":" Welcome to the fifth issue of . In this format we try to inform you about the latest and greatest changes in elasticsearch. We try to cover what happened in the github repositories, as well as all the events happening about elasticsearch and give you a small peek in the future. Elasticsearch core Elasticsearch ecosystem We try to give you some more information about what is happening in the ecosystem we are maintaining around elasticsearch, be it plugin or driver releases or news about kibana. Elasticsearch community Got an interesting open source project, plugin, driver or anything else for elasticsearch? Here is your time to shine! Just drop us a note and we will list it here (and on the .org website, of course!). Meetups Also, if you are interested in a core elasticsearch training, the next locations are San Francisco (limited offer: get three seats for the price of two!) and Boston at the beginning of August. For more locations, check the If you are interested in all this, we are . We are interested in your skills, not in your location. Just drop us a note. ","locales":"","title":"This Week in Elasticsearch - July 31, 2013"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2013-07-17T00:00:00.000Z","url":"/blog/2013-07-17-this-week-in-elasticsearch","seo_title":"","content":" Welcome to the third issue of . In this format we try to inform you about the latest and greatest changes in elasticsearch. We try to cover what happened in the github repositories, as well as all the events happening about elasticsearch and give you a small peek in the future. Elasticsearch core Elasticsearch community Got an interesting open source project, plugin, driver or anything else for elasticsearch? Here is your time to shine! Just drop us a note and we will list it here (and on the .org website, of course!). Meetups Also, if you are interested in a core elasticsearch training, the next locations are San Francisco and Boston in August. For more locations, check the If you are interested in all this, we are . We are interested in your skills, not in your location. Just drop us a note. ","locales":"","title":"This Week in Elasticsearch - July 17, 2013"}
{"index":{}}
{"author":"","category":"","publish_date":"2013-08-20T00:00:00.000Z","url":"/blog/welcome-leslie","seo_title":"","content":" It's my pleasure to welcome Leslie Hawthorn () to our team joining Elasticsearch as our new .    is well know across the open source world spending the last decade creating, cultivating and enabling open source communities. She created the to involve pre-university students in open source software development, launched, received an  in 2010 and has given on many things open source. She’s worked as Program Manager for, Community Manager for  and Community Engineering Team Manager at. I can certainly state Leslie truly understands the spirit of open source. I'm in the lucky position to have worked with Leslie in the past and I am even more happy to work with here again! Given the enormous responsibility that comes with an open source project like Elasticsearch I can't really think of better fit for the role of ensuring community happiness, helping on project culture, and  organising events for devs and geeks. Welcome Leslie!   ps: ...wanna join this team too? We are ! ","locales":"","title":"welcome leslie"}
{"index":{}}
{"author":"Alexander Reelsen","category":"Engineering","publish_date":"2013-08-06T00:00:00.000Z","url":"/blog/0-90-3-released","seo_title":"","content":" The Elasticsearch dev team is pleased to announce the release of , which is based on Lucene 4.4. You can download it .Noticeable changes in 0.90.3 include performance improvements to the has_child query, and improved cluster stability and handling of large cluster states (many indices/shards/aliases). A full list of changes can be found .An exciting new experimental feature that we back ported to 0.90.3 is a new type of suggester called . This is a real-time, prefix oriented suggestion that supports updates in real time while also maintaining an efficient (speed and resource) data structure to provide extreme low latency suggestions cases, specifically as-you-type ones. A blog post describing this exciting new features with actual use cases will be posted shortly.Please , and let us know what you think.: If you use the analysis, mapper attachments or cloud plugins, you'll need to update to the newest version of the plugin to be compatible with the new 0.90.3 release. ","locales":"","title":"0.90.3 Released"}
{"index":{}}
{"author":"","category":"","publish_date":"2013-07-29T00:00:00.000Z","url":"/blog/welcome-zach","seo_title":"","content":" I'd like to welcome Zachary Tong (\"\") to our team (long overdue...). I am sure you've seen Zach around, he has been active about Elasticsearch (on his own blog, and our own), wrote a , , and many . Zach has an amazing gift distilling complex concepts into simple explanations, and he will help us improve our documentation and online resources around Elasticsearch. He will also help push forward Elasticsearch in PHP land. welcome! p.s. we are . ","locales":"","title":"welcome zach"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2013-07-11T00:00:00.000Z","url":"/blog/2013-07-11-this-week-in-elasticsearch","seo_title":"","content":" Welcome to the second issue of . In this format we try to inform you about the latest and greatest changes in elasticsearch. We try to cover what happened in the github repositories, as well as all the events happening about elasticsearch and give you a small peek in the future. Elasticsearch core Elasticsearch ecosystem We try to give you some more information about what is happening in the ecosystem we are maintaining around elasticsearch, be it plugin or driver releases or news about kibana. Elasticsearch community Got an interesting open source project, plugin, driver or anything else for elasticsearch? Here is your time to shine! Just drop us a note and we will list it here (and on the .org website, of course!). Meetups Talks/Slides Also, if you are interested in a core elasticsearch training, the next dates are San Francisco and New York the next week. For more dates, check the If you are interested in all this, we are . We are interested in your skills, not in your location. Just drop us a note. ","locales":"","title":"This Week in Elasticsearch - July 11, 2013"}
{"index":{}}
{"author":"Steven Schuurman","category":"","publish_date":"2013-07-03T00:00:00.000Z","url":"/blog/explosive-growth-fuels-demand-for-talent","seo_title":"","content":" I believe Elasticsearch is destined for greatness, and here’s why: since 2010 we have been constantly innovating Elasticsearch based on our vision for the future for one of the biggest holes in the IT landscape today, and that solution is now being downloaded thousands and thousands of times … per day! But the innovation doesn’t stop there:  we have a talented team hard at work to bring further revolutionary ideas into the market, which you can expect hear more about in the future. We believe this is how software innovation should work: think ahead and create wonderful products that don’t just solve problems, but also create new possibilities. Elasticsearch fits this description very nicely. The product isn’t merely a solution to problem:  we find our customers actually creating new products based on Elasticsearch, that they wouldn’t have been able to drive to market without it. If you look at what we’re currently working on, you’ll undoubtedly agree that Elasticsearch is only the beginning of an exciting journey. The main reason being, that even though there are many other solutions out there that also solve part of the “big data challenge”, no other product was designed from the ground up to provide a single holistic end-to-end experience. Combine that with our team’s near obsession with user friendliness, and you end up with a product that is really easy to use, and that’s getting noticed. Now this is where we are today. However, we have some truly incredibly advanced ideas for releases that we believe will be as groundbreaking as Elasticsearch is today. We are unbelievably passionate about creating Elasticsearch, and know there are more people like us out there. We are driven by our shared passion for making a difference. Creating products that allow others to have new insights into their business, or innovate by driving new products to market, is about as rewarding as it gets. With us progressing Elasticsearch faster than ever before, we have to grow our tech team as well as the group of people that make up the commercial and operational side of our company. We are therefore looking passionate team-players in Sales, Marketing, Technology and Operations that love driving change. We are not interested in merely being ready for the future, we’re creating it. We’re not exactly big believers in tight hierarchical layers and skills set requirements in job descriptions. That’s why you won’t find things like “5 years of experience with X” or “Masters degree in ABC” on our job postings. We need you to excel at what you do, or have the ambition to learn how to excel. Here’s a quick overview of what kind of expertise we’re seeking to expand our team with here at Elasticsearch. We’re looking for Elasticsearch developers that wish to help us progress our product. Whether it’s contributing to the core (in Java) or to the clients (in many other languages) we would love to talk to you. You have the possibility to have a broad and exciting role, where your responsibilities go way beyond writing code. If you want to, you have the possibility to be involved in teaching training courses, creating marketing materials, talking to customers in commercial processes, and speaking at conferences around the world. If you love working with happy customers that are about as excited about a product as you are, you should consider working at Elasticsearch to provide tech support to our customer base. It’s growing fast, and they have questions that need answering. We are as passionate about our business execution as we are about creating new technology. When you speak with us, you will find that our demand generation engine is designed and operated by the very best people in the industry. Our marketing team and our sales teams consist of true industry professionals that are experts in their field. Whenever I talk with anybody on the marketing or sales teams, it’s very reward","locales":"","title":"Explosive Growth Fuels Demand For Talent"}
{"index":{}}
{"author":"Rashid Khan","category":"","publish_date":"2013-08-21T00:00:00.000Z","url":"/blog/kibana-whats-cooking","seo_title":"","content":" Haven't upgraded Kibana lately? You might be missing out on some neat stuff! A lot has changed in Kibana and new panels are only half the story. The entire system behind querying has been refactored to provide consistent colors and legends across the entire dashboard. Interfaces have been standardized and many functions have been modified to provide simpler, faster and more powerful functionality. Let's take a deeper look at some of the recent developments. Terms panel:  Global colors, aliases and queries:  Filters. New Query Input The new query panel replaces the 'stringquery' panel as your method for inputting queries. Gone are the individual query inputs for each panel. You can still specify specific queries for specific panels, but you enter them here first, optionally assigning an alias and color, then select them from within the panel editor. Queries can also be pinned to a collapsable section when not being actively modified. Assigning Queries to panels Assigning queries to panels has been greatly simplified. Queries can be toggled on/off from the panel editor and even if the underlying query is updated or filtered, the alias will remain consistent. You may also notice that settings windows have been segmented into a tabs to provide a cleaner configuration interface. Custom colors and aliases When you assign a color to a query it is instantly reflected on every panel, and the same goes for aliases, which are used as legend values. This makes it easy to assign color variations in logical groups that make sense for your dashboard and data. Hello Terms! A new terms panel has been introduced that represents top field values in 3 unique fashions: pie charts, bar charts and tables. All have click-to-filter functionality that integrates with the new filtering panel. filtering panel? Caught that in the last section, did you? Sharp eye! Filters! Filters allow you to drill down into the dataset without modifying your queries. They can also be removed, toggled and edited at will. Filters have 3 modes. Fields list and micropanel The fields panel has been integrated with the table panel. Field lists are now populated by reading the Elasticsearch endpoint. Note that you may need to update your proxy to reflect this change. The field list is now collapsable to save space and new charts have been added to the micropanel. Hey, whats with the color scheme?! There you go, spotting changes before I can explain! Kibana now allows you to switch between light and dark color schemes to better fit your environment and preferences. That about sums it up! Of course Kibana is constantly evolving so be sure to watch this space, , and follow and on Twitter. Cheers! ","locales":"","title":"Kibana: What's Cooking"}
{"index":{}}
{"author":"","category":"","publish_date":"2013-08-12T00:00:00.000Z","url":"/blog/welcome-lee","seo_title":"","content":" I'd like to welcome Lee Hinman (\"\") to our team. Lee has been a long time elasticsearch user, running and using large elasticsearch clusters. He wrote quite a few plugins to Elasticsearch, such as . Last, Lee is the author of the upcoming Elasticsearch Manning . welcome! p.s. we are . ","locales":"","title":"welcome lee"}
{"index":{}}
{"author":"","category":"","publish_date":"2013-07-31T00:00:00.000Z","url":"/blog/welcome-britta","seo_title":"","content":" I'd like to welcome Britta Weber (\"\") to our team (long overdue...). Britta is almost done with her PhD in computer science, developing Machine Learning algorithms to process electron microscopy image data. She has already contributed to elasticsearch the new  implementation, and a wonderful new query (including support for built in decaying functions). With someone as smart as Britta, I'm excited about what she can contribute to our company and product. welcome! p.s. we are . ","locales":"","title":"welcome britta"}
{"index":{}}
{"author":"","category":"","publish_date":"2013-07-23T00:00:00.000Z","url":"/blog/welcome-boaz","seo_title":"","content":" I'd like to welcome Boaz Leskes (\"\") to our team (long overdue...). Boaz has been an active user of Elasticsearch for a few years now, is an active developer within the Elasticsearch ecosystem (), started the work on multi value memory optimizations in elasticsearch field data, and quite a few contributions to . Last but not least, Boaz is the organizer of the . welcome! p.s. we are . ","locales":"","title":"welcome boaz"}
{"index":{}}
{"author":"Clinton Gormley","category":"Engineering","publish_date":"2013-06-26T00:00:00.000Z","url":"/blog/0-90-2-released","seo_title":"","content":" global wrapper ","locales":"","title":"Elasticsearch 0.90.2 Released"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2013-08-21T00:00:00.000Z","url":"/blog/2013-08-21-this-week-in-elasticsearch","seo_title":"","content":" Welcome to . In this format we try to inform you about the latest and greatest changes in elasticsearch. We try to cover what happened in the github repositories, as well as all the events happening about elasticsearch and give you a small peek in the future. Elasticsearch core Elasticsearch ecosystem We try to give you some more information about what is happening in the ecosystem we are maintaining around elasticsearch, be it plugin or driver releases or news about kibana. Elasticsearch community Got an interesting open source project, plugin, driver or anything else for elasticsearch? Here is your time to shine! Just drop us a note and we will list it here (and on the .org website, of course!). Meetups Also, if you are interested in a core elasticsearch training, the next locations are Toronto at the 29th of August, and London and New York at the 9th of September each. For more locations, check the If you are interested in all this, we are . We are interested in your skills, not in your location. Just drop us a note. ","locales":"","title":"This Week in Elasticsearch - August 21, 2013"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2013-08-14T00:00:00.000Z","url":"/blog/2013-08-14-this-week-in-elasticsearch","seo_title":"","content":" Welcome to the seventh issue of . In this format we try to inform you about the latest and greatest changes in elasticsearch. We try to cover what happened in the github repositories, as well as all the events happening about elasticsearch and give you a small peek in the future. Elasticsearch core Elasticsearch ecosystem We try to give you some more information about what is happening in the ecosystem we are maintaining around elasticsearch, be it plugin or driver releases or news about kibana. Elasticsearch community Got an interesting open source project, plugin, driver or anything else for elasticsearch? Here is your time to shine! Just drop us a note and we will list it here (and on the .org website, of course!). Meetups Talks/Slides Also, if you are interested in a core elasticsearch training, the next locations are Amsterdam at 20th, Austin at the 26th and Toronto at the 29th of August. For more locations, check the If you are interested in all this, we are . We are interested in your skills, not in your location. Just drop us a note. ","locales":"","title":"This Week in Elasticsearch - August 14, 2013"}
{"index":{}}
{"author":"","category":"","publish_date":"2013-08-02T00:00:00.000Z","url":"/blog/welcome-adrien","seo_title":"","content":" I'd like to welcome Adrien Grand (\"\") to our team (long overdue...). Adrien is an Apache Lucene committer, and some of his contributions include , (and many more). He also maintains the compression codec in Java (the fastest Java compression out ). Adrien has been heavily  with Elasticsearch for some time. With someone of Adrien's scale, amazing things will happen for both Apache Lucene and Elasticsearch now that he is fully dedicated to both projects. welcome! p.s. we are . ","locales":"","title":"welcome adrien"}
{"index":{}}
{"author":"Alexander Reelsen","category":"This week in Elasticsearch and Apache Lucene","publish_date":"2013-07-24T00:00:00.000Z","url":"/blog/2013-07-24-this-week-in-elasticsearch","seo_title":"","content":" Welcome to the fourth issue of . In this format we try to inform you about the latest and greatest changes in elasticsearch. We try to cover what happened in the github repositories, as well as all the events happening about elasticsearch and give you a small peek in the future. Elasticsearch core Elasticsearch ecosystem We try to give you some more information about what is happening in the ecosystem we are maintaining around elasticsearch, be it plugin or driver releases or news about kibana. Elasticsearch community Got an interesting open source project, plugin, driver or anything else for elasticsearch? Here is your time to shine! Just drop us a note and we will list it here (and on the .org website, of course!). Meetups Also, if you are interested in a core elasticsearch training, the next locations are San Francisco (limited offer: get three seats for the price of two!) and Boston at the beginning of August. For more locations, check the . If you are interested in all this, we are . We are interested in your skills, not in your location. Just drop us a note. ","locales":"","title":"This Week in Elasticsearch - July 24, 2013"}
{"index":{}}
{"author":"Alexander Reelsen","category":"","publish_date":"2013-07-02T00:00:00.000Z","url":"/blog/2013-07-02-this-week-in-elasticsearch","seo_title":"","content":" Welcome to the first issue of . In this format we try to inform you about the latest and greatest changes in elasticsearch. We try to cover what happened in the github repositories, as well as all the events in the elasticsearch community and give you a small peek into the future. Elasticsearch core Elasticsearch ecosystem Elasticsearch maintains a number of projects outside the core server code. In this section, we will try to give you some more information about what is happening in the ecosystem around elasticsearch, be it plugin or driver releases or news about kibana. Elasticsearch community Got an interesting open source project, plugin, driver or anything else for elasticsearch? Here is your time to shine! Just drop us a note and we will list it here (and on the .org website, of course!). Meetups There are plenty of meetup groups about elasticsearch on , so maybe a meetup for your city already exists. In case you could not find your favorite city for a meetup in the list, contact us and let us try to create an elasticsearch user group! Talks Also, if you are interested in a core elasticsearch training, the next dates are Munich next week, or San Francisco and New York the week after. For more dates, check the Want to work at Elasticsearch? We are ! We are interested in your skills, not in your location. Drop us a note if you are interested. ","locales":"","title":"This Week in Elasticsearch - July 02, 2013"}
{"index":{}}
{"author":"Rashid Khan","category":"","publish_date":"2013-06-14T00:00:00.000Z","url":"/blog/log-analysis-with-elasticsearch","seo_title":"","content":"   With growing infrastructure, expanded automation and increasing server:staff ratios, lean operations teams are leading the charge to extract intelligence from logging data. Join us to learn how Elasticsearch fits into an easy to deploy log analysis architecture. We'll talk about: Rashid Khan will be your host for this live webinar on on Wednesday, June 19th at 9:00am PDT (6pm CET). Rashid is a logging nerd and the author of Kibana, a browser based log and data analysis interface for Elasticsearch ","locales":"","title":"Webinar: Log Analysis with Elasticsearch"}
{"index":{}}
{"author":"Zachary Tong","category":"Engineering","publish_date":"2013-05-06T00:00:00.000Z","url":"/blog/all-about-elasticsearch-filter-bitsets","seo_title":"","content":" global wrapper ","locales":"","title":"All About Elasticsearch Filter BitSets"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2013-03-25T00:00:00.000Z","url":"/blog/0-20-6-released","seo_title":"","content":" version is out. You can download it . This release includes several critical bug fixes, and it is a recommended upgrade for and users. The main critical bug fixed () relates to a problem that has been haunting us for some time now, which can cause full shard deletion when running out of OS system resources. We finally managed to nail it down (you can track the ), and backported a fix to 0.20. ","locales":"","title":"0.20.6 Released"}
{"index":{}}
{"author":"Zachary Tong","category":"Engineering","publish_date":"2013-06-11T00:00:00.000Z","url":"/blog/terms-filter-lookup","seo_title":"","content":" There is a new feature in the 0.90 branch that is pretty awesome: the now supports document lookups. In a normal Terms Filter, you provide a list of Terms that you want to filter against. This is fine for small lists, but what if you have 100 terms? A thousand terms? That is a lot of data to pass over the wire. If that list of terms is stored in your index somewhere, you also have to retrieve it first…just so you can pass it back to Elasticsearch. The new lookup feature tells Elasticsearch to use another document as your terms array. Instead of passing 1000 terms, you simply tell the Terms Filter . Elasticsearch will fetch that document internally, extract the terms and perform your query. Let’s work our way through a concrete example of how it all works. A concrete exampleThe I encourage you to read over it. But I come from a biology background and you can only read so many Twitter examples…so let’s try something a bit different and bioinformatics-y. Imagine you run a bioinformatics search engine and database. You have two indices holding two different types of data. First, you have an index that stores academic research articles. This document represents a single scientific paper. It has two fields, the title of the paper and a list of all proteins relevant to the topic: curl -XPUT localhost:9200/papers/paper/paper789 -d '{ \"title\" : \"Ahi1, whose human ortholog is mutated in Joubert syndrome, is required for Rab8a localization, ciliogenesis and vesicle trafficking.\" \"proteins\" : [ \"Ahi1\", \"Rab8a\" ] }' Next, you have an index which holds data from a . Microarrays are a method to determine whether a gene’s activity is more (“up-regulated”) or less (“down-regulated”) than its normal rate. Microarrays are about the size of a postage stamp and test thousands of genes at once. The resulting “up-regulated” field could potentially be thousands long: curl -XPUT localhost:9200/microarrays/experiment/experiment1234 -d '{ \"upregulated_proteins\" : [ \"Ahi1\", \"Wnt\", \"SHH\", \"CDC42\", \"GSK-3B\", [.......] ] }' Given this data, a very common question might be: . Filtering the papersBefore lookups, the way to accomplish this is a GET plus a Filtered Search. First you have to GET the microarray experiment and extract the array of terms: curl -XGET localhost:9200/microarrays/experiment/experiment1234 # ...Extract array in your app... # Then perform a search query using the filter you need: curl -XGET localhost:9200/papers/paper/_search -d '{ \"query\":{ \"filtered\":{ \"filter\":{ \"terms\" : { \"proteins\":[ \"Ahi1\", \"Wnt\", \"SHH\", \"CDC42\", \"GSK-3B\", [.......] ] } } } } } This works, but you can see why it isn’t ideal. Not only do we need to perform two requests – a GET and a Search – but we have to shuffle a potentially large term array across the wire twice. The lookup feature allows you to bypass this inefficiency. Filtering, this time with LookupsLookups use documents themselves as the list of Terms, which means you can avoid unnecessary requests. Let’s try again, but this time with lookups. The data is organized the same as before, but when we search we skip the extraneous GET phase and go straight to the Terms Filter with the new Lookup syntax: curl -XGET localhost:9200/papers/paper/_search -d '{ \"query\" : { \"filtered\" : { \"filter\" : { \"terms\" : { \"proteins\" : { \"index\" : \"microarrays\", \"type\" : \"experiment\", \"id\" : \"experiment1234\", \"path\" : \"upregulated_proteins\" }, \"_cache_key\" : \"experiment_1234\" } } } } }' Neat! How does it work? Let’s just look at the new Terms Filter syntax line by line: Filter terms in the field…. …matching the array of terms located in the field, inside the document located at … …and save (cache) the result of this filter under the name “experiment_1234″ so we can use it again later without loading the document. More than just convenienceThe new Lookup feature is certainly useful. But it offers more than just convenience: there are tangible performance benefits. Performance can be boosted e","locales":"","title":"Terms Filter Lookup"}
{"index":{}}
{"author":"Zachary Tong","category":"Engineering","publish_date":"2013-06-03T00:00:00.000Z","url":"/blog/customizing-your-document-routing","seo_title":"","content":" global wrapper ","locales":"","title":"Customizing Your Document Routing"}
{"index":{}}
{"author":"Clinton Gormley","category":"News","publish_date":"2013-04-29T00:00:00.000Z","url":"/blog/0-90-0-released","seo_title":"","content":" The Elasticsearch dev team are pleased to announce the release of - the first stable release of Elasticsearch based on Lucene 4. You can download it .Besides all of the performance and memory improvements made by Lucene, version 0.90.0 comes with a host of improvements and new features specific to Elasticsearch itself.  Probably the biggest improvement that our users will notice is much better memory usage when loading for or on a field. Fielddata uses less memory and makes it easier for the garbage collector to do its work, resulting in more stable clusters.  This change alone makes it worth upgrading.Other user-visible features include: On Thursday 2nd May at 9am PST (6pm CET) we will be holding a free (reg. required) live webinar explaining more about the new features in 0.90 and how you can use them.  Come along and ask questions. You can .The next major release will be 1.00, which we have just finished planning at our dev week in Amsterdam. We will post about the new features we intend to include in 1.00 soon, but suffice to say, there are some very cool things planned.  In the meantime, please download and use 0.90, and let us know what you think. ","locales":"","title":"0.90.0 Released"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2013-03-20T00:00:00.000Z","url":"/blog/0-90-0-rc1-released-2","seo_title":"","content":" version is out, the first release candiate for the 0.90 release. You can download it . This release includes an upgrade to Lucene 4.2, many improvements to the suggester feature (including its own dedicated API), another round of memory improvements to field data (long type will now automatically “narrow” to the smallest type when loaded to memory) and several bug fixes. Upgrading to it from previous beta releases is highly recommended. ","locales":"","title":"0.90.0.RC1 Released"}
{"index":{}}
{"author":"","category":"","publish_date":"2013-03-05T00:00:00.000Z","url":"/blog/welcome-costin","seo_title":"","content":" I'd like to welcome Costin Leau () to our team. Costin has spent his last 7+ years mainly in open source around the Java Spring ecosystem with anything and everything from the core container and data access to NoSQL and Hadoop. I personally have known Costin for quite some time now and am really excited about having such a great engineer on-board. welcome! ","locales":"","title":"welcome costin"}
{"index":{}}
{"author":"Boaz Leskes","category":"Engineering","publish_date":"2013-06-24T00:00:00.000Z","url":"/blog/elasticsearch-versioning-support","seo_title":"","content":" Elasticsearch Versioning Support One of the key principles behind Elasticsearch is to allow you to make the most out of your data. Historically, search was a read-only enterprise where a search engine was loaded with data from a single source. As the usage grows and Elasticsearch becomes more central to your application, it happens that data needs to be updated by multiple components. Multiple components lead to concurrency and concurrency leads to conflicts. Elasticsearch's versioning system is there to help cope with those conflicts. The need for versioning - an example To illustrate the situation, let's assume we have a website which people use to rate t-shirt design. The website is simple. It lists all designs and allows users to either give a design a thumbs up or vote them down using a thumbs down icon. For every t-shirt, the website shows the current balance of up votes vs down votes. A record for each search engine looks like this: curl -XPOST 'http://localhost:9200/designs/shirt/1' -d' { \"name\": \"elasticsearch\", \"votes\": 999 }' As you can see, each t-shirt design has a name and a votes counter to keep track of it's current balance. To keeps things simple and scalable, the website is completely stateless. When someone looks at a page and clicks the up vote button, it sends an AJAX request to the server which should indicate to elasticsearch to update the counter. To do so, a naive implementation will take the current votes value, increment it by one and send that to elasticsearch: curl -XPOST 'http://localhost:9200/designs/shirt/1' -d' { \"name\": \"elasticsearch\", \"votes\": 1000 }' This approach has a serious flaw - it may lose votes. Say both Adam and Eve are looking at the same page at the same time. At the moment the page shows 999 votes. Since both are fans, they both click the up vote button. Now Elasticsearch gets two identical copies of the above request to update the document, which it happily does. That means that instead of having a total vote count of 1001, the vote count is now 1000. Oops. Of course, the allows you to be smarter and communicate the fact that the vote can be incremented rather than set to specific value: curl -XPOST 'http://localhost:9200/designs/shirt/1/_update' -d' { \"script\" : \"ctx._source.votes += 1\" }' Doing it this way, means that Elasticsearch first retrieves the document internally, performs the update and indexes it again. While this makes things much more likely to succeed, it still carries the same potential problem as before. During the small window between retrieving and indexing the documents again, things can go wrong. To deal with the above scenario and help with more complex ones, Elasticsearch comes with a built-in versioning system. Elasticsearch's versioning system Every document you store in Elasticsearch has an associated version number. That version number is a positive number between 1 and 2 -1 (inclusive). When you index a document for the very first time, it gets the version 1 and you can see that in the response Elasticsearch returns. This is, for example, the result of the first cURL command in this blog post: { \"ok\": true, \"_index\": \"designs\", \"_type\": \"shirt\", \"_id\": \"1\", \"_version\": 1 } With every write-operation to this document, whether it is an , or , Elasticsearch will increment the version by 1. This increment is atomic and is guaranteed to happen if the operation returned successfully. Elasticsearch will also return the current version of documents with the response of get operations (remember those are real time) and it can also be to return it with every search result. Optimistic locking So back in our toy example, we needed a solution to a scenario where potentially two users try to update the same document at the same time. Traditionally this will be solved with locking: before updating a document, one will acquire a lock on it, do the update and release the lock. When you have a lock on a document, you are guaranteed that no one will ","locales":"","title":"Elasticsearch Versioning Support"}
{"index":{}}
{"author":"Clinton Gormley","category":"","publish_date":"2013-04-25T00:00:00.000Z","url":"/blog/webinar-whats-new-in-elasticsearch-0-90","seo_title":"","content":" (Thursday 2 May @ 9:00am PT, 6pm CET) We're getting ready to release Elasticsearch 0.90, the first version to use Apache Lucene 4. But this release is not just about Lucene 4 - there are a whole lot of goodies that are going to make your life easier, such as: We will be talking about these updates live on Thursday, May 2nd at 9:00am PDT (6pm CET). The webinar will be hosted by Clinton Gormley, the first user of Elasticsearch and author of ElasticSearch.pm, the official Perl API and other Perl modules for Elasticsearch.   ","locales":"","title":"Webinar: What's new in Elasticsearch 0.90?"}
{"index":{}}
{"author":"","category":"","publish_date":"2013-03-20T00:00:00.000Z","url":"/blog/welcome-alexander","seo_title":"","content":" I'd like to welcome Alexander Reelsen () to our team. Alexander has been involved with elasticsarch for quite some time now, with work ranging from building the FST suggester plugin, to the opennlp plugin, to different visualization components (see more of Alexander's work ) and we are very privileged to have him join us. welcome! ","locales":"","title":"welcome alexander!"}
{"index":{}}
{"author":"Zachary Tong","category":"","publish_date":"2013-02-24T00:00:00.000Z","url":"/blog/what-is-an-elasticsearch-index","seo_title":"","content":" What exactly is an index in Elasticsearch? Despite being a very basic question, the answer is surprisingly nuanced. <strong>$ curl -XGET localhost:9200/SubaruFactory/Cars/SubaruImprezza </strong> <strong>$ curl -XGET localhost:9200/logs-2013-02-22,logs-2013-02-21/Errors/_search?query=\"q:Error Message\" </strong> ","locales":"","title":"What is an Elasticsearch Index?"}
{"index":{}}
{"author":"Zachary Tong","category":"Engineering","publish_date":"2013-02-16T00:00:00.000Z","url":"/blog/logging-elasticsearch-events-with-logstash-and-elasticsearch","seo_title":"","content":" global wrapper ","locales":"","title":"Logging Elasticsearch Events with Logstash (and Elasticsearch)"}
{"index":{}}
{"author":"Clinton Gormley","category":"Engineering","publish_date":"2013-06-17T00:00:00.000Z","url":"/blog/changing-mapping-with-zero-downtime","seo_title":"","content":" global wrapper ","locales":"de-de,fr-fr,ko-kr","title":"Changing Mapping with Zero Downtime"}
{"index":{}}
{"author":"Clinton Gormley","category":"Engineering","publish_date":"2013-05-30T00:00:00.000Z","url":"/blog/0-90-1-released","seo_title":"","content":" The Elasticsearch dev team are pleased to announce the release of E, which is based on Lucene 4.3. You can download it .We highly recommend upgrading to 0.90.1 from 0.90.0 as this release includes some key bug fixes, notably a fix to the bool filter which could produce incorrect results () and a fix to the DFS search types ().Besides the bug fixes, 0.90.1 also has some nice new features and enhancements, especially: There are many more changes which you can read about on the . Please download and use 0.90.1, and let us know what you think. ","locales":"","title":"Elasticsearch 0.90.1 Released"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2013-04-08T00:00:00.000Z","url":"/blog/0-90-0-rc2-released","seo_title":"","content":" version is out, the last release candidate before the 0.90 release. You can download it . This version contains a number of bug fixes and enhancements which bring us closer to a stable release of 0.90. It is also our first release to make an RPM available, which should work for RedHat, Fedora and openSUSE. Some of the changes are: Upgrading to it from previous 0.90.* releases is highly recommended. ","locales":"","title":"0.90.0.RC2 Released"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2013-03-20T00:00:00.000Z","url":"/blog/0-90-0-rc1-released","seo_title":"","content":" version is out. You can download it . Why 0.90? More than anything else, it shows where we are heading. The next stable release after 0.90 will be the 1.0 release of elasticsearch. Elasticsearch is being used in production, in mission critical applications, daily, and we feel its time for us to move to that blessed 1.0 release. It is also an indication of the breadth of changes that accompany this release. Down the Road Our aim now is to get 0.90.0 GA out the door. We are working hard on finalizing the last bits to get it done, and hope for a quick cycle to get to the GA. Once its out, we will blog about what we hope to get to in our 1.0 release. Features This release includes many new features, an evidence of the speed of development we have achieved with a team of extremely talented people working daily on the codebase. We are also actively working on documenting all the new features in the guide. Here are some of the highlighted features in this release: New Routing Algorithm () Up to 0.90.0, elasticsearch used a relatively naive algorithm of balancing shards across the cluster, by trying to maintaining an even number of shards across (data) nodes. This created problems with clusters holding many indices, specifically with varied sizes. The new algorithm takes the fact that shards belong to an index into account, trying to balance an index out across all nodes. The new algorithm is based on a weight function, and we will slowly add additional weight options (such as shard size) down the road. Suggst () The suggest feature (part of a search request) allows Elasticsearch to provide suggestions for the given text based on the corpus that is part of the index. The current implemented suggest type uses the Levenshtein distance on a per term basis to potentially provide spelling suggestions. This feature is considered experimental, mainly in the request and response format. We are actively working on additional suggest implementation (such as phrase based suggestion), and this work will be finalized towards the 0.90.0 GA release. Lucene 4 Codecs () Elasticsearch exposes the new codecs infrastructure added by Lucene 4. Codecs allow complete control over how the index is actually stored and read. Though implementing a custom Codec can be considered adventurous (but quite doable as a plugin), elasticsearch exposes several options to control codecs using the mappings. For example, here is an example of mappings that will load the index for the field into memory, and use a bloom filter for the relatively unique field: { \"my_type\" : { \"properties\" : { \"tag\" : {\"type\" : \"string\", \"postings_format\" : \"memory\"}, \"external_id\" : {\"type\" : \"string\", \"postings_format\" : \"bloom_default\"} } } } A nice feature is the ability to change on the fly for existing mapping, which will start to be used for new data as it is indexed, or applied to old data as merging / optimization happens. Field Data Refactoring Field Data in elasticsearch is the data structure used to load field values into memory for sorting and faceting. The data structure has been completely reimplemented, requiring considerably less memory than before, and has been abstracted away to allow for additional future implementations. Multi Value Sorting (, ) Sorting on fields with multiple numeric values now work as expected, properly choosing how to sort based on the sort direction. Sorting on nested documents is now supported as well (with an option filter). On top of it, sorting on multiple values now support the ability to sort based on the , , , and values. Lucene 4 Similarity () Similarity in Lucene allows us to control how relevancy or scoring is done. The new Lucene 4 adds exciting new similarity implementations, such as on top of the current TF/ based one. It also allows us to set similarities per field, which is exposed in elasticsearch through mappings. Compression In 0.19, elasticsearch added compression to stored fields (the ), which had to be enabled by using a specific setting. Now, compr","locales":"","title":"0.90.0.Beta1 Released"}
{"index":{}}
{"author":"Zachary Tong","category":"Engineering","publish_date":"2013-02-20T00:00:00.000Z","url":"/blog/managing-relations-inside-elasticsearch","seo_title":"","content":" Data in the real world is rarely simple - often times it is a jumble of interlocking relations. How do you represent relational data in Elasticsearch? There are a few mechanisms that can be used to provide relation support. Each has their pros and cons, making them useful for different situations. Inner Objects The simplest mechanism are named \"inner objects\". These are JSON objects embedded inside your parent document: { \"name\":\"Zach\", \"car\":{ \"make\":\"Saturn\", \"model\":\"SL\" } } Simple, right? The \"car\" field is just another JSON object, with the inner object having two properties (\"make\" and \"model\"). This inner object mapping will work as long as you have a one-to-one relationship between the root object and the inner object. E.g. every person has at most one \"car\". But what if Zach owns two cars, and we add another person (Bob) who owns just one car? { \"name\" : \"Zach\", \"car\" : [ { \"make\" : \"Saturn\", \"model\" : \"SL\" }, { \"make\" : \"Subaru\", \"model\" : \"Imprezza\" } ] } { \"name\" : \"Bob\", \"car\" : [ { \"make\" : \"Saturn\", \"model\" : \"Imprezza\" } ] } Ignoring the fact that Saturn never made an Imprezza car, what happens when we try to search for it? Logically, only Bob has a \"Saturn Imprezza\", so we should be able to do a query like: `query: car.make=Saturn AND car.model=Imprezza` Right? . If you perform that query, you'll receive both documents as the result. What happens is that Elasticsearch internally flattens inner objects into a single object. So Zach's entry looks like this: { \"name\" : \"Zach\", \"car.make\" : [\"Saturn\", \"Subaru\"] \"car.model\" : [\"SL\", \"Imprezza\"] } Which explains why it was returned as a result. Elasticsearch is fundamentally flat, so internally the documents are represented as flattened fields. Hmm. Nested As an alternative to inner objects, Elasticsearch provides the concept of \" \". Nested documents look identical to inner objects at the document level, but provide the functionality we were missing above (as well as some limitations). Example nested document: { \"name\" : \"Zach\", \"car\" : [ { \"make\" : \"Saturn\", \"model\" : \"SL\" }, { \"make\" : \"Subaru\", \"model\" : \"Imprezza\" } ] } At the mapping level, nested types must be explicitly declared (unlike inner objects, which are automatically detected): { \"person\":{ \"properties\":{ \"name\" : { \"type\" : \"string\" }, \"car\":{ \"type\" : \"nested\" } } } } The problem with inner objects was that each nested JSON object is not treated as an individual component of the document. Instead they were merged with other inner objects sharing the same property names. This is not the case with nested documents. Each nested doc remains independent, and you can perform a query like `car.make=Saturn AND car.model=Imprezza` without a problem. Elasticsearch is still fundamentally flat, but it manages the nested relation internally to give the appearance of nested hierarchy. When you create a nested document, Elasticsearch actually indexes two separate documents (root object and nested object), then relates the two internally. Both docs are stored in the same Lucene block on the same Shard, so read performance is still very fast. This arrangement does come with some disadvantages. Most obvious, you can only access these nested documents using a special ` `. Another big disadvantage comes when you need to update the document, either the root or any of the objects. Since the docs are all stored in the same Lucene block, and Lucene never allows random write access to it's segments, updating one field in the nested doc will force a reindex of the document. This includes the root and any other nested objects, even if they were not modified. Internally, ES will mark the old document as deleted, update the field and then reindex everything into a new Lucene block. If your data changes often, nested documents can have a non-negligible overhead associated with reindexing. Lastly, it is not possible to \"cross reference\" between nested documents. One nested doc cannot \"see\" ano","locales":"","title":"Managing Relations Inside Elasticsearch"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2013-02-14T00:00:00.000Z","url":"/blog/0-20-5-released","seo_title":"","content":" version is out. You can download it . This release includes several critical bug fixes, and it is a recommended upgrade for and users. ","locales":"","title":"0.20.5 Released"}
{"index":{}}
{"author":"Steven Schuurman","category":"News","publish_date":"2013-02-19T00:00:00.000Z","url":"/blog/way-beyond-search","seo_title":"","content":" I am thrilled to announce that we just secured a $24M Series B round of funding from Index Ventures.I know what you're thinking ... “didn't you just close a $10M Series A in November?\" Yes we did, but this was very much the right thing to do for Elasticsearch as an open source project and as a company. Peter Fenton from Benchmark Capital led our A round, and Mike Volpi from Index's San Francisco office took the lead on our B Round - not a bad starting point for a company that's working hard to take Search way beyond merely querying data.In the light of our investment announcement, there are 3 topics we would like to share our thoughts on: Raising Series BWe did not wake up one day with the brilliant idea to set out and raise Series B. In fact, when the plan came together, we were still recovering from our previous fund raising adventure. We did have a great experience talking to some of the greatest minds in venture capital, and received great advice and feedback on how to build our company. Besides that, we learned that the investment community's interest in Elasticsearch was tremendous. This got the ball rolling, and sparked a series of brainstorm-style discussions that got us to start pondering what we would do if we raised an early B round. The answer came to us as quickly as the decision to look for Series B did. Raising a Series B allows us to execute faster on our growth plans for the company and thereby do a better job at serving our rapidly growing customer base on both sides of the Atlantic, in the US and Europe. The driver of this growth is the rapid evolution of search itself.Beyond Just SearchSearch today is so much more than it was even 5 years ago. The technology evolved into a data exploration mechanism and is now used in ways that go way beyond your basic “free text search\" search box on the upper right corner of a website. Search has become top-of-mind for so many people and the world is quickly getting a handle on what search is actually capable of. We feel there are actually a number of reasons why search has made an amazing jump in popularity over the last 12 months: Search solutions like Elasticsearch have a very promising relationship with rapidly expanding data volumes. Whereas search technology and large data volumes fell in love many years ago, we like to believe they recently got engaged and they're currently planning their wedding. It wouldn't surprise us if this celebrity wedding actually lasted.What Elasticsearch made possible with its first release in 2009 (search beyond free text search), is now being adopted by some of the most data intensive organizations in the world. Next generation social network Path very recently announced advanced search functionality in their platform, and as recently as mid-January, Facebook launched its much-discussed Graph Search functionality.Also, with the search technology now capable of powering advanced real time analytics, we see an increase in users depending heavily on search technology to power their dashboards. We know of various companies that use Elasticsearch to drive Google Analytics-like features and can't afford for its service to be disrupted. Even trend analysis is something we see more and more often. For example, a user can query massive data sets though unstructured searches, sprinkle structure on top of the result sets to break down the result over time, and analyze results through a date histogram graph. Let's say that you're applying this formula to Twitter data, you would instantaneously see a trend in time: very powerful, yet extremely easy to realize these days with Elasticsearch.The Future of ElasticsearchIt is our mission to make real-time data exploration available to anyone, and we feel Elasticsearch is very well positioned to achieve just that. Even though Elasticsearch is off to a good start, we feel there are still many places where we can add value into the future. As a company, one of our objectives is to accelerate the Elasticsearch lea","locales":"","title":"Way Beyond Search"}
{"index":{}}
{"author":"Zachary Tong","category":"Engineering","publish_date":"2013-01-15T00:00:00.000Z","url":"/blog/searching-with-shingles","seo_title":"","content":" global wrapper ","locales":"","title":"Searching with Shingles"}
{"index":{}}
{"author":"","category":"","publish_date":"2012-11-14T00:00:00.000Z","url":"/blog/welcome-clinton","seo_title":"","content":" I am extremely happy to announce that Clinton Gormley (\"\") has joined elasticsearch. I still remember those early days in elasticsearch, when we had 3 people on IRC and \"that Perl dude\" kept asking questions on how it works, and gave instrumental insights on the elasticsearch API design and usability. When I created elasticsearch, the goal was always to make it usable and easy to use from any language, and Clinton, with his \"Perlishness\", helped keep elasticsearch honest. As time passed Clint has also become an invaluable source of information and help to the rest of the elasticsearch community, on the mailing list, IRC, and at conferences.Joining our company, Clint will continue to do his thing, but now full time on elasticsearch. He will continue to develop the Perl libraries, help the community around elasticsearch, and sprinkle his high standards when it comes to keeping and making elasticsearch usable and easy to use.Welcome Clint! ","locales":"","title":"Welcome Clinton!"}
{"index":{}}
{"author":"Steven Schuurman","category":"News","publish_date":"2012-11-08T00:00:00.000Z","url":"/blog/elasticsearch-easy-complete-and-robust-just-like-our-product","seo_title":"","content":" With unbelievable excitement we just announced Elasticsearch secured $10M in Series A venture funding from , led by Peter Fenton. Other investors in the round include my SpringSource co-founder Rod Johnson and . Being able to work with Peter Fenton and Rod Johnson again after SpringSource is nothing short of a privilege. Both are established open source visionaries and have already brought a lot of value to the company in terms of insights, network and general advice over the last few weeks.Three reasons stand out why Benchmark, Rod, and Data Collective believe in Elasticsearch: the product and the company: When Shay and I joined forces about 6 months ago, we set out to build a company based on our shared belief that we would be able to create a beacon of light in our industry. Our industry, mostly referred to as the big data market, is undeniably complex and extraordinary technology heavy. Many companies provide a solution that solves one of the many technical challenges companies are facing when aiming to gain competitive advantage from the massive volumes of data it's gathering on a daily basis. I believe that up until Shay created Elasticsearch, very few – if any –of the solutions available were able to make achieving this objective as easy as operating an Apple device.So in a way Elasticsearch already achieved its objective, which makes my life as the CEO of Elasticsearch a lot easier of course. However, we're not done. In fact, we're only getting started…Since we launched the company, I have received many questions on where we want to take the company and what we plan on doing moving forward. The key element to the answer to that question has always been, that our way forward is merely a continuation of the path we're currently on. Please allow me to elaborate.The more we talk to people who use Elasticsearch, the more we are able to distill 3 main reasons why Elasticsearch is so popular. In a nutshell, people love our product because it is: To us, this is an extremely important finding. Predominantly because this confirms that Shay and his team are achieving their objective: to make an extremely easy to use product that reliably delivers on its promise. The mere fact that a product achieve it's objective is actually not as common as it might seem. As products mature, markets mature and as product's established user base grows, it's not uncommon for product roadmaps to deviate from its original path due to heavy pressure from many outside forces. I believe that Elasticsearch was created after a solid vision, rather than based on feature requests from individual users. This has had a massive positive impact on Shay's ability to create a product that is usable for an immensely large audience, instead of just a handful of power users. Shay has not given in to temptations, but rather worked closely together with many users to create a user-friendly product that meets all critical requirements in a large number of use cases.The challenge for our company is to be as focused on being as easy to work with, robust and complete as our product is. Now that doesn't mean that there's nothing left to be desired. Au contraire, we are currently working hard on bullet proofing our support organization by scaling it out fast and fine-tuning our systems to ensure the quality of our support increases while our client base rapidly grows. We are also working on a public training course schedule that covers more cities across the globe, to ensure attendees never have to travel unacceptably far to attend a course. And as is to be expected, we are growing our executive team with talented professional that have the required skills to help grow our reach.With Elasticsearch now having a company behind it to propel it forward, it seems safe to conclude that the Big Data Dark Ages are now finally behind us.Steven & Shay ","locales":"","title":"Elasticsearch: Easy, Complete and Robust – Just like Our Product"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2013-01-29T00:00:00.000Z","url":"/blog/0-20-4-released","seo_title":"","content":" version is out. This release includes important bug fixes over the previous release and an upgrade is highly recommended. ( was also released a couple of days ago, but had a problematic deb package). ","locales":"","title":"0.20.4 Released"}
{"index":{}}
{"author":"Steven Schuurman","category":"News","publish_date":"2013-01-14T00:00:00.000Z","url":"/blog/some-2012-elasticsearch-highlights","seo_title":"","content":" Have you ever had that feeling when - due to sheer excitement - it's hard to decide what to talk about first because there's just too much to tell? Well, that's where I'm at right now… Please allow me to jump right in.Adoption and Series-A Investment with BenchmarkOne of the most illustrative news items of last year was around our download numbers: 2012 was without any doubt the year Elasticsearch took off like a rocket ship. Within a year, our download numbers went from around 50k/month to more than 200k/month and we surpassed the 1.5 million downloads mark. In all my years in open source, including my years at SpringSource, I have never witnessed anything as spectacular as the speed at which the world is adopting Elasticsearch. Personally, I believe the popularity of the product isn't that hard to explain. I remember our investor Peter Fenton tweeting the following last November when we announced our Series-A with Benchmark:“Jaw-dropping momentum, Big Data's killer app has arrived:@elasticsearch Benchmark thrilled to back the team…\"I believe this statement to be spot on as Elasticsearch is groundbreaking in two very relevant aspects:1) Its power to drive information out of extremely large volumes of data 2) Its user friendlinessThese two design characteristics combined with Elasticsearch being fast, real-time and highly scalable make that it lives up to Peter's aforementioned claim.  what I wrote about our Series-A investment from Benchmark, Rod Johnson and Data Collective last November.Elasticsearch in the WildAnother set of important highlights (which get us really fired up, and provide for very rewarding reading material) are the use cases we encounter in the wild. We now know Elasticsearch is literally used everywhere there's data – and as we all know - companies are accumulating more data every day. To put it in perspective, Wired recently described Elasticsearch at “Your Own Private Google.\"  provides for an interesting read, so I recommend checking it out.However, there are many more use cases for Elasticsearch, and though out 2012 many people have been kind enough to share their experiences with us. To pick a few, here are some publications that are interesting to everyone considering making a move on Elasticsearch as their search & analytics runtime of choice. If you're interested in sharing some of your Elasticsearch war-stories, we'd love to . Or, you can of course just  on Twitter and tweet about your findings.At Elasticsearch we're grateful for all those wonderful stories that are being shared online by so many people. It's great inspiration for the entire Elasticsearch team and undoubtedly also for people considering test-driving Elasticsearch for a bit.So thank you very much for sharing – and we look forward to hearing more from you in 2013. ","locales":"","title":"Some 2012 Elasticsearch Highlights"}
{"index":{}}
{"author":"Drew Raines","category":"","publish_date":"2012-12-17T00:00:00.000Z","url":"/blog/new-download-service","seo_title":"","content":" Elasticsearch has been using Github’s static downloads feature as a means of distributing releases for both the core and its plugins for a few years now. Unfortunately, last week, . We’ve been working the past few days to replace this functionality internally. All of the links to elasticsearch core have been updated to the new site through . We’re still migrating our plugins, so you’ll see those change in the next day or two. We are still working on a solution for third-party plugins. The old assets will be removed tomorrow, so if you’re using any links in your provisioning system please update them to the commensurate . ","locales":"","title":"New Download Service"}
{"index":{}}
{"author":"","category":"","publish_date":"2012-10-26T00:00:00.000Z","url":"/blog/welcome-igor","seo_title":"","content":" I am very happy to announce that Igor Motov () is joining Elasticsearch. Igor has extensive knowledge on Elasticsearch, and has written several plugins for it, including a zookeeper discovery module and the jetty plugin. He is also the co-organizer of the . Igor will join our team focusing on continuing and improving Elasticsearch as his full time job.Personally, I am very excited about having Igor join us. We just got a big boost on our ability to execute on our vision that is Elasticsearch. Welcome Igor! ","locales":"","title":"Welcome Igor"}
{"index":{}}
{"author":"Simon Willnauer","category":"","publish_date":"2012-07-13T00:00:00.000Z","url":"/blog/the-rise-of-open-source-search","seo_title":"","content":" Wow! I'm excited to write this blog today being the day that Open-Source Search moved a huge step forward. I have been working with Lucene for over 8 years now spending an enormous amount of time on adding features, fixing bugs, writing documentation and eventually bringing the project forward together with an awesome gang of committers. I think I can state that Lucene has become one of the most successful Open-Source libraries serving million of users worldwide. Yet, there was always something missing in the picture when you need to go out and build a search system especially when it gets to reliability, fail-over and distributed search. You know those things are hard and have always been out of scope for the Lucene library. A couple of years ago Elasticsearch came into the picture offering an extremely promising idea by an extremely passionate engineer. What can I say, “it's grown up\", here comes Elasticsearch the company!With the rise of NoSQL, Hadoop and being Big-Data everywhere Lucene's star didn't shine that bright without offering all these features out of the box and related projects like Solr are slowly catching up with scaling capabilities. But going from an idea to a mature and stable software is a pretty rocky path and it takes time and effort to go. Elasticsearch has gone down that path with all the hard problems in mind from day one. Hey, this is awesome - you know, for search! However, it's not just Buzz! 4 weeks ago at the Berlin Buzzwords conference you could literally feel how much momentum Elasticsearch gained in the last couple of years. Rooms were packed, engineers talking about success stories, limitations, features and improvements all over the place. This is healthy in a lot of ways and a good indicator that it's time to take Elasticsearch to the next level. When Shay, Steven, Uri and myself first talked about the idea to join forces and to establish Elasticsearch as a company it was already obvious to me that this is an enormous chance for Open-Source Search in general as well as Lucene being the underlying technology. As a company we heavily rely on the features, maturity and stability of the software we incorporate into Elasticsearch and in turn on the health of the Open Source community. It's our job and responsibility to maintain it and push it forward. I'm looking forward to a great time working together with great engineers on hard problems making the Elasticsearch user experience as smooth as possible and making Lucene the only choice when you need Search - and trust me you need it! ","locales":"","title":"The Rise of Open-source Search"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2012-06-26T00:00:00.000Z","url":"/blog/0-19-7-released","seo_title":"","content":" version is out. You can download it . The release includes mainly bug fixes, fixing the broken debian package, and a bug in the new store compression option (on reads). ","locales":"","title":"0.19.7 Released"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2012-05-21T00:00:00.000Z","url":"/blog/0-19-4-released","seo_title":"","content":" version is out. You can download it . The release includes serveral bug fixes, mainly around percolator, mapping type, and file based config index templates. It also includes an enahcement allowing to configure the query to be , not failing when wrong format of a value is provided (for example, a text to a numeric field), as well as the ability to search “within” objects using wildcards in the syntax (it was already provided if it was used in the section), for example: . ","locales":"","title":"0.19.4 Released"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2012-03-01T00:00:00.000Z","url":"/blog/0-19-0-released","seo_title":"","content":" version is out. You can download it . It is the final 0.19.0 release (after 3 release candidates). 0.19 major features have been outlined in the previous blog posts for each release candidate release, but, it also marks an important bug fix and stability release over 0.18, upgrade is highly encouraged to all elasticsearch users. ","locales":"","title":"0.19.0 Released"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2012-01-10T00:00:00.000Z","url":"/blog/0-18-7-released","seo_title":"","content":" version is out. You can download it . It includes bug fixes including major bug fix in state storage with shared gateways (s3 for example), and support for nested documents in delete by query. ","locales":"","title":"0.18.7 Released"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2011-10-26T00:00:00.000Z","url":"/blog/0-18-0-released","seo_title":"","content":" version has just been released. You can download it . This is a major release, and includes the following major features: : was quickly released to fix a bug with custom location configuration and fetching inner fields from a source doc. Shard Allocation Awareness and Filtering Shard allocation within a cluster can now be “aware” of which node replicas are allocated on (similar to rack awareness). It allows to make sure that a shard and a replica will be allocated across logical grouping of nodes. The awareness can also be forced, by not over allocating replicas within the same logical node group. Search and Get operations will automatically prefer shards that are allocated within the same logical node group the search is executing on. Filtering allows to explicitly control which nodes indices are allowed, or not allowed, to be allocated on, again, based on custom logical grouping of nodes (called node attributes). Both settings can be updated in realtime using either the cluster wide or index level update settings . More info can be found . Cluster Update Settings The cluster update settings allow to update node level settings (and not index level settings) in realtime on a live cluster. The settings allowed and more docs on the are provided . Timestamp and A document can now automatically be indexed with a timestamp associated with it, and have a (time to live), which, when expired, will cause the document to be deleted. More info can be found and . Improved Geo Execution Geo distance filter and facet good a considerable performance boost by doing bounding box optimization. Also, an option to use indexed lat lon for the checks (must be enabled in the mapping) is provided which can provide even faster executing under certain conditions (compared to in memory checks). More Statistics More statistics are now provided out of the box, including a new index stats (the status should not be used for stats anymore). Statistics for APIs such as index, get, and search are also being aggregated, with search allowing to specific specific stats grouping aggregation (to further distinguish search “types”). Multi Data locations A node can now work with multiple data locations, spreading the index files across those locations in a 0 like behavior. Smaller improvements and bug fixes Many more bug fixes and smaller improvement have went into this release. Full release notes can be found . -shay.banon ","locales":"","title":"0.18.0 Released"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2011-08-12T00:00:00.000Z","url":"/blog/0-17-5-released","seo_title":"","content":" version has just been released. You can download it . This is a bug fix release and minor feature enhancements. Upgrade is highly recommended. ","locales":"","title":"0.17.5 Released"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2011-07-27T00:00:00.000Z","url":"/blog/0-17-2-0-16-5-released","seo_title":"","content":" version and have just been released. You can download it . A major concurrency bug has been fixed when executing search requests, which is the reason for the 0.16.5 release and also included in the release. The release includes mainly bug fixes, and its highly recommended to upgrade to it if is used. ","locales":"","title":"0.17.2 / 0.16.5 Released"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2012-04-30T00:00:00.000Z","url":"/blog/0-19-3-released","seo_title":"","content":" version is out. You can download it . The release includes an upgraded Lucene 3.6 version, and several bug fixes, including better handling of network splits with node client nodes and with minimum master nodes. ","locales":"","title":"0.19.3 Released"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2012-02-21T00:00:00.000Z","url":"/blog/0-19-0-rc3-released","seo_title":"","content":" version is out. You can download it . Its another bug fix released and (hopefully) the final release of 0.19.0 release candidates. It also includes two nice features. The first is the ability to use “date math” on types (especially useful in queries/filters). See more info in the (and documented under the mapping date type). The second feature is a new called “Multi Search” (or ). It allows to construct a single request holding several search requests, executing them in the cluster (in parallel) and returning the results. More info on the format and how to use it can be found in the (and documented under the guide section). ","locales":"","title":"0.19.0.RC3 Released"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2011-12-19T00:00:00.000Z","url":"/blog/0-18-6-released","seo_title":"","content":" version is out. You can download it . It includes minor features and important bug fixes. Changes can be found . ","locales":"","title":"0.18.6 Released"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2011-10-06T00:00:00.000Z","url":"/blog/0-17-8-released","seo_title":"","content":" version has just been released. You can download it . The release includes major bug fixes listed . ","locales":"","title":"0.17.8 Released"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2011-08-05T00:00:00.000Z","url":"/blog/0-17-4-released","seo_title":"","content":" version has just been released. You can download it . This is a quick release post to fix a reported bug which probably doesn’t affect most users, but its still an important one to get out there (for new users) as its a difficult one to track down… . The upgrade is only really needed when using explicit stored fields that have multi values, which can result in not all the values being returned when asked. ","locales":"","title":"0.17.4 Released"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2011-07-21T00:00:00.000Z","url":"/blog/0-17-1-released","seo_title":"","content":" version has just been released. You can download it . This is a quick bug fix release fixing major bugs (listed ). ","locales":"","title":"0.17.1 Released"}
{"index":{}}
{"author":"","category":"","publish_date":"2013-01-16T00:00:00.000Z","url":"/blog/welcome-david","seo_title":"","content":" I'd like to warmly welcome David Pilato () to elasticsearch. David is a prominent elasticsearch user and advocate, and has been doing an amazing work spreading Elasticsearch in France. I just came back from Paris giving our elasticsearch training with and talking at the first France elasticsearch meetup. You can easily see David influence, resulting in 100 people attending the meetup!.David will continue his amazing work promoting elasticsearch, as well as contribute to the development of elasticsearch. You can also check on joining our Elasticsearch. ","locales":"","title":"Welcome David"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2012-12-27T00:00:00.000Z","url":"/blog/0-20-2-released","seo_title":"","content":" version is out. This release includes both important bug fixes as well as upgrades to Lucene (from 3.6.1 to 3.6.2), and other internal libraries. Another change includes how plugins are downloaded. Now, “elasticsearch” plugins are downloaded automatically from our new service, but it can also download plugins from maven directly. Site plugins can stil be downloaded from github as “repositories”. In the future, we will also allow users to upload their own plugins to if they wish to. Enjoy! ","locales":"","title":"0.20.2 Released"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2012-12-04T00:00:00.000Z","url":"/blog/0-19-12-released","seo_title":"","content":" version is out. You can download it . It contains several bug fixes (upgrade when possible…) and a new feature for indexing slow log (similar to search slow log feature). This version also deprecates the snappy compression option (see more ), and the Shared Gateway (more info ). ","locales":"","title":"0.19.12 Released"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2012-10-01T00:00:00.000Z","url":"/blog/0-19-10-released","seo_title":"","content":" version is out. You can download it . It contains several bug fixes but also new features, including: has_parent filter/query filter allows to execute a query on parent documents and result in matching children documents (the inverse of ). More info . Also, the performance of filter/query has been greatly improved. Less overhead of filter cache The filter cache cleanup process is now more optimized, specially when large number of shards exists on a node. Cluster Reroute The allows to explicitly send commands that would move, cancel, and allocate shards. Note, this is an experimental and feedback is welcomed. Bulk The interface allows to have elasticsearch listen using for bulk formatted requests. While comes with its own downsides, it can come in handy, specially when indexing non critical data (ala statsd). This interface is also experimental. ","locales":"","title":"0.19.10 Released"}
{"index":{}}
{"author":"","category":"","publish_date":"2012-09-04T00:00:00.000Z","url":"/blog/welcome-karmi","seo_title":"","content":" I very happy to announce that Karel Minarik () is joining Elasticsearch. Karel is well known in the Elasticsearch community, specifically the ruby community as the author of the popular library, though he is quite the polyglot that dabbles in chef, emberjs and many more.Go ahead and read , I would simply add that we feel lucky that we have Karel on our team, as it greatly helps us realize our vision around Elasticsearch, and our commitment to our community, specifically, the ruby one. ","locales":"","title":"Welcome Karmi"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2012-08-23T00:00:00.000Z","url":"/blog/0-19-9-released","seo_title":"","content":" version is out. You can download it . It contains several bug fixes but also new features, including: Snappy Compression New experimental support for compression on top of the current . Changing to use it is as simple as setting to . Note, anything already compressed with will still work correctly. renamed to The query name gave it great injustice, as it nicely also handles dates and numeric types as well for example. It was renamed to in order to better reflect what it actually does, with backward support for the name as well. On top of that there is now support for query, allowing to more easily run queries on several fields without the verbosity of using query to wrap it. Explain New called , allowing to easily check how and why a specific document matches a query, or even understand why a specific document does not match a specific query. Networking Enhacements Several networking enhancements geared towards reducing further the amount of buffer copies while doing network operations, and better defaults for network buffer sizes. ","locales":"","title":"0.19.9 Released"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2012-07-13T00:00:00.000Z","url":"/blog/you-know-for-search-inc","seo_title":"","content":" When I set down to write the first lines of code for Elasticsearch, about 3 years ago, I looked at my wife and my 2 months old daughter and knew perfectly what I was getting into. Well, as Miracle Max would say, I mostly did. I knew that its going to be a commitment that will take a big chunk of my life to follow through, I knew that I was building something useful that will make developers life simple, and I knew that there is a need for something like Elasticsearch out there.Obviously, I knew all those things, but not many others did. Its funny, as Tim Robbins found out in the classic Cohen brothers movie, “The Hudsucker Proxy\", getting people to see a circle and state “You Know, for Kids!\", and making the leap to understand what it can actually be is not simple.Thomas Jefferson said: “I am a great believer in Luck, and I find that the harder I work, the more I have of it\". Elasticsearch was and is certainly not a walk in the park. Building a distributed system, that can handle massive amounts of data, and still be usable is no simple task. I do feel lucky though, we live in an age where anyone can open a laptop and set out to change the world (a bit), with extra luck points for having an understanding spouse.More than that, I feel lucky with the community that evolved around Elasticsearch. It caught me by surprise the speed at which Elasticsearch got adopted. I knew that something was missing, I just couldn't believe how quickly people will realize it as well. I am very proud of what happened around Elasticsearch, the ecosystem that developed around it, and the users actually taking and using it in real systems and in production.But, dodging buses like crazy for the past couple of years has left me exhausted, and the user's request for having something formal around Elasticsearch, the need to feel safe, has been increasing exponentially. You see, I have seen it happen for 10 years now, it usually starts with “lets just have a search box so people can search some content\", and quickly evolves to using it in many other parts of the stack / system, quickly increasing its importance in the application.Luckily (do you notice a trend?), a few months ago, my good friend from the Compass days Uri Boness mentioned that he was doing something around search, and I got to know Steven, who was leading it. Steven and myself immediately hit it off. You see, Steven and myself think very much alike, yet still differently. It's a rare combination that doesn't happen frequently, but when it does, exciting things happen. And to top it all, Steven comes with an extensive track record when it comes to Open Source projects. When I learned that Simon Willnauer, one of Lucene rock stars is part of the team as well, the inevitable conclusion was not that hard to make. It Just Felt Right.So, I am extremely happy to announce that we now have an Elasticsearch company, your basic one stop shop for anything to do with Elasticsearch. What does it mean? It means that we can basically move Elasticsearch harder, faster, better, and stronger, while providing all the services you might expect from an Open Source company.On top of that, the Elasticsearch team also includes Nick White (the finance wizard), as the CFO. Chris Male, and Martijn van Groningen, both amazingly talented developers and Lucene committers, and the valuable Elissa Nancarrow to handle, well, basically everything else. With the future holding additional talented people joining the company (and if you are up to it, we are hiring!), it really feels like we are on our way to build something really special.I still vividly remember 10 years ago sitting in a one room apartment in London, with no job, first getting into the search space by writing “iCook\" for my wife while she was studying to be a Chef at the Cordon Bleu. Its been a long journey to get to this point, yet it feels like it has just begun... ","locales":"","title":"You Know, for Search! (inc)"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2012-07-02T00:00:00.000Z","url":"/blog/0-19-8-released","seo_title":"","content":" version is out. You can download it . The release includes bug fixes including integer overflow when reading the new compressed stored format, and a fix for failure to of recovery of peer shards due to failure to detected comrpessor (not common). Dangling Indices It also includes better handling of dangling indices. Dangling indices happen when a node that has several indices stored locally, joins a cluster and those local indices do not exists in the cluster metadata. This usually does not happen, especially not with proper gateway.recover_after_nodes flag, but still, users can by mistake get into this state. A new setting setting, with possible values of (never import dangling indices, but also delay the delation of them), (import dangling indices), and (import dangling indices, but in closed state). The default value is . This also allows to recover “other” indices into a cluster, by mv’ing it into an existing node or a new node, where they will be detected as dangling and be recovered (or imported) into the cluster automatically. Actually, it also has a nice trick of possibly using a “new” index name based on the name of the index on the file system. Wildcard Multi Index Syntax Multi index syntax has just become considerably simpler. Current multi index sytax allows to specify several indices to search on using , now, it also support wildcards, for example: . It also support the ability to “add” () and “remove” (), for example: . ","locales":"","title":"0.19.8 Released"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2012-06-25T00:00:00.000Z","url":"/blog/0-19-5-released","seo_title":"","content":" version is out. You can download it . The release includes critical bug fixes in elasticsearch cluster management and peer recovery process, and it is highly recommended for all users. [Update]: Some users reported a problem when starting elasticsearch with due to verification error. This problem has been fixed in . p. [Update 2]: The package seems to be broken in and , a fix will be issued later. p. [Update 3]: has been released fixing the debian packaging problem and a bug in the new compression support (on reads). The release also includes several features, including: Store Compression elasticsearch has had the ability to compress the document for a long time. The problem with it is the fact that small documents don’t end up compressing well, as several documents compressed in a single compression “block” (we use , but Snappy, whihc will be supported also in the future, works the same) will provide a considerable better compression ratio. This version introduces the ability to compress stored fieds using the setting, as well as term vector using the setting. The settings can be set on the index level, and are dynamic, allowing to change them using the index update settings . elasticsearch can handle mixed stored / non stored cases. This allows, for example, to enable compression at a later stage in the index lifecycle, and optimize the index to make use of it (generating new segmetns that use compression). Best compression, comprared to _source level compression, will mainly happen when indexing smaller documents (less than 64k). The price on the other hand is the fact that for each doc returned, a block will need to be decompressed (its fast though) in order to extract the document data. Store Throttling The way Lucene, the IR library elasticsearh uses under the covers, works is by creating immutable segments (up to deletes) and constantly merging them (the merge policy settings allow to control how those merges happen). The merge proces happen in an asyncronous manner without affecting the indexing / search speed. The problem though, especially on systems with low IO, is that the merge process can be expensive and affect search / index operation simply by the fact that the box is now taxed with more IO happening. The store module now allows to have throttling configured for merges (or all) either on the node level, or on the index level. The node level throttling will make sure that out of all the shards allocated on that node, the merge process won’t pass the specific setting bytes per second. It can be set by setting to , and setting to something like . The node level settings can be changed dynamically using the cluster update settings . If specific index level configuration is needed, regardless of the node level settings, it can be set as well using the , and . The default value for the type is , meaning it will throttle based on the node level settings and participate in the global throttling happening. Both settings can be set using the index udpate settings dynamically. ","locales":"","title":"0.19.5 Released"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2012-03-20T00:00:00.000Z","url":"/blog/0-19-1-released","seo_title":"","content":" version is out. You can download it . It is a bug fix release fixing several bugs including a major bug where a thread can get into spin when its relocated and a search/stats operation is executed against it. The release also includes an update to the `read_only` option on an index. The option caused both write and metadata operations to be disabled for the relevant index. Now, in 0.19.1, specific read, write and metadata blocks can be set. See more . ","locales":"","title":"0.19.1 Released"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2012-02-07T00:00:00.000Z","url":"/blog/0-19-0-rc1-released","seo_title":"","content":" version is out. You can download it . It is a major release candidate release of elasticsearch. This is the first time we release a “release candidate” for elasticsearch. The aim is to get users to try it out, fix any problems found with it, and release a final release as soon as possible. Major features include improved indexing performance, more control over shard allocation, more statistics and simpler for stats, an update that accepts a script to perform an update on a document, better state storage when using local gateway, and many bug fixes. Upgrading This release requires a full cluster restart in order to upgrade to the new version (including issuing a flush across all indices before the restart). If you are using the (default) local gateway, it will automatically be upgraded to a new and improved state format (with backups of the original state). The structure of the project has also changed, moving all the plugins to their own repos under . Installing a plugin now requires specifying the location and version to install it from. ","locales":"","title":"0.19.0.RC1 Released"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2011-11-16T00:00:00.000Z","url":"/blog/0-18-3-released","seo_title":"","content":" version has just been released. You can download it . Its an important bug fix release, including a fix for a and upgrade is highly recommended. An version with the bug fixed has also been released. The release also includes additional bug fixes, and some features including , and improved handling for dates. [Update]: Heads up that this release includes a broken plugin script, more info . [Update 2]: has been released fixing the plugin script. -shay.banon ","locales":"","title":"0.18.3 Released"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2011-08-13T00:00:00.000Z","url":"/blog/0-17-6-released","seo_title":"","content":" version has just been released. You can download it . This is is a quick release to fix a bug in automatic date detection when provided as or . ","locales":"","title":"0.17.6 Released"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2011-07-27T00:00:00.000Z","url":"/blog/mailing-list-migration","seo_title":"","content":" I created the mailing list on a google apps account (paying one…) with the hopes of not running into spam problems down the road. The problem with that is the fact that I did not know about the host of problems I will have with google account one… . The latest one is a major problem, where google apps restrict the size of members to 1000 members, and we just hit that. So, I am going to migrate the group to a new group in the “public” google groups, called . Sadly, I don’t think migrating content is possible, but at least I will be able to subscribe all of you. I will automatically subscribe all under a “Send email for each message and update” option, so you will have to change that to your preference (sorry about that…). : Google Groups seems to have spam limits on the number of users that can be added, will add them in batches over time, I guess… . Make sure to now if you can. Luckily, the group has been mirrored on nabble, and it will remain as a mirror to the new group as well. This group will be closed (but not deleted) and old content will be searchable from both nabble and this interface, posting will not be allowed though. And posts should only go to the new group mailing address. I will post details on the new group shortly, but you should get it as well via direct mail once I invite you. I apologize for that. If someone knows of a better way to migrate the mails, it would be great. -shay.banon ","locales":"","title":"Mailing List Migration"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2011-06-01T00:00:00.000Z","url":"/blog/0-16-2-released","seo_title":"","content":" version has just been released. You can download it . The bulk of the work on this release (backported from master) revolves around improved memory usage, especially with the filter cache (there is a new default filter cache, which takes the size in bytes as a limit on works on the level). As usual, bug fixes and other minor improvements. Upgrade is highly recommended. ","locales":"","title":"0.16.2 Released"}
{"index":{}}
{"author":"Zachary Tong","category":"Engineering","publish_date":"2013-02-10T00:00:00.000Z","url":"/blog/understanding-query-then-fetch-vs-dfs-query-then-fetch","seo_title":"","content":" In our , we ran into a situation where the returned scores were suspicious. As a refresher, here is the query in question:$ curl -XGET localhost:9200/startswith/test/_search?pretty -d '{ \"query\": { \"match_phrase_prefix\": { \"title\": { \"query\": \"d\", \"max_expansions\": 5 } } } }' | grep title \"_score\" : 1.0, \"_source\" : {\"title\":\"drunk\"} \"_score\" : 0.30685282, \"_source\" : {\"title\":\"dzone\"} \"_score\" : 0.30685282, \"_source\" : {\"title\":\"data\"} \"_score\" : 0.30685282, \"_source\" : {\"title\":\"drive\"} See how the document “drunk” receives a score of 1.0, while the rest have a score of 0.3? Shouldn’t these docs all have the same score, since they match the query for “d” equally the same? The answer is yes, but there is a very good reason for this scoring discrepancy.Relevancy ScoringPart of the scoring algorithm used by Elasticsearch (and Lucene underneath) includes “” statistics to help calculate relevancy of documents in the index.A lot has been written on the subject of TF-IDF, but it basically says “the more a term appears in a document, the more relevant this document is. But the relevancy is dampened by how often the term appears in the entire index”. Rare terms are only present in a few documents, which means any query matching a rare term becomes highly relevant. Conversely, common terms are found everywhere, so their relevancy to the query is low.Elasticsearch faces an interesting dilemma when you execute a search. Your query needs to find all the relevant documents…but these documents are scattered around any number of shards in your cluster. Each shard is basically a Lucene index, which maintains its own TF and DF statistics. A shard only knows how many times “pineapple” appears within the shard, not the entire cluster.But the relevancy algorithm uses TF-IDF…doesn’t it need to know how the TF and DF for the , not for each shard?Default search type: Query Then FetchThe answer is yes and no. By default, Elasticsearch will use a search type called ““. The way it works is as follows: This system works fine. In most cases, your index has “enough” documents to smooth out the Term/Document frequency statistics. So while each shard may not have complete knowledge of frequencies across the cluster, results are “good enough” because the frequencies are fairly similar everywhere.But in the case of our query mentioned at the beginning of this article, the default search type sometimes fails. DFS Query Then FetchIn the last article, we built an index without specifying shard count – ElasticSearch used the default of 5 shards. We then inserted a measly five documents into the index and demanded ES return relevant results and accurate scores. Not quite fair, is it?The scoring discrepancies were caused by the Query Then Fetch search type. Each shard only contained 1 or 2 documents (the hashing algorithm used by ES ensures a relatively random distribution). When we asked Elastic to compute scores, each shard only had a tiny view of the five-doc index…so scores were inaccurate.Luckily, Elasticsearch doesn’t leave you out to dry. If you have a situation where this scoring discrepancy is problematic, ES provides a search type called “DFS Query Then Fetch”. The procedures is almost identical to Query then Fetch, except it performs a pre-query to calculate global document frequencies. If we apply this new search type to our previous query, we get scoring results that make sense (e.g. they are all identical):$ curl -XGET 'localhost:9200/startswith/test/_search?pretty=true&search_type=dfs_query_then_fetch' -d '{ \"query\": { \"match_phrase_prefix\": { \"title\": { \"query\": \"d\", \"max_expansions\": 5 } } } }' | grep title \"_score\" : 1.9162908, \"_source\" : {\"title\":\"dzone\"} \"_score\" : 1.9162908, \"_source\" : {\"title\":\"data\"} \"_score\" : 1.9162908, \"_source\" : {\"title\":\"drunk\"} \"_score\" : 1.9162908, \"_source\" : {\"title\":\"drive\"} ConclusionOf cours","locales":"","title":"Understanding \"Query Then Fetch\" vs \"DFS Query Then Fetch\""}
{"index":{}}
{"author":"Zachary Tong","category":"Engineering","publish_date":"2013-02-04T00:00:00.000Z","url":"/blog/starts-with-phrase-matching","seo_title":"","content":" “Starts-with” functionality is common in search applications. Either you want to return results as the user types (ala Google Instant) or you simply want to search partial phrases and return any matches. This can be accomplished in Elasticsearch several ways. In this article, we are going to explore phrase matching at query time, instead of building the functionality directly into the index using shingles or nGrams. In particular, we are going to focus on `` query to do the heavy lifting. This query take the normal `match` query and adds phrase support + fuzzy prefix capability. The phrase matching comes from the ability to look at token offsets, allowing the query to know when tokens follow each other in a phrase. The prefix capability will take the last portion of your query and expand it into new query tokens. For example, if your query is “dog f” then will expand this into new queries: Handy, right? Getting Started Let’s go ahead and insert some data into a newly created index. . Our data looks like this: $ curl -XGET localhost:9200/startswith/test/_search?pretty -d '{ \"query\": { \"match_all\" : {} } }' | grep title {\"title\":\"data\"} {\"title\":\"drive\"} {\"title\":\"drunk\"} {\"title\":\"river dog\"} {\"title\":\"dzone\"} Our goal is to do a do a phrase match on the single character “d”. Ideally, we would retrieve all the titles that start with “d”, but not return the [river dog] entry because it starts with an “r”. Let’s go ahead and try the query and see what happens: $ curl -XGET localhost:9200/startswith/test/_search?pretty -d '{ \"query\": { \"match_phrase_prefix\": { \"title\": { \"query\": \"d\", \"max_expansions\": 5 } } } }' | grep title \"_score\" : 0.70710677, \"_source\" : {\"title\":\"data\"} \"_score\" : 0.70710677, \"_source\" : {\"title\":\"drive\"} \"_score\" : 0.70710677, \"_source\" : {\"title\":\"dzone\"} \"_score\" : 0.44194174, \"_source\" : {\"title\":\"river dog\"} \"_score\" : 0.30685282, \"_source\" : {\"title\":\"drunk\"} Hmm…well that didn’t work. Why not? Because we didn’t specify a mapping for this new index, Elasticsearch simply used the default Standard Analyzer. And if you use Standard on [river dog], you’ll end up with this curl -XGET 'localhost:9200/startswith/_analyze?analyzer=standard&pretty' -d 'river dog' | grep token \"token\" : \"river\", \"token\" : \"dog\", And that’s where the problem comes from. Standard tokenizes on whitespace, so [river dog] becomes [river] and [dog]. The query doesn’t match [river], but does match [dog]. You can see this reflected in the score of “river dog”, about half as much as all the other entries because only one token matched. The solution, as is so often the case, comes in the form of a mapping. We want Elasticsearch to look at the entire field as a single token, not parse it into individual tokens. We could simply set the field to not_analyzed, but that will make it case sensitive. Instead, we are going to use the plus the . The tokenizer is simple, and difficult to appreciate until you run into a situation like this one. It simply takes the entire field and emits it as a single token. It doesn’t really tokenize at all in fact, just returns what you give it. `Lowercase` filter is pretty obvious, I should think Go ahead and delete the old index, recreate it with the following mapping and re-insert all the data. { \"settings\":{ \"index\":{ \"analysis\":{ \"analyzer\":{ \"analyzer_startswith\":{ \"tokenizer\":\"keyword\", \"filter\":\"lowercase\" } } } } }, \"mappings\":{ \"test\":{ \"properties\":{ \"title\":{ \"search_analyzer\":\"analyzer_startswith\", \"index_analyzer\":\"analyzer_startswith\", \"type\":\"string\" } } } } } Now, when we execute the same query from above, we get much better results: $ curl -XGET localhost:9200/startswith/test/_search?pretty -d '{ \"query\": { \"match_phrase_prefix\": { \"title\": { \"query\": \"d\", \"max_expansions\": 5 } } } }' | grep title \"_score\" : 1.0, \"_source\" : {\"title\":\"drunk\"} \"_score\" : 0.30685282, \"_source\" : {\"title\":\"dzone\"} \"_score\" : 0.30685282, \"_source\" : {\"title\":\"da","locales":"","title":"Starts-With Phrase Matching"}
{"index":{}}
{"author":"","category":"","publish_date":"2013-01-10T00:00:00.000Z","url":"/blog/welcome-drew-rashid","seo_title":"","content":" I'd like to warmly welcome both Drew Raines () and Rashid Khan () to elasticsearch.Drew has been a long time elasticsearch user, and helped develop, manage and operate very large elasticsearch installations. Drew joined us to help sprinkle a bit more devops qualities to our team, as well as help improve our story in the devops land. His passion for clojure will also have an impact (or already is!).Rashid needs no introduction if you are using logstash. He is the guy behind , the beautiful interface built for logstash (or logstash “structured\" log data). We love the work Rashid has done on Kibana, and obviously are excited about his knowledge of elasticsearch. Rashid will continue and develop Kibana full time with us, as well as dabbling more into our operational and management infrastructure. ","locales":"","title":"Welcome Drew & Rashid"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2012-12-07T00:00:00.000Z","url":"/blog/0-20-0-released","seo_title":"","content":" version is out. This is the GA release of the 0.20.x branch. You can download it . The release mainly include bug fixes over the previous release. Update: released to fix a bug when downloading plugins, please make sure to use it when using plugins. This version also deprecates the snappy compression option (see more ), and the Shared Gateway (more info ). ","locales":"","title":"0.20.0 Released"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2012-10-23T00:00:00.000Z","url":"/blog/0-19-11-0-20-0-rc1-released","seo_title":"","content":" version and are out. You can download it . contains important bug fixes and is a recommended upgrade, is the first release candidate for 0.20 release, major features include: Update Improved update , including the ability to provide a partial document to merge into the existing one, and the ability to “upsert” a document if the document doesn’t exists. geo_shape Support for a type allowing to index complex geo shape (using geo json as the format), and for a query/filter allowing to provide a geo shape and check if it intersects, contains, or disjoint with the indexed shape. It also allows to fetch the “query” shape from the index itself (without needing to provide it as part of the query, just the name of the index/type/id to use). Warmers Provides the ability to register search requests that will be executed before new data becomes available for search, allowing to preload for example data that is needed for sorting, faceting, or parent/child support. This removes the load time from the “user” execution path, allowing to provide consistent search performance. Warmers have full control for registering them, as well as deleting and getting them. They can also be provided as part of the index creation , or as part of index templates. Other A lot of work has going into building the basis for cross major version cluster compatibility, we are not there yet, but we are on our way. Bugs have been fixed (though most major ones have been backported to 0.19), and many other smaller features and enhancements have been implemented. Moving forward, the plan is to have the next version be 0.20.0 final. 0.21 will focus on upgrading to Lucene 4.0 (and exposing its new features), as well as internal refactoring to support future features such as grouping (specifically, optimized for distributed execution). ","locales":"","title":"0.19.11 & 0.20.0.RC1 Released"}
{"index":{}}
{"author":"Karel Minařík","category":"","publish_date":"2012-09-04T00:00:00.000Z","url":"/blog/the-future-is-elastic","seo_title":"","content":" When I first discovered Elasticsearch, in late 2010, I immediately considered it an elaborate hoax.What else can you think when you discover a software project which comes sprinkled with so many buzzwords: a RESTful HTTP interface, working with schema-free documents, talking in JSON through a rich DSL, horizontally scalable and distributed by design, cloud-ready and cloud-friendly? And that software project is a search engine? A hoax, clearly.But then I downloaded the source code, had a look a it... and it seemed quite legit. I became suspicious - it seemed like a little bit too much work for a prank. So I installed said project, and, to my astonishment, it did seem to work the way it was advertised on the tin.At that time, I was working as a lead developer for a social media monitoring application, which was using CouchDB and CouchDB-Lucene as core technologies, so, as you can easily imagine, my neural paths had just exploded like fireworks. Working with schema-free JSON-based documents over HTTP and using full text queries and facets was something I was used to, only this time, it looked way, way better.Being a Ruby shop, we naturally needed a Ruby library for talking to Elasticsearch, and not being satisfied with the offering, I set out to write a . In spite of working hard to make it usable outside of our pretty specific use case, I never imagined it will get nearly nine hundreds of GitHub watchers and be in the process. Or that I will get to solve more then three hundred issues and pull requests, curating other people's code and making sure the library does not crumble under the weight of different styles, approaches and opinions.In no time, I was writing articles for the Elasticsearch site about , , , and in the process got to write a , Elasticsearch data for Ember.js, a monitoring to check for health problems of the cluster. I about Elasticsearch at three conferences. In short, my life became fully immersed in this crazy project. During all that time, Shay was an immensely helpful source of information, advice, and inspiration for cranking out features with incredible speed.Today marks a special day on this journey. As a member of the Elasticsearch.com team, I will be able to put more of my time and energy into Elasticsearch and the surrounding ecosystem — the Ruby library, provisioning and monitoring tools, research and documentation. My specific goal is to make it even more easy for Rubyists to work with full text search in their applications, to understand it better, and to make most of it.After all, search is the primary means for keeping up with the vast amounts of data:  every day, from Google to Mac OS X Spotlight, we rely on search to find information. In fact, the future of our civilization depends on how well we understand the data about ourselves, be it the global Twitter chatter, air pollution metrics, DNA sequences or evidence gathered by the Curiosity rover on Mars. I believe that full text search, and Elasticsearch in particular, will be an important part of this future. ","locales":"","title":"the future is elastic"}
{"index":{}}
{"author":"Uri Boness","category":"","publish_date":"2012-07-13T00:00:00.000Z","url":"/blog/exciting-times-ahead","seo_title":"","content":" About 7 years ago, I met with Shay in a small coffee place in Tel Aviv. We talked about Compass and search in general. I only wished I had recorded that conversation. We talked about the future of information retrieval, about big data (although we didn't use this label yet) and the role that search technology, specifically open source search technology, will have in the future. Fast forward a few years and there I was, with years of Lucene and Solr experience behind me, about to set up a company around open source search. I sat down with Steven and together we laid out the blueprints for the next generation search engine. We had a pretty good idea on the direction the information and data management world is heading. And we also knew what it was missing - A simple yet powerful distributed search engine, built to scale from ground up. Sounds pretty simple, doesn't it :). Few months later, Shay released the first version of Elasticsearch. And as time passed, it became clearer to us all that our visions are too much aligned to be ignored. It took some time and effort, but we finally made it happen - today the Elasticsearch company was born. It is the beginning of a new era. And not in the traditional definition of search, but in the modern definition of information retrieval. The definition by which data becomes accessible in the form of insightful knowledge, or simply put: Making Sense of Your (Big) Data Shay did an amazing job with Elasticsearch. It's simply mind blowing to realize that he, single handedly, managed to build, support, consult and promote such as amazing piece of software and get it to where it is today. But even kimchyman has his limits. The community is immense. The install base is growing by the day (just the other day Elasticsearch reached the 1 million downloads mark on GitHub). The mailing lists and IRC channels are busier than ever. Companies are running their most critical runtime systems on top of Elasticsearch and demand continuous support. Elasticsearch, the company, no longer the missing piece in the success story of this product. It is here to support the continuation and development of the project, provide the much needed security for the ever growing install-base, and help expand the community through training and education. There's a long journey ahead of us, but an exciting one. And we invite you to join us in this journey and share the excitement. ","locales":"","title":"Exciting Times Ahead"}
{"index":{}}
{"author":"Steven Schuurman","category":"News","publish_date":"2012-07-13T00:00:00.000Z","url":"/blog/introducing-elasticsearch-the-company","seo_title":"","content":" Hi everyone,Today is a great day for the Elasticsearch project, all Elasticsearch users, open source in general and not in the least for my colleague, fellow company founder and originator of Elasticsearch - Shay Banon. Today is the day we launch the company behind Elasticsearch! In practice, this means that as of today, users and potential users of Elasticsearch have a definitive source for support, education and guidance with respect to developing, deploying and running Elasticsearch in production environments.Shay and I connected though a mutual acquaintance (company co-founder Uri Boness) and started to share ideas about setting up a company around Elasticsearch. As it turned out, Shay and I shared a vision of creating an innovative company that will really make a difference in the Big Data space, and is focused 100% at creating products that truly meet the needs of users. With my background as co-founder of SpringSource, the company behind the popular Spring Framework, open source was an obvious ingredient of course. It didn't take us long come to the conclusion that cooperating on serving customers and helping companies make successful use of Elasticsearch made a lot of sense. Only months later we setup a company that is committed to the success of Elasticsearch users and that is committed to open source. We take these commitments very seriously. I am therefore very pleased to state that Elasticsearch will also make contributions to the Apache Lucene project, which is for obvious reasons an open source project that's very close to our hearts. All the more reason for me to also be extremely excited of having one of the leaders of the Apache Lucene project, Simon Willnauer, as a co-founder of the company and technology leader on my team.Moving forward, Shay will continue to develop Elasticsearch, and will continue to develop a solution that's exactly what users want and what users need. As of right now, he has a whole team to back him up. During the next few months we will be adding talented Elasticsearch colleagues to the team, so if you're interested in being part of a great team of technologists in the big data search and analytics space, feel free to drop us a line!I truly look forward to working together with my wonderful team towards the continued success of Elasticsearch. It's going to be great.Cheers, Steven Schuurman, CEO ","locales":"","title":"Introducing: Elasticsearch – the Company!"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2012-04-04T00:00:00.000Z","url":"/blog/0-19-2-released","seo_title":"","content":" version is out. You can download it . It is a bug fix release fixing several bugs including a bug not being able to update index level settings on a live index using the index update settings . ","locales":"","title":"0.19.2 Released"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2012-02-08T00:00:00.000Z","url":"/blog/0-19-0-rc2-released","seo_title":"","content":" version is out. You can download it . Its a bug fix release mainly fixing a major bug in how buffers are read and reused over the network (the optimization done in 0.19.0.RC1 was not complete). It also include two nice features, the first is the ability to disallow a shard and its replica to be allocated on the same machine when running more than one instance on said machine. It can be enabled by setting to . The second is an effort to better log GC pause times (without needing to turn on GC logging), with GC type thresholds and logging thresholds (comes with built in default values). ","locales":"","title":"0.19.0.RC2 Released"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2011-11-29T00:00:00.000Z","url":"/blog/0-18-5-released","seo_title":"","content":" version is out. You can download it . It includes an upgraded Lucene version (3.5), featuring bug fixes and memory improvements, as well as more bug fixes in elasticsearch itself. Changes can be found . ","locales":"","title":"0.18.5 Released"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2011-09-19T00:00:00.000Z","url":"/blog/0-17-7-released","seo_title":"","content":" version has just been released. You can download it . This release include the usual list of bug fixes, and also include an upgrade to Lucene 3.4.0 (fixes critical bugs, so make sure you upgrade), as well as improvements to the couchdb river (memory usage wise). ","locales":"","title":"0.17.7 Released"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2011-08-04T00:00:00.000Z","url":"/blog/0-17-3-released","seo_title":"","content":" version has just been released. You can download it . This is another bug fix release with minor enhancements (listed ). This upgrade is highly recommended to all users. Note, when upgrading to this release, due to a bug in previous 0.17.x versions, it is recommended to run flush against all indices before shutting down the cluster (or doing a rolling restart) if “delete_by_query” is used against specific types. ","locales":"","title":"0.17.3 Released"}
{"index":{}}
{"author":"Shay Banon","category":"Engineering","publish_date":"2011-07-19T00:00:00.000Z","url":"/blog/0-17-0-released","seo_title":"","content":" ","locales":"","title":"0.17.0 Released"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2011-05-12T00:00:00.000Z","url":"/blog/0-16-1-released","seo_title":"","content":" version has just been released. You can download it . This is a minor release but it includes some really cool features and enhancements: Text Query Family A new family of queries called . The aim with those is to simplify querying elasticsearch with regards to the analysis process (and should hopefully remove some confusion when using query). More Analysis Options All the analyzers (mainly around language analyzers) part of Lucene are now exposed. On top of that, several token filters were backported from Lucene 4.0 (synonym and word_delimiter). Other Minor additional enhancements (better global facets execution, custom boost per value for type, and more) and important bug fixes are part of this release. Upgrade is highly recommended. ","locales":"","title":"0.16.1 Released"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2011-02-18T00:00:00.000Z","url":"/blog/0-15-0-released","seo_title":"","content":" version has just been released. You can download it . This is another major release that includes several major features and of course, the typical set of bug fixes and enhancements. Some of the major features include: Versioning Versioning got its own detailing how to use the feature. It now allows for a more interesting interaction model where we can use optimistic concurrency control when creating and updating documents. Percolator is an interesting feature, basically turning search upside down: Instead of indexing docs and searching for them, during the indexing process docs are “percolated” to find out which queries match them. I can’t wait to see all of the cool things that will be built on top of utilizing this feature!. Date Histogram One of the more powerful features of elasticsearch are , and one of my favorite facets is the facet (think \"number of comments per month). When using it on a field though, it lacks some important features. The facet comes to solve and enhance the regular histogram by allowing you to “bucket” results by uneven intervals, like months and years. On top of that, you can provide a time zone to control exactly how the “buckets” of hits are defined – something you can’t really do on the client side. Search Filter The new element to the search will now allow you to do facet based navigation in a much simpler way. The new element filters out returned hits, but will be applied to facet calculations. (although each facet can still have its own filters as well using the element). This allows things like navigation-by-facets to be done in a much simpler manner, while controlling which facets are affected by it and which ones still apply only to the original user-supplied query. Many More There are many more performance improvements, enhancements, features, and bug fixes, all listed in the . -shay.banon ","locales":"","title":"0.15.0 Released"}
{"index":{}}
{"author":"Shay Banon","category":"Engineering","publish_date":"2010-11-18T00:00:00.000Z","url":"/blog/0-13-0-released","seo_title":"","content":" ElasticSearch version has just been released. You can download it . This release includes important bug fixes (most important one is a possible corruption in the index data) and some new features, including:More Mapping FeaturesSupport for a new IP (ipv4) type (that is automatically detected) allowing to do range queries (for example) on IP ranges. More enhancements to dynamic templates matching, a compression threshold on the source field, and an analyzer mapping field.Explicit Shard Routing ControlUp until 0.13, documents were routed to the relevant shard based on their respective type and id values. In this version, they are not routed only by the id value by default (check the upgrade notes below), and they can be routed based on a custom value. This means that doing searches can be much faster when knowing in advance the list of routing values the search should executed on.For example, when indexing a blog post as a single document, and a blog comment as another document, the comment document can be routed to the same shard as the blog post using the blog post id. This means that when wanting to search only on things relating to a specific blog post, only a single shard needs to be searched on and not all shards of that index.This is very similar to the multi indices feature (and the ability to search on several of them), but provides a much higher control of routing within an index.Improved Support for Large Number of IndicesLarge number of indices now consume much less resources on the cluster as a whole, and a new allowing to close and open an index has been added.Many More Smaller FeaturesThere are many more smaller features, with the list available under the 0.13 tag.Upgrade NotesTwo important nodes when upgrading to 0.13:The first, due to the change in the default routing mechanism of routing only based on the id and not the type, existing systems upgrading to the new version must set the to in order to maintain the same hashing mechanism as in previous releases.The second, the location of the index files have changed. It used to be stored under the “work” directory, but it has been moved to a new “data” directory. This means two things, if you are using the default, then rename the work directory under the installation to data. If was being explicitly set, change the setting to .This was done in order to simplify the management of files elasticsearch created. Now, there are three places where data is created: for index related data information, for the location of the logs, and for temporal data.-shay.banon ","locales":"","title":"0.13.0 Released"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2010-09-29T00:00:00.000Z","url":"/blog/0-11-0-released","seo_title":"","content":" ElasticSearch version has just been released. You can download it . This is a major release for elasticsearch, both in terms of feature set as well as stability. Major list of features include: Zero Conf Persistency Out of the box long term persistency using a “local” gateway dubbed . River Generic support for which are services running within the elasticsearch cluster indexing streams of data. Rivers exists for , , and . Bulk A new to do bulk indexing of data, greatly increasing the throughput of the indexing process. Details . Thrift Transport A Thrift based transport (mimicking ) for faster execution compared with plain . Minor Features Minor features include and the ability to use them in plain fields, , improved geo support, faster query_string and field queries execution, and of course, bug fixes. -shay.banon ","locales":"","title":"0.11.0 Released"}
{"index":{}}
{"author":"Shay Banon","category":"Engineering","publish_date":"2011-02-08T00:00:00.000Z","url":"/blog/versioning","seo_title":"","content":" ","locales":"","title":"Versioning"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2010-10-18T00:00:00.000Z","url":"/blog/0-12-0-released","seo_title":"","content":" ElasticSearch version has just been released. You can download it . This release is a bit early due to a major bug found with full cluster recovery when using more than one index and the local gateway. Along the way, some features were added as well, those include: More Script Language Support On top of mvel, there is now support for JavaScript, Python, and Groovy for executing scripts. Scripts can be used in different components within elasticsearch, including custom scoring, facets, and others. This support is going to be utilized in future features as well. Dynamic Templates Dynamic schema introduction is already a part of elasticsearch, but sometimes, the defaults are just not good enough. Dynamic templates allow to control different mappings aspects while still retaining the dynamic nature of a document. A dynamic template, for example, can turn a single field automatically into a `multi_field` or have fields matching a template be stored. Geo Overhaul The geo support aim was to support multiple location per document. The current design was problematic in supporting that, so a new design was implemented. This sadly requires a full reindexing if using geo features, as well as requiring to explicitly define `geo_point` mapping type. Facets Improvements Both the `term` and the `statistical` facets now can be executed on more than one field and aggregating the results across all fields values. Query Filters Improvements Fine grained control over which filter results are cached or not, with sensible defaults per filter (as filters behave differently). Other Several small bug fixes and improvements. Note, if you use the thrift client, the thrift protocol has changed not to include some keywords in certain langs. Make sure to regenerate the client code. -shay.banon ","locales":"","title":"0.12.0 Released"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2010-09-28T00:00:00.000Z","url":"/blog/the-river-searchable-couchdb","seo_title":"","content":" allows to easily define data sources and have elasticsearch index them. The example provided in the post is twitter, but a river is an open and can have different implementations. One of those coming out at 0.11 is . The allows to automatically index couchdb and make it searchable using the excellent stream couchdb provides. Setting it up is as simple as executing the following against elasticsearch: curl -XPUT 'localhost:9200/_river/my_db/_meta' -d '{ \"type\" : \"couchdb\", \"couchdb\" : { \"host\" : \"localhost\", \"port\" : 5984, \"db\" : \"my_db\", \"filter\" : null } }' This call will create a river that uses the stream to index all data within couchdb. Moreover, any “future” changes will automatically be indexed as well, making your search index and couchdb synchronized at all times. On top of that, in case of a failover, the couchdb river will automatically be started on another elasticsearch node, and continue indexing from the last indexed seq. As you can see, elasticsearch can easily have several couchdb rivers (and other types of rivers) running at the same time, all pointing to different databases and indexing them into different indices (or the same index, you choose) using the same elasticsearch cluster. This means that being able to search couchdb has just become really really simple. The couchdb river is provided as a plugin (in upcoming 0.11) and can be installed using . -shay.banon ","locales":"","title":"Searchable CouchDB"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2011-03-24T00:00:00.000Z","url":"/blog/new-search-types","seo_title":"","content":" has had the ability to control the since its inception. It includes the ability to control how to execute the “query” and “fetch” phases of a distributed search execution, as well as the ability to compute distributed frequencies across shards (the “dfs” phase). Two things that many users have been asking for is the ability to easily iterate over a very large result set as fast as possible, and the ability to just count results (possibly with facets) without actually fetching any hits back. Both features are now supported as new search types. count The first is the search type. It allows to get back the total number of hits matching a query, with the ability to have facets configured in an optimized (implementation wise and performance wise) manner. For example: curl -XGET 'http://localhost:9200/twitter/tweet/_search?search_type=count' -d '{ \"query\": { \"filtered\" : { \"query\" : { \"query_string\" : { \"query\" : \"some query string here\" } }, \"filter\" : { \"term\" : { \"user\" : \"kimchy\" } } } } } ' The result will not include any hits, just the and optional facets results. scan The new type allows to scroll a very large result set in an optimized manner. When scrolling large result set using one of the other search types, there is an overhead (that increases the more we scroll “into” the result set) that comes from the fact that sorting needs to be computed (either by score, or custom). The type does no sorting, allowing it to optimize the scrolling process. It uses the same scroll mechanism when using other search types. The initial search execution bootstraps the scanning process: curl -XGET 'localhost:9200/_search?search_type=scan&scroll=10m&size=50' -d ' { \"query\" : { \"match_all\" : {} } } ' The parameter indicates two things, the fact that we wish to scroll further into the result set, and the timeout value for maintaing the scroll “open” (the timeout value applies per request, not globally across the scroll process). The parameter controls how many hits we want to get back. Note, the actual number of hits will be the the size provided times the number of shards, which is done in order to further optimize the scrolling process. The result of the search request is a . The should then be used as a parameter to the next search request: curl -XGET 'localhost:9200/_search/scroll?scroll=10m' -d 'c2NhbjsxOjBLMzdpWEtqU2IyZHlmVURPeFJOZnc7MzowSzM3aVhLalNiMmR5ZlVET3hSTmZ3OzU6MEszN2lYS2pTYjJkeWZVRE94Uk5mdzsyOjBLMzdpWEtqU2IyZHlmVURPeFJOZnc7NDowSzM3aVhLalNiMmR5ZlVET3hSTmZ3Ow==' The results now will include hits, and another , which should then be used as the parameter to the next scroll request. Also, the parameter needs to be provided again in order to indicate we wish to continue the scrolling process. The “exit” point from the scrolling process is when no hits are returned back. -shay.banon ","locales":"","title":"New Search Types"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2011-03-23T00:00:00.000Z","url":"/blog/update-settings","seo_title":"","content":" When using , many times the initial usage of an index in the cluster is bulk indexing data into it, and then moving into a more “streamlined” operations that are applied in realtime against it. When doing something like bulk indexing, changing the default settings of the index can improve the speed at which documents are indexed, but, those settings then do not really apply for the case where we want to do real time indexing of data. Some of those settings include , which defaults to . Setting this value to be higher can improve indexing speed. Other settings include low level Lucene settings such as . In the upcoming 0.16 version (and already in master), there has been a lot of work going into being able to update a subset of index level settings in real time to improve that. It uses the same update settings already exposed that allowed to change only the . For example, lets say we are going to do some bulk indexing into an index, we can simply change the relevant index settings to the following: curl -XPUT localhost:9200/test/_settings -d '{ \"index\" : { \"refresh_interval\" : \"-1\", \"merge.policy.merge_factor\" : 30 } }' The above will disable refreshing of the index completely, and change the merge factor to be 30. Once the bulk loading is done, we can get back to the default settings: curl -XPUT localhost:9200/test/_settings -d '{ \"index\" : { \"refresh_interval\" : \"1s\", \"merge.policy.merge_factor\" : 10 } }' This makes even more elastic, allowing to munge it to adapt to what is currently required from it. The following are a list of issues listing the currently updateable settings: If you are missing some settings, ping the mailing list and we can see if they can also be updated dynamically. ","locales":"","title":"Update Settings"}
{"index":{}}
{"author":"Shay Banon","category":"Engineering","publish_date":"2010-12-27T00:00:00.000Z","url":"/blog/0-14-0-released","seo_title":"","content":" ","locales":"","title":"0.14.0 Released"}
{"index":{}}
{"author":"Shay Banon","category":"Engineering","publish_date":"2010-09-28T00:00:00.000Z","url":"/blog/the-river-rabbitmq","seo_title":"","content":" ","locales":"","title":"RabbitMQ River"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2010-09-27T00:00:00.000Z","url":"/blog/zero-conf-persistency","seo_title":"","content":" With the new “local gateway”, upcoming elasticsearch version 0.11 will provide zero conf long term persistency out of the box. In the post, elasticsearch support for long term persistency is explained. The idea is built around providing long term persistency using a shared storage solution. A common storage option between all nodes (shared file system, S3, ) is used to asynchronously write changes in both the cluster meta data (indices created, mappings) and the actual indices to it. The shared storage option has several benefits. One obvious one is the ability to store (parts) the index in memory, and still maintain long term persistency in case of full cluster shutdown. It also provides a native solution if backup is required of the indices (to s3 for example). It does come with an overhead though, the first is the actual need for a shared storage solution for long term persistency, which does complicate things for simpler and get it started scenarios. The other is the fact that potentially very large data set will be stored, where simply using a shared storage is an overhead that is unacceptable (mainly due to cost). In upcoming 0.11 version, another gateway option is provided, called local gateway. The local gateway option allows the cluster to restore both its state and the indices from each node local storage (local file system). And, in order to provide the best out of the box experience, this gateway is now the default one set. The cluster state, which includes the indices created, mappings, and other meta information is versioned and stored on the nodes. In order to recover it, the (or ) should be set to high enough value out of the total expected cluster size in order to ensure latest state recovery. Shards are recovered once a quorum (by default) of the shard with its replicas is found. For this reason (and others) it is recommended to have at least 2 replicas per shard set. Indices (and the transaction log) must be file system based, to provide recoverability in case of full shutdown. The local gateway allows for simple to use long term persistency with elasticsearch and should simplify greatly using elasticsearch with full persistency support. -shay.banon ","locales":"","title":"Zero Conf Persistency"}
{"index":{}}
{"author":"Karel Minařík","category":"Engineering","publish_date":"2011-05-13T00:00:00.000Z","url":"/blog/data-visualization-with-elasticsearch-and-protovis","seo_title":"","content":" The primary purpose of a search engine is, quite unsurprisingly: . You pass it a query, and it returns bunch of matching documents, in the order of relevance. We can get creative with query construction, experimenting with different analyzers for our documents, and the search engine tries hard to provide best results. Nevertheless, a modern full-text search engine can do much more than that. At its core lies the , a highly optimized data structure for efficient lookup of documents matching the query. But it also allows to compute complex of our data, called . The usual purpose of facets is to offer the user a , or . When you search for “camera” at an online store, you can refine your search by choosing different manufacturers, price ranges, or features, usually by clicking on a link, not by fiddling with the query syntax. A canonical example of a is pictured below. Faceted search is one of the few ways to make powerful queries accessible to your users: see Moritz Stefaner’s experiments with for inspiration. But, we can do much more with facets then just displaying these links and checkboxes. We can use the data for makings , which is exactly what we’ll do in this article. Live Dashboards In almost any analytical, monitoring or data-mining service you’ll hit the requirement sooner or later. Because everybody loves dashboards, whether they’re useful or just pretty. As it happens, we can use facets as a pretty powerful analytical engine for our data, without writing any implementations. The screenshot below is from a which uses not only to search and mine the data, but also to provide data aggregation for the interactive dashboard. When the user drills down into the data, adds a keyword, uses a custom query, all the charts change in real-time, thanks to the way how facet aggregation works. The dashboard is not a static snapshot of the data, pre-calculated periodically, but a truly interactive tool for data exploration. In this article, we’ll learn how to retrieve data for charts like these from , and how to create the charts themselves. Pie charts with a facet For the first chart, we’ll use a simple facet in . This facet returns the most frequent terms for a field, together with occurence counts. Let’s index some example data first. curl -X DELETE \"http://localhost:9200/dashboard\" curl -X POST \"http://localhost:9200/dashboard/article\" -d ' { \"title\" : \"One\", \"tags\" : [\"ruby\", \"java\", \"search\"]} ' curl -X POST \"http://localhost:9200/dashboard/article\" -d ' { \"title\" : \"Two\", \"tags\" : [\"java\", \"search\"] } ' curl -X POST \"http://localhost:9200/dashboard/article\" -d ' { \"title\" : \"Three\", \"tags\" : [\"erlang\", \"search\"] } ' curl -X POST \"http://localhost:9200/dashboard/article\" -d ' { \"title\" : \"Four\", \"tags\" : [\"search\"] } ' curl -X POST \"http://localhost:9200/dashboard/_refresh\" As you see, we are storing four articles, each with a couple of tags:  an article can have multiple tags, which is trivial to express in ’s document format, JSON. Now, to retrieve “Top Ten Tags” across the documents, we can simply do: curl -X POST \"http://localhost:9200/dashboard/_search?pretty=true\" -d ' { \"query\" : { \"match_all\" : {} }, \"facets\" : { \"tags\" : { \"terms\" : {\"field\" : \"tags\", \"size\" : 10} } } } ' You can see that we are retrieving all documents, and we have defined a terms facet called “tags”. This query will return something like this: { \"took\" : 2, // ... snip ... \"hits\" : { \"total\" : 4, // ... snip ... }, \"facets\" : { \"tags\" : { \"_type\" : \"terms\", \"missing\" : 1, \"terms\" : [ { \"term\" : \"search\", \"count\" : 4 }, { \"term\" : \"java\", \"count\" : 2 }, { \"term\" : \"ruby\", \"count\" : 1 }, { \"term\" : \"erlang\", \"count\" : 1 } ] } } } We are interested in the section of the JSON, notably in the array. It tells us that we have four articles tagged , two tagged , and so on. (Of course, we could add a parameter to the query, to skip the results altogether.) Suitable visualizati","locales":"","title":"Data Visualization with ElasticSearch and Protovis"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2011-04-23T00:00:00.000Z","url":"/blog/0-16-0-released","seo_title":"","content":" version has just been released. You can download it . This release is quite a big and important one, adding many new features, and fixes several major/critical bugs. Features/enhancements include the ability to update many more index level settings at runtime, several new search types, new facet called term_stats, faster facets execution all around, upgrade to Lucene 3.1, improved support for many indices (memory wise), faster recover post full restart, much improved shard allocation logic to reduce load, and many more. Many bugs have also been fix, with several critical ones revolving around possible data loss. Its highly recommended to upgrade. All changes are listed in the . Upgrade Notes The field is no longer indexed by default. No functionality is lost by it, but, if the field was used in term/terms query/filters, it should be replaced with the new ids query/filter. The issue explaining it is . ","locales":"","title":"0.16.0 Released"}
{"index":{}}
{"author":"Shay Banon","category":"Engineering","publish_date":"2011-02-08T00:00:00.000Z","url":"/blog/percolator","seo_title":"","content":" ","locales":"","title":"Percolator"}
{"index":{}}
{"author":"Shay Banon","category":"Engineering","publish_date":"2010-09-28T00:00:00.000Z","url":"/blog/the-river","seo_title":"","content":" One of the problems elasticsearch aims at solving is “the river” problem. The River is the stream of constant data and somehow finding a way to waddle through it and make something meaningful out of it. That constant data stream can come in different forms and from different sources. It can come directly from a user in an application that uses elasticsearch directly. For example, publishing a new status message, a new blog comment, or a review of a restaurant on apps that automatically apply that change to elasticsearch. Another option is for the data to be pushed to elasticsearch. For example, , cloudera log aggregator, can use an to push log changes to elasticsearch. The last option, and the one discussed here is where data is pulled from one source and applied to elasticsearch. As an example, someone can write a twitter component that listens on twitter stream updates, and apply them to elasticsearch. Those type of components, aside from writing the core code that does it, require additional services to be provided for them. For example, the twitter component will require failover support (if it fails, start it on another node), and possibly state storage (what was the last tweet indexed). in elasticsearch provides just that. A river is a service running within elasticsearch cluster and tries and solve the third type of integration point mentioned above. Rivers are allocated to nodes within the cluster. Are provided with automatic failover in case of node failure, and allow to store state associated with them. The River implementation is a bit of a cheat , rivers are simply represented as different types within an index called . Creating them is as simple as creating a document named within the river (type). Deleting them is just a matter of deleting the river (type). And last, they can easily store state as addition document(s) within the index type. ElasticSearch has a framework support for rivers, and upcoming 0.11 version will come with several different implementations of rivers. The one covered here is the . Here is how it can be created: curl -XPUT localhost:9200/_river/my_twitter_river/_meta -d ' { \"type\" : \"twitter\", \"twitter\" : { \"user\" : \"twitter_user\", \"password\" : \"twitter_passowrd\" } } ' Once created, the will start to be indexed into elasticsearch (including all the relevant metadata, like geo location, places, replies, and so on). Think about the power and capabilities you get with all that data indexed in elasticsearch . Deleting the twitter river is as simple as: curl -XDELETE localhost:9200/_river/my_twitter_river The twitter river will be provided as a plugin in 0.11, and can be easily installed using . ","locales":"","title":"The River"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2010-08-27T00:00:00.000Z","url":"/blog/0-10-0-released","seo_title":"","content":" ElasticSearch version has just been released. You can download it . This is a major release for elasticsearch, both in terms of feature set as well as stability. Major partial list of features include: Geo Support Geo location support has been added, allowing to have geo query based capabilities (distance, bounding box, polygon) as well as facet support (distance based). More info can be found . Update Number of Replicas Dynamically Allow to change the number of replicas an index has using a simple . More Facets Range, filter, and more term facet options. More Mapping Options Ability to compress the field with extensive optimization at decompression only when needed (for example, decompressing directly down into the stream). New Gateway Structure A new gateway structure reducing the chances of gateway corruption as well as building the basis for future options such as saving versions of the gateway and allowing to recover from them. Here is the . Transport Compression The ability to configure the communication between nodes to work in a compressed mode, as well as different components using it by default (for example, peer recovery fetches the index in compressed mode). Minor Enhancements, Bugs Squashing A lot of work has going into improved stability of elasticsearch, better memory management, and major bugs squashing. ElasticSearch is being used by several companies to index very large amount of data with large cluster size successfully with snapshot versions of 0.10. -shay.banon ","locales":"","title":"0.10.0 Released"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2010-08-16T00:00:00.000Z","url":"/blog/geo-location-and-search","seo_title":"","content":" One of the coolest search technology combinations out there are the ability to combine geo and search. Queries such as give me all the restaurants that serves meat ([insert your query here]) within 20 miles from me, or create a distance heat map of them, is slowly becoming a must have for any content website. This is becoming even more relevant with new browsers supporting . Already in (and in the upcoming release), elasticsearch comes with rich support for geo location. Lets take a drive down the geo support path: Indexing Location Aware Documents In general, documents indexed are not required to define any predefined mapping in order to use geo location features, but they should conform to a convention if none is defined. For example, lets take an example of a “pin” that we want to index its location and maybe some tags its associated with: { \"pin\" : { \"location\" : { \"lat\" : 40.12, \"lon\" : -71.34 }, \"tag\" : [\"food\", \"family\"], \"text\" : \"my favorite family restaurant\" } } The element is a “geo enabled” location since it has and properties. Once one follows the above conventions, all geo location features are enabled for . If explicit setting is still required, then its easy to define a mapping that defines a certain property as a . Here is an example: { \"pin\" : { \"properties\" : { \"location\" : { \"type\" : \"geo_point\" } } } } By defining the property as , this means that now we can index location data in many different formats, starting from the lat/lon example above, up to . For information on all the available formats, check out . The automatic mapping of “geo enabled” properties has been disabled since publishing this article. You have to provide the correct mapping for geo properties. Please see the . Find By Location The first thing after indexing location aware documents, is being able to query them. There are several ways to be able to query such information, the simplest one is by . Here is an example: { \"filtered\" : { \"query\" : { \"field\" : { \"text\" : \"restaurant\" } }, \"filter\" : { \"geo_distance\" : { \"distance\" : \"12km\", \"pin.location\" : { \"lat\" : 40, \"lon\" : -70 } } } } } The above will search for all documents with of that exists within of the provided location. The location point can accept several different formats as well, detailed at . The next query supported is a , allowing to restrict the results into a geo box defined by the top left, and bottom right coordinates. Here is an example: { \"filtered\" : { \"query\" : { \"field\" : { \"text\" : \"restaurant\" } }, \"filter\" : { \"geo_bounding_box\" : { \"pin.location\" : { \"top_left\" : { \"lat\" : 40.73, \"lon\" : -74.1 }, \"bottom_right\" : { \"lat\" : 40.717, \"lon\" : -73.99 } } } } } } The last, and the most advance form of geo query is a , here is an example: { \"filtered\" : { \"query\" : { \"field\" : { \"text\" : \"restaurant\" } }, \"filter\" : { \"geo_polygon\" : { \"pin.location\" : { \"points\" : [ {\"lat\" : 40, \"lon\" : -70}, {\"lat\" : 30, \"lon\" : -80}, {\"lat\" : 20, \"lon\" : -90} ] } } } } } Sorting The ability to sort results not just by ranking (how relevant is the document to the query), but also by distance allows for much greater geo usability. There is now a new allowing to sort based on a distance from a specific location: { \"sort\" : [ { \"_geo_distance\" : { \"pin.location\" : [-40, 70], \"order\" : \"asc\", \"unit\" : \"km\" } } ], \"query\" : { \"field\" : { \"text\" : \"restaurant\" } } } On top of that, elasticsearch will now return all the values per hit of fields sorted on, allowing to easily display this important information. Faceting Faceting, the ability to show an aggregated views on top of the search results go hand in hand with geo. For example, one would like to get the number of hits matching the search query within 10 miles, 20 miles, and above from his location. The facet provides just that: { \"query\" : { \"field\" : { \"text\" : \"restaurant\" } }, \"facets\" : { \"geo1\" : { \"geo_distance\" : { \"pin.location\" : { \"lat\" : 40, \"lon\" : -70 }, \"ranges\" : [ { \"to\" : 10 }, { \"from\" : 10, \"to\" : 20 }, { \"fr","locales":"","title":"Geo Location and Search"}
{"index":{}}
{"author":"Shay Banon","category":"Engineering","publish_date":"2010-05-11T00:00:00.000Z","url":"/blog/here-comes-the-cloud","seo_title":"","content":" From the get go, elasticsearch has been designed and built for the cloud. From its internal architecture, to how it works in its distributed nature. In the upcoming 0.7 version, the cloud vision has been fully realized. The Cloud integration revolves around two major components in ElasticSearch: and . Cloud Discovery One of the main problems with running distributed systems on the cloud is discovery. Products that can do “zero conf” discovery use multicast for it (elasticsearch among them), and in most cloud providers (Amazon or Rackspace) multicast is disabled. The typical way to work around it is to use unicast discovery, which requires setting up a specific list of IPs/Hosts (routers or gossip servers). Unicast discovery is problematic when used on the cloud. Machines can come and go, and their IP is not static. Cloud providers work around that by providing the ability to have a set of “elastic IPs”. But, at the end, the management of the cloud installation becomes a pain. At least two servers must be associated with an elastic IP and become a special exception case which needs to be managed. This goes completely against “zero conf” discovery and heavily complicates the cloud installation. ElasticSearch has a new discovery module called “Zen” which was built from the ground up to work well in cloud environments (and integrate well with other elasticsearch modules). The cloud extension to it provides “zero conf” discovery in cloud environments. In a nutshell, when running on the cloud, the list of machines that are already running on the cloud is available through cloud APIs. This information can be used to perform “zero conf” discovery. This follows the motto that the should be embraced by any system running on the cloud: . So, how do you enabled cloud discovery on the cloud? With a few lines of configuration: cloud: account: &lt: Your Amazon AWS Account Here&gt:  key: &lt: Your Amazon AWS Secret Key Here&gt:  compute: type: amazon discovery: type: cloud The above configuration enables auto discovery in Amazon . Simply replace with to work on the Rackspace cloud. There is a long list of compute cloud providers supported, including GoGrid, and Terremark. Gateway ElasticSearch has been designed to do reliable asynchronous long term persistency. This enables several features including the ability for fast local “runtime” storage (including in-memory) while having a long term storage that can be slower. The Gateway concept is described in the post. But first, a step back. When designing a system that would be deployed on the cloud, lets take a search engine for example , things come and go. One of those things that come and go are disks. So, local storage, in cloud environments, is considered transient. In Amazon for example, (Elastic Block Store) was introduced to provide a mountable disk that survives restarts. So, we could configure our search engine to store the index on . But, requires periodic snapshotting to S3 (amazon blob store) for “safe” persistency, since can certainly suffer from failures as well. Of course, this means more money spent on your cloud deployment since now one pays for both and S3. One way to work around this is to persist directly from the local store to S3 by writing some sort of synchronization script / code. But, if the machines fails we will loose all the data up to the point when the script last ran. The next step is to add replication (and sharding for performance) and so on. All of this is provided by elasticsearch out of the box. Here is how elasticsearch can be configured to store both its cluster metadata (to survive full cluster failure) and indices in the cloud: cloud: account: &lt: Your Amazon AWS Account Here&gt:  key: &lt: Your Amazon AWS Secret Key Here&gt:  blobstore: type: amazon gateway: type: cloud cloud: container: mycontainerhere The above simple configuration will store things in Amazon S3. Simply change to to use Rackspace CloudFiles. There is a long li","locales":"","title":"Here Comes the Cloud"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2010-05-17T00:00:00.000Z","url":"/blog/0-7-1-released","seo_title":"","content":" ElasticSearch version has just been released. You can download it . This release fixed a major bug when indexing large documents resulting in storing additional null bytes (and returning them). Version also brings a major feature to elasticsearch, recovery throttling. In elasticsearch, there are two types of recovery. The first, is recovery from the gateway. This happens only when the first shard is allocated in the cluster. The second recovery happens when nodes move or allocate shards around. The recovery process in both cases include recovering both each shard index files, and the transaction log. Up until version , elasticsearch would basically go full force in performing the recovery. If a new node would join the cluster, all the possible shards would be allocated to it, and all will perform recovery in parallel. More over, each single shard index file recovery will happen in parallel as well. This can lead to a heavy load on the nodes, making them less responsive for on going operations performed on them. From version , recovery throttling is enabled, basically allowing only for a controlled number of concurrent recovery operations, and concurrent stream (single shard index file) recovery operation. Both counts are maintained on the node level, regardless of the number of indices or shards. The setting controls the number of concurrent recoveries allowed (shard recoveries). It defaults to the number of cores. The control the concurrent shard index file recoveries, and defaults to the number of cores as well. -shay.banon ","locales":"","title":"0.7.1 Released"}
{"index":{}}
{"author":"Shay Banon","category":"Engineering","publish_date":"2010-04-19T00:00:00.000Z","url":"/blog/elasticseach-just-got-groovy","seo_title":"","content":" ","locales":"","title":"ElasticSearch Just Got Groovy"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2010-07-26T00:00:00.000Z","url":"/blog/0-9-0-released","seo_title":"","content":" ElasticSearch version has just been released. You can download it . This is a major release for elasticsearch, both in terms of feature set as well as stability. Major partial list of features include: Facets Support allow to provide aggregated data view correlating to the search query executed. ElasticSearch now comes with several facets implementations, including the typical “terms” facets (allowing to get the most popular terms, and how often they occur), statistical facets providing statistical information on numeric fields including count, total, mean, min, max, variance, sum of squares, and standard deviation. And, the coolest facet type, histogram facets, which based on a field, break it into buckets and provide data on the relevant buckets derived from the same field, another field, or a script. Scripting Support Added as a general feature within elasticsearch, allows to define scripts that are evaluated at runtime and can be used in different elasticsearch features, such as facets, script search fields, script filter, and so on. More Queries and Filters Additional queries and filters have been added. Thanks to the Query of elasticsearch, adding queries is a snap. Queries include fuzzy query, custom score query (based on scripts), script filter, and/or/not filters, and more. Improved Gateway Recovery A major feature in elasticsearch, allowing to reuse existing index files when recovering from the gateway after a full cluster restart significantly reducing the time it takes to recover from the gateway. This include additions to the gateway behavior including the ability to control when the initial recovery will happen as a factor of the number of nodes in the cluster and time. Also, the shutdown has been enhanced to better handle full cluster shutdown. Script Search Fields The ability to load custom data (based on non stored fields) as part of the search request. Improves Fluent Java / Groovy The Java / Groovy has been greatly enhanced to provide more fluent execution. Cloud Specific Plugin The cloud has been rewritten to use directly the amazon , providing better stability and features when using . The cloud plugin now only works with Amazon . Stability, Bug Squashing, and Memory Usage Improvements A lot of work has going into improved stability of elasticsearch, better memory management, and major bugs squashing. ElasticSearch is being used by several companies to index very large amount of data with large cluster size successfully with snapshot versions of 0.9. -shay.banon ","locales":"","title":"0.9.0 Released"}
{"index":{}}
{"author":"Shay Banon","category":"Engineering","publish_date":"2010-05-27T00:00:00.000Z","url":"/blog/0-8-0-released","seo_title":"","content":" ElasticSearch version has just been released. You can download it . This release includes several bug fixes and memory footprint improvements, and one major feature, Hadoop integration. This allows to use Hadoop as elasticsearch gateway storage, and enabling it is as simple as:Installing the hadoop plugin using .Changing the configuration to include:gateway: type: hdfs hdfs: uri: hdfs://host:port path: path/to/folder -shay.banon ","locales":"","title":"0.8.0 Released"}
{"index":{}}
{"author":"Shay Banon","category":"Engineering","publish_date":"2010-05-14T00:00:00.000Z","url":"/blog/0-7-0-released","seo_title":"","content":" ElasticSearch version has just been released. You can download it . This release brings much improved stability, and several features:Zen DiscoveryA discovery module called built from the ground up to work well, and fast with elasticsearch. This is now the default discovery module, with the jgroups discovery module moving to be provided as a plugin.Groovy ClientA native groovy client providing a Groovyfied build on top of the native Java . More details provided in the blog post. As a side note, anybody up for building a Scala/JRbuy client?CloudFirst and foremost, native cloud support, providing zero conf cloud discovery ( No Special Node™ ) and the ability to persist long term index storage on different cloud providers blob stores. More information can be found in the blog post.Memcached TransportFor that extra oomph when is not fast enough (mainly from other languages), elasticsearch supports a subset of the memcached protocol. Basically, the implementation implements on top of memcached (as much as possible). More info can be found .Simpler Plugin ManagementMany things in elasticsearch are implemented as a plugin. For example, the cloud support or memcached support are implemented as plugins. Now, installing a plugin is as simple as issuing the following command:bin/plugin -install cloud bin/plugin -install transport-memcached Analysis Better support when working with unicode through the analysis plugin. More info .More APIsMore information on nodes using the new Node stats , as well as the ability to restart a node. ClientsSimpler dependency management, requiring only lucene as a dependency.XContentThough currently mainly for internal use, an abstraction on top of has been created, inspired by called . There is support a implementation for it, but also support for , which is a binary format for faster and smaller (message footprint) messages. The Java already uses it automatically (not for indexed documents), and both the and the indexed documents can be either in or format. format will be documented in the near future to allow for non based clients to use it.-shay.banon ","locales":"","title":"0.7.0 Released"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2010-03-05T00:00:00.000Z","url":"/blog/0-5-0-released","seo_title":"","content":" ElasticSearch version has just been released. You can download it . This release brings much improved stability, better handling of mapping definitions, and several features: Several new queries have been added, including , , , with multiple fields. allowing to get terms (from one or more indices) of one or more fields and their respective document frequencies (how often they exists in documents). This can be very handy to implement things like tag clouds or simple auto suggest. for simple indication on the health of the cluster, as well as the ability to wait for the cluster to reach a health status. to search for documents that are like a certain document. exposing all of elasticsearch operations/actions using simple, transport based, async to use with any based language. There are many more minor features and bug fixes, all listed under the tag. -shay.banon ","locales":"","title":"0.5.0 Released"}
{"index":{}}
{"author":"Shay Banon","category":"Engineering","publish_date":"2010-02-25T00:00:00.000Z","url":"/blog/no-sql-yes-search","seo_title":"","content":" ","locales":"","title":"NoSQL, Yes Search"}
{"index":{}}
{"author":"Shay Banon","category":"Engineering","publish_date":"2010-02-16T00:00:00.000Z","url":"/blog/searchengine-time-machine","seo_title":"","content":" ","locales":"","title":"Search Engine Time Machine"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2010-04-09T00:00:00.000Z","url":"/blog/0-6-0-released","seo_title":"","content":" ElasticSearch version has just been released. You can download it . This release brings much improved stability, and several features: First, a big rename has occurred. All the now uses “underscore casing” instead of “CamelCase casing”. This makes elasticsearch more streamlined with other based APIs out there. The is much more flexible now, supporting numbers provided as strings, and boolean values provided as either numbers or strings. This makes using elasticsearch from dynamic languages more easy. field support has been added, automatically creating a field that includes all the different fields in the document for simpler searching (no need to explicitly specify the field name to search on). One of the nice things about the field is that it takes boost level setting of different fields into account. More information on the field can be found . Highlighting is now supported as part of the search request. Simpler Query including support for queries and on queries. allows to create aliases associated with a single index or more and executing other APIs using it instead of the actual index names. A new plugin system has been develop allowing to easily extend elasticsearch with the first plugin being the plugin allowing to index “attachments” such as documents, images, mails, and so on. Internal changes to how communication is handled between nodes resulting in much smaller messages passing around over the low level transport layer and a lower latency/overhead for each . Many bug fixes and performance enhancements slowly making elasticsearch as rock solid as it should be! Last but not least, elasticsearch is now on Maven repository, with a and a . -shay.banon ","locales":"","title":"0.6.0 Released"}
{"index":{}}
{"author":"Shay Banon","category":"Engineering","publish_date":"2010-02-12T00:00:00.000Z","url":"/blog/your-data-your-search","seo_title":"","content":" ","locales":"","title":"Your Data, Your Search"}
{"index":{}}
{"author":"Shay Banon","category":"","publish_date":"2010-02-08T00:00:00.000Z","url":"/blog/you-know-for-search","seo_title":"","content":" ElasticSearch is an open source, distributed, RESTful, search engine which is built on top of internally and enjoys all the features it provides. All the features it has are listed in the , so no need to list them here again (with the extra splash, if I might add). ElasticSearch itself was born out of my frustration with the fact that there isn’t really a good, open source, solution for distributed search engine out there, which also combines what I expect of search engines after building (and on that, I will blog later…). I have been working on this for the past several months, pouring my search and distributed knowledge into this (and portions of my heart and time : ) ), Enjoy!. ","locales":"","title":"You Know, for Search"}
